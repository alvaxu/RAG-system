

## ï¿½ï¿½ **æ–‡æœ¬å¬å›æŠ€æœ¯å®ç°è¯¦ç»†è¯´æ˜**

### **1. ğŸ—ï¸ æ•´ä½“æ¶æ„è®¾è®¡**

æ–‡æœ¬å¬å›ç³»ç»Ÿé‡‡ç”¨**5å±‚ç­–ç•¥é™çº§**çš„æ¶æ„è®¾è®¡ï¼Œç¡®ä¿å³ä½¿é«˜ä¼˜å…ˆçº§ç­–ç•¥å¤±è´¥ï¼Œä»æœ‰ç»“æœè¿”å›ã€‚æ•´ä¸ªç³»ç»ŸåŸºäº `TextEngine` ç±»å®ç°ï¼Œç»§æ‰¿è‡ª `BaseEngine`ã€‚

### **2. ï¿½ï¿½ æ–‡æ¡£åŠ è½½ç®¡ç†**

#### **2.1 ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨é›†æˆ**

```python
def _load_from_document_loader(self):
    """ä»ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨è·å–æ–‡æœ¬æ–‡æ¡£"""
    if self.document_loader:
        try:
            self.text_docs = self.document_loader.get_documents_by_type('text')
            self._docs_loaded = True
            self.logger.info(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–æ–‡æœ¬æ–‡æ¡£: {len(self.text_docs)} ä¸ª")
        except Exception as e:
            self.logger.error(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–æ–‡æœ¬æ–‡æ¡£å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»ŸåŠ è½½æ–¹å¼
            self._load_text_documents()
    else:
        self.logger.warning("æ–‡æ¡£åŠ è½½å™¨æœªæä¾›ï¼Œä½¿ç”¨ä¼ ç»ŸåŠ è½½æ–¹å¼")
        self._load_text_documents()
```

**åŠŸèƒ½è¯´æ˜**ï¼š
- ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨ï¼Œé¿å…é‡å¤åŠ è½½
- æ”¯æŒé™çº§åˆ°ä¼ ç»ŸåŠ è½½æ–¹å¼ï¼Œä¿è¯å‘åå…¼å®¹æ€§
- æä¾›å»¶è¿ŸåŠ è½½æœºåˆ¶ï¼ŒæŒ‰éœ€åŠ è½½æ–‡æ¡£

#### **2.2 ä¼ ç»Ÿæ–‡æ¡£åŠ è½½ï¼ˆé™çº§ç­–ç•¥ï¼‰**

```python
def _load_text_documents(self):
    """åŠ è½½æ–‡æœ¬æ–‡æ¡£åˆ°ç¼“å­˜"""
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # æ¸…ç©ºä¹‹å‰çš„ç¼“å­˜
            self.text_docs = {}
            
            # ä»å‘é‡æ•°æ®åº“åŠ è½½æ‰€æœ‰æ–‡æœ¬æ–‡æ¡£
            for doc_id, doc in self.vector_store.docstore._dict.items():
                chunk_type = doc.metadata.get('chunk_type', '')
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬æ–‡æ¡£ - ç®€åŒ–åˆ¤æ–­é€»è¾‘
                is_text = chunk_type == 'text'
                
                if is_text:
                    self.text_docs[doc_id] = doc
                    self.logger.debug(f"åŠ è½½æ–‡æœ¬æ–‡æ¡£: {doc_id}, chunk_type: {chunk_type}")
            
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(self.text_docs)} ä¸ªæ–‡æœ¬æ–‡æ¡£")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬æ–‡æ¡£ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if not self.text_docs:
                self.logger.warning("æœªæ‰¾åˆ°æ–‡æœ¬æ–‡æ¡£ï¼Œå°è¯•æœç´¢æ‰€æœ‰æ–‡æ¡£...")
                self._search_all_documents_for_texts()
            
            # å¦‚æœæˆåŠŸåŠ è½½äº†æ–‡æ¡£ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
            if len(self.text_docs) > 0:
                self.logger.info(f"æ–‡æœ¬æ–‡æ¡£åŠ è½½æˆåŠŸï¼Œå…± {len(self.text_docs)} ä¸ªæ–‡æ¡£")
                return
            else:
                raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡æ¡£")
                
        except Exception as e:
            retry_count += 1
            self.logger.warning(f"æ–‡æœ¬æ–‡æ¡£åŠ è½½å¤±è´¥ï¼Œç¬¬{retry_count}æ¬¡å°è¯•: {e}")
            
            if retry_count >= max_retries:
                # æœ€ç»ˆå¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶æ¸…ç©ºç¼“å­˜
                self.logger.error(f"æ–‡æœ¬æ–‡æ¡£åŠ è½½æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
                self.text_docs = {}
                return
            else:
                # ç­‰å¾…åé‡è¯•
                import time
                time.sleep(1)
                self.logger.info(f"ç­‰å¾…1ç§’åè¿›è¡Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
```

**åŠŸèƒ½è¯´æ˜**ï¼š
- æ”¯æŒé‡è¯•æœºåˆ¶ï¼Œæœ€å¤šé‡è¯•3æ¬¡
- ä¸¥æ ¼çš„ç±»å‹è¿‡æ»¤ï¼ŒåªåŠ è½½ `chunk_type == 'text'` çš„æ–‡æ¡£
- æä¾›å¤‡é€‰æœç´¢ç­–ç•¥ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°æ–‡æœ¬æ–‡æ¡£
- è¯¦ç»†çš„æ—¥å¿—è®°å½•ï¼Œä¾¿äºè°ƒè¯•å’Œç›‘æ§

### **3. ğŸ” æ ¸å¿ƒæœç´¢ç­–ç•¥å®ç°**

#### **3.1 ä¸»æœç´¢å‡½æ•°**

```python
def _search_texts(self, query: str, **kwargs) -> List[Any]:
    """æ™ºèƒ½æ–‡æœ¬æœç´¢ - V2.0å¢å¼ºç‰ˆï¼ˆä¸¥æ ¼ç±»å‹è¿‡æ»¤ï¼‰"""
    results = []
    
    # ç­–ç•¥1: ä¼˜å…ˆä½¿ç”¨å·²åŠ è½½çš„æ–‡æœ¬æ–‡æ¡£è¿›è¡Œæœç´¢
    if self.text_docs:
        try:
            # å¯¹æ–‡æœ¬æ–‡æ¡£è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
            for doc_id, doc in self.text_docs.items():
                score = self._calculate_text_score(doc, query)
                if score >= self.config.text_similarity_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'text_doc_search'
                    })
            
            self.logger.debug(f"æ–‡æœ¬æ–‡æ¡£æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        except Exception as e:
            self.logger.warning(f"æ–‡æœ¬æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
    
    # ç­–ç•¥2: å¦‚æœæ–‡æœ¬æ–‡æ¡£æœç´¢æ²¡æœ‰ç»“æœï¼Œåœ¨å†…å­˜ä¸­ä½¿ç”¨å®½æ¾æœç´¢ç­–ç•¥
    if not results and self.text_docs:
        try:
            self.logger.debug("ç­–ç•¥1æ— ç»“æœï¼Œå¯ç”¨ç­–ç•¥2ï¼šå†…å­˜å®½æ¾æœç´¢")
            
            # åœ¨å†…å­˜ä¸­æ–‡æ¡£ä¸­ä½¿ç”¨å®½æ¾æœç´¢ç­–ç•¥
            for doc_id, doc in self.text_docs.items():
                # ä½¿ç”¨å®½æ¾çš„è¯„åˆ†ç®—æ³•
                score = self._calculate_text_score_relaxed(doc, query)
                
                # é™ä½é˜ˆå€¼ï¼Œä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…æ¡ä»¶
                relaxed_threshold = self.config.text_similarity_threshold * 0.5
                if score >= relaxed_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'memory_relaxed_search'
                    })
            
            self.logger.debug(f"å†…å­˜å®½æ¾æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        except Exception as e:
            self.logger.warning(f"å†…å­˜å®½æ¾æœç´¢å¤±è´¥: {e}")
    
    # ç­–ç•¥3: å¦‚æœä»ç„¶æ²¡æœ‰ç»“æœï¼Œå°è¯•å…³é”®è¯æœç´¢
    if not results:
        keyword_results = self._keyword_search(query)
        results.extend(keyword_results)
        self.logger.debug(f"å…³é”®è¯æœç´¢æ‰¾åˆ° {len(keyword_results)} ä¸ªç»“æœ")
    
    # ç­–ç•¥4: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆä½†åªé’ˆå¯¹æ–‡æœ¬æ–‡æ¡£ï¼‰
    if not results:
        fuzzy_results = self._fuzzy_search(query)
        results.extend(fuzzy_results)
        self.logger.debug(f"æ¨¡ç³Šæœç´¢æ‰¾åˆ° {len(fuzzy_results)} ä¸ªç»“æœ")
    
    # ç­–ç•¥5: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œåœ¨å†…å­˜ä¸­é™ä½é˜ˆå€¼é‡æ–°æœç´¢
    if not results and self.config.text_similarity_threshold > 0.05 and self.text_docs:
        self.logger.debug("ç­–ç•¥5ï¼šåœ¨å†…å­˜ä¸­é™ä½é˜ˆå€¼é‡æ–°æœç´¢...")
        original_threshold = self.config.text_similarity_threshold
        self.config.text_similarity_threshold = 0.05
        
        # åœ¨å†…å­˜ä¸­æ–‡æ¡£ä¸­é‡æ–°æœç´¢
        try:
            for doc_id, doc in self.text_docs.items():
                score = self._calculate_text_score(doc, query)
                if score >= self.config.text_similarity_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'memory_low_threshold_search'
                    })
            
            self.logger.debug(f"å†…å­˜ä½é˜ˆå€¼æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        except Exception as e:
            self.logger.warning(f"å†…å­˜ä½é˜ˆå€¼æœç´¢å¤±è´¥: {e}")
        
        # æ¢å¤åŸå§‹é˜ˆå€¼
        self.config.text_similarity_threshold = original_threshold
    
    # å»é‡å’Œæ’åº
    seen_ids = set()
    unique_results = []
    for result in results:
        doc_id = result.get('doc_id', 'unknown')
        if doc_id not in seen_ids:
            seen_ids.add(doc_id)
            unique_results.append(result)
    
    # æŒ‰åˆ†æ•°æ’åº
    unique_results.sort(key=lambda x: x['score'], reverse=True)
    
    self.logger.debug(f"æœ€ç»ˆå»é‡åå¾—åˆ° {len(unique_results)} ä¸ªç»“æœ")
    return unique_results
```

**åŠŸèƒ½è¯´æ˜**ï¼š
- **5å±‚ç­–ç•¥é™çº§**ï¼šç¡®ä¿å³ä½¿é«˜ä¼˜å…ˆçº§ç­–ç•¥å¤±è´¥ï¼Œä»æœ‰ç»“æœè¿”å›
- **æ™ºèƒ½ç­–ç•¥åˆ‡æ¢**ï¼šæ ¹æ®ç»“æœæ•°é‡è‡ªåŠ¨å¯ç”¨ä¸‹ä¸€å±‚ç­–ç•¥
- **å†…å­˜ä¼˜å…ˆä¼˜åŒ–**ï¼šç­–ç•¥2å’Œç­–ç•¥5éƒ½åœ¨å†…å­˜ä¸­æ‰§è¡Œï¼Œé¿å…é‡å¤I/Oæ“ä½œ
- **é˜ˆå€¼åŠ¨æ€è°ƒæ•´**ï¼šç­–ç•¥5ä¸´æ—¶é™ä½é˜ˆå€¼åˆ°0.05ï¼Œç¡®ä¿æœ‰ç»“æœè¿”å›
- **ç»“æœå»é‡æ’åº**ï¼šé¿å…é‡å¤å†…å®¹ï¼ŒæŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº

### **4. ï¿½ï¿½ è¯„åˆ†ç®—æ³•è¯¦è§£**

#### **4.1 ä¸¥æ ¼è¯„åˆ†ç®—æ³•ï¼ˆç­–ç•¥1ï¼‰**

```python
def _calculate_text_score(self, doc: Any, query: str) -> float:
    """è®¡ç®—æ–‡æœ¬åŒ¹é…åˆ†æ•° - æ™ºèƒ½ç»¼åˆè¯„åˆ†ï¼ˆä¸¥æ ¼ç›¸å…³æ€§åˆ¤æ–­ï¼‰"""
    score = 0.0
    
    # è·å–æ–‡æœ¬å†…å®¹
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight
    
    # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        # å…³é”®è¯åŒ¹é…å¿…é¡»è¾¾åˆ°ä¸€å®šé˜ˆå€¼æ‰ç»™åˆ†
        if keyword_score > 0.3:  # è‡³å°‘30%çš„å…³é”®è¯åŒ¹é…
            score += keyword_score * self.config.keyword_weight
        else:
            # å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œå¤§å¹…é™ä½åˆ†æ•°
            score *= 0.3
    
    # 3. å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¦‚æœæœ‰å‘é‡åµŒå…¥ï¼‰
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.3  # é™ä½é»˜è®¤å‘é‡åˆ†æ•°
        score += vector_score * self.config.vector_weight
    
    # 4. æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±ï¼ˆé™ä½å¥–åŠ±ï¼‰
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.05  # é™ä½æ–‡æœ¬æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±
    
    # 5. å†…å®¹é•¿åº¦å¥–åŠ±ï¼ˆé™ä½å¥–åŠ±ï¼‰
    if len(content) > 100:
        score += 0.02  # é™ä½å†…å®¹é•¿åº¦å¥–åŠ±
    
    # 6. ç›¸å…³æ€§æƒ©ç½šæœºåˆ¶
    if semantic_score < 0.1:  # è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡ä½
        score *= 0.5  # å¤§å¹…é™ä½åˆ†æ•°
    
    return min(score, 1.0)
```

**è¯„åˆ†æƒé‡é…ç½®**ï¼š
- `semantic_weight`: 0.3 (è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡)
- `keyword_weight`: 0.5 (å…³é”®è¯åŒ¹é…æƒé‡)  
- `vector_weight`: 0.2 (å‘é‡ç›¸ä¼¼åº¦æƒé‡)

**ä¸¥æ ¼æ€§ç‰¹ç‚¹**ï¼š
- å…³é”®è¯åŒ¹é…å¿…é¡»è¾¾åˆ°30%æ‰ç»™åˆ†ï¼Œå¦åˆ™å¤§å¹…é™åˆ†
- è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡ä½æ—¶ï¼Œæ€»åˆ†ä¹˜ä»¥0.5æƒ©ç½š
- é™ä½æ–‡æ¡£ç±»å‹å’Œå†…å®¹é•¿åº¦çš„å¥–åŠ±åˆ†æ•°

#### **4.2 å®½æ¾è¯„åˆ†ç®—æ³•ï¼ˆç­–ç•¥2ï¼‰**

```python
def _calculate_text_score_relaxed(self, doc: Any, query: str) -> float:
    """è®¡ç®—æ–‡æœ¬åŒ¹é…åˆ†æ•° - å®½æ¾è¯„åˆ†ç®—æ³•ï¼ˆç”¨äºç­–ç•¥2ï¼‰"""
    score = 0.0
    
    # è·å–æ–‡æœ¬å†…å®¹
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå®½æ¾å¤„ç†ï¼‰
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight
    
    # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆå®½æ¾åŒ¹é…ï¼‰
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        # å®½æ¾å¤„ç†ï¼šå³ä½¿å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œä¹Ÿç»™äºˆä¸€å®šåˆ†æ•°
        if keyword_score > 0.1:  # é™ä½åˆ°10%çš„å…³é”®è¯åŒ¹é…
            score += keyword_score * self.config.keyword_weight
        else:
            # å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œè½»å¾®é™ä½åˆ†æ•°
            score *= 0.7  # ä»0.3æå‡åˆ°0.7
    
    # 3. å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¦‚æœæœ‰å‘é‡åµŒå…¥ï¼‰
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.4  # æé«˜é»˜è®¤å‘é‡åˆ†æ•°
        score += vector_score * self.config.vector_weight
    
    # 4. æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±ï¼ˆæé«˜å¥–åŠ±ï¼‰
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.1  # æé«˜æ–‡æœ¬æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±
    
    # 5. å†…å®¹é•¿åº¦å¥–åŠ±ï¼ˆæé«˜å¥–åŠ±ï¼‰
    if len(content) > 100:
        score += 0.05  # æé«˜å†…å®¹é•¿åº¦å¥–åŠ±
    
    # 6. ç›¸å…³æ€§æƒ©ç½šæœºåˆ¶ï¼ˆå®½æ¾å¤„ç†ï¼‰
    if semantic_score < 0.05:  # ä»0.1é™ä½åˆ°0.05
        score *= 0.7  # ä»0.5æå‡åˆ°0.7
    
    return min(score, 1.0)
```

**å®½æ¾æ€§ç‰¹ç‚¹**ï¼š
- å…³é”®è¯åŒ¹é…é˜ˆå€¼é™ä½åˆ°10%
- æƒ©ç½šç³»æ•°ä»0.3æå‡åˆ°0.7ï¼Œæ›´å®½æ¾
- è¯­ä¹‰ç›¸ä¼¼åº¦æƒ©ç½šé˜ˆå€¼ä»0.1é™ä½åˆ°0.05
- æé«˜æ–‡æ¡£ç±»å‹å’Œå†…å®¹é•¿åº¦çš„å¥–åŠ±åˆ†æ•°

### **5. ï¿½ï¿½ å…³é”®è¯å¤„ç†æœºåˆ¶**

#### **5.1 å…³é”®è¯æå–**

```python
def _extract_keywords(self, query: str) -> List[str]:
    """æå–æŸ¥è¯¢å…³é”®è¯"""
    stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'äº†', 'å—', 'å‘¢', 'å•Š'}
    
    import re
    clean_query = re.sub(r'[^\w\s]', '', query)
    
    words = clean_query.split()
    keywords = [word for word in words if word not in stop_words and len(word) > 1]
    
    return keywords
```

**åŠŸèƒ½è¯´æ˜**ï¼š
- å»é™¤åœç”¨è¯ï¼ˆä¸­æ–‡å¸¸è§è™šè¯ï¼‰
- æ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
- è¿‡æ»¤é•¿åº¦å°äº2çš„å•è¯
- è¿”å›æ¸…æ´—åçš„å…³é”®è¯åˆ—è¡¨

#### **5.2 å…³é”®è¯è¯„åˆ†**

```python
def _calculate_keyword_score(self, doc: Any, keywords: List[str]) -> float:
    """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
    if not keywords:
        return 0.0
    
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    total_score = 0.0
    for keyword in keywords:
        if keyword in content:
            total_score += 1.0
    
    return min(total_score / len(keywords), 1.0)
```

**è¯„åˆ†é€»è¾‘**ï¼š
- ç»Ÿè®¡æ–‡æ¡£å†…å®¹ä¸­åŒ…å«çš„å…³é”®è¯æ•°é‡
- åˆ†æ•° = åŒ¹é…å…³é”®è¯æ•° / æ€»å…³é”®è¯æ•°
- æœ€é«˜åˆ†æ•°ä¸º1.0

### **6. ğŸŒ è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—**

```python
def _calculate_text_similarity(self, query: str, text: str) -> float:
    """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
    if not text or not query:
        return 0.0
    
    # ç®€å•çš„è¯æ±‡é‡å è®¡ç®—
    query_words = set(query.lower().split())
    text_words = set(text.lower().split())
    
    if not query_words or not text_words:
        return 0.0
    
    intersection = query_words.intersection(text_words)
    union = query_words.union(text_words)
    
    if union:
        return len(intersection) / len(union)
    return 0.0
```

**è®¡ç®—åŸç†**ï¼š
- ä½¿ç”¨Jaccardç›¸ä¼¼åº¦ï¼šäº¤é›†å¤§å° / å¹¶é›†å¤§å°
- å°†æŸ¥è¯¢å’Œæ–‡æ¡£è½¬æ¢ä¸ºè¯æ±‡é›†åˆ
- è®¡ç®—è¯æ±‡é‡å ç¨‹åº¦
- è¿”å›0-1ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°

### **7. ğŸ” æ™ºèƒ½æ¨¡ç³Šæœç´¢**

```python
def _fuzzy_search(self, query: str) -> List[Any]:
    """æ™ºèƒ½æ¨¡ç³Šæœç´¢ - åªåœ¨çœŸæ­£ç›¸å…³æ—¶æ‰å¯ç”¨"""
    results = []
    query_lower = query.lower()
    
    # åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ä¸­èŠ¯å›½é™…ç›¸å…³
    smic_keywords = ['ä¸­èŠ¯å›½é™…', 'ä¸­èŠ¯', 'æ™¶åœ†', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'IC', 'ä»£å·¥']
    query_has_smic_context = any(keyword in query_lower for keyword in smic_keywords)
    
    # å¦‚æœæŸ¥è¯¢ä¸ä¸­èŠ¯å›½é™…æ— å…³ï¼Œä¸å¯ç”¨æ¨¡ç³Šæœç´¢
    if not query_has_smic_context:
        self.logger.debug(f"æŸ¥è¯¢ '{query}' ä¸ä¸­èŠ¯å›½é™…æ— å…³ï¼Œè·³è¿‡æ¨¡ç³Šæœç´¢")
        return results
    
    # æå–æŸ¥è¯¢ä¸­çš„å…³é”®æ¦‚å¿µï¼ˆåªé’ˆå¯¹ä¸­èŠ¯å›½é™…ç›¸å…³å†…å®¹ï¼‰
    key_concepts = ['ä¸­èŠ¯å›½é™…', 'æ™¶åœ†', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'æŠ€æœ¯', 'ä¸šåŠ¡', 'å¸‚åœº', 'å‘å±•', 'è¥æ”¶', 'åˆ©æ¶¦']
    
    for doc_id, doc in self.text_docs.items():
        content = doc.page_content if hasattr(doc, 'page_content') else ''
        content_lower = content.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®æ¦‚å¿µ
        concept_matches = sum(1 for concept in key_concepts if concept in content_lower)
        if concept_matches > 0:
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼Œè¦æ±‚è‡³å°‘2ä¸ªæ¦‚å¿µåŒ¹é…
            if concept_matches >= 2:
                score = min(concept_matches * 0.15, 0.8)  # é™ä½åˆ†æ•°ï¼Œæœ€é«˜0.8
                results.append({
                    'doc_id': doc_id,
                    'doc': doc,
                    'score': score,
                    'match_type': 'smart_fuzzy_search'
                })
    
    return results
```

**æ™ºèƒ½ç‰¹ç‚¹**ï¼š
- **é¢†åŸŸç›¸å…³æ€§åˆ¤æ–­**ï¼šåªåœ¨æŸ¥è¯¢ä¸ä¸­èŠ¯å›½é™…ç›¸å…³æ—¶å¯ç”¨
- **æ¦‚å¿µåŒ¹é…è®¡æ•°**ï¼šç»Ÿè®¡æ–‡æ¡£ä¸­åŒ…å«çš„å…³é”®æ¦‚å¿µæ•°é‡
- **æœ€ä½åŒ¹é…è¦æ±‚**ï¼šè¦æ±‚è‡³å°‘2ä¸ªæ¦‚å¿µåŒ¹é…æ‰è¿”å›ç»“æœ
- **åˆ†æ•°ä¸Šé™æ§åˆ¶**ï¼šæœ€é«˜åˆ†æ•°é™åˆ¶åœ¨0.8ï¼Œé¿å…è¿‡åº¦åŒ¹é…

### **8. ï¿½ï¿½ ç»“æœæ’åºå’Œä¼˜åŒ–**

#### **8.1 æ™ºèƒ½æ’åº**

```python
def _rank_text_results(self, results: List[Any], query: str) -> List[Any]:
    """å¯¹æ–‡æœ¬ç»“æœè¿›è¡Œæ™ºèƒ½æ’åº"""
    if not results:
        return []
    
    # æŒ‰åˆ†æ•°æ’åº
    sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)
    
    # é™åˆ¶ç»“æœæ•°é‡
    return sorted_results[:self.config.max_results]
```

**æ’åºé€»è¾‘**ï¼š
- æŒ‰ç›¸å…³æ€§åˆ†æ•°é™åºæ’åˆ—
- é™åˆ¶è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤10ä¸ªï¼‰
- ç¡®ä¿æœ€ç›¸å…³çš„ç»“æœæ’åœ¨å‰é¢

#### **8.2 ç»“æœå»é‡**

```python
# å»é‡å’Œæ’åº
seen_ids = set()
unique_results = []
for result in results:
    doc_id = result.get('doc_id', 'unknown')
    if doc_id not in seen_ids:
        seen_ids.add(doc_id)
        unique_results.append(result)
```

**å»é‡æœºåˆ¶**ï¼š
- åŸºäºæ–‡æ¡£IDå»é‡
- ä½¿ç”¨é›†åˆï¼ˆsetï¼‰æé«˜æŸ¥æ‰¾æ•ˆç‡
- ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„é‡å¤ç»“æœ

### **9. âš™ï¸ é…ç½®ç®¡ç†**

#### **9.1 é…ç½®éªŒè¯**

```python
def _validate_config(self):
    """éªŒè¯æ–‡æœ¬å¼•æ“é…ç½® - æ”¯æŒä¸¤ç§é…ç½®ç±»å‹"""
            # é…ç½®ç±»å‹æ£€æŸ¥
        from ..config.v2_config import TextEngineConfigV2
        
        if not isinstance(self.config, TextEngineConfigV2):
            raise ValueError("é…ç½®å¿…é¡»æ˜¯TextEngineConfigV2ç±»å‹")
    
    # è·å–ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œæ”¯æŒä¸¤ç§é…ç½®ç±»å‹
    threshold = getattr(self.config, 'text_similarity_threshold', 0.7)
    if threshold < 0 or threshold > 1:
        raise ValueError("æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
```

**é…ç½®ç‰¹ç‚¹**ï¼š
- æ”¯æŒæ–°æ—§ä¸¤ç§é…ç½®ç±»å‹
- éªŒè¯ç›¸ä¼¼åº¦é˜ˆå€¼çš„æœ‰æ•ˆæ€§
- æä¾›é»˜è®¤å€¼æ”¯æŒ

### **10. ï¿½ï¿½ æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡**

#### **10.1 æ–‡æ¡£ç»Ÿè®¡**

```python
def get_text_statistics(self) -> Dict[str, Any]:
    """è·å–æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯"""
    return {
        'total_texts': len(self.text_docs),
        'total_chars': sum(len(doc.page_content) if hasattr(doc, 'page_content') else 0 
                         for doc in self.text_docs.values())
    }
```

**ç»Ÿè®¡ä¿¡æ¯**ï¼š
- æ–‡æœ¬æ–‡æ¡£æ€»æ•°
- æ€»å­—ç¬¦æ•°
- ä¾¿äºç›‘æ§ç³»ç»ŸçŠ¶æ€å’Œæ€§èƒ½

#### **10.2 ç¼“å­˜ç®¡ç†**

```python
def refresh_text_cache(self):
    """åˆ·æ–°æ–‡æœ¬ç¼“å­˜"""
    self._load_text_documents()
    self.logger.info("æ–‡æœ¬ç¼“å­˜å·²åˆ·æ–°")

def get_text_by_id(self, text_id: str) -> Optional[Any]:
    """æ ¹æ®IDè·å–æ–‡æœ¬"""
    return self.text_docs.get(text_id)

def get_all_texts(self) -> List[Any]:
    """è·å–æ‰€æœ‰æ–‡æœ¬"""
    return list(self.text_docs.values())
```

**ç¼“å­˜åŠŸèƒ½**ï¼š
- æ”¯æŒæ‰‹åŠ¨åˆ·æ–°ç¼“å­˜
- æä¾›æŒ‰IDæŸ¥è¯¢æ–‡æ¡£çš„æ¥å£
- æ”¯æŒè·å–æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨

## ï¿½ï¿½ **æŠ€æœ¯å®ç°æ€»ç»“**

æ–‡æœ¬å¬å›ç³»ç»Ÿé€šè¿‡ä»¥ä¸‹æŠ€æœ¯ç‰¹ç‚¹å®ç°äº†é«˜æ€§èƒ½ã€é«˜å‡†ç¡®æ€§çš„æœç´¢ï¼š

1. **5å±‚ç­–ç•¥é™çº§**ï¼šç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§å’Œç»“æœå¯ç”¨æ€§
2. **å†…å­˜ä¼˜å…ˆä¼˜åŒ–**ï¼šå‡å°‘I/Oæ“ä½œï¼Œæå‡å“åº”é€Ÿåº¦
3. **æ™ºèƒ½è¯„åˆ†ç®—æ³•**ï¼šä¸¥æ ¼å’Œå®½æ¾ä¸¤ç§æ¨¡å¼ï¼Œé€‚åº”ä¸åŒåœºæ™¯
4. **é¢†åŸŸç›¸å…³æœç´¢**ï¼šé’ˆå¯¹ä¸­èŠ¯å›½é™…ä¸šåŠ¡åœºæ™¯çš„æ™ºèƒ½æ¨¡ç³Šæœç´¢
5. **å‘åå…¼å®¹æ€§**ï¼šæ”¯æŒæ–°æ—§é…ç½®å’Œä¼ ç»ŸåŠ è½½æ–¹å¼
6. **æ€§èƒ½ç›‘æ§**ï¼šè¯¦ç»†çš„æ—¥å¿—è®°å½•å’Œç»Ÿè®¡ä¿¡æ¯
7. **é”™è¯¯å¤„ç†**ï¼šé‡è¯•æœºåˆ¶å’Œé™çº§ç­–ç•¥ï¼Œæé«˜ç³»ç»Ÿç¨³å®šæ€§

è¿™ä¸ªè®¾è®¡å……åˆ†ä½“ç°äº†**æ™ºèƒ½åŒ–**ã€**æ€§èƒ½ä¼˜åŒ–**å’Œ**å·¥ç¨‹åŒ–**çš„ç‰¹ç‚¹ï¼Œä¸ºRAGç³»ç»Ÿæä¾›äº†å¯é çš„æ–‡æœ¬å¬å›èƒ½åŠ›ã€‚



æˆ‘æ¥è¯¦ç»†è§£é‡Šä¸€ä¸‹ `text_engine.py` ä¸­ `_search_texts` æ–¹æ³•å®šä¹‰çš„5ç§åŒ¹é…ç­–ç•¥ã€‚è®©æˆ‘å…ˆä»”ç»†é˜…è¯»è¿™ä¸ªæ–¹æ³•ï¼Œç„¶åç”¨è‡ªç„¶è¯­è¨€è¯¦ç»†æè¿°æ¯ç§ç­–ç•¥ã€‚

## ğŸ” æ–‡æœ¬æœç´¢5ç§åŒ¹é…ç­–ç•¥è¯¦è§£

### ğŸ“‹ ç­–ç•¥æ¦‚è§ˆ

`_search_texts` æ–¹æ³•é‡‡ç”¨äº†**åˆ†å±‚é€’è¿›**çš„æœç´¢ç­–ç•¥ï¼Œä»ä¸¥æ ¼åŒ¹é…é€æ­¥æ”¾å®½åˆ°æ¨¡ç³ŠåŒ¹é…ï¼Œç¡®ä¿æ—¢èƒ½æ‰¾åˆ°é«˜ç›¸å…³æ€§ç»“æœï¼Œåˆä¸ä¼šé—æ¼æ½œåœ¨ç›¸å…³å†…å®¹ã€‚

---

### ğŸ¯ **ç­–ç•¥1ï¼šæ–‡æœ¬æ–‡æ¡£ä¸¥æ ¼æœç´¢**
**ç›®æ ‡**ï¼šåœ¨å·²åŠ è½½çš„æ–‡æœ¬æ–‡æ¡£ä¸­å¯»æ‰¾é«˜ç›¸å…³æ€§å†…å®¹

**æ‰§è¡Œè¿‡ç¨‹**ï¼š
1. éå†æ‰€æœ‰å·²ç¼“å­˜çš„æ–‡æœ¬æ–‡æ¡£ï¼ˆ`self.text_docs`ï¼‰
2. å¯¹æ¯ä¸ªæ–‡æ¡£è®¡ç®—**ä¸¥æ ¼è¯„åˆ†**ï¼ˆä½¿ç”¨ `_calculate_text_score` æ–¹æ³•ï¼‰
3. åªä¿ç•™åˆ†æ•°è¾¾åˆ°é…ç½®é˜ˆå€¼çš„ç»“æœ
4. è®°å½•åŒ¹é…ç±»å‹ä¸º `'text_doc_search'`

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨**ä¸¥æ ¼çš„ç›¸å…³æ€§åˆ¤æ–­**
- è¦æ±‚å…³é”®è¯åŒ¹é…è‡³å°‘30%æ‰ç»™åˆ†
- è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡ä½ä¼šå¤§å¹…é™ä½åˆ†æ•°
- è¿™æ˜¯**æœ€ç²¾ç¡®**çš„æœç´¢ç­–ç•¥

---

### ğŸ”„ **ç­–ç•¥2ï¼šå†…å­˜å®½æ¾æœç´¢**
**è§¦å‘æ¡ä»¶**ï¼šç­–ç•¥1æ²¡æœ‰æ‰¾åˆ°ç»“æœæ—¶å¯ç”¨

**æ‰§è¡Œè¿‡ç¨‹**ï¼š
1. åœ¨åŒæ ·çš„æ–‡æœ¬æ–‡æ¡£ä¸­é‡æ–°æœç´¢
2. ä½¿ç”¨**å®½æ¾è¯„åˆ†ç®—æ³•**ï¼ˆ`_calculate_text_score_relaxed`ï¼‰
3. é™ä½é˜ˆå€¼åˆ°åŸå§‹é˜ˆå€¼çš„50%
4. è®°å½•åŒ¹é…ç±»å‹ä¸º `'memory_relaxed_search'`

**ç‰¹ç‚¹**ï¼š
- å…³é”®è¯åŒ¹é…è¦æ±‚é™ä½åˆ°10%
- ç›¸å…³æ€§æƒ©ç½šæœºåˆ¶æ›´å®½æ¾
- æ–‡æ¡£ç±»å‹å’Œé•¿åº¦å¥–åŠ±æ›´é«˜
- è¿™æ˜¯**é€‚åº¦æ”¾å®½**çš„æœç´¢ç­–ç•¥

---

### ï¿½ï¿½ **ç­–ç•¥3ï¼šå…³é”®è¯æœç´¢**
**è§¦å‘æ¡ä»¶**ï¼šç­–ç•¥1å’Œ2éƒ½æ²¡æœ‰ç»“æœæ—¶å¯ç”¨

**æ‰§è¡Œè¿‡ç¨‹**ï¼š
1. æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯ï¼ˆè¿‡æ»¤åœç”¨è¯ï¼‰
2. åœ¨æ–‡æœ¬æ–‡æ¡£ä¸­å¯»æ‰¾åŒ…å«è¿™äº›å…³é”®è¯çš„å†…å®¹
3. è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
4. è®°å½•åŒ¹é…ç±»å‹ä¸º `'keyword_search'`

**ç‰¹ç‚¹**ï¼š
- åŸºäº**è¯æ±‡åŒ¹é…**è€Œéè¯­ä¹‰ç†è§£
- é€‚åˆæŸ¥æ‰¾åŒ…å«ç‰¹å®šæœ¯è¯­çš„æ–‡æ¡£
- ä¸è€ƒè™‘ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ç›¸å…³æ€§
- è¿™æ˜¯**è¯æ±‡å±‚é¢**çš„æœç´¢ç­–ç•¥

---

### ï¿½ï¿½ï¸ **ç­–ç•¥4ï¼šæ™ºèƒ½æ¨¡ç³Šæœç´¢**
**è§¦å‘æ¡ä»¶**ï¼šå‰ä¸‰ç§ç­–ç•¥éƒ½æ²¡æœ‰ç»“æœæ—¶å¯ç”¨

**æ‰§è¡Œè¿‡ç¨‹**ï¼š
1. åˆ†ææŸ¥è¯¢æ˜¯å¦ä¸**ä¸­èŠ¯å›½é™…**ç›¸å…³
2. å¦‚æœç›¸å…³ï¼Œæå–å…³é”®æ¦‚å¿µï¼ˆå¦‚ï¼šæ™¶åœ†ã€èŠ¯ç‰‡ã€åŠå¯¼ä½“ç­‰ï¼‰
3. åœ¨æ–‡æ¡£ä¸­å¯»æ‰¾åŒ…å«è¿™äº›æ¦‚å¿µçš„æ–‡æœ¬
4. è¦æ±‚è‡³å°‘2ä¸ªæ¦‚å¿µåŒ¹é…æ‰ç»™åˆ†
5. è®°å½•åŒ¹é…ç±»å‹ä¸º `'smart_fuzzy_search'`

**ç‰¹ç‚¹**ï¼š
- åªåœ¨**ç‰¹å®šä¸šåŠ¡é¢†åŸŸ**å¯ç”¨
- åŸºäº**æ¦‚å¿µåŒ¹é…**è€Œéç²¾ç¡®è¯æ±‡
- åˆ†æ•°ä¸Šé™ä¸º0.8ï¼ˆé¿å…è¿‡åº¦åŒ¹é…ï¼‰
- è¿™æ˜¯**ä¸šåŠ¡å¯¼å‘**çš„æ¨¡ç³Šæœç´¢

---

### ï¿½ï¿½ **ç­–ç•¥5ï¼šå†…å­˜ä½é˜ˆå€¼æœç´¢**
**è§¦å‘æ¡ä»¶**ï¼šæ‰€æœ‰ç­–ç•¥éƒ½æ²¡æœ‰ç»“æœï¼Œä¸”é…ç½®é˜ˆå€¼å¤§äº0.05æ—¶å¯ç”¨

**æ‰§è¡Œè¿‡ç¨‹**ï¼š
1. ä¸´æ—¶å°†ç›¸ä¼¼åº¦é˜ˆå€¼é™ä½åˆ°0.05
2. åœ¨å†…å­˜ä¸­æ–‡æ¡£ä¸­é‡æ–°æœç´¢
3. ä½¿ç”¨æ ‡å‡†è¯„åˆ†ç®—æ³•
4. æœç´¢å®Œæˆåæ¢å¤åŸå§‹é˜ˆå€¼
5. è®°å½•åŒ¹é…ç±»å‹ä¸º `'memory_low_threshold_search'`

**ç‰¹ç‚¹**ï¼š
- è¿™æ˜¯**æœ€åçš„å…œåº•ç­–ç•¥**
- é˜ˆå€¼é™ä½åˆ°æä½æ°´å¹³ï¼ˆ0.05ï¼‰
- ç¡®ä¿å³ä½¿ç›¸å…³æ€§å¾ˆä½ä¹Ÿèƒ½æ‰¾åˆ°ç»“æœ
- æœç´¢å®Œæˆåä¼šæ¢å¤åŸå§‹é…ç½®

---

## ï¿½ï¿½ **ç­–ç•¥æ‰§è¡Œæµç¨‹**

```
å¼€å§‹æœç´¢
    â†“
ç­–ç•¥1ï¼šä¸¥æ ¼æœç´¢ â†’ æœ‰ç»“æœï¼Ÿ â†’ æ˜¯ â†’ è¿”å›ç»“æœ
    â†“ å¦
ç­–ç•¥2ï¼šå®½æ¾æœç´¢ â†’ æœ‰ç»“æœï¼Ÿ â†’ æ˜¯ â†’ è¿”å›ç»“æœ
    â†“ å¦
ç­–ç•¥3ï¼šå…³é”®è¯æœç´¢ â†’ æœ‰ç»“æœï¼Ÿ â†’ æ˜¯ â†’ è¿”å›ç»“æœ
    â†“ å¦
ç­–ç•¥4ï¼šæ™ºèƒ½æ¨¡ç³Šæœç´¢ â†’ æœ‰ç»“æœï¼Ÿ â†’ æ˜¯ â†’ è¿”å›ç»“æœ
    â†“ å¦
ç­–ç•¥5ï¼šä½é˜ˆå€¼æœç´¢ â†’ æœ‰ç»“æœï¼Ÿ â†’ æ˜¯ â†’ è¿”å›ç»“æœ
    â†“ å¦
è¿”å›ç©ºç»“æœ
```

## ğŸ’¡ **è®¾è®¡ç†å¿µ**

è¿™ç§åˆ†å±‚ç­–ç•¥ä½“ç°äº†**"ç²¾ç¡®ä¼˜å…ˆï¼Œé€æ­¥æ”¾å®½"**çš„è®¾è®¡æ€æƒ³ï¼š

1. **ä¼˜å…ˆä¿è¯è´¨é‡**ï¼šå‰ä¸¤ç§ç­–ç•¥ç¡®ä¿ç»“æœçš„é«˜ç›¸å…³æ€§
2. **é¿å…é—æ¼å†…å®¹**ï¼šåç»­ç­–ç•¥ç¡®ä¿èƒ½æ‰¾åˆ°æ½œåœ¨ç›¸å…³å†…å®¹
3. **æ™ºèƒ½ä¸šåŠ¡ç†è§£**ï¼šç­–ç•¥4é’ˆå¯¹ç‰¹å®šä¸šåŠ¡åœºæ™¯ä¼˜åŒ–
4. **å…œåº•ä¿éšœ**ï¼šç­–ç•¥5ç¡®ä¿å³ä½¿ä½ç›¸å…³æ€§ä¹Ÿèƒ½è¿”å›ç»“æœ

## ï¿½ï¿½ **å®é™…åº”ç”¨åœºæ™¯**

- **ç­–ç•¥1-2**ï¼šé€‚åˆç²¾ç¡®æŸ¥è¯¢ï¼Œå¦‚"ä¸­èŠ¯å›½é™…2023å¹´è¥æ”¶"
- **ç­–ç•¥3**ï¼šé€‚åˆæœ¯è¯­æŸ¥è¯¢ï¼Œå¦‚"æ™¶åœ†ä»£å·¥"
- **ç­–ç•¥4**ï¼šé€‚åˆä¸šåŠ¡ç›¸å…³æŸ¥è¯¢ï¼Œå¦‚"åŠå¯¼ä½“æŠ€æœ¯å‘å±•"
- **ç­–ç•¥5**ï¼šç¡®ä¿ä»»ä½•æŸ¥è¯¢éƒ½èƒ½è¿”å›ç»“æœï¼Œé¿å…ç©ºç»“æœ

è¿™æ ·çš„è®¾è®¡æ—¢ä¿è¯äº†æœç´¢è´¨é‡ï¼Œåˆç¡®ä¿äº†æœç´¢çš„è¦†ç›–é¢å’Œç”¨æˆ·ä½“éªŒï¼

å¥½çš„ï¼æˆ‘æ¥è¯¦ç»†è§£é‡Šæ¯ä¸€æ­¥çš„ç®—åˆ†æœºåˆ¶ã€‚è®©æˆ‘ä»”ç»†åˆ†æä»£ç ä¸­çš„è¯„åˆ†ç®—æ³•ï¼Œç”¨å…·ä½“çš„ä¾‹å­æ¥è¯´æ˜ã€‚

## ï¿½ï¿½ æ–‡æœ¬æœç´¢è¯„åˆ†ç®—æ³•è¯¦è§£

### ï¿½ï¿½ **ç­–ç•¥1ï¼šä¸¥æ ¼è¯„åˆ†ç®—æ³•** (`_calculate_text_score`)

```python
def _calculate_text_score(self, doc: Any, query: str) -> float:
    score = 0.0
    
    # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆæƒé‡æœ€é«˜ï¼‰
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight  # é»˜è®¤0.3
    
    # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        if keyword_score > 0.3:  # è‡³å°‘30%çš„å…³é”®è¯åŒ¹é…
            score += keyword_score * self.config.keyword_weight  # é»˜è®¤0.5
        else:
            score *= 0.3  # å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œå¤§å¹…é™ä½åˆ†æ•°
    
    # 3. å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.3
        score += vector_score * self.config.vector_weight  # é»˜è®¤0.2
    
    # 4. æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.05
    
    # 5. å†…å®¹é•¿åº¦å¥–åŠ±
    if len(content) > 100:
        score += 0.02
    
    # 6. ç›¸å…³æ€§æƒ©ç½šæœºåˆ¶
    if semantic_score < 0.1:  # è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡ä½
        score *= 0.5  # å¤§å¹…é™ä½åˆ†æ•°
    
    return min(score, 1.0)
```

**å…·ä½“ç®—åˆ†ç¤ºä¾‹**ï¼š
å‡è®¾æŸ¥è¯¢ï¼š"ä¸­èŠ¯å›½é™…2023å¹´è¥æ”¶æƒ…å†µ"

- **è¯­ä¹‰ç›¸ä¼¼åº¦**ï¼š0.6 Ã— 0.3 = 0.18
- **å…³é”®è¯åŒ¹é…**ï¼š0.8 Ã— 0.5 = 0.4ï¼ˆå‡è®¾80%å…³é”®è¯åŒ¹é…ï¼‰
- **å‘é‡ç›¸ä¼¼åº¦**ï¼š0.3 Ã— 0.2 = 0.06
- **æ–‡æ¡£ç±»å‹å¥–åŠ±**ï¼š+0.05
- **å†…å®¹é•¿åº¦å¥–åŠ±**ï¼š+0.02
- **æœ€ç»ˆåˆ†æ•°**ï¼š0.18 + 0.4 + 0.06 + 0.05 + 0.02 = 0.71

---

### ï¿½ï¿½ **ç­–ç•¥2ï¼šå®½æ¾è¯„åˆ†ç®—æ³•** (`_calculate_text_score_relaxed`)

```python
def _calculate_text_score_relaxed(self, doc: Any, query: str) -> float:
    score = 0.0
    
    # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå®½æ¾å¤„ç†ï¼‰
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight  # 0.3
    
    # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆå®½æ¾åŒ¹é…ï¼‰
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        if keyword_score > 0.1:  # é™ä½åˆ°10%çš„å…³é”®è¯åŒ¹é…
            score += keyword_score * self.config.keyword_weight  # 0.5
        else:
            score *= 0.7  # è½»å¾®é™ä½åˆ†æ•°ï¼ˆä»0.3æå‡åˆ°0.7ï¼‰
    
    # 3. å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆæé«˜é»˜è®¤åˆ†æ•°ï¼‰
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.4  # ä»0.3æå‡åˆ°0.4
        score += vector_score * self.config.vector_weight  # 0.2
    
    # 4. æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±ï¼ˆæé«˜å¥–åŠ±ï¼‰
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.1  # ä»0.05æå‡åˆ°0.1
    
    # 5. å†…å®¹é•¿åº¦å¥–åŠ±ï¼ˆæé«˜å¥–åŠ±ï¼‰
    if len(content) > 100:
        score += 0.05  # ä»0.02æå‡åˆ°0.05
    
    # 6. ç›¸å…³æ€§æƒ©ç½šæœºåˆ¶ï¼ˆå®½æ¾å¤„ç†ï¼‰
    if semantic_score < 0.05:  # ä»0.1é™ä½åˆ°0.05
        score *= 0.7  # ä»0.5æå‡åˆ°0.7
    
    return min(score, 1.0)
```

**å…·ä½“ç®—åˆ†ç¤ºä¾‹**ï¼š
åŒæ ·çš„æŸ¥è¯¢ï¼š"ä¸­èŠ¯å›½é™…2023å¹´è¥æ”¶æƒ…å†µ"

- **è¯­ä¹‰ç›¸ä¼¼åº¦**ï¼š0.6 Ã— 0.3 = 0.18
- **å…³é”®è¯åŒ¹é…**ï¼š0.8 Ã— 0.5 = 0.4
- **å‘é‡ç›¸ä¼¼åº¦**ï¼š0.4 Ã— 0.2 = 0.08
- **æ–‡æ¡£ç±»å‹å¥–åŠ±**ï¼š+0.1
- **å†…å®¹é•¿åº¦å¥–åŠ±**ï¼š+0.05
- **æœ€ç»ˆåˆ†æ•°**ï¼š0.18 + 0.4 + 0.08 + 0.1 + 0.05 = 0.81

**å®½æ¾ç­–ç•¥çš„ä¼˜åŠ¿**ï¼šåˆ†æ•°æ›´é«˜ï¼Œæ›´å®¹æ˜“è¾¾åˆ°é˜ˆå€¼ï¼

---

### ï¿½ï¿½ **ç­–ç•¥3ï¼šå…³é”®è¯åŒ¹é…è¯„åˆ†** (`_calculate_keyword_score`)

```python
def _calculate_keyword_score(self, doc: Any, keywords: List[str]) -> float:
    if not keywords:
        return 0.0
    
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    total_score = 0.0
    for keyword in keywords:
        if keyword in content:  # ç®€å•åŒ…å«åˆ¤æ–­
            total_score += 1.0
    
    return min(total_score / len(keywords), 1.0)
```

**å…·ä½“ç®—åˆ†ç¤ºä¾‹**ï¼š
æŸ¥è¯¢ï¼š"ä¸­èŠ¯å›½é™…2023å¹´è¥æ”¶æƒ…å†µ"
æå–å…³é”®è¯ï¼š["ä¸­èŠ¯å›½é™…", "2023", "è¥æ”¶", "æƒ…å†µ"]

å‡è®¾æ–‡æ¡£åŒ…å«ï¼š["ä¸­èŠ¯å›½é™…", "è¥æ”¶", "æƒ…å†µ"]ï¼ˆ3ä¸ªå…³é”®è¯åŒ¹é…ï¼‰

- **å…³é”®è¯åˆ†æ•°**ï¼š3 Ã· 4 = 0.75

---

### ï¿½ï¿½ **ç­–ç•¥4ï¼šæ™ºèƒ½æ¨¡ç³Šæœç´¢è¯„åˆ†**

```python
def _smart_fuzzy_search(self, query: str) -> List[Any]:
    # åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ä¸­èŠ¯å›½é™…ç›¸å…³
    smic_keywords = ['ä¸­èŠ¯å›½é™…', 'ä¸­èŠ¯', 'æ™¶åœ†', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'IC', 'ä»£å·¥']
    query_has_smic_context = any(keyword in query_lower for keyword in smic_keywords)
    
    if not query_has_smic_context:
        return []  # ä¸ç›¸å…³å°±è·³è¿‡
    
    # æå–å…³é”®æ¦‚å¿µ
    key_concepts = ['ä¸­èŠ¯å›½é™…', 'æ™¶åœ†', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'æŠ€æœ¯', 'ä¸šåŠ¡', 'å¸‚åœº', 'å‘å±•', 'è¥æ”¶', 'åˆ©æ¶¦']
    
    for doc_id, doc in self.text_docs.items():
        content_lower = content.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®æ¦‚å¿µ
        concept_matches = sum(1 for concept in key_concepts if concept in content_lower)
        if concept_matches > 0:
            if concept_matches >= 2:  # è‡³å°‘2ä¸ªæ¦‚å¿µåŒ¹é…
                score = min(concept_matches * 0.15, 0.8)  # æœ€é«˜0.8åˆ†
                results.append({
                    'doc_id': doc_id,
                    'doc': doc,
                    'score': score,
                    'match_type': 'smart_fuzzy_search'
                })
```

**å…·ä½“ç®—åˆ†ç¤ºä¾‹**ï¼š
æŸ¥è¯¢ï¼š"åŠå¯¼ä½“æŠ€æœ¯å‘å±•"

- **æ¦‚å¿µåŒ¹é…æ£€æŸ¥**ï¼š
  - æ–‡æ¡£åŒ…å«ï¼š["åŠå¯¼ä½“", "æŠ€æœ¯", "å‘å±•"]ï¼ˆ3ä¸ªæ¦‚å¿µï¼‰
  - **åˆ†æ•°è®¡ç®—**ï¼š3 Ã— 0.15 = 0.45ï¼ˆä½†æœ€é«˜0.8åˆ†ï¼‰

---

### ğŸ“‰ **ç­–ç•¥5ï¼šä½é˜ˆå€¼æœç´¢è¯„åˆ†**

```python
# ä¸´æ—¶é™ä½é˜ˆå€¼
original_threshold = self.config.text_similarity_threshold
self.config.text_similarity_threshold = 0.05  # é™ä½åˆ°0.05

try:
    # ä½¿ç”¨æ ‡å‡†è¯„åˆ†ç®—æ³•ï¼Œä½†é˜ˆå€¼æä½
    for doc_id, doc in self.text_docs.items():
        score = self._calculate_text_score(doc, query)
        if score >= 0.05:  # æä½é˜ˆå€¼
            results.append({
                'doc_id': doc_id,
                'doc': doc,
                'score': score,
                'match_type': 'memory_low_threshold_search'
            })
finally:
    # æ¢å¤åŸå§‹é˜ˆå€¼
    self.config.text_similarity_threshold = original_threshold
```

**å…·ä½“ç®—åˆ†ç¤ºä¾‹**ï¼š
å‡è®¾åŸå§‹é˜ˆå€¼æ˜¯0.7ï¼Œç°åœ¨ä¸´æ—¶é™ä½åˆ°0.05

- ä½¿ç”¨**ç­–ç•¥1çš„ä¸¥æ ¼è¯„åˆ†ç®—æ³•**
- ä½†æ¥å—åˆ†æ•°â‰¥0.05çš„ç»“æœ
- è¿™æ ·å³ä½¿ç›¸å…³æ€§å¾ˆä½ä¹Ÿèƒ½æ‰¾åˆ°ç»“æœ

---

## ğŸ¯ **è¯„åˆ†æƒé‡é…ç½®**

```python
# é»˜è®¤æƒé‡é…ç½®
self.config.semantic_weight = 0.3      # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡30%
self.config.keyword_weight = 0.5       # å…³é”®è¯åŒ¹é…æƒé‡50%
self.config.vector_weight = 0.2        # å‘é‡ç›¸ä¼¼åº¦æƒé‡20%
```

## ğŸ” **è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—** (`_calculate_text_similarity`)

```python
def _calculate_text_similarity(self, query: str, text: str) -> float:
    query_words = set(query.lower().split())
    text_words = set(text.lower().split())
    
    intersection = query_words.intersection(text_words)  # äº¤é›†
    union = query_words.union(text_words)               # å¹¶é›†
    
    if union:
        return len(intersection) / len(union)  # Jaccardç›¸ä¼¼åº¦
    return 0.0
```

**å…·ä½“ç®—åˆ†ç¤ºä¾‹**ï¼š
æŸ¥è¯¢ï¼š"ä¸­èŠ¯å›½é™…è¥æ”¶"
æ–‡æ¡£ï¼š"ä¸­èŠ¯å›½é™…2023å¹´è¥æ”¶æƒ…å†µ"

- **æŸ¥è¯¢è¯æ±‡**ï¼š{"ä¸­èŠ¯å›½é™…", "è¥æ”¶"}
- **æ–‡æ¡£è¯æ±‡**ï¼š{"ä¸­èŠ¯å›½é™…", "2023", "å¹´", "è¥æ”¶", "æƒ…å†µ"}
- **äº¤é›†**ï¼š{"ä¸­èŠ¯å›½é™…", "è¥æ”¶"}ï¼ˆ2ä¸ªï¼‰
- **å¹¶é›†**ï¼š{"ä¸­èŠ¯å›½é™…", "2023", "å¹´", "è¥æ”¶", "æƒ…å†µ"}ï¼ˆ5ä¸ªï¼‰
- **ç›¸ä¼¼åº¦**ï¼š2 Ã· 5 = 0.4

## ï¿½ï¿½ **å…³é”®ç†è§£ç‚¹**

1. **æƒé‡åˆ†é…**ï¼šå…³é”®è¯åŒ¹é…å 50%ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦å 30%ï¼Œå‘é‡ç›¸ä¼¼åº¦å 20%
2. **æƒ©ç½šæœºåˆ¶**ï¼šå…³é”®è¯åŒ¹é…ä¸è¶³ä¼šå¤§å¹…é™ä½åˆ†æ•°
3. **å¥–åŠ±æœºåˆ¶**ï¼šæ–‡æ¡£ç±»å‹ã€å†…å®¹é•¿åº¦ç­‰ç»™äºˆå°é¢å¥–åŠ±
4. **é˜ˆå€¼æ§åˆ¶**ï¼šä¸åŒç­–ç•¥ä½¿ç”¨ä¸åŒé˜ˆå€¼ï¼Œç¡®ä¿ç»“æœè´¨é‡
5. **å…œåº•ä¿éšœ**ï¼šç­–ç•¥5ç¡®ä¿å³ä½¿ä½ç›¸å…³æ€§ä¹Ÿèƒ½è¿”å›ç»“æœ

è¿™æ ·çš„è¯„åˆ†æœºåˆ¶æ—¢ä¿è¯äº†æœç´¢çš„ç²¾ç¡®æ€§ï¼Œåˆç¡®ä¿äº†æœç´¢çš„è¦†ç›–é¢å’Œç”¨æˆ·ä½“éªŒï¼