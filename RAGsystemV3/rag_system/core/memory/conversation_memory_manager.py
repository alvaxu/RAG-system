"""
å¯¹è¯è®°å¿†ç®¡ç†å™¨

åŸºäºRAGç³»ç»ŸV3çš„å‘é‡æ•°æ®åº“æœºåˆ¶ï¼Œå®ç°å¤šè½®å¯¹è¯è®°å¿†ç®¡ç†åŠŸèƒ½
"""

import logging
import sqlite3
import json
import time
import os
import math
import re
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from pathlib import Path

# ç”¨äºä¸‰å±‚æ£€ç´¢ç­–ç•¥çš„å¯¼å…¥
try:
    import jieba
    import jieba.posseg as pseg
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("jiebaæˆ–sklearnæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„å…³é”®è¯æå–")

from .models import ConversationSession, MemoryChunk, MemoryQuery, CompressionRequest
from .exceptions import (
    MemoryError, SessionNotFoundError, MemoryRetrievalError, 
    MemoryStorageError, DatabaseError, ValidationError
)
# PathManageré€šè¿‡ä¾èµ–æ³¨å…¥ä¼ å…¥ï¼Œé¿å…è·¨åŒ…å¯¼å…¥
from .memory_config_manager import MemoryConfigManager
from .memory_compression_engine import MemoryCompressionEngine

logger = logging.getLogger(__name__)


class ConversationMemoryManager:
    """
    å¯¹è¯è®°å¿†ç®¡ç†å™¨
    
    åŸºäºç°æœ‰å‘é‡æ•°æ®åº“æœºåˆ¶ï¼Œå®ç°å¤šè½®å¯¹è¯è®°å¿†ç®¡ç†
    """
    
    def __init__(self, config_manager: MemoryConfigManager, 
                 vector_db_integration=None, llm_caller=None, path_manager=None):
        """
        åˆå§‹åŒ–å¯¹è¯è®°å¿†ç®¡ç†å™¨
        
        Args:
            config_manager: è®°å¿†é…ç½®ç®¡ç†å™¨å®ä¾‹
            vector_db_integration: å‘é‡æ•°æ®åº“é›†æˆå®ä¾‹ï¼ˆå¯é€‰ï¼‰
            llm_caller: LLMè°ƒç”¨å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            path_manager: è·¯å¾„ç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.config_manager = config_manager
        self.vector_db_integration = vector_db_integration
        self.llm_caller = llm_caller
        self.path_manager = path_manager
        
        # è·å–é…ç½®
        self.database_path = config_manager.get_database_path()
        self.retrieval_config = config_manager.get_retrieval_config()
        self.session_config = config_manager.get_session_config()
        
        # åˆå§‹åŒ–å‹ç¼©å¼•æ“
        self.compression_engine = MemoryCompressionEngine(config_manager, llm_caller)
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._initialize_database()
        
        # è®°å¿†ç»Ÿè®¡
        self.memory_stats = {
            'total_sessions': 0,
            'total_memories': 0,
            'total_queries': 0,
            'avg_memories_per_session': 0.0,
            'last_activity': None
        }
        
        logger.info("å¯¹è¯è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_database(self) -> None:
        """
        åˆå§‹åŒ–è®°å¿†æ•°æ®åº“
        
        Raises:
            DatabaseError: æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥
        """
        try:
            # å¤„ç†æ•°æ®åº“è·¯å¾„
            if self.path_manager:
                # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆåŸºäºé¡¹ç›®æ ¹ç›®å½•ï¼‰
                self.database_path = self.path_manager.get_absolute_path(self.database_path)
            else:
                # å¦‚æœæ²¡æœ‰è·¯å¾„ç®¡ç†å™¨ï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ä½œä¸ºåŸºå‡†
                if not os.path.isabs(self.database_path):
                    self.database_path = os.path.abspath(self.database_path)
            
            # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
            db_path = Path(self.database_path)
            db_path.parent.mkdir(parents=True, exist_ok=True)
            
            # è¿æ¥æ•°æ®åº“
            self.conn = sqlite3.connect(self.database_path, check_same_thread=False)
            self.conn.row_factory = sqlite3.Row
            
            # åˆ›å»ºè¡¨
            self._create_tables()
            
            logger.info(f"è®°å¿†æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.database_path}")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise DatabaseError(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}") from e
    
    def _create_tables(self) -> None:
        """
        åˆ›å»ºæ•°æ®åº“è¡¨
        """
        cursor = self.conn.cursor()
        
        # åˆ›å»ºä¼šè¯è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS conversation_sessions (
                session_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                status TEXT NOT NULL DEFAULT 'active',
                metadata TEXT,
                memory_count INTEGER DEFAULT 0,
                last_query TEXT
            )
        """)
        
        # åˆ›å»ºè®°å¿†è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS memory_chunks (
                chunk_id TEXT PRIMARY KEY,
                session_id TEXT NOT NULL,
                content TEXT NOT NULL,
                content_type TEXT NOT NULL DEFAULT 'text',
                relevance_score REAL DEFAULT 0.0,
                importance_score REAL DEFAULT 0.0,
                created_at TEXT NOT NULL,
                metadata TEXT,
                vector_embedding TEXT,
                FOREIGN KEY (session_id) REFERENCES conversation_sessions (session_id)
            )
        """)
        
        # åˆ›å»ºå‹ç¼©è®°å½•è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS compression_records (
                record_id TEXT PRIMARY KEY,
                session_id TEXT NOT NULL,
                original_count INTEGER NOT NULL,
                compressed_count INTEGER NOT NULL,
                compression_ratio REAL NOT NULL,
                strategy TEXT NOT NULL,
                created_at TEXT NOT NULL,
                metadata TEXT,
                FOREIGN KEY (session_id) REFERENCES conversation_sessions (session_id)
            )
        """)
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_sessions_user_id ON conversation_sessions (user_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_sessions_status ON conversation_sessions (status)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_memories_session_id ON memory_chunks (session_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_memories_content_type ON memory_chunks (content_type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_memories_created_at ON memory_chunks (created_at)")
        
        self.conn.commit()
    
    def create_session(self, user_id: str, metadata: Dict[str, Any] = None) -> ConversationSession:
        """
        åˆ›å»ºæ–°çš„å¯¹è¯ä¼šè¯
        
        Args:
            user_id: ç”¨æˆ·æ ‡è¯†
            metadata: ä¼šè¯å…ƒæ•°æ®
            
        Returns:
            ConversationSession: åˆ›å»ºçš„ä¼šè¯å¯¹è±¡
            
        Raises:
            MemoryError: ä¼šè¯åˆ›å»ºå¤±è´¥
        """
        try:
            # æ£€æŸ¥ç”¨æˆ·ä¼šè¯æ•°é‡é™åˆ¶
            if not self._check_session_limit(user_id):
                raise MemoryError(f"ç”¨æˆ· {user_id} çš„ä¼šè¯æ•°é‡å·²è¾¾åˆ°ä¸Šé™")
            
            # åˆ›å»ºä¼šè¯å¯¹è±¡
            session = ConversationSession(
                user_id=user_id,
                metadata=metadata or {},
                created_at=datetime.now(),
                updated_at=datetime.now()
            )
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            cursor = self.conn.cursor()
            cursor.execute("""
                INSERT INTO conversation_sessions 
                (session_id, user_id, created_at, updated_at, status, metadata, memory_count, last_query)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                session.session_id,
                session.user_id,
                session.created_at.isoformat(),
                session.updated_at.isoformat(),
                session.status,
                json.dumps(session.metadata),
                session.memory_count,
                session.last_query
            ))
            
            self.conn.commit()
            
            # æ›´æ–°ç»Ÿè®¡
            self.memory_stats['total_sessions'] += 1
            self.memory_stats['last_activity'] = datetime.now()
            
            logger.info(f"ä¼šè¯åˆ›å»ºæˆåŠŸ: {session.session_id} (ç”¨æˆ·: {user_id})")
            return session
            
        except Exception as e:
            logger.error(f"åˆ›å»ºä¼šè¯å¤±è´¥: {e}")
            raise MemoryError(f"åˆ›å»ºä¼šè¯å¤±è´¥: {e}") from e
    
    def get_session(self, session_id: str) -> ConversationSession:
        """
        è·å–ä¼šè¯ä¿¡æ¯
        
        Args:
            session_id: ä¼šè¯ID
            
        Returns:
            ConversationSession: ä¼šè¯å¯¹è±¡
            
        Raises:
            SessionNotFoundError: ä¼šè¯ä¸å­˜åœ¨
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT * FROM conversation_sessions WHERE session_id = ?
            """, (session_id,))
            
            row = cursor.fetchone()
            if not row:
                raise SessionNotFoundError(session_id)
            
            # æ„å»ºä¼šè¯å¯¹è±¡
            session = ConversationSession(
                session_id=row['session_id'],
                user_id=row['user_id'],
                created_at=datetime.fromisoformat(row['created_at']),
                updated_at=datetime.fromisoformat(row['updated_at']),
                status=row['status'],
                metadata=json.loads(row['metadata']) if row['metadata'] and row['metadata'].strip() else {},
                memory_count=row['memory_count'],
                last_query=row['last_query'] or ''
            )
            
            return session
            
        except SessionNotFoundError:
            raise
        except Exception as e:
            logger.error(f"è·å–ä¼šè¯å¤±è´¥: {e}")
            raise MemoryError(f"è·å–ä¼šè¯å¤±è´¥: {e}") from e
    
    def list_sessions(self, user_id: str = None, status: str = "active", 
                     limit: int = 100) -> List[ConversationSession]:
        """
        åˆ—å‡ºä¼šè¯
        
        Args:
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
            status: ä¼šè¯çŠ¶æ€
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            List[ConversationSession]: ä¼šè¯åˆ—è¡¨
        """
        try:
            cursor = self.conn.cursor()
            
            if user_id:
                cursor.execute("""
                    SELECT * FROM conversation_sessions 
                    WHERE user_id = ? AND status = ?
                    ORDER BY updated_at DESC
                    LIMIT ?
                """, (user_id, status, limit))
            else:
                cursor.execute("""
                    SELECT * FROM conversation_sessions 
                    WHERE status = ?
                    ORDER BY updated_at DESC
                    LIMIT ?
                """, (status, limit))
            
            rows = cursor.fetchall()
            sessions = []
            
            for row in rows:
                session = ConversationSession(
                    session_id=row['session_id'],
                    user_id=row['user_id'],
                    created_at=datetime.fromisoformat(row['created_at']),
                    updated_at=datetime.fromisoformat(row['updated_at']),
                    status=row['status'],
                    metadata=json.loads(row['metadata']) if row['metadata'] and row['metadata'].strip() else {},
                    memory_count=row['memory_count'],
                    last_query=row['last_query'] or ''
                )
                sessions.append(session)
            
            return sessions
            
        except Exception as e:
            logger.error(f"åˆ—å‡ºä¼šè¯å¤±è´¥: {e}")
            raise MemoryError(f"åˆ—å‡ºä¼šè¯å¤±è´¥: {e}") from e
    
    def delete_session(self, session_id: str) -> bool:
        """
        åˆ é™¤ä¼šè¯åŠå…¶æ‰€æœ‰è®°å¿†
        
        Args:
            session_id: ä¼šè¯ID
            
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
            
        Raises:
            SessionNotFoundError: ä¼šè¯ä¸å­˜åœ¨
            MemoryError: åˆ é™¤å¤±è´¥
        """
        try:
            # éªŒè¯ä¼šè¯å­˜åœ¨
            self.get_session(session_id)
            
            cursor = self.conn.cursor()
            
            # åˆ é™¤ä¼šè¯çš„æ‰€æœ‰è®°å¿†
            cursor.execute("DELETE FROM memory_chunks WHERE session_id = ?", (session_id,))
            deleted_memories = cursor.rowcount
            
            # åˆ é™¤ä¼šè¯
            cursor.execute("DELETE FROM conversation_sessions WHERE session_id = ?", (session_id,))
            deleted_sessions = cursor.rowcount
            
            self.conn.commit()
            
            # æ›´æ–°ç»Ÿè®¡
            self.memory_stats['total_sessions'] = max(0, self.memory_stats['total_sessions'] - deleted_sessions)
            self.memory_stats['total_memories'] = max(0, self.memory_stats['total_memories'] - deleted_memories)
            self.memory_stats['last_activity'] = datetime.now()
            
            logger.info(f"ä¼šè¯åˆ é™¤æˆåŠŸ: {session_id}, åˆ é™¤è®°å¿†: {deleted_memories} æ¡")
            return True
            
        except SessionNotFoundError:
            raise
        except Exception as e:
            logger.error(f"åˆ é™¤ä¼šè¯å¤±è´¥: {e}")
            raise MemoryError(f"åˆ é™¤ä¼šè¯å¤±è´¥: {e}") from e
    
    def add_memory(self, session_id: str, content: str, content_type: str = "text",
                   relevance_score: float = 0.0, importance_score: float = 0.0,
                   metadata: Dict[str, Any] = None) -> MemoryChunk:
        """
        æ·»åŠ è®°å¿†
        
        Args:
            session_id: ä¼šè¯ID
            content: è®°å¿†å†…å®¹
            content_type: å†…å®¹ç±»å‹
            relevance_score: ç›¸å…³æ€§åˆ†æ•°
            importance_score: é‡è¦æ€§åˆ†æ•°
            metadata: è®°å¿†å…ƒæ•°æ®
            
        Returns:
            MemoryChunk: åˆ›å»ºçš„è®°å¿†å¯¹è±¡
            
        Raises:
            SessionNotFoundError: ä¼šè¯ä¸å­˜åœ¨
            MemoryStorageError: è®°å¿†å­˜å‚¨å¤±è´¥
        """
        try:
            # éªŒè¯ä¼šè¯å­˜åœ¨
            self.get_session(session_id)
            
            # åˆ›å»ºè®°å¿†å¯¹è±¡
            memory = MemoryChunk(
                session_id=session_id,
                content=content,
                content_type=content_type,
                relevance_score=relevance_score,
                importance_score=importance_score,
                created_at=datetime.now(),
                metadata=metadata or {}
            )
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            cursor = self.conn.cursor()
            cursor.execute("""
                INSERT INTO memory_chunks 
                (chunk_id, session_id, content, content_type, relevance_score, 
                 importance_score, created_at, metadata, vector_embedding)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                memory.chunk_id,
                memory.session_id,
                memory.content,
                memory.content_type,
                memory.relevance_score,
                memory.importance_score,
                memory.created_at.isoformat(),
                json.dumps(memory.metadata),
                json.dumps(memory.vector_embedding) if memory.vector_embedding else None
            ))
            
            # æ›´æ–°ä¼šè¯è®°å¿†æ•°é‡
            cursor.execute("""
                UPDATE conversation_sessions 
                SET memory_count = memory_count + 1, updated_at = ?
                WHERE session_id = ?
            """, (datetime.now().isoformat(), session_id))
            
            self.conn.commit()
            
            # æ›´æ–°ç»Ÿè®¡
            self.memory_stats['total_memories'] += 1
            self.memory_stats['last_activity'] = datetime.now()
            
            logger.info(f"è®°å¿†æ·»åŠ æˆåŠŸ: {memory.chunk_id} (ä¼šè¯: {session_id})")
            return memory
            
        except SessionNotFoundError:
            raise
        except Exception as e:
            logger.error(f"æ·»åŠ è®°å¿†å¤±è´¥: {e}")
            raise MemoryStorageError(f"æ·»åŠ è®°å¿†å¤±è´¥: {e}") from e
    
    def retrieve_memories(self, query: MemoryQuery) -> List[MemoryChunk]:
        """
        æ£€ç´¢ç›¸å…³è®°å¿†
        
        Args:
            query: è®°å¿†æŸ¥è¯¢å¯¹è±¡
            
        Returns:
            List[MemoryChunk]: ç›¸å…³è®°å¿†åˆ—è¡¨
            
        Raises:
            MemoryRetrievalError: è®°å¿†æ£€ç´¢å¤±è´¥
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨æ–‡æœ¬æ£€ç´¢ï¼Œå› ä¸ºè®°å¿†å­˜å‚¨åœ¨SQLiteä¸­
            # å‘é‡æ£€ç´¢éœ€è¦è®°å¿†å†…å®¹ä¹Ÿå­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œç›®å‰æ²¡æœ‰å®ç°
            logger.info(f"ä½¿ç”¨æ–‡æœ¬æ£€ç´¢è®°å¿†: session_id={query.session_id}, query_text={query.query_text[:50]}...")
            return self._text_retrieve_memories(query)
                
        except Exception as e:
            logger.error(f"è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
            raise MemoryRetrievalError(f"è®°å¿†æ£€ç´¢å¤±è´¥: {e}") from e
    
    def _vector_retrieve_memories(self, query: MemoryQuery) -> List[MemoryChunk]:
        """
        åŸºäºå‘é‡çš„è®°å¿†æ£€ç´¢
        
        Args:
            query: è®°å¿†æŸ¥è¯¢å¯¹è±¡
            
        Returns:
            List[MemoryChunk]: ç›¸å…³è®°å¿†åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ç°æœ‰å‘é‡æ•°æ®åº“è¿›è¡Œæ£€ç´¢
            # è¿™é‡Œéœ€è¦é€‚é…ç°æœ‰çš„å‘é‡æ£€ç´¢æœºåˆ¶
            results = self.vector_db_integration.search_texts(
                query=query.query_text,
                k=query.max_results,
                similarity_threshold=query.similarity_threshold
            )
            
            # è½¬æ¢ä¸ºè®°å¿†å¯¹è±¡
            memories = []
            for result in results:
                # æ£€æŸ¥æ˜¯å¦å±äºç›®æ ‡ä¼šè¯
                if result.get('session_id') == query.session_id:
                    memory = MemoryChunk(
                        chunk_id=result.get('chunk_id', ''),
                        session_id=result.get('session_id', ''),
                        content=result.get('content', ''),
                        content_type=result.get('content_type', 'text'),
                        relevance_score=result.get('similarity_score', 0.0),
                        importance_score=result.get('importance_score', 0.0),
                        created_at=datetime.fromisoformat(result.get('created_at', datetime.now().isoformat())),
                        metadata=result.get('metadata', {}),
                        vector_embedding=result.get('vector_embedding')
                    )
                    memories.append(memory)
            
            return memories
            
        except Exception as e:
            logger.error(f"å‘é‡è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
            raise MemoryRetrievalError(f"å‘é‡è®°å¿†æ£€ç´¢å¤±è´¥: {e}") from e
    
    def _text_retrieve_memories(self, query: MemoryQuery) -> List[MemoryChunk]:
        """
        åŸºäºæ–‡æœ¬çš„è®°å¿†æ£€ç´¢ - ä¸‰å±‚æ£€ç´¢ç­–ç•¥
        
        Args:
            query: è®°å¿†æŸ¥è¯¢å¯¹è±¡
            
        Returns:
            List[MemoryChunk]: ç›¸å…³è®°å¿†åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹ä¸‰å±‚è®°å¿†æ£€ç´¢: session_id={query.session_id}, query_text='{query.query_text}'")
            
            # 1. åŸºç¡€è¿‡æ»¤ï¼šæŒ‰session_idå’Œæ—¶é—´èŒƒå›´è¿‡æ»¤
            all_memories = self._get_filtered_memories(query)
            
            if not all_memories:
                logger.info("ğŸ” æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆåŸºç¡€è¿‡æ»¤æ¡ä»¶çš„è®°å¿†ã€‚")
                return []
            
            logger.info(f"ğŸ” ç¬¬ä¸€å±‚ï¼šæ—¶é—´è¡°å‡ç­›é€‰ï¼Œå€™é€‰è®°å¿†æ•°é‡: {len(all_memories)}")
            
            # 2. å…³é”®è¯åŒ¹é… (åˆæ­¥ç­›é€‰)
            keyword_matched_memories = self._keyword_match_memories(query.query_text, all_memories)
            
            if not keyword_matched_memories:
                logger.info("ğŸ” å…³é”®è¯åŒ¹é…æœªæ‰¾åˆ°ç›¸å…³è®°å¿†ï¼Œè¿”å›ç©ºç»“æœã€‚")
                return []
            
            logger.info(f"ğŸ” ç¬¬äºŒå±‚ï¼šå…³é”®è¯åŒ¹é…ï¼ŒåŒ¹é…è®°å¿†æ•°é‡: {len(keyword_matched_memories)}")
            
            # 3. è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®— (ç²¾ç»†æ’åº)
            if keyword_matched_memories:
                logger.info(f"ğŸ” ç¬¬ä¸‰å±‚ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¯¹ {len(keyword_matched_memories)} æ¡è®°å¿†è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—...")
                scored_memories = self._score_memories_by_relevance(query.query_text, keyword_matched_memories)
                
                # 4. æ’åºå’Œæˆªæ–­
                sorted_memories = sorted(scored_memories, key=lambda m: m.relevance_score, reverse=True)
                final_memories = sorted_memories[:query.max_results]
                
                # 5. é™çº§æœºåˆ¶ï¼šå¦‚æœç»“æœä¸ºç©ºï¼Œé™ä½é˜ˆå€¼é‡è¯•
                if not final_memories:
                    logger.warning("ğŸ” è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—åæ— ç»“æœï¼Œå°è¯•é™çº§æ£€ç´¢...")
                    final_memories = self._fallback_retrieval(query, keyword_matched_memories)
                
                logger.info(f"âœ… ä¸‰å±‚æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_memories)} æ¡è®°å¿†ã€‚")
                return final_memories
            else:
                logger.info("ğŸ” å…³é”®è¯åŒ¹é…æœªæ‰¾åˆ°ç›¸å…³è®°å¿†ï¼Œè·³è¿‡è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ã€‚")
                return []
            
        except Exception as e:
            logger.error(f"ä¸‰å±‚è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
            raise MemoryRetrievalError(f"ä¸‰å±‚è®°å¿†æ£€ç´¢å¤±è´¥: {e}") from e
    
    def _get_filtered_memories(self, query: MemoryQuery) -> List[MemoryChunk]:
        """
        è·å–åŸºç¡€è¿‡æ»¤åçš„è®°å¿†åˆ—è¡¨ï¼ˆç¬¬ä¸€å±‚ï¼šæ—¶é—´è¡°å‡ç­›é€‰ï¼‰
        
        Args:
            query: è®°å¿†æŸ¥è¯¢å¯¹è±¡
            
        Returns:
            List[MemoryChunk]: åŸºç¡€è¿‡æ»¤åçš„è®°å¿†åˆ—è¡¨
        """
        try:
            cursor = self.conn.cursor()
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            conditions = ["session_id = ?"]
            params = [query.session_id]
            
            # æ—¶é—´çª—å£è¿‡æ»¤
            time_window_hours = self.config_manager.get_time_window_hours()
            if time_window_hours > 0:
                time_threshold = datetime.now() - timedelta(hours=time_window_hours)
                conditions.append("created_at >= ?")
                params.append(time_threshold.isoformat())
            
            if query.content_types:
                placeholders = ','.join(['?' for _ in query.content_types])
                conditions.append(f"content_type IN ({placeholders})")
                params.extend(query.content_types)
            
            if query.time_range:
                if 'start' in query.time_range:
                    conditions.append("created_at >= ?")
                    params.append(query.time_range['start'].isoformat())
                if 'end' in query.time_range:
                    conditions.append("created_at <= ?")
                    params.append(query.time_range['end'].isoformat())
            
            # æ‰§è¡ŒæŸ¥è¯¢
            where_clause = " AND ".join(conditions)
            cursor.execute(f"""
                SELECT * FROM memory_chunks 
                WHERE {where_clause}
                ORDER BY created_at DESC
            """, params)
            
            rows = cursor.fetchall()
            memories = []
            
            for row in rows:
                memory = MemoryChunk(
                    chunk_id=row['chunk_id'],
                    session_id=row['session_id'],
                    content=row['content'],
                    content_type=row['content_type'],
                    relevance_score=row['relevance_score'],
                    importance_score=row['importance_score'],
                    created_at=datetime.fromisoformat(row['created_at']),
                    metadata=json.loads(row['metadata']) if row['metadata'] and row['metadata'].strip() else {},
                    vector_embedding=json.loads(row['vector_embedding']) if row['vector_embedding'] else None
                )
                memories.append(memory)
            
            return memories
            
        except Exception as e:
            logger.error(f"åŸºç¡€è®°å¿†è¿‡æ»¤å¤±è´¥: {e}")
            raise MemoryRetrievalError(f"åŸºç¡€è®°å¿†è¿‡æ»¤å¤±è´¥: {e}") from e
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        ä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–å…³é”®è¯ - æ”¯æŒjiebaåˆ†è¯
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        try:
            if not text or not text.strip():
                return []
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨jiebaåˆ†è¯
            if JIEBA_AVAILABLE and self.config_manager.is_jieba_enabled():
                return self._extract_keywords_with_jieba(text)
            else:
                return self._extract_keywords_simple(text)
                
        except Exception as e:
            logger.warning(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    def _extract_keywords_with_jieba(self, text: str) -> List[str]:
        """
        ä½¿ç”¨jiebaåˆ†è¯æå–å…³é”®è¯
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨jiebaåˆ†è¯å’Œè¯æ€§æ ‡æ³¨
            words = pseg.cut(text)
            
            # åœç”¨è¯é›†åˆ
            stop_words = {
                'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 
                'å®ƒ', 'ä»–', 'å¥¹', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 
                'å—', 'å‘¢', 'å§', 'å•Š', 'å‘€', 'å“¦', 'å—¯', 'å°±', 'éƒ½', 'ä¹Ÿ', 
                'è¿˜', 'è¦', 'ä¼š', 'èƒ½', 'å¯ä»¥', 'åº”è¯¥', 'å¯èƒ½', 'å·²ç»', 'ä¸€ç›´'
            }
            
            # ä¿ç•™çš„è¯æ€§
            keep_pos = {'n', 'v', 'a', 'nr', 'ns', 'nt', 'nw', 'nz', 'vn', 'an'}
            
            keywords = []
            for word, pos in words:
                word = word.strip()
                if (len(word) >= 2 and 
                    word not in stop_words and 
                    pos in keep_pos):
                    keywords.append(word)
            
            logger.debug(f"jiebaåˆ†è¯ç»“æœ: {keywords}")
            return keywords
            
        except Exception as e:
            logger.warning(f"jiebaåˆ†è¯å¤±è´¥: {e}")
            return self._extract_keywords_simple(text)
    
    def _extract_keywords_simple(self, text: str) -> List[str]:
        """
        ç®€å•å…³é”®è¯æå–ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        try:
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—æ¯å’Œæ•°å­—
            cleaned_text = re.sub(r'[^\u4e00-\u9fff\w\s]', ' ', text)
            
            # æŒ‰ç©ºæ ¼åˆ†å‰²
            words = cleaned_text.split()
            
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            stop_words = {
                'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 
                'å®ƒ', 'ä»–', 'å¥¹', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 
                'å—', 'å‘¢', 'å§', 'å•Š', 'å‘€', 'å“¦', 'å—¯', 'å°±', 'éƒ½', 'ä¹Ÿ', 
                'è¿˜', 'è¦', 'ä¼š', 'èƒ½', 'å¯ä»¥', 'åº”è¯¥', 'å¯èƒ½', 'å·²ç»', 'ä¸€ç›´'
            }
            
            keywords = []
            for word in words:
                word = word.strip()
                if len(word) >= 2 and word not in stop_words:
                    keywords.append(word)
            
            logger.debug(f"ç®€å•åˆ†è¯ç»“æœ: {keywords}")
            return keywords
            
        except Exception as e:
            logger.warning(f"ç®€å•å…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    def _keyword_match_memories(self, query_text: str, memories: List[MemoryChunk]) -> List[MemoryChunk]:
        """
        å…³é”®è¯åŒ¹é…è®°å¿†ï¼ˆç¬¬äºŒå±‚ï¼šå…³é”®è¯åŒ¹é…ï¼‰
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            memories: å€™é€‰è®°å¿†åˆ—è¡¨
            
        Returns:
            List[MemoryChunk]: å…³é”®è¯åŒ¹é…çš„è®°å¿†åˆ—è¡¨
        """
        try:
            if not query_text or not memories:
                return []
            
            query_keywords = set(self._extract_keywords(query_text))
            if not query_keywords:
                logger.warning("æŸ¥è¯¢æ–‡æœ¬æ— æ³•æå–å…³é”®è¯")
                return []
            
            matched_memories = []
            keyword_threshold = self.config_manager.get_keyword_threshold()
            
            for memory in memories:
                memory_keywords = set(self._extract_keywords(memory.content))
                if not memory_keywords:
                    continue
                
                # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                intersection = len(query_keywords & memory_keywords)
                union = len(query_keywords | memory_keywords)
                
                if union > 0:
                    jaccard_similarity = intersection / union
                    
                    # åº”ç”¨æ—¶é—´è¡°å‡
                    time_score = self._calculate_time_decay_score(memory, datetime.now())
                    
                    # ç»¼åˆè¯„åˆ†
                    final_score = jaccard_similarity * 0.7 + time_score * 0.3
                    
                    if final_score >= keyword_threshold:
                        # æ›´æ–°è®°å¿†çš„ç›¸å…³æ€§åˆ†æ•°
                        memory.relevance_score = final_score
                        matched_memories.append(memory)
                        
                        logger.debug(f"å…³é”®è¯åŒ¹é…: {memory.chunk_id}, ç›¸ä¼¼åº¦: {jaccard_similarity:.3f}, æ—¶é—´åˆ†æ•°: {time_score:.3f}, ç»¼åˆåˆ†æ•°: {final_score:.3f}")
            
            logger.info(f"å…³é”®è¯åŒ¹é…å®Œæˆ: {len(matched_memories)}/{len(memories)} æ¡è®°å¿†é€šè¿‡åŒ¹é…")
            return matched_memories
            
        except Exception as e:
            logger.error(f"å…³é”®è¯åŒ¹é…å¤±è´¥: {e}")
            return []
    
    def _calculate_time_decay_score(self, memory: MemoryChunk, current_time: datetime) -> float:
        """
        è®¡ç®—æ—¶é—´è¡°å‡åˆ†æ•°
        
        Args:
            memory: è®°å¿†å¯¹è±¡
            current_time: å½“å‰æ—¶é—´
            
        Returns:
            float: æ—¶é—´è¡°å‡åˆ†æ•°
        """
        try:
            time_diff = (current_time - memory.created_at).total_seconds() / 3600  # è½¬æ¢ä¸ºå°æ—¶
            decay_factor = self.config_manager.get_time_decay_factor()
            return math.exp(-decay_factor * time_diff)
        except Exception as e:
            logger.warning(f"æ—¶é—´è¡°å‡è®¡ç®—å¤±è´¥: {e}")
            return 1.0  # é»˜è®¤åˆ†æ•°
    
    def _score_memories_by_relevance(self, query_text: str, memories: List[MemoryChunk]) -> List[MemoryChunk]:
        """
        åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—è®°å¿†ç›¸å…³æ€§åˆ†æ•°ï¼ˆç¬¬ä¸‰å±‚ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            memories: å€™é€‰è®°å¿†åˆ—è¡¨
            
        Returns:
            List[MemoryChunk]: å¸¦ç›¸å…³æ€§åˆ†æ•°çš„è®°å¿†åˆ—è¡¨
        """
        try:
            if not query_text or not memories:
                return []
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨TF-IDF
            if JIEBA_AVAILABLE and self.config_manager.is_tfidf_enabled():
                return self._score_memories_with_tfidf(query_text, memories)
            else:
                return self._score_memories_simple(query_text, memories)
                
        except Exception as e:
            logger.error(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return memories  # è¿”å›åŸå§‹è®°å¿†åˆ—è¡¨
    
    def _score_memories_with_tfidf(self, query_text: str, memories: List[MemoryChunk]) -> List[MemoryChunk]:
        """
        ä½¿ç”¨TF-IDFè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            memories: å€™é€‰è®°å¿†åˆ—è¡¨
            
        Returns:
            List[MemoryChunk]: å¸¦ç›¸å…³æ€§åˆ†æ•°çš„è®°å¿†åˆ—è¡¨
        """
        try:
            # å‡†å¤‡æ–‡æ¡£é›†åˆ
            documents = [query_text]
            memory_contents = []
            
            for memory in memories:
                memory_contents.append(memory.content)
                documents.append(memory.content)
            
            # åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
            tfidf_vectorizer = TfidfVectorizer(
                tokenizer=self._extract_keywords,
                lowercase=False,
                max_features=1000,
                min_df=1,
                max_df=0.95
            )
            
            # è®¡ç®—TF-IDFçŸ©é˜µ
            tfidf_matrix = tfidf_vectorizer.fit_transform(documents)
            
            # è®¡ç®—æŸ¥è¯¢ä¸æ¯ä¸ªè®°å¿†çš„ä½™å¼¦ç›¸ä¼¼åº¦
            query_vector = tfidf_matrix[0:1]
            memory_vectors = tfidf_matrix[1:]
            
            similarities = cosine_similarity(query_vector, memory_vectors)[0]
            
            # æ›´æ–°è®°å¿†çš„ç›¸å…³æ€§åˆ†æ•°
            semantic_threshold = self.config_manager.get_semantic_threshold()
            
            for i, memory in enumerate(memories):
                semantic_score = similarities[i]
                
                # åº”ç”¨æ—¶é—´è¡°å‡
                time_score = self._calculate_time_decay_score(memory, datetime.now())
                
                # ç»¼åˆè¯„åˆ†ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ + æ—¶é—´è¡°å‡
                final_score = semantic_score * 0.6 + time_score * 0.4
                
                # æ›´æ–°è®°å¿†çš„ç›¸å…³æ€§åˆ†æ•°
                memory.relevance_score = final_score
                
                logger.debug(f"è¯­ä¹‰ç›¸ä¼¼åº¦: {memory.chunk_id}, è¯­ä¹‰åˆ†æ•°: {semantic_score:.3f}, æ—¶é—´åˆ†æ•°: {time_score:.3f}, ç»¼åˆåˆ†æ•°: {final_score:.3f}")
            
            # è¿‡æ»¤ä½åˆ†è®°å¿†
            filtered_memories = [m for m in memories if m.relevance_score >= semantic_threshold]
            
            logger.info(f"TF-IDFè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ: {len(filtered_memories)}/{len(memories)} æ¡è®°å¿†é€šè¿‡è¯­ä¹‰åŒ¹é…")
            return filtered_memories
            
        except Exception as e:
            logger.warning(f"TF-IDFè®¡ç®—å¤±è´¥: {e}")
            return self._score_memories_simple(query_text, memories)
    
    def _score_memories_simple(self, query_text: str, memories: List[MemoryChunk]) -> List[MemoryChunk]:
        """
        ç®€å•è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            memories: å€™é€‰è®°å¿†åˆ—è¡¨
            
        Returns:
            List[MemoryChunk]: å¸¦ç›¸å…³æ€§åˆ†æ•°çš„è®°å¿†åˆ—è¡¨
        """
        try:
            query_keywords = set(self._extract_keywords(query_text))
            if not query_keywords:
                return memories
            
            for memory in memories:
                memory_keywords = set(self._extract_keywords(memory.content))
                if not memory_keywords:
                    continue
                
                # è®¡ç®—å…³é”®è¯é‡å åº¦
                intersection = len(query_keywords & memory_keywords)
                union = len(query_keywords | memory_keywords)
                
                if union > 0:
                    keyword_similarity = intersection / union
                    
                    # åº”ç”¨æ—¶é—´è¡°å‡
                    time_score = self._calculate_time_decay_score(memory, datetime.now())
                    
                    # ç»¼åˆè¯„åˆ†
                    final_score = keyword_similarity * 0.7 + time_score * 0.3
                    memory.relevance_score = final_score
                    
                    logger.debug(f"ç®€å•è¯­ä¹‰è®¡ç®—: {memory.chunk_id}, å…³é”®è¯ç›¸ä¼¼åº¦: {keyword_similarity:.3f}, æ—¶é—´åˆ†æ•°: {time_score:.3f}, ç»¼åˆåˆ†æ•°: {final_score:.3f}")
            
            logger.info(f"ç®€å•è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ: {len(memories)} æ¡è®°å¿†")
            return memories
            
        except Exception as e:
            logger.error(f"ç®€å•è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return memories
    
    def _fallback_retrieval(self, query: MemoryQuery, keyword_matched_memories: List[MemoryChunk]) -> List[MemoryChunk]:
        """
        é™çº§æ£€ç´¢æœºåˆ¶ï¼šå½“æ­£å¸¸æ£€ç´¢æ— ç»“æœæ—¶ï¼Œä½¿ç”¨æ›´å®½æ¾çš„æ¡ä»¶
        
        Args:
            query: è®°å¿†æŸ¥è¯¢å¯¹è±¡
            keyword_matched_memories: å…³é”®è¯åŒ¹é…çš„è®°å¿†åˆ—è¡¨
            
        Returns:
            List[MemoryChunk]: é™çº§æ£€ç´¢çš„è®°å¿†åˆ—è¡¨
        """
        try:
            if not keyword_matched_memories:
                return []
            
            logger.info("ğŸ” æ‰§è¡Œé™çº§æ£€ç´¢ï¼šä½¿ç”¨æ›´å®½æ¾çš„è¯­ä¹‰é˜ˆå€¼")
            
            # ä½¿ç”¨æ›´å®½æ¾çš„è¯­ä¹‰é˜ˆå€¼
            original_threshold = self.config_manager.get_semantic_threshold()
            fallback_threshold = min(0.1, original_threshold * 0.5)  # é™ä½åˆ°åŸé˜ˆå€¼çš„50%æˆ–0.1
            
            # ä¸´æ—¶æ›´æ–°é˜ˆå€¼
            self.config_manager.update_config('retrieval.semantic_threshold', fallback_threshold)
            
            try:
                # é‡æ–°è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                scored_memories = self._score_memories_by_relevance(query.query_text, keyword_matched_memories)
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œç›´æ¥è¿”å›æŒ‰æ—¶é—´æ’åºçš„è®°å¿†
                if not scored_memories:
                    logger.warning("ğŸ” é™çº§æ£€ç´¢ä»æ— ç»“æœï¼Œè¿”å›æŒ‰æ—¶é—´æ’åºçš„è®°å¿†")
                    # æŒ‰æ—¶é—´æ’åºï¼Œè¿”å›æœ€è¿‘çš„è®°å¿†
                    sorted_memories = sorted(keyword_matched_memories, key=lambda m: m.created_at, reverse=True)
                    return sorted_memories[:query.max_results]
                
                # æ’åºå’Œæˆªæ–­
                sorted_memories = sorted(scored_memories, key=lambda m: m.relevance_score, reverse=True)
                return sorted_memories[:query.max_results]
                
            finally:
                # æ¢å¤åŸå§‹é˜ˆå€¼
                self.config_manager.update_config('retrieval.semantic_threshold', original_threshold)
                
        except Exception as e:
            logger.error(f"é™çº§æ£€ç´¢å¤±è´¥: {e}")
            # æœ€åçš„é™çº§ï¼šè¿”å›æŒ‰æ—¶é—´æ’åºçš„è®°å¿†
            sorted_memories = sorted(keyword_matched_memories, key=lambda m: m.created_at, reverse=True)
            return sorted_memories[:query.max_results]
    
    def get_session_memories(self, session_id: str, max_results: int = 100) -> List[MemoryChunk]:
        """
        è·å–ä¼šè¯çš„æ‰€æœ‰è®°å¿†ï¼ˆä¸è¿›è¡Œè¯­ä¹‰æœç´¢ï¼‰
        
        Args:
            session_id: ä¼šè¯ID
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            
        Returns:
            List[MemoryChunk]: è®°å¿†åˆ—è¡¨
            
        Raises:
            SessionNotFoundError: ä¼šè¯ä¸å­˜åœ¨
            MemoryRetrievalError: è·å–è®°å¿†å¤±è´¥
        """
        try:
            # éªŒè¯ä¼šè¯å­˜åœ¨
            self.get_session(session_id)
            
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT * FROM memory_chunks 
                WHERE session_id = ?
                ORDER BY created_at DESC
                LIMIT ?
            """, (session_id, max_results))
            
            rows = cursor.fetchall()
            memories = []
            
            for row in rows:
                memory = MemoryChunk(
                    chunk_id=row['chunk_id'],
                    session_id=row['session_id'],
                    content=row['content'],
                    content_type=row['content_type'],
                    relevance_score=row['relevance_score'],
                    importance_score=row['importance_score'],
                    created_at=datetime.fromisoformat(row['created_at']),
                    metadata=json.loads(row['metadata']) if row['metadata'] and row['metadata'].strip() else {},
                    vector_embedding=row['vector_embedding']
                )
                memories.append(memory)
            
            logger.info(f"è·å–ä¼šè¯ {session_id} çš„è®°å¿†: {len(memories)} æ¡")
            return memories
            
        except SessionNotFoundError:
            raise
        except Exception as e:
            logger.error(f"è·å–ä¼šè¯è®°å¿†å¤±è´¥: {e}")
            raise MemoryRetrievalError(f"è·å–ä¼šè¯è®°å¿†å¤±è´¥: {e}") from e
    
    def compress_session_memories(self, session_id: str, 
                                 request: CompressionRequest = None) -> Tuple[List[MemoryChunk], Dict[str, Any]]:
        """
        å‹ç¼©ä¼šè¯è®°å¿†
        
        Args:
            session_id: ä¼šè¯ID
            request: å‹ç¼©è¯·æ±‚ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Tuple[List[MemoryChunk], Dict[str, Any]]: å‹ç¼©åçš„è®°å¿†åˆ—è¡¨å’Œå‹ç¼©ä¿¡æ¯
        """
        try:
            # éªŒè¯ä¼šè¯å­˜åœ¨
            self.get_session(session_id)
            
            # è·å–ä¼šè¯çš„æ‰€æœ‰è®°å¿†
            query = MemoryQuery(session_id=session_id, max_results=1000)
            memories = self.retrieve_memories(query)
            
            if not memories:
                logger.info(f"ä¼šè¯ {session_id} æ²¡æœ‰è®°å¿†éœ€è¦å‹ç¼©")
                return [], {}
            
            # åˆ›å»ºå‹ç¼©è¯·æ±‚
            if not request:
                request = CompressionRequest(
                    session_id=session_id,
                    strategy=self.config_manager.get_config_value('compression.strategy', 'semantic'),
                    threshold=self.config_manager.get_config_value('compression.threshold', 20),
                    max_ratio=self.config_manager.get_config_value('compression.max_compression_ratio', 0.3)
                )
            
            # æ‰§è¡Œå‹ç¼©
            compressed_memories, compression_record = self.compression_engine.compress_memories(memories, request)
            
            # å¦‚æœå‹ç¼©æˆåŠŸï¼Œæ›´æ–°æ•°æ®åº“
            if len(compressed_memories) < len(memories):
                self._update_session_memories(session_id, compressed_memories, compression_record)
            
            # æ„å»ºå‹ç¼©ä¿¡æ¯
            compression_info = {
                'original_count': len(memories),
                'compressed_count': len(compressed_memories),
                'compression_ratio': compression_record.compression_ratio,
                'strategy': compression_record.strategy,
                'compression_time': compression_record.created_at.isoformat()
            }
            
            logger.info(f"ä¼šè¯è®°å¿†å‹ç¼©å®Œæˆ: {session_id}, å‹ç¼©æ¯”ä¾‹: {compression_record.compression_ratio:.2f}")
            return compressed_memories, compression_info
            
        except Exception as e:
            logger.error(f"å‹ç¼©ä¼šè¯è®°å¿†å¤±è´¥: {e}")
            raise MemoryError(f"å‹ç¼©ä¼šè¯è®°å¿†å¤±è´¥: {e}") from e
    
    def _update_session_memories(self, session_id: str, compressed_memories: List[MemoryChunk], 
                                compression_record) -> None:
        """
        æ›´æ–°ä¼šè¯è®°å¿†ï¼ˆæ›¿æ¢ä¸ºå‹ç¼©åçš„è®°å¿†ï¼‰
        
        Args:
            session_id: ä¼šè¯ID
            compressed_memories: å‹ç¼©åçš„è®°å¿†åˆ—è¡¨
            compression_record: å‹ç¼©è®°å½•
        """
        try:
            cursor = self.conn.cursor()
            
            # åˆ é™¤åŸæœ‰è®°å¿†
            cursor.execute("DELETE FROM memory_chunks WHERE session_id = ?", (session_id,))
            
            # æ’å…¥å‹ç¼©åçš„è®°å¿†
            for memory in compressed_memories:
                cursor.execute("""
                    INSERT INTO memory_chunks 
                    (chunk_id, session_id, content, content_type, relevance_score, 
                     importance_score, created_at, metadata, vector_embedding)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    memory.chunk_id,
                    memory.session_id,
                    memory.content,
                    memory.content_type,
                    memory.relevance_score,
                    memory.importance_score,
                    memory.created_at.isoformat(),
                    json.dumps(memory.metadata),
                    json.dumps(memory.vector_embedding) if memory.vector_embedding else None
                ))
            
            # ä¿å­˜å‹ç¼©è®°å½•
            cursor.execute("""
                INSERT INTO compression_records 
                (record_id, session_id, original_count, compressed_count, 
                 compression_ratio, strategy, created_at, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                compression_record.record_id,
                compression_record.session_id,
                compression_record.original_count,
                compression_record.compressed_count,
                compression_record.compression_ratio,
                compression_record.strategy,
                compression_record.created_at.isoformat(),
                json.dumps(compression_record.metadata)
            ))
            
            # æ›´æ–°ä¼šè¯è®°å¿†æ•°é‡
            cursor.execute("""
                UPDATE conversation_sessions 
                SET memory_count = ?, updated_at = ?
                WHERE session_id = ?
            """, (len(compressed_memories), datetime.now().isoformat(), session_id))
            
            self.conn.commit()
            
        except Exception as e:
            logger.error(f"æ›´æ–°ä¼šè¯è®°å¿†å¤±è´¥: {e}")
            raise MemoryError(f"æ›´æ–°ä¼šè¯è®°å¿†å¤±è´¥: {e}") from e
    
    def _check_session_limit(self, user_id: str) -> bool:
        """
        æ£€æŸ¥ç”¨æˆ·ä¼šè¯æ•°é‡é™åˆ¶
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            bool: æ˜¯å¦åœ¨é™åˆ¶èŒƒå›´å†…
        """
        try:
            max_sessions = self.session_config.get('max_sessions_per_user', 100)
            
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT COUNT(*) FROM conversation_sessions 
                WHERE user_id = ? AND status = 'active'
            """, (user_id,))
            
            current_count = cursor.fetchone()[0]
            return current_count < max_sessions
            
        except Exception as e:
            logger.warning(f"æ£€æŸ¥ä¼šè¯é™åˆ¶å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶å…è®¸åˆ›å»ºä¼šè¯
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """
        è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            cursor = self.conn.cursor()
            
            # è·å–ä¼šè¯ç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) FROM conversation_sessions WHERE status = 'active'")
            active_sessions = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM memory_chunks")
            total_memories = cursor.fetchone()[0]
            
            # è®¡ç®—å¹³å‡è®°å¿†æ•°
            avg_memories = total_memories / active_sessions if active_sessions > 0 else 0
            
            stats = self.memory_stats.copy()
            stats.update({
                'active_sessions': active_sessions,
                'total_memories': total_memories,
                'avg_memories_per_session': avg_memories
            })
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–è®°å¿†ç»Ÿè®¡å¤±è´¥: {e}")
            return self.memory_stats.copy()
    
    def close(self) -> None:
        """
        å…³é—­æ•°æ®åº“è¿æ¥
        """
        try:
            if hasattr(self, 'conn'):
                self.conn.close()
                logger.info("è®°å¿†æ•°æ®åº“è¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
