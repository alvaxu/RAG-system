"""
ç»Ÿä¸€æœåŠ¡æ¥å£æ¨¡å—

RAGç³»ç»Ÿçš„ç»Ÿä¸€æœåŠ¡æ¥å£ï¼Œä¸ºæ‰€æœ‰æŸ¥è¯¢ç±»å‹æä¾›ç»Ÿä¸€çš„æœåŠ¡è°ƒç”¨
ä¸¥æ ¼æŒ‰ç…§33.V3_RAGæŸ¥è¯¢å¤„ç†æ¨¡å—è¯¦ç»†è®¾è®¡æ–‡æ¡£å®ç°
"""

import logging
from typing import Dict, List, Any, Optional
import time

from .config_integration import ConfigIntegration
from .retrieval import RetrievalEngine
from .reranking_enhanced import MultiModelReranker
from .llm_caller import LLMCaller
from .context_manager import ContextChunk
from .exceptions import (
    ServiceInitializationError,
    RetrievalError,
    RerankingError,
    LLMServiceError,
    ContentProcessingError,
    ConfigurationError
)

logger = logging.getLogger(__name__)


class UnifiedServices:
    """ç»Ÿä¸€æœåŠ¡æ¥å£ - æ‰€æœ‰æŸ¥è¯¢ç±»å‹å¤ç”¨"""
    
    def __init__(self, config_integration: ConfigIntegration):
        """
        åˆå§‹åŒ–ç»Ÿä¸€æœåŠ¡æ¥å£
        
        :param config_integration: é…ç½®é›†æˆç®¡ç†å™¨å®ä¾‹
        """
        self.config = config_integration
        
        try:
            # åˆå§‹åŒ–å„ä¸ªæœåŠ¡
            # æ­£ç¡®åˆå§‹åŒ–å‘é‡æ•°æ®åº“é›†æˆç®¡ç†å™¨
            from .vector_db_integration import VectorDBIntegration
            self.vector_db_integration = VectorDBIntegration(config_integration)
            
            self.retrieval_service = RetrievalEngine(config_integration, self.vector_db_integration)
            self.reranking_service = MultiModelReranker(config_integration)
            self.llm_service = LLMCaller(config_integration)
            
            logger.info("ç»Ÿä¸€æœåŠ¡æ¥å£åˆå§‹åŒ–å®Œæˆ")
            
        except (ServiceInitializationError, ConfigurationError) as e:
            logger.error(f"ç»Ÿä¸€æœåŠ¡æ¥å£åˆå§‹åŒ–å¤±è´¥: {e}")
            raise ServiceInitializationError(f"ç»Ÿä¸€æœåŠ¡æ¥å£åˆå§‹åŒ–å¤±è´¥: {e}") from e
        except Exception as e:
            logger.error(f"ç»Ÿä¸€æœåŠ¡æ¥å£åˆå§‹åŒ–å¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰: {e}")
            raise ServiceInitializationError(f"ç»Ÿä¸€æœåŠ¡æ¥å£åˆå§‹åŒ–å¤±è´¥: {e}") from e
    
    async def retrieve(self, query: str, content_types: List[str] = None, 
                      options: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        ç»Ÿä¸€æ£€ç´¢æœåŠ¡ - æ”¯æŒå¤šç±»å‹å†…å®¹æ£€ç´¢
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param content_types: å†…å®¹ç±»å‹åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰ç±»å‹
        :param options: æ£€ç´¢é€‰é¡¹
        :return: æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            if content_types is None:
                content_types = ['text', 'image', 'table']
            
            if options is None:
                options = {}
            
            max_results = options.get('max_results', 10)
            relevance_threshold = options.get('relevance_threshold', 0.5)
            
            # æ ¹æ®å†…å®¹ç±»å‹è¿›è¡Œæ£€ç´¢
            all_results = []
            
            if 'text' in content_types:
                text_results = self.retrieval_service.retrieve_texts(query, max_results, relevance_threshold)
                all_results.extend(text_results)
            
            if 'image' in content_types:
                image_results = self.retrieval_service.retrieve_images(query, max_results, relevance_threshold)
                all_results.extend(image_results)
            
            if 'table' in content_types:
                table_results = self.retrieval_service.retrieve_tables(query, max_results, relevance_threshold)
                all_results.extend(table_results)
            
            logger.info(f"ç»Ÿä¸€æ£€ç´¢å®Œæˆï¼ŒæŸ¥è¯¢: {query}ï¼Œè¿”å›ç»“æœ: {len(all_results)}")
            return all_results
            
        except RetrievalError as e:
            logger.error(f"ç»Ÿä¸€æ£€ç´¢å¤±è´¥: {e}")
            return []
        except Exception as e:
            logger.error(f"ç»Ÿä¸€æ£€ç´¢å¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰: {e}")
            return []
    
    async def rerank(self, query: str, results: List[Any]) -> List[Dict[str, Any]]:
        """
        é‡æ’åºæœåŠ¡ - å®Œå…¨å¤ç”¨
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param results: å¾…é‡æ’åºçš„ç»“æœåˆ—è¡¨
        :return: é‡æ’åºåçš„ç»“æœ
        """
        try:
            if not results:
                return []
            
            logger.info(f"å¼€å§‹ç»Ÿä¸€é‡æ’åºï¼Œå¤„ç† {len(results)} ä¸ªç»“æœ")
            
            # è°ƒç”¨é‡æ’åºæœåŠ¡
            reranked_results = self.reranking_service.rerank(query, results)
            
            logger.info(f"ç»Ÿä¸€é‡æ’åºå®Œæˆ")
            return reranked_results
            
        except RerankingError as e:
            logger.error(f"ç»Ÿä¸€é‡æ’åºå¤±è´¥: {e}")
            # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œè¿”å›åŸå§‹æ’åº
            return self._fallback_sort(results)
        except Exception as e:
            logger.error(f"ç»Ÿä¸€é‡æ’åºå¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰: {e}")
            # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œè¿”å›åŸå§‹æ’åº
            return self._fallback_sort(results)
    
    async def generate_answer(self, query: str, results: List[Any], context_memories: List[Dict[str, Any]] = None) -> str:
        """
        LLMæœåŠ¡ - å¤ç”¨+é€‚é…ï¼Œæ”¯æŒå†å²è®°å¿†ä¸Šä¸‹æ–‡
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param results: æ£€ç´¢ç»“æœåˆ—è¡¨
        :param context_memories: å†å²è®°å¿†ä¸Šä¸‹æ–‡
        :return: ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        try:
            if not results:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
            
            logger.info("å¼€å§‹ç”ŸæˆLLMç­”æ¡ˆ")
            
            # æ„å»ºç»Ÿä¸€ä¸Šä¸‹æ–‡
            context_chunks = self._build_unified_context(results)
            
            # æ£€æŸ¥å†å²è®°å¿†é›†æˆé…ç½®
            context_integration_config = self.config.get('rag_system.memory_module.context_integration', {})
            memory_enabled = context_integration_config.get('enabled', True)
            
            # å¦‚æœæœ‰å†å²è®°å¿†ä¸”é…ç½®å¯ç”¨ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
            if context_memories and memory_enabled:
                logger.info(f"ğŸ§  UnifiedServicesæ”¶åˆ°å†å²è®°å¿†:")
                logger.info(f"  - æ•°é‡: {len(context_memories)}")
                logger.info(f"  - å†…å®¹é¢„è§ˆ:")
                for i, memory in enumerate(context_memories[:3]):
                    logger.info(f"    {i+1}. {memory.get('content', '')[:50]}...")
                logger.info(f"ğŸ”§ æ·»åŠ  {len(context_memories)} æ¡å†å²è®°å¿†åˆ°ä¸Šä¸‹æ–‡")
                memory_context = self._build_memory_context(context_memories, context_integration_config)
                logger.info(f"ğŸ“Š æ„å»ºçš„memory_contextæ•°é‡: {len(memory_context)}")
                context_chunks.extend(memory_context)
                logger.info(f"âœ… åˆå¹¶åcontext_chunksæ€»æ•°: {len(context_chunks)}")
            elif context_memories and not memory_enabled:
                logger.info("â­ï¸ å†å²è®°å¿†é›†æˆå·²ç¦ç”¨ï¼Œè·³è¿‡è®°å¿†ä¸Šä¸‹æ–‡")
            else:
                logger.info("âŒ UnifiedServices: æ²¡æœ‰æ”¶åˆ°å†å²è®°å¿†")
            
            # è°ƒè¯•ï¼šæŸ¥çœ‹ä¼ é€’ç»™LLMçš„å®Œæ•´ä¸Šä¸‹æ–‡
            logger.info("ğŸ” ä¼ é€’ç»™LLMçš„å®Œæ•´ä¸Šä¸‹æ–‡:")
            for i, chunk in enumerate(context_chunks):
                logger.info(f"  - ä¸Šä¸‹æ–‡{i+1}: ç±»å‹={chunk.content_type}, æ¥æº={chunk.source}, å†…å®¹={chunk.content[:200]}...")
            
            # è°ƒç”¨LLMæœåŠ¡ï¼Œä¼ é€’ContextChunkåˆ—è¡¨
            llm_response = self.llm_service.generate_answer(query, context_chunks)
            
            logger.info(f"LLMç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(llm_response.answer)} å­—ç¬¦")
            return llm_response.answer
            
        except LLMServiceError as e:
            logger.error(f"LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_fallback_answer(query, results)
        except Exception as e:
            logger.error(f"LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰: {e}")
            return self._generate_fallback_answer(query, results)
    
    def _build_unified_context(self, results: List[Any]) -> List[ContextChunk]:
        """
        æ„å»ºç»Ÿä¸€ä¸Šä¸‹æ–‡
        
        :param results: ç»“æœåˆ—è¡¨
        :return: ContextChunkå¯¹è±¡åˆ—è¡¨
        """
        try:
            if not results:
                return []
            
            # è·å–é…ç½®çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            max_context_length = self.config.get_rag_config('query_processing.max_context_length', 4000)
            
            # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€ç›¸å…³çš„å†…å®¹
            sorted_results = sorted(results, key=lambda x: x.get('similarity_score', 0.0), reverse=True)
            
            context_chunks = []
            current_length = 0
            
            for result in sorted_results:
                # æå–å†…å®¹
                content = self._extract_content(result)
                if not content:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé•¿åº¦é™åˆ¶
                if current_length + len(content) > max_context_length:
                    # æˆªæ–­å†…å®¹ä»¥é€‚åº”é•¿åº¦é™åˆ¶
                    remaining_length = max_context_length - current_length
                    if remaining_length > 100:  # è‡³å°‘ä¿ç•™100å­—ç¬¦
                        content = content[:remaining_length] + "..."
                    else:
                        break
                
                # åˆ›å»ºContextChunkå¯¹è±¡
                context_chunk = self._dict_to_context_chunk(result, content)
                context_chunks.append(context_chunk)
                current_length += len(content)
                
                # å¦‚æœå·²ç»è¾¾åˆ°ç›®æ ‡é•¿åº¦ï¼Œåœæ­¢æ·»åŠ 
                if current_length >= max_context_length:
                    break
            
            logger.info(f"ç»Ÿä¸€ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼ŒContextChunkæ•°é‡: {len(context_chunks)}")
            
            return context_chunks
            
        except Exception as e:
            logger.error(f"æ„å»ºç»Ÿä¸€ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            # è¿”å›å‰å‡ ä¸ªç»“æœçš„ContextChunkå¯¹è±¡
            fallback_chunks = []
            for r in results[:3]:
                content = self._extract_content(r)
                if content:
                    chunk = self._dict_to_context_chunk(r, content)
                    fallback_chunks.append(chunk)
            return fallback_chunks
    
    def _build_unified_prompt(self, query: str, context: str) -> str:
        """
        æ„å»ºç»Ÿä¸€Prompt
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        :return: å®Œæ•´çš„Prompt
        """
        try:
            # è·å–ç³»ç»Ÿæç¤ºè¯
            system_prompt = self.config.get_rag_config('models.llm.system_prompt', 
                'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤ŸåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆå‡†ç¡®ã€ç›¸å…³ã€å®Œæ•´çš„ç­”æ¡ˆã€‚')
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            prompt = f"""
{system_prompt}

åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š
{query}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ç­”æ¡ˆï¼š
"""
            return prompt.strip()
            
        except Exception as e:
            logger.error(f"æ„å»ºç»Ÿä¸€Promptå¤±è´¥: {e}")
            # è¿”å›ç®€å•çš„æç¤ºè¯
            return f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n\nä¸Šä¸‹æ–‡ï¼š{context}\n\né—®é¢˜ï¼š{query}"
    
    def _dict_to_context_chunk(self, result: Dict[str, Any], content: str) -> ContextChunk:
        """
        å®‰å…¨åœ°å°†å­—å…¸è½¬æ¢ä¸ºContextChunkå¯¹è±¡
        
        :param result: åŸå§‹ç»“æœå­—å…¸
        :param content: æå–çš„å†…å®¹
        :return: ContextChunkå¯¹è±¡
        """
        try:
            return ContextChunk(
                content=content,
                chunk_id=result.get('chunk_id', ''),
                content_type=result.get('chunk_type', 'text'),
                relevance_score=float(result.get('similarity_score', 0.0)),
                source=result.get('document_name', ''),
                metadata=result
            )
        except Exception as e:
            logger.warning(f"è½¬æ¢ContextChunkå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return ContextChunk(
                content=content,
                chunk_id='',
                content_type='text',
                relevance_score=0.0,
                source='',
                metadata={}
            )
    
    def _extract_content(self, result: Any) -> str:
        """
        ä»ç»“æœä¸­æå–å†…å®¹
        
        :param result: æ£€ç´¢ç»“æœ
        :return: æå–çš„å†…å®¹
        """
        try:
            # å°è¯•ä¸åŒçš„å†…å®¹å­—æ®µ
            if hasattr(result, 'content') and result.content:
                return str(result.content)
            elif hasattr(result, 'text') and result.text:
                return str(result.text)
            elif hasattr(result, 'page_content') and result.page_content:
                return str(result.page_content)
            elif hasattr(result, 'description') and result.description:
                return str(result.description)
            elif isinstance(result, dict):
                # å¦‚æœæ˜¯å­—å…¸ç±»å‹
                if 'content' in result:
                    return str(result['content'])
                elif 'text' in result:
                    return str(result['text'])
                elif 'page_content' in result:
                    return str(result['page_content'])
                elif 'description' in result:
                    return str(result['description'])
            else:
                return str(result)
        except Exception:
            return ""
    
    def _fallback_sort(self, results: List[Any]) -> List[Dict[str, Any]]:
        """
        é‡æ’åºå¤±è´¥æ—¶çš„å›é€€ç­–ç•¥
        
        :param results: åŸå§‹ç»“æœåˆ—è¡¨
        :return: æ’åºåçš„ç»“æœ
        """
        try:
            # æŒ‰åˆ†æ•°æ’åº
            sorted_results = sorted(results, key=lambda x: x.get('score', 0.0), reverse=True)
            return sorted_results
        except Exception as e:
            logger.error(f"å›é€€æ’åºå¤±è´¥: {e}")
            return results
    
    def _generate_fallback_answer(self, query: str, results: List[Any]) -> str:
        """
        LLMæœåŠ¡å¤±è´¥æ—¶çš„å›é€€ç­–ç•¥
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param results: æ£€ç´¢ç»“æœåˆ—è¡¨
        :return: å›é€€ç­”æ¡ˆ
        """
        try:
            if not results:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯ã€‚"
            
            # ç®€å•çš„ç­”æ¡ˆç”Ÿæˆï¼Œä¸ä¾èµ–LLM
            top_results = results[:3]
            summary = f"æ‰¾åˆ°{len(results)}ä¸ªç›¸å…³ç»“æœï¼Œå…¶ä¸­ï¼š\n"
            
            for i, result in enumerate(top_results, 1):
                content_type = result.get('chunk_type', 'unknown')
                score = result.get('score', 0.0)
                summary += f"{i}. {content_type}ç±»å‹ç»“æœï¼ˆç›¸å…³æ€§ï¼š{score:.2f}ï¼‰\n"
            
            return summary
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›é€€ç­”æ¡ˆå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„æŸ¥è¯¢ï¼Œè¯·ç¨åé‡è¯•ã€‚"
    
    def get_service_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€ä¿¡æ¯"""
        return {
            'status': 'ready',
            'service_type': 'UnifiedServices',
            'services': {
                'retrieval_service': self.retrieval_service is not None,
                'reranking_service': self.reranking_service is not None,
                'llm_service': self.llm_service is not None
            },
            'features': [
                'unified_retrieval',
                'unified_reranking',
                'unified_llm',
                'context_building',
                'prompt_building',
                'fallback_strategies',
                'memory_context_integration'
            ]
        }
    
    def _build_memory_context(self, context_memories: List[Dict[str, Any]], config: Dict[str, Any] = None) -> List[ContextChunk]:
        """
        æ„å»ºå†å²è®°å¿†ä¸Šä¸‹æ–‡
        
        :param context_memories: å†å²è®°å¿†åˆ—è¡¨
        :param config: ä¸Šä¸‹æ–‡é›†æˆé…ç½®
        :return: ContextChunkå¯¹è±¡åˆ—è¡¨
        """
        try:
            if not config:
                config = {}
            
            # ä»é…ç½®ä¸­è·å–å‚æ•°
            max_memories = config.get('max_memories_in_prompt', 5)
            min_relevance = config.get('min_relevance_score', 0.1)
            max_length = config.get('max_memory_length', 1000)
            include_metadata = config.get('include_memory_metadata', True)
            
            logger.info(f"ğŸ”§ å¼€å§‹æ„å»ºå†å²è®°å¿†ä¸Šä¸‹æ–‡ï¼Œè¾“å…¥è®°å¿†æ•°é‡: {len(context_memories)}")
            logger.info(f"ğŸ”§ é…ç½®å‚æ•°: max_memories={max_memories}, min_relevance={min_relevance}, max_length={max_length}")
            
            if context_memories:
                logger.info(f"ğŸ”§ è¾“å…¥è®°å¿†å†…å®¹é¢„è§ˆ:")
                for i, memory in enumerate(context_memories[:2]):
                    logger.info(f"  - è®°å¿†{i+1}: {memory}")
            
            memory_chunks = []
            
            # æŒ‰ç›¸å…³æ€§æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_memories = sorted(context_memories, key=lambda x: x.get('relevance_score', 0.0), reverse=True)
            filtered_memories = []
            
            for memory in sorted_memories[:max_memories]:
                # æ£€æŸ¥ç›¸å…³æ€§é˜ˆå€¼
                relevance_score = memory.get('relevance_score', 0.0)
                if relevance_score >= min_relevance:
                    # æ£€æŸ¥é•¿åº¦é™åˆ¶
                    content = memory.get('content', '')
                    if len(content) <= max_length:
                        filtered_memories.append(memory)
                    else:
                        # æˆªæ–­è¿‡é•¿çš„è®°å¿†
                        memory['content'] = content[:max_length-3] + "..."
                        filtered_memories.append(memory)
            
            logger.info(f"ğŸ”§ è¿‡æ»¤åè®°å¿†æ•°é‡: {len(filtered_memories)} (åŸå§‹: {len(context_memories)})")
            
            for memory in filtered_memories:
                # æ„å»ºå…ƒæ•°æ®
                metadata = {
                    'importance_score': memory.get('importance_score', 0.0),
                    'created_at': memory.get('created_at', ''),
                    'memory_id': memory.get('chunk_id', '')
                }
                
                # å¦‚æœé…ç½®è¦æ±‚åŒ…å«æ›´å¤šå…ƒæ•°æ®
                if include_metadata:
                    metadata.update({
                        'relevance_score': memory.get('relevance_score', 0.0),
                        'content_type': memory.get('content_type', 'text'),
                        'user_query': memory.get('user_query', '')
                    })
                
                # åˆ›å»ºContextChunkå¯¹è±¡
                memory_chunk = ContextChunk(
                    content=memory['content'],
                    chunk_id=memory.get('chunk_id', ''),
                    content_type='memory',
                    relevance_score=memory.get('relevance_score', 0.0),
                    source='conversation_memory',
                    metadata=metadata
                )
                memory_chunks.append(memory_chunk)
            
            logger.info(f"æ„å»ºå†å²è®°å¿†ä¸Šä¸‹æ–‡å®Œæˆï¼ŒContextChunkæ•°é‡: {len(memory_chunks)}")
            return memory_chunks
            
        except Exception as e:
            logger.error(f"æ„å»ºå†å²è®°å¿†ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return []
