"""
å¢å¼ºé‡æ’åºæ¨¡å—æµ‹è¯•

æµ‹è¯•æ–°å®ç°çš„RerankingåŠŸèƒ½ï¼š
1. å¤šæ¨¡å‹é‡æ’åºæ”¯æŒ
2. æ··åˆæ’åºç­–ç•¥
3. æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜
4. é‡æ’åºè´¨é‡è¯„ä¼°
"""

import sys
import os
import logging
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.reranking_enhanced import MultiModelReranker, RerankCandidate, RerankResult

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

class MockConfigIntegration:
    """æ¨¡æ‹Ÿé…ç½®é›†æˆç®¡ç†å™¨"""
    
    def __init__(self):
        self.config = {
            'rag_system.models.reranking.dashscope.enabled': True,
            'rag_system.models.reranking.dashscope.model_name': 'gte-rerank-v2',
            'rag_system.models.reranking.dashscope.weight': 0.6,
            'rag_system.models.reranking.rule_based.enabled': True,
            'rag_system.models.reranking.rule_based.weight': 0.4,
            'rag_system.models.reranking.batch_size': 32,
            'rag_system.models.reranking.max_candidates': 100,
            'rag_system.models.reranking.cache.enabled': True
        }
    
    def get(self, key: str, default=None):
        """æ¨¡æ‹Ÿé…ç½®è·å–"""
        return self.config.get(key, default)
    
    def get_env_var(self, key: str):
        """æ¨¡æ‹Ÿç¯å¢ƒå˜é‡è·å–"""
        env_vars = {
            'DASHSCOPE_API_KEY': 'mock_key'
        }
        return env_vars.get(key)

def create_test_candidates():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„å€™é€‰ç»“æœ"""
    return [
        RerankCandidate(
            chunk_id="chunk_001",
            content="äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºå¼€å‘èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            content_type="text",
            original_score=0.9,
            metadata={"source": "AI_intro.txt", "quality_score": 0.95}
        ),
        RerankCandidate(
            chunk_id="chunk_002",
            content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚",
            content_type="text",
            original_score=0.8,
            metadata={"source": "ML_intro.txt", "quality_score": 0.88}
        ),
        RerankCandidate(
            chunk_id="chunk_003",
            content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚",
            content_type="text",
            original_score=0.7,
            metadata={"source": "DL_intro.txt", "quality_score": 0.82}
        ),
        RerankCandidate(
            chunk_id="chunk_004",
            content="è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦åº”ç”¨é¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
            content_type="text",
            original_score=0.6,
            metadata={"source": "NLP_intro.txt", "quality_score": 0.75}
        ),
        RerankCandidate(
            chunk_id="chunk_005",
            content="è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œè‡´åŠ›äºè®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œåˆ†æè§†è§‰ä¿¡æ¯ã€‚",
            content_type="text",
            original_score=0.5,
            metadata={"source": "CV_intro.txt", "quality_score": 0.70}
        )
    ]

def test_reranker_initialization():
    """æµ‹è¯•é‡æ’åºå™¨åˆå§‹åŒ–"""
    logger.info("æµ‹è¯•é‡æ’åºå™¨åˆå§‹åŒ–...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        mock_config = MockConfigIntegration()
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        reranker = MultiModelReranker(mock_config)
        
        # æ£€æŸ¥åŸºæœ¬é…ç½®
        logger.info(f"  é‡æ’åºå™¨ç±»å‹: {type(reranker)}")
        logger.info(f"  å¯ç”¨æ¨¡å‹: {reranker.models}")
        logger.info(f"  æ‰¹å¤„ç†å¤§å°: {reranker.batch_size}")
        logger.info(f"  æœ€å¤§å€™é€‰æ•°: {reranker.max_candidates}")
        logger.info(f"  ç¼“å­˜å¯ç”¨: {reranker.cache_enabled}")
        
        # æ£€æŸ¥æœåŠ¡çŠ¶æ€
        status = reranker.get_service_status()
        logger.info(f"  æœåŠ¡çŠ¶æ€: {status['status']}")
        
        if status['status'] == 'ready':
            logger.info("âœ… é‡æ’åºå™¨åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
            return True
        else:
            logger.error("âŒ é‡æ’åºå™¨çŠ¶æ€å¼‚å¸¸")
            return False
        
    except Exception as e:
        logger.error(f"âŒ é‡æ’åºå™¨åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_basic_reranking():
    """æµ‹è¯•åŸºç¡€é‡æ’åºåŠŸèƒ½"""
    logger.info("æµ‹è¯•åŸºç¡€é‡æ’åºåŠŸèƒ½...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        mock_config = MockConfigIntegration()
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        reranker = MultiModelReranker(mock_config)
        
        # åˆ›å»ºæµ‹è¯•å€™é€‰
        candidates = create_test_candidates()
        query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ï¼Ÿ"
        
        # æ‰§è¡Œé‡æ’åº
        start_time = time.time()
        reranked_results = reranker.rerank(query, candidates)
        processing_time = time.time() - start_time
        
        logger.info(f"  æŸ¥è¯¢: {query}")
        logger.info(f"  å€™é€‰æ•°é‡: {len(candidates)}")
        logger.info(f"  é‡æ’åºç»“æœæ•°é‡: {len(reranked_results)}")
        logger.info(f"  å¤„ç†æ—¶é—´: {processing_time:.3f}s")
        
        # éªŒè¯ç»“æœ
        if len(reranked_results) != len(candidates):
            logger.error("âŒ é‡æ’åºç»“æœæ•°é‡ä¸åŒ¹é…")
            return False
        
        # æ£€æŸ¥æ’åºç»“æœ
        for i, result in enumerate(reranked_results):
            logger.info(f"    æ’å{i+1}: {result.chunk_id}, åˆ†æ•°: {result.final_score:.3f}, ç½®ä¿¡åº¦: {result.confidence:.3f}")
            
            # éªŒè¯åŸºæœ¬å­—æ®µ
            if not result.chunk_id or result.final_score < 0 or result.confidence < 0:
                logger.error(f"âŒ é‡æ’åºç»“æœå­—æ®µæ— æ•ˆ: {result}")
                return False
        
        # æ£€æŸ¥æ˜¯å¦æŒ‰åˆ†æ•°é™åºæ’åˆ—
        scores = [result.final_score for result in reranked_results]
        if scores != sorted(scores, reverse=True):
            logger.error("âŒ é‡æ’åºç»“æœæœªæŒ‰åˆ†æ•°é™åºæ’åˆ—")
            return False
        
        logger.info("âœ… åŸºç¡€é‡æ’åºåŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ åŸºç¡€é‡æ’åºåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_cache_functionality():
    """æµ‹è¯•ç¼“å­˜åŠŸèƒ½"""
    logger.info("æµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        mock_config = MockConfigIntegration()
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        reranker = MultiModelReranker(mock_config)
        
        # åˆ›å»ºæµ‹è¯•å€™é€‰
        candidates = create_test_candidates()
        query = "æµ‹è¯•ç¼“å­˜åŠŸèƒ½"
        
        # ç¬¬ä¸€æ¬¡é‡æ’åºï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        start_time = time.time()
        results1 = reranker.rerank(query, candidates)
        time1 = time.time() - start_time
        
        # ç¬¬äºŒæ¬¡é‡æ’åºï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
        start_time = time.time()
        results2 = reranker.rerank(query, candidates)
        time2 = time.time() - start_time
        
        logger.info(f"  ç¬¬ä¸€æ¬¡é‡æ’åºæ—¶é—´: {time1:.3f}s")
        logger.info(f"  ç¬¬äºŒæ¬¡é‡æ’åºæ—¶é—´: {time2:.3f}s")
        logger.info(f"  ç¼“å­˜å‘½ä¸­ç‡: {reranker.performance_stats['cache_hits']}")
        logger.info(f"  ç¼“å­˜æœªå‘½ä¸­ç‡: {reranker.performance_stats['cache_misses']}")
        
        # éªŒè¯ç¼“å­˜æ•ˆæœ
        if time2 < time1:
            logger.info("âœ… ç¼“å­˜åŠ é€Ÿæ•ˆæœæ˜æ˜¾")
        else:
            logger.warning("âš ï¸ ç¼“å­˜åŠ é€Ÿæ•ˆæœä¸æ˜æ˜¾")
        
        # éªŒè¯ç»“æœä¸€è‡´æ€§
        if len(results1) != len(results2):
            logger.error("âŒ ç¼“å­˜å‰åç»“æœæ•°é‡ä¸ä¸€è‡´")
            return False
        
        # æ£€æŸ¥åˆ†æ•°ä¸€è‡´æ€§
        for r1, r2 in zip(results1, results2):
            if abs(r1.final_score - r2.final_score) > 0.001:
                logger.error(f"âŒ ç¼“å­˜å‰ååˆ†æ•°ä¸ä¸€è‡´: {r1.final_score} vs {r2.final_score}")
                return False
        
        logger.info("âœ… ç¼“å­˜åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_performance_optimization():
    """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½"""
    logger.info("æµ‹è¯•æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        mock_config = MockConfigIntegration()
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        reranker = MultiModelReranker(mock_config)
        
        # åˆ›å»ºå¤§é‡å€™é€‰è¿›è¡Œæ€§èƒ½æµ‹è¯•
        large_candidates = []
        for i in range(50):
            candidate = RerankCandidate(
                chunk_id=f"chunk_{i:03d}",
                content=f"è¿™æ˜¯ç¬¬{i+1}ä¸ªæµ‹è¯•å€™é€‰å†…å®¹ï¼Œç”¨äºæ€§èƒ½æµ‹è¯•ã€‚",
                content_type="text",
                original_score=0.9 - i * 0.01,
                metadata={"source": f"test_{i}.txt", "quality_score": 0.9 - i * 0.01}
            )
            large_candidates.append(candidate)
        
        query = "æ€§èƒ½æµ‹è¯•æŸ¥è¯¢"
        
        # æ‰§è¡Œé‡æ’åº
        start_time = time.time()
        reranked_results = reranker.rerank(query, large_candidates)
        processing_time = time.time() - start_time
        
        logger.info(f"  å€™é€‰æ•°é‡: {len(large_candidates)}")
        logger.info(f"  å¤„ç†æ—¶é—´: {processing_time:.3f}s")
        logger.info(f"  å¹³å‡å¤„ç†æ—¶é—´: {reranker.performance_stats['avg_time']:.3f}s")
        
        # éªŒè¯æ€§èƒ½
        if processing_time < 5.0:  # 50ä¸ªå€™é€‰åº”è¯¥åœ¨5ç§’å†…å®Œæˆ
            logger.info("âœ… æ€§èƒ½è¡¨ç°è‰¯å¥½")
        else:
            logger.warning("âš ï¸ æ€§èƒ½è¡¨ç°ä¸€èˆ¬")
        
        # éªŒè¯ç»“æœæ•°é‡é™åˆ¶
        if len(reranked_results) <= reranker.max_candidates:
            logger.info("âœ… å€™é€‰æ•°é‡é™åˆ¶ç”Ÿæ•ˆ")
        else:
            logger.error("âŒ å€™é€‰æ•°é‡é™åˆ¶æœªç”Ÿæ•ˆ")
        
        logger.info("âœ… æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_fallback_mechanism():
    """æµ‹è¯•å›é€€æœºåˆ¶"""
    logger.info("æµ‹è¯•å›é€€æœºåˆ¶...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®ï¼ˆç¦ç”¨æ‰€æœ‰æ¨¡å‹ï¼‰
        mock_config = MockConfigIntegration()
        mock_config.config['rag_system.models.reranking.dashscope.enabled'] = False
        mock_config.config['rag_system.models.reranking.rule_based.enabled'] = False
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        reranker = MultiModelReranker(mock_config)
        
        # åˆ›å»ºæµ‹è¯•å€™é€‰
        candidates = create_test_candidates()
        query = "æµ‹è¯•å›é€€æœºåˆ¶"
        
        # æ‰§è¡Œé‡æ’åºï¼ˆåº”è¯¥è§¦å‘å›é€€ï¼‰
        reranked_results = reranker.rerank(query, candidates)
        
        logger.info(f"  å›é€€ç»“æœæ•°é‡: {len(reranked_results)}")
        logger.info(f"  å¯ç”¨æ¨¡å‹: {reranker.get_service_status()['available_models']}")
        
        # éªŒè¯å›é€€ç»“æœ
        if len(reranked_results) == len(candidates):
            logger.info("âœ… å›é€€æœºåˆ¶æ­£å¸¸å·¥ä½œ")
            
            # æ£€æŸ¥å›é€€ç»“æœçš„åŸºæœ¬å±æ€§
            for result in reranked_results:
                if result.final_score == result.original_score:
                    logger.info("âœ… å›é€€ç»“æœä½¿ç”¨åŸå§‹åˆ†æ•°")
                else:
                    logger.warning("âš ï¸ å›é€€ç»“æœåˆ†æ•°è¢«ä¿®æ”¹")
            
            return True
        else:
            logger.error("âŒ å›é€€æœºåˆ¶å¼‚å¸¸")
            return False
        
    except Exception as e:
        logger.error(f"âŒ å›é€€æœºåˆ¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("å¢å¼ºé‡æ’åºæ¨¡å—æµ‹è¯•å¼€å§‹")
    logger.info("=" * 60)
    
    tests = [
        ("é‡æ’åºå™¨åˆå§‹åŒ–", test_reranker_initialization),
        ("åŸºç¡€é‡æ’åºåŠŸèƒ½", test_basic_reranking),
        ("ç¼“å­˜åŠŸèƒ½", test_cache_functionality),
        ("æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½", test_performance_optimization),
        ("å›é€€æœºåˆ¶", test_fallback_mechanism)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"\nğŸ“‹ è¿è¡Œæµ‹è¯•: {test_name}")
        try:
            if test_func():
                logger.info(f"âœ… {test_name} é€šè¿‡")
                passed += 1
            else:
                logger.error(f"âŒ {test_name} å¤±è´¥")
        except Exception as e:
            logger.error(f"ğŸ’¥ {test_name} æ‰§è¡Œå¼‚å¸¸: {e}")
    
    logger.info(f"\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    logger.info(f"   æ€»æµ‹è¯•æ•°: {total}")
    logger.info(f"   é€šè¿‡æ•°é‡: {passed}")
    logger.info(f"   å¤±è´¥æ•°é‡: {total - passed}")
    logger.info(f"   é€šè¿‡ç‡: {(passed/total)*100:.1f}%")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼ºé‡æ’åºæ¨¡å—å®ç°æˆåŠŸ")
        return 0
    else:
        logger.error("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼éœ€è¦æ£€æŸ¥å®ç°")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
