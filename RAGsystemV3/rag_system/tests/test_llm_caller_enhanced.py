"""
LLMè°ƒç”¨å™¨å¢å¼ºåŠŸèƒ½æµ‹è¯•æ¨¡å—

æµ‹è¯•æ–°å®ç°çš„LLMè°ƒç”¨å™¨åŠŸèƒ½ï¼š
1. æç¤ºè¯ç®¡ç†å™¨ï¼ˆPromptManagerï¼‰
2. ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆContextManagerï¼‰
3. Tokenç»Ÿè®¡åŠŸèƒ½
4. è¿”å›æ•°æ®ç»“æ„ä¼˜åŒ–
"""

import sys
import os
import logging
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.llm_caller import LLMCaller, LLMResponse
from core.prompt_manager import PromptManager, PromptTemplate
from core.context_manager import ContextManager, ContextChunk
from core.config_integration import ConfigIntegration

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

class MockConfigIntegration:
    """æ¨¡æ‹Ÿé…ç½®é›†æˆç®¡ç†å™¨"""
    
    def __init__(self):
        self.config = {
            'rag_system.models.llm.model_name': 'qwen-turbo',
            'rag_system.models.llm.max_tokens': 2048,
            'rag_system.models.llm.temperature': 0.7,
            'rag_system.query_processing.max_context_length': 4000,
            'rag_system.query_processing.max_chunks': 10,
            'rag_system.query_processing.relevance_threshold': 0.7
        }
    
    def get(self, key: str, default=None):
        """æ¨¡æ‹Ÿé…ç½®è·å–"""
        return self.config.get(key, default)
    
    def get_rag_config(self, key: str, default=None):
        """æ¨¡æ‹ŸRAGé…ç½®è·å–"""
        config_key = f'rag_system.{key}'
        return self.config.get(config_key, default)
    
    def get_env_var(self, key: str):
        """æ¨¡æ‹Ÿç¯å¢ƒå˜é‡è·å–"""
        env_vars = {
            'DASHSCOPE_API_KEY': 'mock_key'
        }
        return env_vars.get(key)

def create_test_context_chunks():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„ä¸Šä¸‹æ–‡å—"""
    return [
        ContextChunk(
            content="äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºå¼€å‘èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            chunk_id="chunk_001",
            content_type="text",
            relevance_score=0.9,
            source="AI_intro.txt",
            metadata={"category": "definition", "language": "zh"}
        ),
        ContextChunk(
            content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚",
            chunk_id="chunk_002",
            content_type="text",
            relevance_score=0.8,
            source="ML_intro.txt",
            metadata={"category": "subfield", "language": "zh"}
        ),
        ContextChunk(
            content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚",
            chunk_id="chunk_003",
            content_type="text",
            relevance_score=0.7,
            source="DL_intro.txt",
            metadata={"category": "technique", "language": "zh"}
        ),
        ContextChunk(
            content="è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦åº”ç”¨é¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
            chunk_id="chunk_004",
            content_type="text",
            relevance_score=0.6,
            source="NLP_intro.txt",
            metadata={"category": "application", "language": "zh"}
        )
    ]

def test_prompt_manager():
    """æµ‹è¯•æç¤ºè¯ç®¡ç†å™¨"""
    logger.info("æµ‹è¯•æç¤ºè¯ç®¡ç†å™¨...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        mock_config = MockConfigIntegration()
        
        # åˆå§‹åŒ–æç¤ºè¯ç®¡ç†å™¨
        prompt_manager = PromptManager(mock_config)
        
        # æµ‹è¯•1: è·å–é»˜è®¤æ¨¡æ¿
        templates = prompt_manager.list_templates()
        logger.info(f"  é»˜è®¤æ¨¡æ¿æ•°é‡: {len(templates)}")
        
        if len(templates) < 3:
            logger.error("  é”™è¯¯: é»˜è®¤æ¨¡æ¿æ•°é‡ä¸è¶³")
            return False
        
        # æµ‹è¯•2: ç”Ÿæˆæç¤ºè¯
        test_params = {
            'context': 'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯...',
            'query': 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ'
        }
        
        prompt = prompt_manager.generate_prompt('rag_qa', test_params)
        logger.info(f"  ç”Ÿæˆçš„æç¤ºè¯é•¿åº¦: {len(prompt)}")
        
        if not prompt or len(prompt) < 50:
            logger.error("  é”™è¯¯: ç”Ÿæˆçš„æç¤ºè¯æ— æ•ˆ")
            return False
        
        # æµ‹è¯•3: æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿
        custom_template = PromptTemplate(
            name='custom_qa',
            template="åŸºäºä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\nä¸Šä¸‹æ–‡ï¼š{context}\né—®é¢˜ï¼š{query}\nç­”æ¡ˆï¼š",
            description='è‡ªå®šä¹‰é—®ç­”æ¨¡æ¿',
            category='custom',
            version='1.0.0',
            parameters=['context', 'query'],
            examples=[],
            created_at=time.time(),
            updated_at=time.time()
        )
        
        success = prompt_manager.add_template(custom_template)
        if not success:
            logger.error("  é”™è¯¯: æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿å¤±è´¥")
            return False
        
        # æµ‹è¯•4: æœç´¢æ¨¡æ¿
        search_results = prompt_manager.search_templates('åˆ†æ')
        logger.info(f"  æœç´¢'åˆ†æ'çš„ç»“æœæ•°é‡: {len(search_results)}")
        
        # æµ‹è¯•5: è·å–ä½¿ç”¨ç»Ÿè®¡
        usage_stats = prompt_manager.get_usage_stats()
        logger.info(f"  ä½¿ç”¨ç»Ÿè®¡: {usage_stats}")
        
        logger.info("âœ… æç¤ºè¯ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æç¤ºè¯ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_context_manager():
    """æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    logger.info("æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        mock_config = MockConfigIntegration()
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        context_manager = ContextManager(mock_config)
        
        # åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡å—
        context_chunks = create_test_context_chunks()
        
        # æµ‹è¯•1: ä¸Šä¸‹æ–‡ä¼˜åŒ–
        query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ï¼Ÿ"
        optimized_context = context_manager.optimize_context(context_chunks, query)
        
        logger.info(f"  åŸå§‹ä¸Šä¸‹æ–‡å—æ•°é‡: {len(context_chunks)}")
        logger.info(f"  ä¼˜åŒ–åä¸Šä¸‹æ–‡é•¿åº¦: {len(optimized_context)}")
        
        if not optimized_context:
            logger.error("  é”™è¯¯: ä¸Šä¸‹æ–‡ä¼˜åŒ–å¤±è´¥")
            return False
        
        # æµ‹è¯•2: é•¿åº¦é™åˆ¶
        short_context = context_manager.optimize_context(context_chunks, query, max_length=100)
        logger.info(f"  é™åˆ¶é•¿åº¦åçš„ä¸Šä¸‹æ–‡é•¿åº¦: {len(short_context)}")
        
        if len(short_context) > 100:
            logger.error("  é”™è¯¯: é•¿åº¦é™åˆ¶æœªç”Ÿæ•ˆ")
            return False
        
        # æµ‹è¯•3: ç›¸å…³æ€§æ’åº
        # ä¿®æ”¹ä¸€äº›å—çš„åˆ†æ•°
        context_chunks[0].relevance_score = 0.5
        context_chunks[1].relevance_score = 0.9
        
        sorted_chunks = context_manager._sort_by_relevance(context_chunks, query)
        logger.info(f"  æ’åºåæœ€é«˜ç›¸å…³æ€§: {sorted_chunks[0].relevance_score:.3f}")
        
        if sorted_chunks[0].relevance_score != 0.9:
            logger.error("  é”™è¯¯: ç›¸å…³æ€§æ’åºå¤±è´¥")
            return False
        
        # æµ‹è¯•4: è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = context_manager.get_context_stats()
        logger.info(f"  ä¸Šä¸‹æ–‡å¤„ç†ç»Ÿè®¡: {stats}")
        
        logger.info("âœ… ä¸Šä¸‹æ–‡ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä¸Šä¸‹æ–‡ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_llm_caller_enhanced():
    """æµ‹è¯•å¢å¼ºçš„LLMè°ƒç”¨å™¨"""
    logger.info("æµ‹è¯•å¢å¼ºçš„LLMè°ƒç”¨å™¨...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        mock_config = MockConfigIntegration()
        
        # åˆå§‹åŒ–LLMè°ƒç”¨å™¨
        llm_caller = LLMCaller(mock_config)
        
        # åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡å—
        context_chunks = create_test_context_chunks()
        
        # æµ‹è¯•1: ç”Ÿæˆç­”æ¡ˆï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰
        query = "è¯·è§£é‡Šäººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å…³ç³»"
        response = llm_caller.generate_answer(query, context_chunks, 'rag_qa')
        
        logger.info(f"  å“åº”ç±»å‹: {type(response)}")
        logger.info(f"  å“åº”æˆåŠŸ: {response.success}")
        logger.info(f"  ç­”æ¡ˆé•¿åº¦: {len(response.answer)}")
        logger.info(f"  å¤„ç†æ—¶é—´: {response.processing_time:.3f}s")
        
        if not isinstance(response, LLMResponse):
            logger.error("  é”™è¯¯: è¿”å›ç±»å‹ä¸æ˜¯LLMResponse")
            return False
        
        if not response.success:
            logger.error("  é”™è¯¯: å“åº”ç”Ÿæˆå¤±è´¥")
            return False
        
        # æµ‹è¯•2: Tokenç»Ÿè®¡
        logger.info(f"  Prompt Tokens: {response.prompt_tokens}")
        logger.info(f"  Completion Tokens: {response.completion_tokens}")
        logger.info(f"  Total Tokens: {response.total_tokens}")
        
        if response.total_tokens <= 0:
            logger.error("  é”™è¯¯: Tokenç»Ÿè®¡æ— æ•ˆ")
            return False
        
        # æµ‹è¯•3: å…ƒæ•°æ®
        metadata = response.metadata
        logger.info(f"  å…ƒæ•°æ®: {metadata}")
        
        if not metadata or 'context_chunks_count' not in metadata:
            logger.error("  é”™è¯¯: å…ƒæ•°æ®ä¸å®Œæ•´")
            return False
        
        # æµ‹è¯•4: è·å–Tokenç»Ÿè®¡
        token_stats = llm_caller.get_token_stats()
        logger.info(f"  ç´¯è®¡Tokenç»Ÿè®¡: {token_stats}")
        
        if token_stats['total_tokens'] <= 0:
            logger.error("  é”™è¯¯: ç´¯è®¡Tokenç»Ÿè®¡æ— æ•ˆ")
            return False
        
        # æµ‹è¯•5: æœåŠ¡çŠ¶æ€
        service_status = llm_caller.get_service_status()
        logger.info(f"  æœåŠ¡çŠ¶æ€: {service_status}")
        
        if service_status['status'] != 'ready':
            logger.error("  é”™è¯¯: æœåŠ¡çŠ¶æ€å¼‚å¸¸")
            return False
        
        logger.info("âœ… å¢å¼ºçš„LLMè°ƒç”¨å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¢å¼ºçš„LLMè°ƒç”¨å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_integration():
    """æµ‹è¯•ç»„ä»¶é›†æˆ"""
    logger.info("æµ‹è¯•ç»„ä»¶é›†æˆ...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        mock_config = MockConfigIntegration()
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        prompt_manager = PromptManager(mock_config)
        context_manager = ContextManager(mock_config)
        llm_caller = LLMCaller(mock_config)
        
        # æµ‹è¯•1: ç»„ä»¶çŠ¶æ€
        prompt_status = prompt_manager.get_service_status()
        context_status = context_manager.get_service_status()
        llm_status = llm_caller.get_service_status()
        
        logger.info(f"  æç¤ºè¯ç®¡ç†å™¨çŠ¶æ€: {prompt_status['status']}")
        logger.info(f"  ä¸Šä¸‹æ–‡ç®¡ç†å™¨çŠ¶æ€: {context_status['status']}")
        logger.info(f"  LLMè°ƒç”¨å™¨çŠ¶æ€: {llm_status['status']}")
        
        if not all(status['status'] == 'ready' for status in [prompt_status, context_status, llm_status]):
            logger.error("  é”™è¯¯: ç»„ä»¶çŠ¶æ€å¼‚å¸¸")
            return False
        
        # æµ‹è¯•2: å®Œæ•´æµç¨‹
        context_chunks = create_test_context_chunks()
        query = "æ€»ç»“äººå·¥æ™ºèƒ½çš„ä¸»è¦æŠ€æœ¯"
        
        # ä½¿ç”¨ä¸åŒçš„æç¤ºè¯æ¨¡æ¿
        templates = ['rag_qa', 'rag_summary', 'rag_analysis']
        
        for template in templates:
            response = llm_caller.generate_answer(query, context_chunks, template)
            logger.info(f"  æ¨¡æ¿ {template} å“åº”æˆåŠŸ: {response.success}")
            
            if not response.success:
                logger.error(f"  é”™è¯¯: æ¨¡æ¿ {template} å“åº”å¤±è´¥")
                return False
        
        logger.info("âœ… ç»„ä»¶é›†æˆæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç»„ä»¶é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("LLMè°ƒç”¨å™¨å¢å¼ºåŠŸèƒ½æµ‹è¯•å¼€å§‹")
    logger.info("=" * 60)
    
    tests = [
        ("æç¤ºè¯ç®¡ç†å™¨", test_prompt_manager),
        ("ä¸Šä¸‹æ–‡ç®¡ç†å™¨", test_context_manager),
        ("å¢å¼ºçš„LLMè°ƒç”¨å™¨", test_llm_caller_enhanced),
        ("ç»„ä»¶é›†æˆ", test_integration)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"\nğŸ“‹ è¿è¡Œæµ‹è¯•: {test_name}")
        try:
            if test_func():
                logger.info(f"âœ… {test_name} é€šè¿‡")
                passed += 1
            else:
                logger.error(f"âŒ {test_name} å¤±è´¥")
        except Exception as e:
            logger.error(f"ğŸ’¥ {test_name} æ‰§è¡Œå¼‚å¸¸: {e}")
    
    logger.info(f"\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    logger.info(f"   æ€»æµ‹è¯•æ•°: {total}")
    logger.info(f"   é€šè¿‡æ•°é‡: {passed}")
    logger.info(f"   å¤±è´¥æ•°é‡: {total - passed}")
    logger.info(f"   é€šè¿‡ç‡: {(passed/total)*100:.1f}%")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMè°ƒç”¨å™¨å¢å¼ºåŠŸèƒ½å®ç°æˆåŠŸ")
        return 0
    else:
        logger.error("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼éœ€è¦æ£€æŸ¥å®ç°")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
