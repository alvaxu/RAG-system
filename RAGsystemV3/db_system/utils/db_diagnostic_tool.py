#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
V3ç‰ˆæœ¬æ•°æ®åº“è¯Šæ–­å·¥å…·

åŠŸèƒ½ï¼š
1. åˆ†æå‘é‡æ•°æ®åº“ç»“æ„å’Œå†…å®¹
2. æ£€æŸ¥æ–‡æ¡£ç±»å‹åˆ†å¸ƒ
3. åˆ†æå…ƒæ•°æ®å­—æ®µ
4. è¾“å‡ºè¯Šæ–­æŠ¥å‘Š
5. æ”¯æŒä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
6. ğŸ”¢ å‘é‡æ•°æ®æ·±åº¦åˆ†æï¼ˆèŒƒæ•°åˆ†å¸ƒã€ç»´åº¦éªŒè¯ï¼‰
7. ğŸ”— å‘é‡ç›¸ä¼¼åº¦åˆ†æï¼ˆè´¨é‡è¯„ä¼°ã€å¼‚å¸¸æ£€æµ‹ï¼‰
8. ğŸ“‹ æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Šï¼ˆ100åˆ†åˆ¶è¯„åˆ†ï¼‰
9. ğŸ“‹ è¯¦ç»†æ–‡æ¡£åˆ—è¡¨æ˜¾ç¤ºï¼ˆå¯é€‰äº¤äº’åŠŸèƒ½ï¼‰
'''

import os
import json
import logging
import pickle
import numpy as np
from typing import Dict, List, Any, Optional
from pathlib import Path
from collections import defaultdict

from core.vector_store_manager import LangChainVectorStoreManager
from config.config_manager import ConfigManager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class DatabaseDiagnosticTool:
    """æ•°æ®åº“è¯Šæ–­å·¥å…·"""
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–è¯Šæ–­å·¥å…·"""
        try:
            if config_path:
                self.config_manager = ConfigManager(config_path)
            else:
                self.config_manager = ConfigManager()
            
            # åŠ è½½é…ç½®
            if not self.config_manager.load_config():
                logging.warning("é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                # å¦‚æœé…ç½®åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„
                if not self.config_manager.is_loaded:
                    logging.info("å°è¯•ä½¿ç”¨é»˜è®¤é…ç½®è·¯å¾„é‡æ–°åŠ è½½...")
                    default_config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "config", "v3_config.json")
                    if os.path.exists(default_config_path):
                        self.config_manager = ConfigManager(default_config_path)
                        if not self.config_manager.load_config():
                            logging.error("é»˜è®¤é…ç½®è·¯å¾„åŠ è½½ä¹Ÿå¤±è´¥")
            
            self.vector_store_manager = LangChainVectorStoreManager(self.config_manager)
            logging.info("æ•°æ®åº“è¯Šæ–­å·¥å…·åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def run_diagnostic(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®åº“è¯Šæ–­"""
        print("ğŸ” V3ç‰ˆæœ¬æ•°æ®åº“è¯Šæ–­å·¥å…·å¯åŠ¨")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½å‘é‡æ•°æ®åº“
            print("ğŸ“š åŠ è½½å‘é‡æ•°æ®åº“...")
            if not self.vector_store_manager.load():
                print("âŒ æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“")
                return {'success': False, 'error': 'æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“'}
            
            # 2. è·å–æ•°æ®åº“åŸºæœ¬ä¿¡æ¯
            print("ğŸ“Š è·å–æ•°æ®åº“åŸºæœ¬ä¿¡æ¯...")
            basic_info = self._get_basic_info()
            
            # 3. åˆ†ææ–‡æ¡£ç»“æ„
            print("ğŸ” åˆ†ææ–‡æ¡£ç»“æ„...")
            structure_info = self._analyze_document_structure()
            
            # 4. åˆ†æå…ƒæ•°æ®
            print("ğŸ“‹ åˆ†æå…ƒæ•°æ®...")
            metadata_info = self._analyze_metadata()
            
            # 5. æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£
            print("ğŸ“· æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£...")
            image_info = self._check_image_docs()
            
            # 6. æ£€æŸ¥è¡¨æ ¼æ–‡æ¡£
            print("ğŸ“Š æ£€æŸ¥è¡¨æ ¼æ–‡æ¡£...")
            table_info = self._check_table_docs()
            
            # 7. æ£€æŸ¥æ–‡æœ¬æ–‡æ¡£
            print("ğŸ“ æ£€æŸ¥æ–‡æœ¬æ–‡æ¡£...")
            text_info = self._check_text_docs()

            # 8. åˆ†æå‘é‡æ•°æ®
            print("ğŸ”¢ åˆ†æå‘é‡æ•°æ®...")
            vector_info = self._analyze_vector_data(self.vector_store_manager.vector_store)
            
            # 8.1 æŒ‰vector_typeåˆ†æå‘é‡æ•°æ®
            print("ğŸ”¢ æŒ‰å‘é‡ç±»å‹åˆ†æå‘é‡æ•°æ®...")
            vector_type_info = self._analyze_vector_data_by_type(self.vector_store_manager.vector_store)

            # 9. ç”Ÿæˆæ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š
            print("ğŸ“‹ ç”Ÿæˆæ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š...")
            quality_report = self._generate_quality_report(structure_info, image_info, table_info, vector_info)

            # 10. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
            print("ğŸ“‹ ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š...")
            diagnostic_report = {
                'success': True,
                'basic_info': basic_info,
                'structure_info': structure_info,
                'metadata_info': metadata_info,
                'image_info': image_info,
                'table_info': table_info,
                'text_info': text_info,
                'vector_info': vector_info,
                'vector_type_info': vector_type_info,
                'quality_report': quality_report
            }

            # 11. æ˜¾ç¤ºè¯¦ç»†æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            show_detailed = input("\næ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„æ–‡æ¡£åˆ—è¡¨? (y/n): ").strip().lower()
            if show_detailed == 'y':
                self._show_detailed_document_list(structure_info)

            # 12. è¯¢é—®æ˜¯å¦ä¿å­˜ç»“æœ
            save_choice = input("\næ˜¯å¦ä¿å­˜è¯Šæ–­ç»“æœåˆ°æ–‡ä»¶? (y/n): ").strip().lower()
            if save_choice == 'y':
                output_file = "v3_db_diagnostic_results.json"
                self._save_results(diagnostic_report, output_file)
                print(f"ğŸ’¾ è¯Šæ–­ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            print("\nâœ… æ•°æ®åº“è¯Šæ–­å®Œæˆï¼")
            return diagnostic_report
            
        except Exception as e:
            logging.error(f"æ•°æ®åº“è¯Šæ–­å¤±è´¥: {e}")
            print(f"âŒ æ•°æ®åº“è¯Šæ–­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}
    
    def _get_basic_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“åŸºæœ¬ä¿¡æ¯"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            total_docs = len(docstore)
            
            # è·å–å‘é‡æ•°æ®åº“è·¯å¾„ - ä¿®å¤é…ç½®åŠ è½½é—®é¢˜
            vector_db_path = None
            if self.config_manager.is_loaded:
                vector_db_path = self.config_manager.get_path('vector_db_dir')
            else:
                # å¦‚æœé…ç½®æœªåŠ è½½ï¼Œå°è¯•é‡æ–°åŠ è½½
                logging.info("é…ç½®æœªåŠ è½½ï¼Œå°è¯•é‡æ–°åŠ è½½...")
                if self.config_manager.load_config():
                    vector_db_path = self.config_manager.get_path('vector_db_dir')
                else:
                    # ä½¿ç”¨é»˜è®¤è·¯å¾„
                    vector_db_path = "./central/vector_db"
                    logging.warning("é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„")
            
            basic_info = {
                'total_docs': total_docs,  # ä¿®å¤å­—æ®µåä¸ä¸€è‡´é—®é¢˜
                'vector_db_path': vector_db_path,
                'vector_db_exists': os.path.exists(vector_db_path) if vector_db_path else False
            }
            
            # æ£€æŸ¥å‘é‡æ•°æ®åº“æ–‡ä»¶
            if vector_db_path and os.path.exists(vector_db_path):
                files = list(Path(vector_db_path).glob('*'))
                basic_info['vector_db_files'] = [
                    {
                        'name': f.name,
                        'size': f.stat().st_size if f.is_file() else 0,
                        'type': 'file' if f.is_file() else 'directory'
                    }
                    for f in files
                ]
            
            print(f"ğŸ“š æ€»æ–‡æ¡£æ•°: {total_docs}")
            print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
            print(f"âœ… å‘é‡æ•°æ®åº“å­˜åœ¨: {basic_info['vector_db_exists']}")
            
            return basic_info
            
        except Exception as e:
            logging.error(f"è·å–åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_document_structure(self) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£ç»“æ„"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            
            # ç»Ÿè®¡æ–‡æ¡£ç±»å‹åˆ†å¸ƒ
            chunk_types = defaultdict(int)
            vector_types = defaultdict(int)
            document_names = set()
            all_fields = set()
            
            for doc_id, doc in docstore.items():
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                
                # ç»Ÿè®¡åˆ†å—ç±»å‹
                chunk_type = metadata.get('chunk_type', 'unknown')
                chunk_types[chunk_type] += 1
                
                # ç»Ÿè®¡å‘é‡ç±»å‹
                vector_type = metadata.get('vector_type', 'unknown')
                vector_types[vector_type] += 1
                
                # ç›´æ¥è·å–æ–‡æ¡£åç§°
                doc_name = metadata.get('document_name', 'unknown')
                document_names.add(doc_name)
                
                # æ”¶é›†æ‰€æœ‰å­—æ®µ
                all_fields.update(metadata.keys())
            
            structure_info = {
                'chunk_type_distribution': dict(chunk_types),
                'vector_type_distribution': dict(vector_types),
                'unique_document_names': list(document_names),
                'total_unique_documents': len(document_names),
                'all_metadata_fields': list(all_fields),
                'total_metadata_fields': len(all_fields)
            }
            
            print(f"\nğŸ“Š åˆ†å—ç±»å‹åˆ†å¸ƒ:")
            for chunk_type, count in sorted(chunk_types.items()):
                print(f"   {chunk_type}: {count}")
            
            print(f"\nğŸ”¢ å‘é‡ç±»å‹åˆ†å¸ƒ:")
            for vector_type, count in sorted(vector_types.items()):
                print(f"   {vector_type}: {count}")
            
            print(f"\nğŸ“š æ–‡æ¡£ç»Ÿè®¡:")
            print(f"  æ€»æ–‡æ¡£æ•°: {len(docstore)}")
            print(f"  å”¯ä¸€æ–‡æ¡£å: {len(document_names)}")
            print(f"  å…·ä½“æ–‡æ¡£å: {list(document_names)}")
            print(f"  å…ƒæ•°æ®å­—æ®µæ•°: {len(all_fields)}")
            
            return structure_info
            
        except Exception as e:
            logging.error(f"åˆ†ææ–‡æ¡£ç»“æ„å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_metadata(self) -> Dict[str, Any]:
        """åˆ†æå…ƒæ•°æ®ç»“æ„"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            
            # åˆ†æå­—æ®µç±»å‹å’Œç¤ºä¾‹
            field_types = defaultdict(set)
            field_examples = defaultdict(list)
            chunk_type_fields = defaultdict(set)
            
            for doc_id, doc in docstore.items():
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                chunk_type = metadata.get('chunk_type', 'unknown')
                
                for field, value in metadata.items():
                    field_types[field].add(type(value).__name__)
                    chunk_type_fields[chunk_type].add(field)
                    
                    # ä¿å­˜å‰3ä¸ªç¤ºä¾‹
                    if len(field_examples[field]) < 3:
                        field_examples[field].append({
                            'chunk_type': chunk_type,
                            'value': str(value)[:100] + '...' if len(str(value)) > 100 else str(value)
                        })
            
            metadata_info = {
                'field_types': {k: list(v) for k, v in field_types.items()},
                'chunk_type_fields': {k: list(v) for k, v in chunk_type_fields.items()},
                'field_examples': {k: v for k, v in field_examples.items()}
            }
            
            print(f"\nğŸ” å­—æ®µç±»å‹åˆ†æ:")
            for field, types in sorted(field_types.items()):
                print(f"  {field}: {', '.join(sorted(types))}")
            
            return metadata_info
            
        except Exception as e:
            logging.error(f"åˆ†æå…ƒæ•°æ®å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _check_image_docs(self) -> Dict[str, Any]:
        """æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            image_docs = []
            
            for doc_id, doc in docstore.items():
                if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'image':
                    image_docs.append((doc_id, doc))
            
            image_info = {
                'total_image_docs': len(image_docs),
                'enhanced_description_stats': {
                    'with_enhanced': 0,
                    'without_enhanced': 0
                },
                'dual_vector_storage_stats': {
                    'visual_embedding_count': 0,
                    'description_embedding_count': 0,
                    'dual_storage_count': 0
                },
                'samples': []
            }
            
            print(f"\nğŸ“· æ‰¾åˆ° {len(image_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
            
            # åˆ†æenhanced_descriptionå­—æ®µ
            enhanced_count = 0
            empty_count = 0
            
            # åˆ†æåŒé‡å‘é‡å­˜å‚¨
            visual_embedding_count = 0
            description_embedding_count = 0
            dual_storage_count = 0
            
            # æŒ‰image_idåˆ†ç»„ç»Ÿè®¡åŒé‡å‘é‡å­˜å‚¨
            image_id_groups = defaultdict(list)
            for doc_id, doc in image_docs:
                image_id = doc.metadata.get('image_id', 'unknown')
                vector_type = doc.metadata.get('vector_type', 'unknown')
                image_id_groups[image_id].append(vector_type)
            
            for image_id, vector_types in image_id_groups.items():
                if 'visual_embedding' in vector_types:
                    visual_embedding_count += 1
                if 'description_embedding' in vector_types:
                    description_embedding_count += 1
                if 'visual_embedding' in vector_types and 'description_embedding' in vector_types:
                    dual_storage_count += 1
            
            for doc_id, doc in image_docs:
                # ç›´æ¥ä»metadataè·å–enhanced_description
                enhanced_desc = doc.metadata.get('enhanced_description', '')
                if enhanced_desc:
                    enhanced_count += 1
                else:
                    empty_count += 1
            
            image_info['enhanced_description_stats']['with_enhanced'] = enhanced_count
            image_info['enhanced_description_stats']['without_enhanced'] = empty_count
            image_info['dual_vector_storage_stats']['visual_embedding_count'] = visual_embedding_count
            image_info['dual_vector_storage_stats']['description_embedding_count'] = description_embedding_count
            image_info['dual_vector_storage_stats']['dual_storage_count'] = dual_storage_count
            
            print(f"âœ… æœ‰enhanced_descriptionçš„å›¾ç‰‡: {enhanced_count}")
            print(f"âŒ æ— enhanced_descriptionçš„å›¾ç‰‡: {empty_count}")
            if (enhanced_count + empty_count) > 0:
                print(f"ğŸ“ˆ è¦†ç›–ç‡: {enhanced_count/(enhanced_count+empty_count)*100:.1f}%")
            
            print(f"\nğŸ”¢ åŒé‡å‘é‡å­˜å‚¨ç»Ÿè®¡:")
            print(f"  visual_embeddingå‘é‡: {visual_embedding_count}")
            print(f"  description_embeddingå‘é‡: {description_embedding_count}")
            print(f"  åŒé‡å­˜å‚¨çš„å›¾ç‰‡: {dual_storage_count}")
            if len(image_id_groups) > 0:
                print(f"ğŸ“ˆ åŒé‡å­˜å‚¨è¦†ç›–ç‡: {dual_storage_count/len(image_id_groups)*100:.1f}%")
            
            # æ˜¾ç¤ºå‰5ä¸ªå›¾ç‰‡æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
            for i, (doc_id, doc) in enumerate(image_docs[:5]):
                # ç›´æ¥ä»metadataè·å–ä¿¡æ¯
                document_name = doc.metadata.get('document_name', 'N/A')
                page_number = doc.metadata.get('page_number', 'N/A')
                image_id = doc.metadata.get('image_id', 'N/A')
                vector_type = doc.metadata.get('vector_type', 'N/A')
                enhanced_description = doc.metadata.get('enhanced_description', '')
                
                sample_info = {
                    'index': i+1,
                    'doc_id': doc_id,
                    'document_name': document_name,
                    'page_number': page_number,
                    'image_id': image_id,
                    'vector_type': vector_type,
                    'enhanced_description': enhanced_description[:100] + '...' if len(enhanced_description) > 100 else enhanced_description
                }
                image_info['samples'].append(sample_info)
                
                print(f"\nğŸ“· å›¾ç‰‡æ–‡æ¡£ {i+1}:")
                print(f"  ID: {doc_id}")
                print(f"  æ–‡æ¡£å: {document_name}")
                print(f"  é¡µç : {page_number}")
                print(f"  å›¾ç‰‡ID: {image_id}")
                print(f"  å‘é‡ç±»å‹: {vector_type}")
                print(f"  å¢å¼ºæè¿°: {enhanced_description[:100] + '...' if len(enhanced_description) > 100 else enhanced_description}")
            
            if len(image_docs) > 5:
                print(f"\n... è¿˜æœ‰ {len(image_docs) - 5} ä¸ªå›¾ç‰‡æ–‡æ¡£")
            
            return image_info
            
        except Exception as e:
            logging.error(f"æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _check_table_docs(self) -> Dict[str, Any]:
        """æ£€æŸ¥è¡¨æ ¼æ–‡æ¡£"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            table_docs = []
            
            for doc_id, doc in docstore.items():
                if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'table':
                    table_docs.append((doc_id, doc))
            
            table_info = {
                'total_table_docs': len(table_docs),
                'metadata_fields': set(),
                'table_types': defaultdict(int),
                'document_names': set(),
                'samples': []
            }
            
            print(f"\nğŸ“Š æ‰¾åˆ° {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
            
            # åˆ†æå‰å‡ ä¸ªè¡¨æ ¼æ–‡æ¡£
            for i, (doc_id, doc) in enumerate(table_docs[:3]):
                # ç›´æ¥ä»metadataè·å–ä¿¡æ¯
                document_name = doc.metadata.get('document_name', 'N/A')
                page_number = doc.metadata.get('page_number', 'N/A')
                table_id = doc.metadata.get('table_id', 'N/A')
                table_type = doc.metadata.get('table_type', 'N/A')
                
                sample_info = {
                    'index': i+1,
                    'doc_id': doc_id,
                    'document_name': document_name,
                    'page_number': page_number,
                    'table_id': table_id,
                    'table_type': table_type
                }
                table_info['samples'].append(sample_info)
                
                print(f"\nğŸ“„ è¡¨æ ¼æ–‡æ¡£ {i+1}:")
                print(f"  æ–‡æ¡£ID: {doc_id}")
                print(f"  æ–‡æ¡£å: {document_name}")
                print(f"  é¡µç : {page_number}")
                print(f"  è¡¨æ ¼ID: {table_id}")
                print(f"  è¡¨æ ¼ç±»å‹: {table_type}")
                
                # æ”¶é›†å…ƒæ•°æ®å­—æ®µ
                if hasattr(doc, 'metadata') and doc.metadata:
                    table_info['metadata_fields'].update(doc.metadata.keys())
                    
                    # ç»Ÿè®¡ç‰¹å®šå­—æ®µ
                    if table_type and table_type != 'N/A':
                        table_info['table_types'][table_type] += 1
                    if document_name and document_name != 'N/A':
                        table_info['document_names'].add(document_name)
            
            # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
            table_info['metadata_fields'] = list(table_info['metadata_fields'])
            table_info['document_names'] = list(table_info['document_names'])
            table_info['table_types'] = dict(table_info['table_types'])
            
            print(f"\nğŸ“Š è¡¨æ ¼æ–‡æ¡£ç»Ÿè®¡:")
            print(f"  æ€»æ–‡æ¡£æ•°: {table_info['total_table_docs']}")
            print(f"  å…ƒæ•°æ®å­—æ®µæ•°: {len(table_info['metadata_fields'])}")
            
            return table_info
            
        except Exception as e:
            logging.error(f"æ£€æŸ¥è¡¨æ ¼æ–‡æ¡£å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _check_text_docs(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡æœ¬æ–‡æ¡£"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            text_docs = []
            
            for doc_id, doc in docstore.items():
                if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'text':
                    text_docs.append((doc_id, doc))
            
            text_info = {
                'total_text_docs': len(text_docs),
                'document_names': set(),
                'page_numbers': set(),
                'samples': []
            }
            
            print(f"\nğŸ“ æ‰¾åˆ° {len(text_docs)} ä¸ªæ–‡æœ¬æ–‡æ¡£")
            
            # åˆ†æå‰å‡ ä¸ªæ–‡æœ¬æ–‡æ¡£
            for i, (doc_id, doc) in enumerate(text_docs[:3]):
                # ç›´æ¥ä»metadataè·å–ä¿¡æ¯
                document_name = doc.metadata.get('document_name', 'N/A')
                page_number = doc.metadata.get('page_number', 'N/A')
                chunk_index = doc.metadata.get('chunk_index', 'N/A')
                
                sample_info = {
                    'index': i+1,
                    'doc_id': doc_id,
                    'document_name': document_name,
                    'page_number': page_number,
                    'chunk_index': chunk_index,
                    'content_preview': doc.page_content[:100] + '...' if hasattr(doc, 'page_content') and len(doc.page_content) > 100 else (doc.page_content if hasattr(doc, 'page_content') else 'N/A')
                }
                text_info['samples'].append(sample_info)
                
                print(f"\nğŸ“ æ–‡æœ¬æ–‡æ¡£ {i+1}:")
                print(f"  æ–‡æ¡£ID: {doc_id}")
                print(f"  æ–‡æ¡£å: {doc.metadata.get('document_name', 'N/A')}")
                print(f"  é¡µç : {doc.metadata.get('page_number', 'N/A')}")
                print(f"  å—ç´¢å¼•: {doc.metadata.get('chunk_index', 'N/A')}")
                print(f"  å†…å®¹é¢„è§ˆ: {doc.page_content[:100] + '...' if hasattr(doc, 'page_content') and len(doc.page_content) > 100 else (doc.page_content if hasattr(doc, 'page_content') else 'N/A')}")
                
                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                if doc.metadata.get('document_name'):
                    text_info['document_names'].add(doc.metadata['document_name'])
                if doc.metadata.get('page_number'):
                    text_info['page_numbers'].add(doc.metadata['page_number'])
            
            # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
            text_info['document_names'] = list(text_info['document_names'])
            text_info['page_numbers'] = list(text_info['page_numbers'])
            
            print(f"\nğŸ“ æ–‡æœ¬æ–‡æ¡£ç»Ÿè®¡:")
            print(f"  æ€»æ–‡æ¡£æ•°: {text_info['total_text_docs']}")
            print(f"  å”¯ä¸€æ–‡æ¡£å: {len(text_info['document_names'])}")
            print(f"  é¡µç èŒƒå›´: {min(text_info['page_numbers']) if text_info['page_numbers'] else 'N/A'} - {max(text_info['page_numbers']) if text_info['page_numbers'] else 'N/A'}")
            
            return text_info

        except Exception as e:
            logging.error(f"æ£€æŸ¥æ–‡æœ¬æ–‡æ¡£å¤±è´¥: {e}")
            return {'error': str(e)}

    def _analyze_vector_data(self, vector_store) -> Dict[str, Any]:
        """åˆ†æå‘é‡æ•°æ®çš„åˆ†å¸ƒå’Œæ ·æœ¬"""
        print("\nğŸ”¢ å‘é‡æ•°æ®åˆ†æ")
        print("=" * 60)

        if not vector_store:
            print("âŒ å‘é‡å­˜å‚¨å¯¹è±¡æ— æ•ˆ")
            return None

        try:
            # è·å–å‘é‡æ•°æ®
            vectors = vector_store.index.reconstruct_n(0, vector_store.index.ntotal)
            vector_info = {
                'vector_count': vector_store.index.ntotal,
                'vector_dimension': vector_store.index.d,
                'vectors': vectors
            }

            print(f"ğŸ“Š å‘é‡ç»Ÿè®¡:")
            print(f"  å‘é‡æ•°é‡: {vector_info['vector_count']}")
            print(f"  å‘é‡ç»´åº¦: {vector_info['vector_dimension']}")

            if vector_info['vector_count'] > 0:
                # å‘é‡èŒƒæ•°åˆ†æ
                norms = np.linalg.norm(vectors, axis=1)
                vector_info['norm_stats'] = {
                    'min': float(norms.min()),
                    'max': float(norms.max()),
                    'mean': float(norms.mean()),
                    'std': float(norms.std())
                }

                print(f"\nğŸ“ å‘é‡èŒƒæ•°ç»Ÿè®¡:")
                print(f"  æœ€å°å€¼: {norms.min():.4f}")
                print(f"  æœ€å¤§å€¼: {norms.max():.4f}")
                print(f"  å¹³å‡å€¼: {norms.mean():.4f}")
                print(f"  æ ‡å‡†å·®: {norms.std():.4f}")

                # æ˜¾ç¤ºå‘é‡æ ·æœ¬
                show_samples = input("\næ˜¯å¦æ˜¾ç¤ºå‘é‡æ•°æ®æ ·æœ¬? (y/n): ").strip().lower()
                if show_samples == 'y':
                    print(f"\nğŸ” å‘é‡æ ·æœ¬:")
                    sample_count = min(5, vector_info['vector_count'])
                    for i in range(sample_count):
                        print(f"  å‘é‡ {i+1}: [{vectors[i][0]:.4f}, {vectors[i][1]:.4f}, ..., {vectors[i][-1]:.4f}]")
                        print(f"    èŒƒæ•°: {norms[i]:.4f}")

                    # åˆ†æå‘é‡ç›¸ä¼¼åº¦
                    if vector_info['vector_count'] > 1:
                        self._analyze_vector_similarity(vectors, vector_info)

            return vector_info

        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åˆ†æå¤±è´¥: {e}")
            return None

    def _analyze_vector_similarity(self, vectors, vector_info):
        """åˆ†æå‘é‡ç›¸ä¼¼åº¦ - è¯„ä¼°å‘é‡åŒ–è´¨é‡å’Œå‘ç°æ½œåœ¨é—®é¢˜"""
        print("\nğŸ”— å‘é‡ç›¸ä¼¼åº¦åˆ†æ")
        print("-" * 40)
        print("ğŸ¯ ä½œç”¨ï¼šè¯„ä¼°å‘é‡åŒ–è´¨é‡ï¼Œå‘ç°é‡å¤å†…å®¹ï¼Œä¼˜åŒ–æ£€ç´¢å‚æ•°")

        try:
            # è®¡ç®—å‰10ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦
            sample_size = min(10, vector_info['vector_count'])
            sample_vectors = vectors[:sample_size]

            # å½’ä¸€åŒ–å‘é‡ç”¨äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            norms = np.linalg.norm(sample_vectors, axis=1, keepdims=True)
            normalized_vectors = sample_vectors / norms

            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = np.dot(normalized_vectors, normalized_vectors.T)

            print(f"ğŸ“ˆ å‰{sample_size}ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦çŸ©é˜µ:")
            print("     ", end="")
            for i in range(sample_size):
                print("8d")
            print()

            for i in range(sample_size):
                print("3d", end="")
                for j in range(sample_size):
                    if i == j:
                        print("1.0000", end="")
                    else:
                        sim = similarity_matrix[i, j]
                        # ç”¨ä¸åŒæ ‡è¯†ç¬¦è¡¨ç¤ºç›¸ä¼¼åº¦çº§åˆ«
                        if sim > 0.8:
                            print(".4f", end="")
                        elif sim > 0.6:
                            print(".4f", end="")
                        elif sim > 0.4:
                            print(".4f", end="")
                        else:
                            print(".4f", end="")
                print()

            # åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ
            similarities = []
            for i in range(sample_size):
                for j in range(i+1, sample_size):
                    similarities.append(similarity_matrix[i, j])

            if similarities:
                similarities = np.array(similarities)
                print("\nğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡:")
                print(f"  å¹³å‡ç›¸ä¼¼åº¦: {similarities.mean():.4f}")
                print(f"  æœ€å¤§ç›¸ä¼¼åº¦: {similarities.max():.4f}")
                print(f"  æœ€å°ç›¸ä¼¼åº¦: {similarities.min():.4f}")
                print(f"  ç›¸ä¼¼åº¦æ ‡å‡†å·®: {similarities.std():.4f}")
                # è´¨é‡è¯„ä¼°
                print("\nğŸ” è´¨é‡è¯„ä¼°:")
                if similarities.mean() > 0.7:
                    print("  âš ï¸  ç›¸ä¼¼åº¦è¿‡é«˜ï¼šå¯èƒ½å­˜åœ¨å¤§é‡é‡å¤å†…å®¹")
                elif similarities.mean() > 0.5:
                    print("  âœ… ç›¸ä¼¼åº¦é€‚ä¸­ï¼šå†…å®¹å¤šæ ·æ€§è‰¯å¥½")
                elif similarities.mean() > 0.3:
                    print("  âœ… ç›¸ä¼¼åº¦æ­£å¸¸ï¼šå†…å®¹åŒºåˆ†åº¦è‰¯å¥½")
                else:
                    print("  âœ… ç›¸ä¼¼åº¦è¾ƒä½ï¼šå†…å®¹é«˜åº¦å¤šæ ·åŒ–")

                # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é«˜çš„ç›¸ä¼¼åº¦
                high_sim_pairs = [(i, j) for i in range(sample_size) for j in range(i+1, sample_size) if similarity_matrix[i, j] > 0.9]
                if high_sim_pairs:
                    print(f"  ğŸš¨ å‘ç° {len(high_sim_pairs)} å¯¹é«˜åº¦ç›¸ä¼¼çš„å‘é‡ (>0.9)")
                    print("     å¯èƒ½è¡¨ç¤ºé‡å¤å†…å®¹æˆ–å‘é‡åŒ–å¼‚å¸¸")

        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {e}")

    def _analyze_vector_data_by_type(self, vector_store) -> Dict[str, Any]:
        """æŒ‰vector_typeåˆ†æå‘é‡æ•°æ®"""
        print("\nğŸ”¢ æŒ‰å‘é‡ç±»å‹åˆ†æå‘é‡æ•°æ®")
        print("=" * 60)

        if not vector_store:
            print("âŒ å‘é‡å­˜å‚¨å¯¹è±¡æ— æ•ˆ")
            return None

        try:
            docstore = vector_store.docstore._dict
            
            # æŒ‰vector_typeåˆ†ç»„ç»Ÿè®¡
            vector_type_stats = defaultdict(list)
            vector_type_counts = defaultdict(int)
            
            for doc_id, doc in docstore.items():
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                vector_type = metadata.get('vector_type', 'unknown')
                vector_type_counts[vector_type] += 1
                
                # è·å–å¯¹åº”çš„å‘é‡ç´¢å¼•
                try:
                    # è¿™é‡Œéœ€è¦æ ¹æ®doc_idæ‰¾åˆ°å¯¹åº”çš„å‘é‡ç´¢å¼•
                    # ç”±äºFAISSç´¢å¼•å’Œdocstoreçš„å¯¹åº”å…³ç³»ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼è·å–
                    vector_type_stats[vector_type].append(doc_id)
                except:
                    pass
            
            print(f"ğŸ“Š å‘é‡ç±»å‹ç»Ÿè®¡:")
            for vector_type, count in sorted(vector_type_counts.items()):
                print(f"  {vector_type}: {count}")
            
            # åˆ†ææ¯ç§å‘é‡ç±»å‹çš„è´¨é‡
            vector_type_info = {}
            for vector_type, doc_ids in vector_type_stats.items():
                if vector_type != 'unknown' and len(doc_ids) > 0:
                    print(f"\nğŸ” {vector_type} å‘é‡åˆ†æ:")
                    print(f"  æ•°é‡: {len(doc_ids)}")
                    
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„å‘é‡è´¨é‡åˆ†æ
                    vector_type_info[vector_type] = {
                        'count': len(doc_ids),
                        'doc_ids': doc_ids[:5]  # åªä¿å­˜å‰5ä¸ªä½œä¸ºç¤ºä¾‹
                    }
            
            return vector_type_info

        except Exception as e:
            print(f"âŒ æŒ‰å‘é‡ç±»å‹åˆ†æå¤±è´¥: {e}")
            return None

    def _generate_quality_report(self, structure_info, image_info, table_info, vector_info) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š"""
        print("\nğŸ“‹ æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š")
        print("=" * 60)

        report = {
            'overall_score': 0,
            'issues': [],
            'recommendations': []
        }

        # æ£€æŸ¥1: æ–‡æ¡£æ•°é‡åˆç†æ€§
        if structure_info and structure_info.get('total_docs', 0) > 0:
            print("âœ… æ–‡æ¡£æ•°é‡æ£€æŸ¥é€šè¿‡")
            report['overall_score'] += 20
        else:
            print("âŒ æ–‡æ¡£æ•°é‡å¼‚å¸¸")
            report['issues'].append("æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
            report['recommendations'].append("æ£€æŸ¥æ–‡æ¡£å¤„ç†æµç¨‹æ˜¯å¦æ­£å¸¸")

        # æ£€æŸ¥2: å…ƒæ•°æ®å®Œæ•´æ€§
        if structure_info and len(structure_info.get('all_metadata_fields', [])) > 10:
            print("âœ… å…ƒæ•°æ®å­—æ®µä¸°å¯Œ")
            report['overall_score'] += 25
        else:
            print("âŒ å…ƒæ•°æ®å­—æ®µä¸è¶³")
            report['issues'].append("å…ƒæ•°æ®å­—æ®µæ•°é‡ä¸è¶³")
            report['recommendations'].append("æ£€æŸ¥metadataæå–é€»è¾‘")

        # æ£€æŸ¥3: å›¾ç‰‡æ–‡æ¡£è´¨é‡
        if image_info and image_info['total_image_docs'] > 0:
            enhanced_ratio = image_info['enhanced_description_stats']['with_enhanced'] / max(1, image_info['total_image_docs'])
            if enhanced_ratio > 0.8:
                print("âœ… å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡è‰¯å¥½")
                report['overall_score'] += 15
            elif enhanced_ratio > 0.5:
                print("âš ï¸  å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡ä¸€èˆ¬")
                report['overall_score'] += 10
                report['recommendations'].append("æå‡å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡")
            else:
                print("âŒ å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡ä¸è¶³")
                report['issues'].append("å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡ä½")
                report['recommendations'].append("æ£€æŸ¥å›¾ç‰‡å¢å¼ºå¤„ç†æµç¨‹")
            
            # æ£€æŸ¥6: åŒé‡å‘é‡å­˜å‚¨å®Œæ•´æ€§
            dual_stats = image_info.get('dual_vector_storage_stats', {})
            if dual_stats:
                dual_ratio = dual_stats.get('dual_storage_count', 0) / max(1, len(set(doc.metadata.get('image_id', '') for doc in self.vector_store_manager.vector_store.docstore._dict.values() if doc.metadata.get('chunk_type') == 'image')))
                if dual_ratio > 0.8:
                    print("âœ… åŒé‡å‘é‡å­˜å‚¨è¦†ç›–ç‡è‰¯å¥½")
                    report['overall_score'] += 15
                elif dual_ratio > 0.5:
                    print("âš ï¸  åŒé‡å‘é‡å­˜å‚¨è¦†ç›–ç‡ä¸€èˆ¬")
                    report['overall_score'] += 10
                    report['recommendations'].append("æå‡åŒé‡å‘é‡å­˜å‚¨è¦†ç›–ç‡")
                else:
                    print("âŒ åŒé‡å‘é‡å­˜å‚¨è¦†ç›–ç‡ä¸è¶³")
                    report['issues'].append("åŒé‡å‘é‡å­˜å‚¨è¦†ç›–ç‡ä½")
                    report['recommendations'].append("æ£€æŸ¥åŒé‡å‘é‡å­˜å‚¨æµç¨‹")
        else:
            print("âš ï¸  æ²¡æœ‰å›¾ç‰‡æ–‡æ¡£")
            report['recommendations'].append("æ£€æŸ¥å›¾ç‰‡å¤„ç†æµç¨‹")

        # æ£€æŸ¥4: è¡¨æ ¼æ–‡æ¡£è´¨é‡
        if table_info and table_info['total_table_docs'] > 0:
            processed_ratio = table_info.get('has_processed_content', 0) / max(1, table_info['total_table_docs'])
            if processed_ratio > 0.8:
                print("âœ… è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†è¦†ç›–ç‡è‰¯å¥½")
                report['overall_score'] += 15
            else:
                print("âŒ è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†è¦†ç›–ç‡ä¸è¶³")
                report['issues'].append("è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†è¦†ç›–ç‡ä½")
                report['recommendations'].append("æ£€æŸ¥è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†æµç¨‹")
        else:
            print("âš ï¸  æ²¡æœ‰è¡¨æ ¼æ–‡æ¡£")

        # æ£€æŸ¥5: å‘é‡æ•°æ®è´¨é‡
        if vector_info and vector_info['vector_count'] > 0:
            if 'norm_stats' in vector_info:
                norm_std = vector_info['norm_stats']['std']
                if norm_std < 1.0:
                    print("âœ… å‘é‡èŒƒæ•°åˆ†å¸ƒå‡åŒ€")
                    report['overall_score'] += 20
                else:
                    print("âš ï¸  å‘é‡èŒƒæ•°åˆ†å¸ƒä¸å‡åŒ€")
                    report['overall_score'] += 15
                    report['recommendations'].append("æ£€æŸ¥å‘é‡æ ‡å‡†åŒ–å¤„ç†")

            # æ£€æŸ¥å‘é‡ç»´åº¦ä¸€è‡´æ€§
            if vector_info['vector_dimension'] == 1536:  # DashScopeé»˜è®¤ç»´åº¦
                print("âœ… å‘é‡ç»´åº¦æ­£ç¡®")
            else:
                print(f"âš ï¸  å‘é‡ç»´åº¦å¼‚å¸¸: {vector_info['vector_dimension']}")
                report['issues'].append(f"å‘é‡ç»´åº¦å¼‚å¸¸: {vector_info['vector_dimension']}")
                report['recommendations'].append("æ£€æŸ¥embeddingæ¨¡å‹é…ç½®")
        else:
            print("âŒ æ²¡æœ‰å‘é‡æ•°æ®")
            report['issues'].append("æ²¡æœ‰å‘é‡æ•°æ®")
            report['recommendations'].append("æ£€æŸ¥å‘é‡åŒ–æµç¨‹")

        # ç”Ÿæˆæ€»ä½“è¯„ä»·
        print("\nğŸ† æ€»ä½“è¯„ä»·:")
        if report['overall_score'] >= 90:
            print("  ğŸ‰ ä¼˜ç§€ - æ•°æ®è´¨é‡éå¸¸è‰¯å¥½")
        elif report['overall_score'] >= 70:
            print("  âœ… è‰¯å¥½ - æ•°æ®è´¨é‡åŸºæœ¬æ»¡è¶³è¦æ±‚")
        elif report['overall_score'] >= 50:
            print("  âš ï¸  ä¸€èˆ¬ - æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›")
        else:
            print("  âŒ è¾ƒå·® - æ•°æ®è´¨é‡å­˜åœ¨ä¸¥é‡é—®é¢˜")

        print(f"  ğŸ“Š ç»¼åˆå¾—åˆ†: {report['overall_score']}/100")

        if report['issues']:
            print("\nğŸ”§ å‘ç°çš„é—®é¢˜:")
            for issue in report['issues']:
                print(f"  â€¢ {issue}")

        if report['recommendations']:
            print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for rec in report['recommendations']:
                print(f"  â€¢ {rec}")

        report['overall_score'] = report['overall_score']
        return report

    def _show_detailed_document_list(self, structure_info):
        """æ˜¾ç¤ºè¯¦ç»†çš„æ–‡æ¡£åˆ—è¡¨"""
        print("\nğŸ“‹ è¯¦ç»†æ–‡æ¡£åˆ—è¡¨")
        print("=" * 80)

        # ä»docstoreä¸­è·å–è¯¦ç»†ä¿¡æ¯
        vector_store = self.vector_store_manager.vector_store
        if not vector_store or not hasattr(vector_store, 'docstore'):
            print("âŒ æ— æ³•è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯")
            return

        docstore = vector_store.docstore._dict

        # æŒ‰chunk_typeåˆ†ç»„æ˜¾ç¤º
        from collections import defaultdict
        docs_by_type = defaultdict(list)

        for doc_id, doc in docstore.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            chunk_type = metadata.get('chunk_type', 'unknown')

            doc_info = {
                'doc_id': doc_id,
                'chunk_type': chunk_type,
                'document_name': metadata.get('document_name', 'N/A'),
                'page_number': metadata.get('page_number', 'N/A'),
                'content_length': len(doc.page_content) if hasattr(doc, 'page_content') else 0,
                'metadata_fields_count': len(metadata),
                'metadata_keys': list(metadata.keys())
            }
            docs_by_type[chunk_type].append(doc_info)

        for chunk_type, docs in docs_by_type.items():
            print(f"\nğŸ”¹ {chunk_type.upper()} æ–‡æ¡£ ({len(docs)} ä¸ª):")
            print("-" * 60)

            # æ˜¾ç¤ºå‰10ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
            for i, doc in enumerate(docs[:10]):
                print("2d")
                print(f"      æ–‡æ¡£å: {doc['document_name']}")
                print(f"      é¡µç : {doc['page_number']}")
                print(f"      å†…å®¹é•¿åº¦: {doc['content_length']}")
                print(f"      å…ƒæ•°æ®å­—æ®µæ•°: {doc['metadata_fields_count']}")
                print(f"      å…ƒæ•°æ®å­—æ®µ: {', '.join(doc['metadata_keys'][:8])}...")
                if len(doc['metadata_keys']) > 8:
                    print(f"                   ... è¿˜æœ‰ {len(doc['metadata_keys']) - 8} ä¸ªå­—æ®µ")

            if len(docs) > 10:
                print(f"      ... è¿˜æœ‰ {len(docs) - 10} ä¸ªæ–‡æ¡£")

            # æ˜¾ç¤ºè¯¥ç±»å‹çš„ç»Ÿè®¡ä¿¡æ¯
            total_content_length = sum(doc['content_length'] for doc in docs)
            avg_content_length = total_content_length / len(docs) if docs else 0
            print(f"\n  ğŸ“Š {chunk_type} ç»Ÿè®¡:")
            print(f"    æ€»å†…å®¹é•¿åº¦: {total_content_length}")
            print(f"    å¹³å‡å†…å®¹é•¿åº¦: {avg_content_length:.1f}")
            print(f"    å¹³å‡å…ƒæ•°æ®å­—æ®µæ•°: {sum(doc['metadata_fields_count'] for doc in docs) / len(docs):.1f}")
    
    def _show_type_overview(self, chunk_type: str):
        """æ˜¾ç¤ºæŸä¸ªç±»å‹çš„åŸºæœ¬æƒ…å†µ"""
        print(f"\nğŸ“Š {chunk_type.upper()} ç±»å‹åŸºæœ¬æƒ…å†µ")
        print("=" * 60)
        
        vector_store = self.vector_store_manager.vector_store
        if not vector_store or not hasattr(vector_store, 'docstore'):
            print("âŒ æ— æ³•è·å–å‘é‡å­˜å‚¨ä¿¡æ¯")
            return
        
        docstore = vector_store.docstore._dict
        
        # ç­›é€‰æŒ‡å®šç±»å‹çš„æ–‡æ¡£
        type_docs = []
        for doc_id, doc in docstore.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            if metadata.get('chunk_type') == chunk_type:
                type_docs.append((doc_id, doc))
        
        if not type_docs:
            print(f"âŒ æœªæ‰¾åˆ° {chunk_type} ç±»å‹çš„æ–‡æ¡£")
            return
        
        print(f"ğŸ“š æ‰¾åˆ° {len(type_docs)} ä¸ª {chunk_type} æ–‡æ¡£")
        
        # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
        total_content_length = 0
        metadata_fields = set()
        document_names = set()
        
        for doc_id, doc in type_docs:
            if hasattr(doc, 'page_content'):
                total_content_length += len(doc.page_content)
            
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            metadata_fields.update(metadata.keys())
            
            # ç›´æ¥è·å–æ–‡æ¡£å
            doc_name = metadata.get('document_name', 'N/A')
            if doc_name != 'N/A':
                document_names.add(doc_name)
        
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"  æ€»æ–‡æ¡£æ•°: {len(type_docs)}")
        print(f"  æ€»å†…å®¹é•¿åº¦: {total_content_length}")
        print(f"  å¹³å‡å†…å®¹é•¿åº¦: {total_content_length / len(type_docs):.1f}")
        print(f"  å”¯ä¸€æ–‡æ¡£å: {len(document_names)}")
        print(f"  å…ƒæ•°æ®å­—æ®µæ•°: {len(metadata_fields)}")
        
        if document_names:
            print(f"\nğŸ“š æ–‡æ¡£ååˆ—è¡¨:")
            for i, name in enumerate(sorted(document_names)[:5], 1):
                print(f"  {i}. {name}")
            if len(document_names) > 5:
                print(f"  ... è¿˜æœ‰ {len(document_names) - 5} ä¸ªæ–‡æ¡£")
        
        print(f"\nğŸ”§ å…ƒæ•°æ®å­—æ®µ:")
        for i, field in enumerate(sorted(metadata_fields)[:10], 1):
            print(f"  {i}. {field}")
        if len(metadata_fields) > 10:
            print(f"  ... è¿˜æœ‰ {len(metadata_fields) - 10} ä¸ªå­—æ®µ")
    
    def _show_type_detailed_metadata(self, chunk_type: str):
        """æ˜¾ç¤ºæŸä¸ªç±»å‹ä¸­æ‰€æœ‰æ–‡æ¡£çš„è¯¦ç»†metadata"""
        print(f"\nğŸ” {chunk_type.upper()} ç±»å‹è¯¦ç»†Metadataåˆ†æ")
        print("=" * 80)
        
        vector_store = self.vector_store_manager.vector_store
        if not vector_store or not hasattr(vector_store, 'docstore'):
            print("âŒ æ— æ³•è·å–å‘é‡å­˜å‚¨ä¿¡æ¯")
            return
        
        docstore = vector_store.docstore._dict
        
        # ç­›é€‰æŒ‡å®šç±»å‹çš„æ–‡æ¡£
        type_docs = []
        for doc_id, doc in docstore.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            if metadata.get('chunk_type') == chunk_type:
                type_docs.append((doc_id, doc))
        
        if not type_docs:
            print(f"âŒ æœªæ‰¾åˆ° {chunk_type} ç±»å‹çš„æ–‡æ¡£")
            return
        
        print(f"ğŸ“š æ‰¾åˆ° {len(type_docs)} ä¸ª {chunk_type} æ–‡æ¡£")
        print("ğŸ” å¼€å§‹åˆ†ææ¯ä¸ªæ–‡æ¡£çš„å®Œæ•´metadata...")
        
        # åˆ†ææ¯ä¸ªæ–‡æ¡£çš„metadata
        for i, (doc_id, doc) in enumerate(type_docs, 1):
            print(f"\nğŸ“„ {chunk_type.upper()} æ–‡æ¡£ {i}:")
            print("-" * 60)
            print(f"æ–‡æ¡£ID: {doc_id}")
            
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            
            # æ˜¾ç¤ºå¤–å±‚metadata
            print(f"\nğŸ”§ å¤–å±‚Metadata ({len(metadata)} ä¸ªå­—æ®µ):")
            for key, value in sorted(metadata.items()):
                if key in ['description_embedding', 'image_embedding', 'text_embedding', 'table_embedding'] and isinstance(value, (list, tuple)):
                    # é™åˆ¶embeddingå­—æ®µçš„æ˜¾ç¤ºé•¿åº¦ï¼Œåªæ˜¾ç¤ºå‰3ä¸ªå€¼
                    if len(value) > 3:
                        print(f"  {key}: [{value[0]}, {value[1]}, {value[2]}, ...] (å…±{len(value)}ä¸ªå…ƒç´ )")
                    else:
                        print(f"  {key}: {list(value)}")
                else:
                    # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
                    if isinstance(value, str) and len(str(value)) > 100:
                        display_value = str(value)[:100] + "..."
                    else:
                        display_value = str(value)
                    print(f"  {key}: {display_value}")
            
            # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
            if hasattr(doc, 'page_content'):
                # æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©æ­£ç¡®çš„å†…å®¹å­—æ®µ
                if chunk_type == 'text' and metadata.get('text'):
                    # æ–‡æœ¬æ–‡æ¡£ï¼šæ˜¾ç¤ºtextå­—æ®µå†…å®¹
                    content = metadata['text']
                    if len(content) > 200:
                        content_preview = content[:200] + "..."
                    else:
                        content_preview = content
                    print(f"\nğŸ“ æ–‡æœ¬å†…å®¹é¢„è§ˆ:")
                    print(f"  {content_preview}")
                elif chunk_type == 'table' and metadata.get('table_content'):
                    # è¡¨æ ¼æ–‡æ¡£ï¼šæ˜¾ç¤ºtable_contentå­—æ®µå†…å®¹
                    content = metadata['table_content']
                    if len(content) > 200:
                        content_preview = content[:200] + "..."
                    else:
                        content_preview = content
                    print(f"\nğŸ“Š è¡¨æ ¼å†…å®¹é¢„è§ˆ:")
                    print(f"  {content_preview}")
                elif chunk_type == 'image' and metadata.get('enhanced_description'):
                    # å›¾åƒæ–‡æ¡£ï¼šæ˜¾ç¤ºenhanced_descriptionå­—æ®µå†…å®¹
                    content = metadata['enhanced_description']
                    if len(content) > 200:
                        content_preview = content[:200] + "..."
                    else:
                        content_preview = content
                    print(f"\nğŸ–¼ï¸ å›¾åƒæè¿°é¢„è§ˆ:")
                    print(f"  {content_preview}")
                else:
                    # å…¶ä»–æƒ…å†µï¼šæ˜¾ç¤ºpage_contentæˆ–å°è¯•å…¶ä»–å­—æ®µ
                    content = doc.page_content if hasattr(doc, 'page_content') else "æ— å†…å®¹"
                    if len(content) > 200:
                        content_preview = content[:200] + "..."
                    else:
                        content_preview = content
                    print(f"\nğŸ“ å†…å®¹é¢„è§ˆ:")
                    print(f"  {content_preview}")
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­æ˜¾ç¤ºä¸‹ä¸€ä¸ªæ–‡æ¡£
            if i < len(type_docs):
                print(f"\næ˜¯å¦ç»§ç»­æ˜¾ç¤ºä¸‹ä¸€ä¸ªæ–‡æ¡£? (y/n): ", end="")
                try:
                    user_input = input().strip().lower()
                    if user_input != 'y':
                        print("åœæ­¢æ˜¾ç¤ºè¯¦ç»†metadata")
                        break
                except KeyboardInterrupt:
                    print("\nç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢æ˜¾ç¤º")
                    break
                except:
                    print("ç»§ç»­æ˜¾ç¤ºä¸‹ä¸€ä¸ªæ–‡æ¡£...")
    
    def _show_interactive_menu(self):
        """æ˜¾ç¤ºäº¤äº’å¼èœå•"""
        print("\nğŸ¯ æ•°æ®åº“è¯Šæ–­å·¥å…· - äº¤äº’å¼èœå•")
        print("=" * 60)
        print("è¯·é€‰æ‹©è¦æ‰§è¡Œçš„æ“ä½œ:")
        print("1. ğŸ“Š å±•ç¤ºæ•°æ®åº“æ•´ä½“æƒ…å†µ")
        print("2. ğŸ“· å±•ç¤ºå›¾ç‰‡ç±»å‹åŸºæœ¬æƒ…å†µ")
        print("3. ğŸ“Š å±•ç¤ºè¡¨æ ¼ç±»å‹åŸºæœ¬æƒ…å†µ")
        print("4. ğŸ“ å±•ç¤ºæ–‡æœ¬ç±»å‹åŸºæœ¬æƒ…å†µ")
        print("5. ğŸ” å±•ç¤ºå›¾ç‰‡ç±»å‹æ‰€æœ‰å­—æ®µå’Œå€¼")
        print("6. ğŸ” å±•ç¤ºè¡¨æ ¼ç±»å‹æ‰€æœ‰å­—æ®µå’Œå€¼")
        print("7. ğŸ” å±•ç¤ºæ–‡æœ¬ç±»å‹æ‰€æœ‰å­—æ®µå’Œå€¼")
        print("8. ğŸ”¢ å±•ç¤ºåŒé‡å‘é‡å­˜å‚¨åˆ†æ")
        print("9. ğŸ”¢ æŒ‰vector_typeåˆ†æå‘é‡æ•°æ®")
        print("10. ğŸ“‹ è¿è¡Œå®Œæ•´è¯Šæ–­")
        print("11. ğŸšª é€€å‡º")
        print("-" * 60)
    
    def run_interactive_mode(self):
        """è¿è¡Œäº¤äº’å¼æ¨¡å¼"""
        print("ğŸ” V3ç‰ˆæœ¬æ•°æ®åº“è¯Šæ–­å·¥å…·å¯åŠ¨")
        print("=" * 60)
        
        try:
            # åŠ è½½å‘é‡æ•°æ®åº“
            print("ğŸ“š åŠ è½½å‘é‡æ•°æ®åº“...")
            if not self.vector_store_manager.load():
                print("âŒ æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“")
                return
            
            print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
            
            while True:
                self._show_interactive_menu()
                
                try:
                    choice = input("è¯·è¾“å…¥é€‰æ‹© (1-11): ").strip()
                    
                    if choice == '1':
                        # å±•ç¤ºæ•°æ®åº“æ•´ä½“æƒ…å†µ
                        print("\nğŸ“Š æ•°æ®åº“æ•´ä½“æƒ…å†µ")
                        print("=" * 60)
                        basic_info = self._get_basic_info()
                        structure_info = self._analyze_document_structure()
                        
                        print(f"ğŸ“š æ€»æ–‡æ¡£æ•°: {basic_info.get('total_docs', 'N/A')}")
                        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {basic_info.get('vector_db_path', 'N/A')}")
                        print(f"âœ… å‘é‡æ•°æ®åº“å­˜åœ¨: {basic_info.get('vector_db_exists', 'N/A')}")
                        
                        print(f"\nğŸ“Š åˆ†å—ç±»å‹åˆ†å¸ƒ:")
                        for chunk_type, count in structure_info.get('chunk_types', {}).items():
                            print(f"  {chunk_type}: {count}")
                        
                        print(f"\nğŸ“š æ–‡æ¡£ç»Ÿè®¡:")
                        print(f"  æ€»æ–‡æ¡£æ•°: {structure_info.get('total_docs', 'N/A')}")
                        print(f"  å”¯ä¸€æ–‡æ¡£å: {structure_info.get('unique_document_names', 'N/A')}")
                        
                        doc_names = structure_info.get('document_names', [])
                        if doc_names:
                            print(f"  å…·ä½“æ–‡æ¡£å: {doc_names[:3]}")
                            if len(doc_names) > 3:
                                print(f"  ... è¿˜æœ‰ {len(doc_names) - 3} ä¸ªæ–‡æ¡£")
                    
                    elif choice == '2':
                        # å±•ç¤ºå›¾ç‰‡ç±»å‹åŸºæœ¬æƒ…å†µ
                        self._show_type_overview('image')
                    
                    elif choice == '3':
                        # å±•ç¤ºè¡¨æ ¼ç±»å‹åŸºæœ¬æƒ…å†µ
                        self._show_type_overview('table')
                    
                    elif choice == '4':
                        # å±•ç¤ºæ–‡æœ¬ç±»å‹åŸºæœ¬æƒ…å†µ
                        self._show_type_overview('text')
                    
                    elif choice == '5':
                        # å±•ç¤ºå›¾ç‰‡ç±»å‹æ‰€æœ‰å­—æ®µå’Œå€¼
                        self._show_type_detailed_metadata('image')
                    
                    elif choice == '6':
                        # å±•ç¤ºè¡¨æ ¼ç±»å‹æ‰€æœ‰å­—æ®µå’Œå€¼
                        self._show_type_detailed_metadata('table')
                    
                    elif choice == '7':
                        # å±•ç¤ºæ–‡æœ¬ç±»å‹æ‰€æœ‰å­—æ®µå’Œå€¼
                        self._show_type_detailed_metadata('text')
                    
                    elif choice == '8':
                        # å±•ç¤ºåŒé‡å‘é‡å­˜å‚¨åˆ†æ
                        print("\nğŸ”¢ åŒé‡å‘é‡å­˜å‚¨åˆ†æ")
                        print("=" * 60)
                        image_info = self._check_image_docs()
                        
                    elif choice == '9':
                        # æŒ‰vector_typeåˆ†æå‘é‡æ•°æ®
                        print("\nğŸ”¢ æŒ‰å‘é‡ç±»å‹åˆ†æå‘é‡æ•°æ®")
                        print("=" * 60)
                        vector_type_info = self._analyze_vector_data_by_type(self.vector_store_manager.vector_store)
                        
                    elif choice == '10':
                        # è¿è¡Œå®Œæ•´è¯Šæ–­
                        print("\nğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´è¯Šæ–­...")
                        self.run_diagnostic()
                    
                    elif choice == '11':
                        # é€€å‡º
                        print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨æ•°æ®åº“è¯Šæ–­å·¥å…·ï¼")
                        break
                    
                    else:
                        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-11 ä¹‹é—´çš„æ•°å­—")
                    
                    if choice != '11':
                        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
                        print("\n" + "="*60)
                
                except KeyboardInterrupt:
                    print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
                    break
                except Exception as e:
                    print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                    input("æŒ‰å›è½¦é”®ç»§ç»­...")
        
        except Exception as e:
            print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def _save_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜è¯Šæ–­ç»“æœåˆ°æ–‡ä»¶"""
        try:
            # æ¸…ç†numpyæ•°ç»„å’Œå…¶ä»–ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            cleaned_results = self._clean_for_json(results)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(cleaned_results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ è¯Šæ–­ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            logging.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def _clean_for_json(self, obj):
        """æ¸…ç†å¯¹è±¡ä¸­çš„numpyæ•°ç»„å’Œå…¶ä»–ä¸å¯JSONåºåˆ—åŒ–çš„å¯¹è±¡"""
        import numpy as np
        
        if isinstance(obj, dict):
            return {key: self._clean_for_json(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._clean_for_json(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating)):
            # å°†numpyæ•°å€¼ç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
            return obj.item()
        elif hasattr(obj, '__dict__'):
            # å¯¹äºå…¶ä»–å¯¹è±¡ï¼Œå°è¯•è½¬æ¢ä¸ºå­—å…¸
            try:
                return self._clean_for_json(obj.__dict__)
            except:
                return str(obj)
        else:
            return obj

def main():
    """ä¸»å‡½æ•°"""
    try:
        tool = DatabaseDiagnosticTool()
        tool.run_interactive_mode()
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
