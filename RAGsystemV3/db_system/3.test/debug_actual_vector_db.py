#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š

## 1. è°ƒè¯•è„šæœ¬ï¼šæ£€æŸ¥å®é™…å‘é‡æ•°æ®åº“çŠ¶æ€
## 2. åˆ†æä¸ºä»€ä¹ˆæ˜¾ç¤º33å¼ æœªå®Œæˆå›¾ç‰‡
## 3. å¯¹æ¯”è®¾è®¡æ–‡æ¡£è¦æ±‚ä¸å®é™…çŠ¶æ€

"""

import os
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.vector_store_manager import LangChainVectorStoreManager
from config.config_manager import ConfigManager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def debug_vector_db_status():
    """è°ƒè¯•å‘é‡æ•°æ®åº“çŠ¶æ€"""
    try:
        print("ğŸ” å¼€å§‹è°ƒè¯•å‘é‡æ•°æ®åº“çŠ¶æ€...")
        
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        config_manager = ConfigManager()
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨
        vector_store_manager = LangChainVectorStoreManager(config_manager)
        
        # å°è¯•åŠ è½½å‘é‡æ•°æ®åº“
        print("ğŸ“‚ å°è¯•åŠ è½½å‘é‡æ•°æ®åº“...")
        load_success = vector_store_manager.load()
        
        if not load_success:
            print("âŒ æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“")
            return
        
        print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        
        # è·å–æ•°æ®åº“çŠ¶æ€
        status = vector_store_manager.get_status()
        print(f"\nğŸ“Š æ•°æ®åº“çŠ¶æ€:")
        print(f"  - æ˜¯å¦åˆå§‹åŒ–: {status.get('is_initialized')}")
        print(f"  - å‘é‡æ€»æ•°: {status.get('total_vectors')}")
        print(f"  - ç´¢å¼•ç±»å‹: {status.get('index_type')}")
        print(f"  - ç´¢å¼•å‘é‡æ•°: {status.get('index_ntotal')}")
        
        # æ£€æŸ¥å‘é‡å­˜å‚¨å¯¹è±¡
        if not vector_store_manager.vector_store:
            print("âŒ å‘é‡å­˜å‚¨å¯¹è±¡ä¸ºç©º")
            return
        
        # æ£€æŸ¥docstoreä¸­çš„å†…å®¹
        docstore = vector_store_manager.vector_store.docstore
        if not hasattr(docstore, '_dict'):
            print("âŒ docstoreæ²¡æœ‰_dictå±æ€§")
            return
        
        docstore_dict = docstore._dict
        print(f"\nğŸ“š Docstoreå†…å®¹:")
        print(f"  - æ–‡æ¡£æ€»æ•°: {len(docstore_dict)}")
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„chunk
        chunk_types = {}
        image_chunks = []
        
        for doc_id, doc in docstore_dict.items():
            metadata = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else {}
            chunk_type = metadata.get('chunk_type', 'unknown')
            
            # ç»Ÿè®¡chunkç±»å‹
            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
            
            # æ”¶é›†å›¾ç‰‡chunkçš„è¯¦ç»†ä¿¡æ¯
            if chunk_type == 'image':
                image_info = {
                    'doc_id': doc_id,
                    'image_path': metadata.get('image_path', ''),
                    'image_id': metadata.get('image_id', ''),
                    'document_name': metadata.get('document_name', ''),
                    'enhancement_status': metadata.get('enhancement_status', 'unknown'),
                    'vectorization_status': metadata.get('vectorization_status', 'unknown'),
                    'enhanced_description': metadata.get('enhanced_description', ''),
                    'image_embedding': metadata.get('image_embedding', []),
                    'description_embedding': metadata.get('description_embedding', [])
                }
                image_chunks.append(image_info)
        
        print(f"\nğŸ“Š Chunkç±»å‹ç»Ÿè®¡:")
        for chunk_type, count in chunk_types.items():
            print(f"  - {chunk_type}: {count}")
        
        print(f"\nğŸ–¼ï¸ å›¾ç‰‡Chunkè¯¦ç»†ä¿¡æ¯ (å…±{len(image_chunks)}å¼ ):")
        for i, img in enumerate(image_chunks):
            print(f"  å›¾ç‰‡ {i+1}:")
            print(f"    - ID: {img['image_id']}")
            print(f"    - æ–‡æ¡£: {img['document_name']}")
            print(f"    - å¢å¼ºçŠ¶æ€: {img['enhancement_status']}")
            print(f"    - å‘é‡åŒ–çŠ¶æ€: {img['vectorization_status']}")
            print(f"    - å¢å¼ºæè¿°: {'æœ‰' if img['enhanced_description'] else 'æ— '}")
            print(f"    - å›¾ç‰‡å‘é‡: {'æœ‰' if img['image_embedding'] else 'æ— '}")
            print(f"    - æè¿°å‘é‡: {'æœ‰' if img['description_embedding'] else 'æ— '}")
            print()
        
        # åˆ†ææœªå®Œæˆçš„åŸå› 
        print("ğŸ” åˆ†ææœªå®ŒæˆåŸå› :")
        enhancement_pending = sum(1 for img in image_chunks if img['enhancement_status'] != 'success')
        vectorization_pending = sum(1 for img in image_chunks if img['vectorization_status'] != 'success')
        no_enhanced_desc = sum(1 for img in image_chunks if not img['enhanced_description'])
        no_image_embedding = sum(1 for img in image_chunks if not img['image_embedding'])
        no_desc_embedding = sum(1 for img in image_chunks if not img['description_embedding'])
        
        print(f"  - å¢å¼ºçŠ¶æ€ä¸æ˜¯'success': {enhancement_pending}")
        print(f"  - å‘é‡åŒ–çŠ¶æ€ä¸æ˜¯'success': {vectorization_pending}")
        print(f"  - ç¼ºå°‘å¢å¼ºæè¿°: {no_enhanced_desc}")
        print(f"  - ç¼ºå°‘å›¾ç‰‡å‘é‡: {no_image_embedding}")
        print(f"  - ç¼ºå°‘æè¿°å‘é‡: {no_desc_embedding}")
        
        # æ£€æŸ¥get_unfinished_imagesçš„é€»è¾‘
        print(f"\nğŸ” æµ‹è¯•get_unfinished_imagesé€»è¾‘:")
        unfinished_images = vector_store_manager.get_unfinished_images()
        print(f"  - get_unfinished_imagesè¿”å›: {len(unfinished_images)} å¼ ")
        
        if unfinished_images:
            print(f"  - ç¬¬ä¸€å¼ æœªå®Œæˆå›¾ç‰‡çš„çŠ¶æ€:")
            first_unfinished = unfinished_images[0]
            print(f"    - éœ€è¦å¢å¼º: {first_unfinished.get('needs_enhancement')}")
            print(f"    - éœ€è¦å‘é‡åŒ–: {first_unfinished.get('needs_vectorization')}")
            print(f"    - å…ƒæ•°æ®: {first_unfinished.get('metadata', {})}")
        
    except Exception as e:
        logger.error(f"è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_vector_db_status()
