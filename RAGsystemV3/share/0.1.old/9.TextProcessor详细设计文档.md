## TextProcessor详细设计文档（修正版）

### 1. 模块概述

**TextProcessor** 是V3版本文档处理系统的核心组件之一，负责处理各种类型的文本内容，包括从PDF解析出的文本、Markdown文件内容、以及JSON格式的结构化文本数据。

**重要说明**：本设计文档已根据MinerU JSON输出的实际结构进行了更新，主要字段映射如下：
- `text`：文本内容
- `text_level`：标题级别（1级标题等）
- `page_idx`：页码索引
- `type`：内容类型（固定为"text"）

#### 1.1 设计目标
- **统一处理**：统一处理各种来源的文本内容，提供一致的处理接口
- **智能分块**：根据配置进行文本分块，保持语义完整性
- **质量保证**：确保文本内容的完整性和准确性
- **向量化集成**：通过ModelCaller进行文本向量化，生成高质量的文本向量
- **元数据标准化**：严格按照设计文档的TEXT_METADATA_SCHEMA生成元数据

#### 1.2 核心功能
- 文本内容提取和清理
- 基于配置的文本分块
- 文本向量化处理（通过ModelCaller）
- 标准化元数据生成
- 批量文本处理

### 2. 文本处理流程

#### 2.1 整体流程

TextProcessor采用简化的处理流程，专注于核心功能：

```
输入文本 → 预处理 → 文本分块 → 向量化 → 元数据生成 → 输出结果
   ↓         ↓         ↓         ↓         ↓         ↓
文本内容   格式清理   智能分块   向量生成   标准元数据   最终结果
```

#### 2.2 处理阶段说明

**1. 预处理阶段**
- 文本编码检测和转换
- 特殊字符清理和标准化
- 空白字符规范化

**2. 文本分块阶段**
- 基于配置的chunk_size进行分块
- 保持句子和段落的完整性
- 支持chunk_overlap配置

**3. 向量化阶段**
- 通过ModelCaller调用文本嵌入模型
- 为每个文本块生成向量
- 错误处理和重试机制

**4. 元数据生成阶段**
- 严格按照设计文档的TEXT_METADATA_SCHEMA
- 生成标准化的元数据
- 包含向量化状态信息

### 3. 核心类设计

#### 3.1 TextProcessor主类

```python
class TextProcessor:
    """
    文本处理器主类
    
    功能：
    - 统一处理各种来源的文本内容
    - 基于配置的文本分块
    - 通过ModelCaller进行文本向量化
    - 生成标准化的元数据（使用TEXT_METADATA_SCHEMA）
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.model_caller = ModelCaller(config_manager)  # 使用统一的ModelCaller
        self._load_configuration()
    
    def process_text(self, text_content: str, source_type: str = None, 
                    metadata: Dict = None) -> List[Dict]:
        """
        处理文本内容
        
        :param text_content: 文本内容
        :param source_type: 文本来源类型
        :param metadata: 相关元数据
        :return: 处理结果列表（每个chunk一个元数据字典）
        """
        try:
            # 1. 预处理文本
            preprocessed_text = self._preprocess_text(text_content)
            
            # 2. 文本分块
            chunks = self._chunk_text(preprocessed_text)
            
            # 3. 文本向量化
            vectorized_chunks = self._vectorize_chunks(chunks)
            
            # 4. 生成标准化的元数据（使用TEXT_METADATA_SCHEMA）
            result_metadata = []
            for i, chunk in enumerate(vectorized_chunks):
                chunk_metadata = self._build_text_metadata(
                    chunk['text'], i, metadata, chunk
                )
                result_metadata.append(chunk_metadata)
            
            return result_metadata
            
        except Exception as e:
            logging.error(f"文本处理失败: {str(e)}")
            raise
    
    def process_batch(self, text_contents: List[Dict]) -> List[List[Dict]]:
        """
        批量处理文本内容
        
        :param text_contents: 文本内容列表，每个元素包含text和metadata
        :return: 处理结果列表（每个文本对应一个chunk元数据列表）
        """
        results = []
        
        for i, text_item in enumerate(text_contents):
            try:
                result = self.process_text(
                    text_content=text_item['text'],
                    source_type=text_item.get('source_type'),
                    metadata=text_item.get('metadata', {})
                )
                results.append(result)
                
            except Exception as e:
                logging.error(f"批量处理第{i+1}个文本失败: {str(e)}")
                # 记录失败信息，继续处理其他文本
                results.append([{
                    'chunk_id': f"text_error_{int(time.time())}_{i}",
                    'chunk_type': 'text',
                    'error': True,
                    'error_message': str(e),
                    'original_text': text_item.get('text', ''),
                    'metadata': text_item.get('metadata', {})
                }])
        
        return results
    
    def _preprocess_text(self, text: str) -> str:
        """预处理文本"""
        if not text:
            return ""
        
        # 1. 编码标准化
        text = self._normalize_encoding(text)
        
        # 2. 特殊字符清理
        text = self._clean_special_characters(text)
        
        # 3. 空白字符规范化
        text = self._normalize_whitespace(text)
        
        return text
    
    def _normalize_encoding(self, text: str) -> str:
        """编码标准化"""
        try:
            # 确保是UTF-8编码
            if not isinstance(text, str):
                text = str(text)
            
            # 处理常见的编码问题
            text = text.encode('utf-8', errors='ignore').decode('utf-8')
            
            return text
            
        except Exception as e:
            logging.warning(f"编码转换失败: {str(e)}")
            return text
    
    def _clean_special_characters(self, text: str) -> str:
        """清理特殊字符"""
        # 替换常见的特殊字符
        replacements = {
            '\u00a0': ' ',      # 不间断空格
            '\u200b': '',       # 零宽空格
            '\u200c': '',       # 零宽非连接符
            '\u200d': '',       # 零宽连接符
            '\u2060': '',       # 词连接符
            '\ufeff': '',       # 字节顺序标记
        }
        
        for old_char, new_char in replacements.items():
            text = text.replace(old_char, new_char)
        
        # 清理控制字符（保留换行符和制表符）
        text = ''.join(char for char in text if char.isprintable() or char in '\n\t')
        
        return text
    
    def _normalize_whitespace(self, text: str) -> str:
        """规范化空白字符"""
        # 将多个空白字符替换为单个空格
        text = re.sub(r'\s+', ' ', text)
        
        # 清理行首行尾空白
        text = text.strip()
        
        # 规范化换行符
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        # 清理多余的空行
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text
    
    def _chunk_text(self, text: str) -> List[Dict]:
        """文本分块"""
        chunks = []
        
        if not text:
            return chunks
        
        # 按句子分割
        sentences = self._split_sentences(text)
        
        current_chunk = ""
        chunk_index = 0
        
        for sentence in sentences:
            # 检查添加这个句子是否会超过最大块大小
            if len(current_chunk + sentence) > self.chunk_size:
                # 保存当前块
                if current_chunk.strip():
                    chunks.append(self._create_chunk_info(
                        current_chunk.strip(), chunk_index, len(chunks)
                    ))
                    chunk_index += 1
                
                # 开始新块
                current_chunk = sentence
            else:
                current_chunk += sentence
        
        # 添加最后一个块
        if current_chunk.strip():
            chunks.append(self._create_chunk_info(
                current_chunk.strip(), chunk_index, len(chunks)
            ))
        
        return chunks
    
    def _create_chunk_info(self, text: str, chunk_index: int, global_index: int) -> Dict:
        """创建分块信息"""
        return {
            'text': text,
            'chunk_index': chunk_index,
            'global_index': global_index,
            'text_length': len(text),
            'is_complete': True
        }
    
    def _split_sentences(self, text: str) -> List[str]:
        """分割句子"""
        # 中文句子分割
        sentences = re.split(r'([。！？；])', text)
        
        # 重新组合句子和标点
        result = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                sentence = sentences[i] + sentences[i + 1]
                if sentence.strip():
                    result.append(sentence.strip())
        
        # 处理最后一个句子（如果没有标点）
        if len(sentences) % 2 == 1 and sentences[-1].strip():
            result.append(sentences[-1].strip())
        
        return result
    
    def _vectorize_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """为文本块生成向量"""
        vectorized_chunks = []
        
        for i, chunk in enumerate(chunks):
            try:
                # 调用ModelCaller生成文本向量
                embedding = self.model_caller.call_text_embedding(
                    chunk['text'], 
                    metadata={'chunk_index': i, 'chunk_type': 'text'}
                )
                
                # 创建向量化的块
                vectorized_chunk = chunk.copy()
                vectorized_chunk['embedding'] = embedding
                vectorized_chunk['vectorization_status'] = 'completed'
                vectorized_chunk['vectorization_timestamp'] = time.time()
                # 使用ModelCaller的get_model_info方法获取模型信息
                model_info = self.model_caller.get_model_info('text_embedding')
                vectorized_chunk['embedding_model'] = model_info.get('model_name', 'unknown')
                
                vectorized_chunks.append(vectorized_chunk)
                
            except Exception as e:
                logging.error(f"文本块{i}向量化失败: {str(e)}")
                # 创建失败的块，包含错误信息
                failed_chunk = chunk.copy()
                failed_chunk['embedding'] = []
                failed_chunk['vectorization_status'] = 'failed'
                failed_chunk['vectorization_error'] = str(e)
                vectorized_chunks.append(failed_chunk)
        
        return vectorized_chunks
    
    def _build_text_metadata(self, text_content: str, chunk_index: int, 
                            metadata: Dict, chunk_info: Dict) -> Dict:
        """
        构建标准化的文本元数据（严格按照设计文档要求）
        
        :param text_content: 文本内容
        :param chunk_index: 分块索引
        :param metadata: 原始元数据
        :param chunk_info: 分块信息
        :return: 标准化的文本元数据（符合TEXT_METADATA_SCHEMA）
        """
        current_timestamp = int(time.time())
        
        # 使用设计文档中定义的TEXT_METADATA_SCHEMA结构
        result = {
            # 通用元数据字段（COMMON_METADATA_FIELDS）
            'chunk_id': f"text_{int(time.time())}_{chunk_index}_{uuid.uuid4().hex[:8]}",
            'chunk_type': 'text',
            'source_type': metadata.get('source_type', 'mineru_output'),
            'document_name': metadata.get('document_name', ''),
            'document_path': metadata.get('document_path', ''),
            'page_number': metadata.get('page_number', 1),
            'page_idx': metadata.get('page_idx', 0),
            'created_timestamp': current_timestamp,
            'updated_timestamp': current_timestamp,
            'processing_version': 'V3.0.0',
            'vectorized': chunk_info.get('vectorization_status') == 'completed',
            'vectorization_timestamp': chunk_info.get('vectorization_timestamp'),
            'embedding_model': chunk_info.get('embedding_model'),
            
            # 文本特有字段（TEXT_METADATA_SCHEMA）
            'text_content': text_content,
            'text_length': len(text_content),
            'chunk_size': len(text_content),
            'chunk_overlap': 0,  # 当前实现中不处理重叠
            'chunk_position': {
                'start': chunk_info.get('global_index', 0) * self.chunk_size,
                'end': chunk_info.get('global_index', 0) * self.chunk_size + len(text_content)
            },
            'related_images': metadata.get('related_images', []),
            'related_tables': metadata.get('related_tables', []),
            'parent_chunk_id': metadata.get('parent_chunk_id'),
            
            # 向量化信息
            'text_embedding': chunk_info.get('embedding', [])
        }
        
        return result
    
    def _load_configuration(self):
        """加载配置（使用配置管理文档中的配置结构）"""
        self.config = self.config_manager.get_config()
        
        # 使用配置管理文档中定义的配置结构
        doc_processing_config = self.config.get('document_processing', {})
        self.chunk_size = doc_processing_config.get('chunk_size', 1000)
        self.chunk_overlap = doc_processing_config.get('chunk_overlap', 200)
        
        # 获取路径配置
        paths_config = self.config.get('paths', {})
        self.input_dir = paths_config.get('input_pdf_dir', './document/orig_pdf')
        self.output_dir = paths_config.get('mineru_output_dir', './document/md')
        self.vector_db_dir = paths_config.get('vector_db_dir', './central/vector_db')
    
    def get_vectorization_status(self) -> Dict:
        """获取向量化状态信息"""
        return {
            'model_info': self.model_caller.get_model_info('text_embedding'),
            'statistics': self.model_caller.get_statistics(),
            'configuration': {
                'chunk_size': self.chunk_size,
                'chunk_overlap': self.chunk_overlap
            },
            'paths': {
                'input_dir': self.input_dir,
                'output_dir': self.output_dir,
                'vector_db_dir': self.vector_db_dir
            }
        }
```

### 4. 使用示例

#### 4.1 基本使用

```python
# 创建文本处理器
config_manager = ConfigManager()
text_processor = TextProcessor(config_manager)

# 处理单个文本
text_content = "这是一个测试文本。它包含多个句子。我们用它来测试文本处理功能。"
metadata = {
    'source_type': 'mineru_output',
    'document_name': 'test_document.pdf',
    'document_path': './document/md/test_document_1.json',
    'page_number': 1,
    'page_idx': 0,
    'related_images': ['image_001.jpg'],
    'related_tables': ['table_001']
}

result = text_processor.process_text(
    text_content=text_content,
    source_type="test",
    metadata=metadata
)

print(f"处理结果:")
print(f"  原始文本长度: {len(text_content)}")
print(f"  分块数量: {len(result)}")

# 显示分块结果
for i, chunk_metadata in enumerate(result):
    print(f"\n块 {i+1}:")
    print(f"  Chunk ID: {chunk_metadata['chunk_id']}")
    print(f"  文本: {chunk_metadata['text_content'][:100]}...")
    print(f"  长度: {chunk_metadata['text_length']}")
    print(f"  是否已向量化: {chunk_metadata['vectorized']}")
    print(f"  向量化模型: {chunk_metadata['embedding_model']}")
    if chunk_metadata.get('text_embedding'):
        print(f"  向量维度: {len(chunk_metadata['text_embedding'])}")
```

#### 4.2 批量处理

```python
# 批量处理文本
text_contents = [
    {
        'text': "第一个文本内容。包含一些信息。",
        'source_type': 'document1',
        'metadata': {
            'id': 1, 
            'title': '文档1',
            'source_type': 'mineru_output',
            'document_name': 'document1.pdf',
            'document_path': './document/md/document1_1.json',
            'page_number': 1,
            'page_idx': 0
        }
    },
    {
        'text': "第二个文本内容。包含其他信息。",
        'source_type': 'document2',
        'metadata': {
            'id': 2, 
            'title': '文档2',
            'source_type': 'mineru_output',
            'document_name': 'document2.pdf',
            'document_path': './document/md/document2_1.json',
            'page_number': 1,
            'page_idx': 0
        }
    }
]

results = text_processor.process_batch(text_contents)

for i, result in enumerate(results):
    if not result[0].get('error'):
        print(f"文档 {i+1} 处理成功:")
        print(f"  分块数量: {len(result)}")
        successful_chunks = [c for c in result if c.get('vectorized')]
        print(f"  成功向量化: {len(successful_chunks)}/{len(result)}")
    else:
        print(f"文档 {i+1} 处理失败: {result[0]['error_message']}")
```

#### 4.3 获取向量化状态

```python
# 查询向量化状态
vectorization_status = text_processor.get_vectorization_status()
print(f"\n=== 向量化状态信息 ===")
print(f"模型信息: {vectorization_status['model_info']}")
print(f"调用统计: {vectorization_status['statistics']}")
print(f"配置信息: {vectorization_status['configuration']}")
print(f"路径配置: {vectorization_status['paths']}")
```

### 5. 配置参数

#### 5.1 文本处理配置（使用配置管理文档中的结构）

TextProcessor只使用配置管理文档中定义的配置参数：

```json
{
  "document_processing": {
    "chunk_size": 1000,           // 文本分块大小
    "chunk_overlap": 200          // 分块重叠大小
  },
  "paths": {
    "input_pdf_dir": "./document/orig_pdf",
    "mineru_output_dir": "./document/md",
    "vector_db_dir": "./central/vector_db"
  }
}
```

### 6. 设计优势

#### 6.1 严格遵循设计文档要求
- **元数据结构**：完全使用设计文档中定义的 `TEXT_METADATA_SCHEMA`
- **通用字段**：包含所有必需的 `COMMON_METADATA_FIELDS`
- **字段命名**：与设计文档保持一致

#### 6.2 简化设计
- **专注核心功能**：只保留必要的文本处理功能
- **配置一致性**：只使用配置管理文档中定义的参数
- **移除过度功能**：删除复杂的质量分析、性能监控等

#### 6.3 标准化元数据
- **严格遵循设计文档**：使用设计文档要求的TEXT_METADATA_SCHEMA
- **包含所有必需字段**：确保元数据结构的一致性
- **与ModelCaller集成**：通过统一的ModelCaller接口进行向量化

#### 6.4 性能优化
- **批量处理支持**：支持批量文本处理
- **高效的向量化调用**：通过ModelCaller统一管理
- **内存使用优化**：避免不必要的内存占用

#### 6.5 错误处理
- **完善的错误处理机制**：与ModelCaller保持一致
- **失败重试支持**：通过ModelCaller的重试机制
- **详细的错误信息记录**：便于调试和问题排查

### 7. 与ModelCaller的集成

#### 7.1 接口设计

TextProcessor通过ModelCaller的统一接口进行文本向量化：

```python
# 在TextProcessor中使用ModelCaller
def _vectorize_chunks(self, chunks: List[Dict]) -> List[Dict]:
    """为文本块生成向量"""
    vectorized_chunks = []
    
    for i, chunk in enumerate(chunks):
        try:
            # 调用ModelCaller的统一接口
            embedding = self.model_caller.call_text_embedding(
                chunk['text'], 
                metadata={'chunk_index': i, 'chunk_type': 'text'}
            )
            
            # 创建向量化的块
            vectorized_chunk = chunk.copy()
            vectorized_chunk['embedding'] = embedding
            vectorized_chunk['vectorization_status'] = 'completed'
            vectorized_chunk['vectorization_timestamp'] = time.time()
            # 使用ModelCaller的get_model_info方法获取模型信息
            model_info = self.model_caller.get_model_info('text_embedding')
            vectorized_chunk['embedding_model'] = model_info.get('model_name', 'unknown')
            
            vectorized_chunks.append(vectorized_chunk)
            
        except Exception as e:
            logging.error(f"文本块{i}向量化失败: {str(e)}")
            # 创建失败的块，包含错误信息
            failed_chunk = chunk.copy()
            failed_chunk['embedding'] = []
            failed_chunk['vectorization_status'] = 'failed'
            failed_chunk['vectorization_error'] = str(e)
            vectorized_chunks.append(failed_chunk)
    
    return vectorized_chunks
```

#### 7.2 错误处理

TextProcessor与ModelCaller的错误处理机制保持一致：

```python
def _handle_vectorization_error(self, chunk: Dict, error: Exception) -> Dict:
    """处理向量化错误"""
    failed_chunk = chunk.copy()
    failed_chunk.update({
        'embedding': [],
        'vectorization_status': 'failed',
        'vectorization_error': str(error),
        'vectorization_timestamp': time.time()
    })
    
    # 记录错误信息
    logging.error(f"文本块向量化失败: {str(error)}")
    
    return failed_chunk
```

### 8. 与配置管理的集成

#### 8.1 路径配置

```python
# 使用配置管理文档中定义的路径配置
def _load_configuration(self):
    """加载配置（使用配置管理文档中的配置结构）"""
    self.config = self.config_manager.get_config()
    
    # 使用配置管理文档中定义的配置结构
    doc_processing_config = self.config.get('document_processing', {})
    self.chunk_size = doc_processing_config.get('chunk_size', 1000)
    self.chunk_overlap = doc_processing_config.get('chunk_overlap', 200)
    
    # 获取路径配置
    paths_config = self.config.get('paths', {})
    self.input_dir = paths_config.get('input_pdf_dir', './document/orig_pdf')
    self.output_dir = paths_config.get('mineru_output_dir', './document/md')
    self.vector_db_dir = paths_config.get('vector_db_dir', './central/vector_db')
```

#### 8.2 处理参数配置

```python
# 使用配置管理文档中定义的文档处理配置
doc_processing_config = self.config.get('document_processing', {})
self.chunk_size = doc_processing_config.get('chunk_size', 1000)
self.chunk_overlap = doc_processing_config.get('chunk_overlap', 200)
```

## 主要修改点

1. **修正元数据结构**：完全按照设计文档的 `TEXT_METADATA_SCHEMA` 和 `COMMON_METADATA_FIELDS` 实现
2. **简化配置参数**：只使用配置管理文档中定义的配置结构，移除过度配置
3. **完善ModelCaller集成**：正确使用 `get_model_info` 方法，添加配置验证
4. **修正元数据生成方法**：完全按照设计文档要求实现 `_build_text_metadata`
5. **添加路径配置支持**：使用配置管理文档中定义的路径配置
6. **统一错误处理**：与ModelCaller的错误处理机制保持一致

这个修正版的TextProcessor设计文档确保了：

1. **元数据结构**：完全按照设计文档的 `TEXT_METADATA_SCHEMA` 和 `COMMON_METADATA_FIELDS` 实现
2. **配置管理**：使用配置管理文档中定义的配置结构，避免过度配置
3. **ModelCaller集成**：正确使用ModelCaller提供的方法，包括配置验证和模型信息获取
4. **路径配置**：使用配置管理文档中定义的路径配置
5. **错误处理**：与ModelCaller的错误处理机制保持一致