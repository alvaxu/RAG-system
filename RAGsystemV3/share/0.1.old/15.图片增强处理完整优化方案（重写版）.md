# **å›¾ç‰‡å¢å¼ºå¤„ç†å®Œæ•´ä¼˜åŒ–æ–¹æ¡ˆï¼ˆé‡å†™ç‰ˆï¼‰**

## **ï¿½ï¿½ æ–¹æ¡ˆæ¦‚è¿°**

é‡æ–°è®¾è®¡å›¾ç‰‡å¢å¼ºå¤„ç†æµç¨‹ï¼Œå®ç°ï¼š
1. **ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å¢å¼ºä¿¡æ¯**ï¼Œé¿å…å¤šå±‚å¤„ç†å¯¼è‡´çš„é‡å¤
2. **æ™ºèƒ½å»é‡æœºåˆ¶**ï¼Œç¡®ä¿ä¿¡æ¯è´¨é‡å’Œä¸€è‡´æ€§
3. **å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ**ï¼Œä½¿ç”¨æ ‡å‡†åŒ–çš„ç±»åå’Œæ¨¡å—ç»“æ„
4. **ä¿ç•™ä¸Šä¸€ç‰ˆæœ¬ä¼˜ç§€è®¾è®¡**ï¼ŒåŒ…æ‹¬åˆ†å±‚æè¿°å’ŒåŒé‡å‘é‡åŒ–
5. **å…ƒæ•°æ®å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ**ï¼Œéµå¾ªIMAGE_METADATA_SCHEMA

**é‡è¦è¯´æ˜**ï¼šæœ¬è®¾è®¡æ–‡æ¡£å·²æ ¹æ®MinerU JSONè¾“å‡ºçš„å®é™…ç»“æ„è¿›è¡Œäº†æ›´æ–°ï¼Œä¸»è¦å­—æ®µæ˜ å°„å¦‚ä¸‹ï¼š

**è¡¨æ ¼ç±»å‹**ï¼š
- `table_body`ï¼šè¡¨æ ¼çš„HTMLå†…å®¹ï¼ˆç”¨äºå‘é‡åŒ–å’Œwebå±•ç°ï¼‰
- `table_caption`ï¼šè¡¨æ ¼æ ‡é¢˜æ•°ç»„
- `table_footnote`ï¼šè¡¨æ ¼è„šæ³¨æ•°ç»„

**å›¾ç‰‡ç±»å‹**ï¼š
- `img_path`ï¼šå›¾ç‰‡åœ¨imagesç›®å½•ä¸‹çš„ç›¸å¯¹è·¯å¾„
- `img_caption`ï¼šå›¾ç‰‡æ ‡é¢˜æ•°ç»„
- `img_footnote`ï¼šå›¾ç‰‡è„šæ³¨æ•°ç»„

**æ–‡æœ¬ç±»å‹**ï¼š
- `text`ï¼šæ–‡æœ¬å†…å®¹
- `text_level`ï¼šæ ‡é¢˜çº§åˆ«

---

## **ï¿½ï¿½ ç¬¬ä¸€éƒ¨åˆ†ï¼šå›¾ç‰‡å¢å¼ºå¤„ç†å™¨ï¼ˆImageEnhancerï¼‰**

### **1.1 æ ¸å¿ƒç±»è®¾è®¡**

```python
class ImageEnhancer:
    """
    å›¾ç‰‡å¢å¼ºå¤„ç†å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å¢å¼ºä¿¡æ¯ï¼Œé¿å…é‡å¤å†…å®¹
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼Œä½äºprocessorsæ¨¡å—ä¸‹
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # ä½¿ç”¨é…ç½®ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.enhancement_model = self.config.get('image_processing.enhancement_model', 'qwen-vl-plus')
        self.enhancement_model_api = self.config.get('image_processing.enhancement_model_api', 'dashscope')
        
        # ä½¿ç”¨é…ç½®ç®¡ç†æ–‡æ¡£ä¸­å®é™…å­˜åœ¨çš„é…ç½®é”®å
        self.batch_size = self.config.get('api_rate_limiting.enhancement_batch_size', 5)
        self.delay_seconds = self.config.get('api_rate_limiting.enhancement_delay_seconds', 2)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.failure_handler = config_manager.get_failure_handler()
        
        # åˆå§‹åŒ–APIå¯†é’¥
        self.dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')
        if not self.dashscope_api_key:
            raise ValueError("æœªè®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
        
        # åˆå§‹åŒ–DashScope
        import dashscope
        dashscope.api_key = self.dashscope_api_key
        
        # åŠ è½½å¤„ç†æ ‡è®°é…ç½®ï¼ˆä½¿ç”¨ç®€å•çš„é»˜è®¤å€¼ï¼Œä¸ä¾èµ–å¤æ‚é…ç½®ï¼‰
        self._load_processing_markers()
        
        logging.info("å›¾ç‰‡å¢å¼ºå¤„ç†å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰åˆå§‹åŒ–å®Œæˆ")
    
    def _load_processing_markers(self):
        """
        ä»é…ç½®åŠ è½½å¤„ç†æ ‡è®°ï¼ˆä½¿ç”¨é…ç½®ç®¡ç†æ–‡æ¡£ä¸­å­˜åœ¨çš„é…ç½®ï¼‰
        """
        # ä½¿ç”¨é…ç½®ç®¡ç†æ–‡æ¡£ä¸­å®é™…å­˜åœ¨çš„é…ç½®
        self.enhancement_model = self.config.get('image_processing.enhancement_model', 'qwen-vl-plus')
        self.enhancement_model_api = self.config.get('image_processing.enhancement_model_api', 'dashscope')
        
        # ä½¿ç”¨ç®€å•çš„é»˜è®¤å€¼ï¼Œä¸ä¾èµ–å¤æ‚çš„é…ç½®ç»“æ„
        self.batch_size = self.config.get('api_rate_limiting.enhancement_batch_size', 5)
        self.delay_seconds = self.config.get('api_rate_limiting.enhancement_delay_seconds', 2)
        
        # é»˜è®¤æ ‡è®°é…ç½®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        default_markers = {
            'layer_markers': [
                'åŸºç¡€è§†è§‰æè¿°', 'å†…å®¹ç†è§£æè¿°', 'æ•°æ®è¶‹åŠ¿æè¿°', 'è¯­ä¹‰ç‰¹å¾æè¿°'
            ],
            'structure_markers': [
                'å›¾è¡¨ç±»å‹', 'æ•°æ®ç‚¹', 'è¶‹åŠ¿åˆ†æ', 'å…³é”®æ´å¯Ÿ'
            ],
            'format_variants': [
                '**', '-', 'ï¼š', ':'
            ]
        }
        
        # åŠ¨æ€ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æ ‡è®°ç»„åˆ
        self.all_markers = self._generate_marker_combinations(default_markers)
    
    def _generate_marker_combinations(self, marker_config: Dict) -> List[str]:
        """
        åŠ¨æ€ç”Ÿæˆæ ‡è®°ç»„åˆï¼ˆä¿ç•™ä¸Šä¸€ç‰ˆæœ¬çš„ä¼˜ç§€è®¾è®¡ï¼‰
        """
        all_markers = []
        
        # ä¸ºæ¯ä¸ªæ ‡è®°ç”Ÿæˆæ‰€æœ‰æ ¼å¼å˜ä½“
        for marker in marker_config.get('layer_markers', []):
            for variant in marker_config.get('format_variants', []):
                all_markers.append(f"{variant}{marker}{variant}")
                all_markers.append(f"{variant}{marker}")
                all_markers.append(f"{marker}{variant}")
                all_markers.append(marker)
        
        # æ·»åŠ ç»“æ„æ ‡è®°
        for marker in marker_config.get('structure_markers', []):
            for variant in marker_config.get('format_variants', []):
                all_markers.append(f"{variant}{marker}{variant}")
                all_markers.append(f"{variant}{marker}")
                all_markers.append(f"{marker}{variant}")
                all_markers.append(marker)
        
        # å»é‡å¹¶æ’åºï¼ˆæŒ‰é•¿åº¦é™åºï¼Œç¡®ä¿é•¿æ ‡è®°ä¼˜å…ˆåŒ¹é…ï¼‰
        all_markers = sorted(list(set(all_markers)), key=len, reverse=True)
        
        return all_markers
```

### **1.2 ä¸€æ¬¡æ€§å¢å¼ºæ–¹æ³•**

```python
def enhance_image_complete(self, image_path: str, mineru_info: Dict) -> Dict[str, Any]:
    """
    ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´çš„å›¾ç‰‡å¢å¼ºä¿¡æ¯ï¼Œé¿å…é‡å¤
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£çš„IMAGE_METADATA_SCHEMAè§„èŒƒ
    """
    try:
        # 1. è·å–MinerUåŸå§‹ä¿¡æ¯
        img_caption = mineru_info.get('img_caption', [])
        img_footnote = mineru_info.get('img_footnote', [])
        
        # 2. è°ƒç”¨è§†è§‰æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ
        vision_response = self._call_vision_model(image_path)
        if not vision_response:
            # å¦‚æœè§†è§‰æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯
            return self._create_fallback_description(img_caption, img_footnote)
        
        # 3. æ™ºèƒ½ç”Ÿæˆå®Œæ•´æè¿°ï¼ˆé¿å…é‡å¤ï¼‰
        complete_description = self._generate_complete_description(
            img_caption, img_footnote, vision_response
        )
        
        # 4. æå–åˆ†å±‚æè¿°å’Œç»“æ„åŒ–ä¿¡æ¯
        layered_descriptions = self._extract_layered_descriptions(vision_response)
        structured_info = self._extract_structured_info(vision_response)
        
        # 5. è¿”å›å®Œæ•´ç»“æœï¼ˆç¬¦åˆIMAGE_METADATA_SCHEMAï¼‰
        return {
            'enhanced_description': complete_description,
            'layered_descriptions': layered_descriptions,
            'structured_info': structured_info,
            'enhancement_timestamp': int(time.time()),
            'enhancement_status': 'success',
            'enhancement_model': self.enhancement_model,
            'enhancement_api': self.enhancement_model_api,
            'mineru_original': {
                'img_caption': img_caption,
                'img_footnote': img_footnote,
                'img_path': mineru_info.get('img_path', '')
            },
            'vision_analysis': {
                'raw_response': vision_response,
                'analysis_timestamp': int(time.time())
            }
        }
        
    except Exception as e:
        logging.error(f"å›¾ç‰‡å¢å¼ºå¤±è´¥: {e}")
        return self._create_fallback_description(img_caption, img_footnote, str(e))

def _create_fallback_description(self, img_caption: List[str], img_footnote: List[str], error: str = None) -> Dict[str, Any]:
    """
    åˆ›å»ºå›é€€æè¿°ï¼ˆå½“å¢å¼ºå¤±è´¥æ—¶ï¼‰
    """
    description_parts = []
    
    if img_caption:
        description_parts.append(' '.join(img_caption))
    if img_footnote:
        description_parts.append(' '.join(img_footnote))
    
    fallback_description = ' | '.join(description_parts) if description_parts else 'å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥'
    
    return {
        'enhanced_description': fallback_description,
        'layered_descriptions': {},
        'structured_info': {},
        'enhancement_timestamp': int(time.time()),
        'enhancement_status': 'failed',
        'enhancement_error': error or 'è§†è§‰æ¨¡å‹è°ƒç”¨å¤±è´¥',
        'enhancement_model': self.enhancement_model,
        'enhancement_api': self.enhancement_model_api,
        'mineru_original': {
            'img_caption': img_caption,
            'img_footnote': img_footnote,
            'img_path': ''
        }
    }
```

---

## **ğŸ”§ ç¬¬äºŒéƒ¨åˆ†ï¼šå›¾ç‰‡å‘é‡åŒ–å™¨ï¼ˆImageVectorizerï¼‰**

### **2.1 æ ¸å¿ƒç±»è®¾è®¡**

```python
class ImageVectorizer:
    """
    å›¾ç‰‡å‘é‡åŒ–å™¨
    å®ç°åŒé‡embeddingç­–ç•¥ï¼šè§†è§‰embedding + è¯­ä¹‰embedding
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼Œä½äºvectorizationæ¨¡å—ä¸‹
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # ä½¿ç”¨é…ç½®ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.image_embedding_model = self.config.get('vectorization.image_embedding_model', 'multimodal-embedding-one-peace-v1')
        self.text_embedding_model = self.config.get('vectorization.text_embedding_model', 'text-embedding-v1')
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.failure_handler = config_manager.get_failure_handler()
        
        # åˆå§‹åŒ–ModelCaller
        self.model_caller = ModelCaller(config_manager)
        
        logging.info("å›¾ç‰‡å‘é‡åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def vectorize_image(self, image_path: str, enhanced_description: str) -> Dict[str, Any]:
        """
        å¯¹å•å¼ å›¾ç‰‡è¿›è¡ŒåŒé‡å‘é‡åŒ–
        å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£çš„IMAGE_METADATA_SCHEMAè§„èŒƒ
        """
        try:
            # 1. è§†è§‰å‘é‡åŒ–ï¼ˆä½¿ç”¨One_Peaceæ¨¡å‹ï¼‰
            image_vector = self.model_caller.call_visual_embedding(image_path)
            
            # 2. è¯­ä¹‰å‘é‡åŒ–ï¼ˆä½¿ç”¨text-embeddingæ¨¡å‹ï¼‰
            description_vector = self.model_caller.call_text_embedding(enhanced_description)
            
            return {
                # åŒé‡embeddingå­—æ®µï¼ˆç¬¦åˆIMAGE_METADATA_SCHEMAï¼‰
                'image_embedding': image_vector,
                'description_embedding': description_vector,
                'image_embedding_model': self.image_embedding_model,
                'description_embedding_model': self.text_embedding_model,
                
                # å‘é‡åŒ–çŠ¶æ€ä¿¡æ¯
                'vectorization_status': 'success',
                'vectorization_timestamp': int(time.time()),
                'vector_dimensions': {
                    'image': len(image_vector) if image_vector else 0,
                    'description': len(description_vector) if description_vector else 0
                }
            }
            
        except Exception as e:
            logging.error(f"å›¾ç‰‡åŒé‡å‘é‡åŒ–å¤±è´¥: {e}")
            return {
                'vectorization_status': 'failed',
                'vectorization_error': str(e),
                'image_embedding': None,
                'description_embedding': None
            }
    
    def vectorize_images_batch(self, images: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡å‘é‡åŒ–å›¾ç‰‡
        """
        vectorized_images = []
        
        for i, image in enumerate(images):
            try:
                print(f"  æ­£åœ¨å‘é‡åŒ–å›¾ç‰‡ {i+1}/{len(images)}: {os.path.basename(image.get('image_path', ''))}")
                
                # åŒé‡å‘é‡åŒ–
                vectorization_result = self.vectorize_image(
                    image.get('image_path', ''),
                    image.get('enhanced_description', '')
                )
                
                # æ›´æ–°å›¾ç‰‡ä¿¡æ¯
                image.update(vectorization_result)
                vectorized_images.append(image)
                
                print(f"  âœ… å›¾ç‰‡å‘é‡åŒ–å®Œæˆ: {os.path.basename(image.get('image_path', ''))}")
                
            except Exception as e:
                error_msg = f"å›¾ç‰‡å‘é‡åŒ–å¤±è´¥: {os.path.basename(image.get('image_path', ''))}, é”™è¯¯: {e}"
                print(f"  âš ï¸ {error_msg}")
                
                # è®°å½•å¤±è´¥
                self.failure_handler.record_failure(
                    image, 'image_vectorization', str(e)
                )
                
                # æ ‡è®°å¤±è´¥çŠ¶æ€
                image['vectorization_status'] = 'failed'
                image['vectorization_error'] = str(e)
                vectorized_images.append(image)
        
        return vectorized_images
```

---

## **ğŸ”§ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå›¾ç‰‡å¤„ç†å™¨ï¼ˆImageProcessorï¼‰**

### **3.1 æ ¸å¿ƒç±»è®¾è®¡**

```python
class ImageProcessor:
    """
    å›¾ç‰‡å¤„ç†å™¨
    æ•´åˆï¼šå¤åˆ¶ â†’ å¢å¼º â†’ å‘é‡åŒ– â†’ å­˜å‚¨
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼Œä½äºprocessorsæ¨¡å—ä¸‹
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.image_enhancer = ImageEnhancer(config_manager)
        self.image_vectorizer = ImageVectorizer(config_manager)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("å›¾ç‰‡å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_images(self, images: List[Dict]) -> List[Dict]:
        """
        å®Œæ•´çš„å›¾ç‰‡å¤„ç†æµç¨‹
        """
        try:
            print(f" å¼€å§‹å¤„ç† {len(images)} å¼ å›¾ç‰‡...")
            
            # æ­¥éª¤1: å›¾ç‰‡å¤åˆ¶åˆ°æœ€ç»ˆç›®å½•
            print("æ­¥éª¤1: å›¾ç‰‡å¤åˆ¶...")
            copied_images = self._copy_images_to_final_dir(images)
            success_count = sum(1 for img in copied_images if img.get('copy_status') == 'success')
            print(f"âœ… å›¾ç‰‡å¤åˆ¶å®Œæˆ: {success_count}/{len(images)} æˆåŠŸ")
            
            # æ­¥éª¤2: ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å¢å¼ºä¿¡æ¯ï¼ˆé¿å…é‡å¤ï¼‰
            print("æ­¥éª¤2: å›¾ç‰‡å¢å¼ºæè¿°...")
            enhanced_images = []
            for i, image in enumerate(copied_images):
                if image.get('copy_status') == 'success':
                    print(f"  ï¿½ï¿½ï¸ å¢å¼ºå›¾ç‰‡ {i+1}/{len(copied_images)}: {os.path.basename(image.get('final_image_path', ''))}")
                    
                    # ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å¢å¼ºä¿¡æ¯
                    enhancement_result = self.image_enhancer.enhance_image_complete(
                        image.get('final_image_path', ''),
                        {
                            'img_caption': image.get('img_caption', []),
                            'img_footnote': image.get('img_footnote', []),
                            'img_path': image.get('img_path', '')
                        }
                    )
                    
                    # æ›´æ–°å›¾ç‰‡ä¿¡æ¯
                    image.update(enhancement_result)
                    enhanced_images.append(image)
                    
                    print(f"  âœ… å›¾ç‰‡å¢å¼ºå®Œæˆ: {os.path.basename(image.get('final_image_path', ''))}")
                else:
                    enhanced_images.append(image)
            
            success_count = sum(1 for img in enhanced_images if img.get('enhancement_status') == 'success')
            print(f"âœ… å›¾ç‰‡å¢å¼ºå®Œæˆ: {success_count}/{len(images)} æˆåŠŸ")
            
            # æ­¥éª¤3: å›¾ç‰‡åŒé‡å‘é‡åŒ–
            print("æ­¥éª¤3: å›¾ç‰‡åŒé‡å‘é‡åŒ–...")
            vectorized_images = self.image_vectorizer.vectorize_images_batch(enhanced_images)
            success_count = sum(1 for img in vectorized_images if img.get('vectorization_status') == 'success')
            print(f"âœ… å›¾ç‰‡å‘é‡åŒ–å®Œæˆ: {success_count}/{len(images)} æˆåŠŸ")
            
            # æ­¥éª¤4: ç”Ÿæˆå®Œæ•´å…ƒæ•°æ®
            print("æ­¥éª¤4: ç”Ÿæˆå®Œæ•´å…ƒæ•°æ®...")
            final_images = []
            for image in vectorized_images:
                complete_metadata = self._create_complete_image_metadata(image)
                final_images.append(complete_metadata)
            
            print(f"âœ… å›¾ç‰‡å¤„ç†æµç¨‹å®Œæˆ: {len(final_images)} å¼ å›¾ç‰‡")
            return final_images
            
        except Exception as e:
            error_msg = f"å›¾ç‰‡å¤„ç†æµç¨‹å¤±è´¥: {e}"
            logging.error(error_msg)
            self.failure_handler.record_failure('image_pipeline', 'image_processing_pipeline', str(e))
            raise RuntimeError(error_msg)
    
    def _copy_images_to_final_dir(self, images: List[Dict]) -> List[Dict]:
        """
        å°†å›¾ç‰‡å¤åˆ¶åˆ°æœ€ç»ˆç›®å½•
        """
        copied_images = []
        
        for image in images:
            try:
                source_path = image.get('source_image_path', '')
                target_path = image.get('final_image_path', '')
                
                if os.path.exists(source_path):
                    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(target_path), exist_ok=True)
                    
                    # å¤åˆ¶å›¾ç‰‡
                    shutil.copy2(source_path, target_path)
                    
                    # æ›´æ–°å›¾ç‰‡ä¿¡æ¯
                    image['copy_status'] = 'success'
                    image['final_image_path'] = target_path
                    image['image_size'] = os.path.getsize(target_path)
                    
                    # è·å–å›¾ç‰‡å°ºå¯¸
                    image['image_dimensions'] = self._get_image_dimensions(target_path)
                    
                    copied_images.append(image)
                    logging.info(f"å›¾ç‰‡å¤åˆ¶æˆåŠŸ: {os.path.basename(source_path)}")
                else:
                    image['copy_status'] = 'failed'
                    image['error'] = 'æºæ–‡ä»¶ä¸å­˜åœ¨'
                    self.failure_handler.record_failure(source_path, 'image_copy', 'æºæ–‡ä»¶ä¸å­˜åœ¨')
                    
            except Exception as e:
                image['copy_status'] = 'failed'
                image['error'] = str(e)
                self.failure_handler.record_failure(source_path, 'image_copy', str(e))
                logging.error(f"å›¾ç‰‡å¤åˆ¶å¤±è´¥: {source_path}, é”™è¯¯: {e}")
        
        return copied_images
    
    def _get_image_dimensions(self, image_path: str) -> Dict[str, int]:
        """è·å–å›¾ç‰‡å°ºå¯¸"""
        try:
            from PIL import Image
            with Image.open(image_path) as img:
                return {
                    'width': img.width,
                    'height': img.height
                }
        except Exception as e:
            logging.warning(f"è·å–å›¾ç‰‡å°ºå¯¸å¤±è´¥: {e}")
            return {'width': 0, 'height': 0}
    
    def _create_complete_image_metadata(self, image: Dict) -> Dict[str, Any]:
        """
        åˆ›å»ºå®Œæ•´çš„å›¾ç‰‡å…ƒæ•°æ®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£çš„IMAGE_METADATA_SCHEMAè§„èŒƒ
        """
        return {
            # åŸºç¡€æ ‡è¯†å­—æ®µï¼ˆç¬¦åˆCOMMON_METADATA_FIELDSï¼‰
            'chunk_id': image.get('chunk_id', ''),
            'chunk_type': 'image',
            'source_type': 'pdf',
            'document_name': image.get('document_name', ''),
            'document_path': image.get('document_path', ''),
            'page_number': image.get('page_number', 1),
            'page_idx': image.get('page_idx', 1),
            'created_timestamp': image.get('created_timestamp', int(time.time())),
            'updated_timestamp': int(time.time()),
            'processing_version': '3.0.0',
            
            # å‘é‡åŒ–ä¿¡æ¯å­—æ®µ
            'vectorized': image.get('vectorization_status') == 'success',
            'vectorization_timestamp': image.get('vectorization_timestamp'),
            'embedding_model': f"{image.get('image_embedding_model', '')}+{image.get('description_embedding_model', '')}" if image.get('image_embedding_model') and image.get('description_embedding_model') else None,
            
            # å›¾ç‰‡ç‰¹æœ‰å­—æ®µï¼ˆç¬¦åˆIMAGE_METADATA_SCHEMAï¼‰
            'image_id': image.get('image_id', ''),
            'image_path': image.get('final_image_path', ''),
            'image_filename': image.get('image_filename', ''),
            'image_type': image.get('image_type', 'general'),
            'image_format': image.get('image_format', 'UNKNOWN'),
            'image_dimensions': image.get('image_dimensions', {'width': 0, 'height': 0}),
            
            # å†…å®¹æè¿°å­—æ®µï¼ˆä¿ç•™ç°æœ‰ç³»ç»Ÿçš„ä¼˜ç§€éƒ¨åˆ†ï¼‰
            'basic_description': image.get('basic_description', ''),
            'enhanced_description': image.get('enhanced_description', ''),
            'layered_descriptions': image.get('layered_descriptions', {}),
            'structured_info': image.get('structured_info', {}),
            
            # å›¾ç‰‡æ ‡é¢˜å’Œè„šæ³¨ï¼ˆä¿ç•™ç°æœ‰ç³»ç»Ÿçš„ä¼˜ç§€éƒ¨åˆ†ï¼‰
            'img_caption': image.get('img_caption', []),
            'img_footnote': image.get('img_footnote', []),
            
            # å¢å¼ºå¤„ç†å­—æ®µï¼ˆæ”¯æŒå¤±è´¥å¤„ç†å’Œè¡¥åšï¼‰
            'enhancement_enabled': image.get('enhancement_enabled', True),
            'enhancement_model': image.get('enhancement_model', ''),
            'enhancement_status': image.get('enhancement_status', 'unknown'),
            'enhancement_timestamp': image.get('enhancement_timestamp'),
            'enhancement_error': image.get('enhancement_error', ''),
            
            # åŒé‡embeddingå­—æ®µï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
            'image_embedding': image.get('image_embedding', []),
            'description_embedding': image.get('description_embedding', []),
            'image_embedding_model': image.get('image_embedding_model', ''),
            'description_embedding_model': image.get('description_embedding_model', ''),
            
            # å…³è”ä¿¡æ¯å­—æ®µ
            'related_text_chunks': image.get('related_text_chunks', []),
            'related_table_chunks': image.get('related_table_chunks', []),
            'parent_document_id': image.get('parent_document_id', ''),
            
            # å¤„ç†çŠ¶æ€ä¿¡æ¯
            'copy_status': image.get('copy_status', 'unknown'),
            'enhancement_status': image.get('enhancement_status', 'unknown'),
            'vectorization_status': image.get('vectorization_status', 'unknown'),
            
            # åŸå§‹ä¿¡æ¯
            'mineru_original': image.get('mineru_original', {}),
            'vision_analysis': image.get('vision_analysis', {}),
            
            # æ¶æ„æ ‡è¯†
            'metadata_schema': 'IMAGE_METADATA_SCHEMA',
            'metadata_version': '3.0.0',
            'processing_pipeline': 'MinerU_Enhancement_Pipeline',
            'optimization_features': [
                'one_time_enhancement',
                'smart_deduplication',
                'complete_metadata',
                'dual_vectorization'
            ]
        }
```

---

## **ğŸ”§ ç¬¬å››éƒ¨åˆ†ï¼šå†…å®¹å…ƒæ•°æ®æå–å™¨ï¼ˆContentMetadataExtractorï¼‰**

### **4.1 æ ¸å¿ƒç±»è®¾è®¡**

```python
class ContentMetadataExtractor:
    """
    å†…å®¹å…ƒæ•°æ®æå–å™¨
    åŸºäºMinerUè§£æçš„JSONæ–‡ä»¶æå–textã€tableã€imageçš„å…ƒæ•°æ®
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£çš„å…ƒæ•°æ®è§„èŒƒ
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # ä½¿ç”¨é…ç½®ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.chunk_size = self.config.get('document_processing.chunk_size', 1000)
        self.chunk_overlap = self.config.get('document_processing.chunk_overlap', 200)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("å†…å®¹å…ƒæ•°æ®æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_metadata_from_json(self, json_path: str, doc_name: str) -> Dict[str, Any]:
        """
        ä»JSONæ–‡ä»¶æå–å…ƒæ•°æ®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        """
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æå–æ–‡æœ¬å—
            text_chunks = self._extract_text_chunks(data, doc_name)
            
            # æå–è¡¨æ ¼ä¿¡æ¯
            tables = self._extract_table_info(data, doc_name)
            
            # æå–å›¾ç‰‡ä¿¡æ¯
            images = self._extract_image_info(data, doc_name)
            
            return {
                'text_chunks': text_chunks,
                'tables': tables,
                'images': images,
                'document_name': doc_name,
                'total_items': len(data)
            }
            
        except Exception as e:
            self.failure_handler.record_failure(json_path, 'metadata_extraction', str(e))
            logging.error(f"å…ƒæ•°æ®æå–å¤±è´¥: {json_path}, é”™è¯¯: {e}")
            return {'text_chunks': [], 'tables': [], 'images': []}
    
    def _extract_text_chunks(self, data: List[Dict], doc_name: str) -> List[Dict]:
        """
        æå–æ–‡æœ¬å—ï¼Œå®Œå…¨ç¬¦åˆTEXT_METADATA_SCHEMAè§„èŒƒ
        """
        text_chunks = []
        chunk_index = 0
        
        for item in data:
            if item.get('type') == 'text':
                # è·å–æ–‡æœ¬å†…å®¹
                text_content = item.get('content', '')
                if not text_content.strip():
                    continue
                
                # æ™ºèƒ½åˆ†å—å¤„ç†
                chunks = self._smart_text_chunking(text_content, chunk_index)
                
                for i, chunk_content in enumerate(chunks):
                    chunk = {
                        # åŸºç¡€æ ‡è¯†å­—æ®µï¼ˆç¬¦åˆCOMMON_METADATA_FIELDSï¼‰
                        'chunk_id': f"{doc_name}_text_{chunk_index}_{i}",
                        'chunk_type': 'text',
                        'source_type': 'pdf',
                        'document_name': doc_name,
                        'document_path': f"{doc_name}.pdf",
                        'page_number': item.get('page_idx', 1),
                        'page_idx': item.get('page_idx', 1),
                        'created_timestamp': int(time.time()),
                        'updated_timestamp': int(time.time()),
                        'processing_version': '3.0.0',
                        
                        # å‘é‡åŒ–ä¿¡æ¯å­—æ®µ
                        'vectorized': False,
                        'vectorization_timestamp': None,
                        'embedding_model': None,
                        
                        # æ–‡æœ¬ç‰¹æœ‰å­—æ®µï¼ˆç¬¦åˆTEXT_METADATA_SCHEMAï¼‰
                        'text_content': chunk_content,
                        'text_length': len(chunk_content),
                        'chunk_size': len(chunk_content),
                        'chunk_overlap': 0,
                        'chunk_position': {
                            'start_char': i * self.chunk_size,
                            'end_char': min((i + 1) * self.chunk_size, len(text_content)),
                            'chunk_index': i,
                            'total_chunks': len(chunks)
                        },
                        
                        # å…³è”ä¿¡æ¯å­—æ®µ
                        'related_images': [],
                        'related_tables': [],
                        'parent_chunk_id': None
                    }
                    
                    text_chunks.append(chunk)
                    chunk_index += 1
        
        return text_chunks
    
    def _extract_table_info(self, data: List[Dict], doc_name: str) -> List[Dict]:
        """
        æå–è¡¨æ ¼ä¿¡æ¯ï¼Œå®Œå…¨ç¬¦åˆTABLE_METADATA_SCHEMAè§„èŒƒ
        """
        tables = []
        table_index = 0
        
        for item in data:
            if item.get('type') == 'table':
                # è·å–è¡¨æ ¼å†…å®¹ï¼ˆæ ¹æ®MinerU JSONå®é™…ç»“æ„ï¼‰
                table_body = item.get('table_body', '')
                if not table_body.strip():
                    continue
                
                # åˆ†æè¡¨æ ¼ç»“æ„
                table_structure = self._analyze_table_structure(table_body)
                
                # æ™ºèƒ½åˆ†å—å¤„ç†ï¼ˆå¤§è¡¨æ ¼åˆ†å—ï¼‰
                table_chunks = self._smart_table_chunking(table_body, table_structure)
                
                for i, chunk_content in enumerate(table_chunks):
                    table = {
                        # åŸºç¡€æ ‡è¯†å­—æ®µï¼ˆç¬¦åˆCOMMON_METADATA_FIELDSï¼‰
                        'chunk_id': f"{doc_name}_table_{table_index}_{i}",
                        'chunk_type': 'table',
                        'source_type': 'pdf',
                        'document_name': doc_name,
                        'document_path': f"{doc_name}.pdf",
                        'page_number': item.get('page_idx', 1),
                        'page_idx': item.get('page_idx', 1),
                        'created_timestamp': int(time.time()),
                        'updated_timestamp': int(time.time()),
                        'processing_version': '3.0.0',
                        
                        # å‘é‡åŒ–ä¿¡æ¯å­—æ®µ
                        'vectorized': False,
                        'vectorization_timestamp': None,
                        'embedding_model': None,
                        
                        # è¡¨æ ¼ç‰¹æœ‰å­—æ®µï¼ˆç¬¦åˆTABLE_METADATA_SCHEMAï¼‰
                        'table_id': f"{doc_name}_table_{table_index}_{i}",
                        'table_type': 'data_table',
                        'table_rows': table_structure.get('rows', 0),
                        'table_columns': table_structure.get('columns', 0),
                        'table_headers': table_structure.get('headers', []),
                        'table_title': item.get('table_title', ''),
                        'table_summary': self._generate_table_summary(chunk_content),
                        
                        # å†…å®¹å­—æ®µï¼ˆæ ¹æ®MinerU JSONå®é™…ç»“æ„ï¼‰
                        'table_body': chunk_content,
                        'table_caption': item.get('table_caption', []),
                        'table_footnote': item.get('table_footnote', []),
                        
                        # åˆ†å—ä¿¡æ¯å­—æ®µï¼ˆæ”¯æŒå¤§è¡¨æ ¼åˆ†å—ï¼‰
                        'is_subtable': len(table_chunks) > 1,
                        'parent_table_id': f"{doc_name}_table_{table_index}" if len(table_chunks) > 1 else None,
                        'subtable_index': i if len(table_chunks) > 1 else None,
                        'chunk_start_row': i * self.chunk_size if len(table_chunks) > 1 else 0,
                        'chunk_end_row': min((i + 1) * self.chunk_size, table_structure.get('rows', 0)) if len(table_chunks) > 1 else table_structure.get('rows', 0),
                        
                        # å…³è”ä¿¡æ¯å­—æ®µ
                        'related_text': item.get('related_text', ''),
                        'related_images': [],
                        'related_text_chunks': [],
                        'table_context': item.get('table_context', '')
                    }
                    
                    tables.append(table)
                    table_index += 1
        
        return tables
    
    def _extract_image_info(self, data: List[Dict], doc_name: str) -> List[Dict]:
        """
        æå–å›¾ç‰‡ä¿¡æ¯ï¼Œå®Œå…¨ç¬¦åˆIMAGE_METADATA_SCHEMAè§„èŒƒ
        """
        images = []
        image_index = 0
        
        for item in data:
            if item.get('type') == 'image':
                # è·å–å›¾ç‰‡è·¯å¾„
                img_path = item.get('img_path', '')
                
                # æ„å»ºå®Œæ•´è·¯å¾„
                mineru_output_dir = self.config_manager.get_path('mineru_output_dir')
                source_image_path = os.path.join(mineru_output_dir, 'images', os.path.basename(img_path))
                
                # æ„å»ºæœ€ç»ˆå›¾ç‰‡è·¯å¾„
                final_image_dir = self.config_manager.get_path('final_image_dir')
                final_image_path = os.path.join(final_image_dir, os.path.basename(img_path))
                
                image = {
                    # åŸºç¡€æ ‡è¯†å­—æ®µï¼ˆç¬¦åˆCOMMON_METADATA_FIELDSï¼‰
                    'chunk_id': f"{doc_name}_image_{image_index}",
                    'chunk_type': 'image',
                    'source_type': 'pdf',
                    'document_name': doc_name,
                    'document_path': f"{doc_name}.pdf",
                    'page_number': item.get('page_idx', 1),
                    'page_idx': item.get('page_idx', 1),
                    'created_timestamp': int(time.time()),
                    'updated_timestamp': int(time.time()),
                    'processing_version': '3.0.0',
                    
                    # å‘é‡åŒ–ä¿¡æ¯å­—æ®µ
                    'vectorized': False,
                    'vectorization_timestamp': None,
                    'embedding_model': None,
                    
                    # å›¾ç‰‡ç‰¹æœ‰å­—æ®µï¼ˆç¬¦åˆIMAGE_METADATA_SCHEMAï¼‰
                    'image_id': f"{doc_name}_image_{image_index}",
                    'image_path': final_image_path,
                    'image_filename': os.path.basename(img_path),
                    'image_type': 'general',
                    'image_format': self._get_image_format(img_path),
                    'image_dimensions': {'width': 0, 'height': 0},  # ç¨åå¡«å……
                    
                    # å†…å®¹æè¿°å­—æ®µï¼ˆä¿ç•™ç°æœ‰ç³»ç»Ÿçš„ä¼˜ç§€éƒ¨åˆ†ï¼‰
                    'basic_description': ' | '.join(item.get('img_caption', [])),
                    'enhanced_description': '',  # ç¨åå¡«å……
                    'layered_descriptions': {},  # ç¨åå¡«å……
                    'structured_info': {},  # ç¨åå¡«å……
                    
                    # å›¾ç‰‡æ ‡é¢˜å’Œè„šæ³¨ï¼ˆä¿ç•™ç°æœ‰ç³»ç»Ÿçš„ä¼˜ç§€éƒ¨åˆ†ï¼‰
                    'img_caption': item.get('img_caption', []),
                    'img_footnote': item.get('img_footnote', []),
                    
                    # å¢å¼ºå¤„ç†å­—æ®µï¼ˆæ”¯æŒå¤±è´¥å¤„ç†å’Œè¡¥åšï¼‰
                    'enhancement_enabled': True,
                    'enhancement_model': None,  # ç¨åå¡«å……
                    'enhancement_status': 'pending',
                    'enhancement_timestamp': None,
                    'enhancement_error': None,
                    
                    # åŒé‡embeddingå­—æ®µï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
                    'image_embedding': None,  # ç¨åå¡«å……
                    'description_embedding': None,  # ç¨åå¡«å……
                    'image_embedding_model': None,  # ç¨åå¡«å……
                    'description_embedding_model': None,  # ç¨åå¡«å……
                    
                    # å…³è”ä¿¡æ¯å­—æ®µ
                    'related_text_chunks': [],
                    'related_table_chunks': [],
                    'parent_document_id': doc_name,
                    
                    # åŸå§‹è·¯å¾„ä¿¡æ¯
                    'source_image_path': source_image_path,
                    'img_path': img_path
                }
                
                images.append(image)
                image_index += 1
        
        return images
```

---

## **ï¿½ï¿½ ç¬¬äº”éƒ¨åˆ†ï¼šå‘é‡åŒ–ç®¡ç†å™¨ï¼ˆVectorizationManagerï¼‰**

### **5.1 æ ¸å¿ƒç±»è®¾è®¡**

```python
class VectorizationManager:
    """
    å‘é‡åŒ–ç®¡ç†å™¨
    ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å†…å®¹çš„å‘é‡åŒ–ï¼Œæ”¯æŒåŒé‡embeddingç­–ç•¥
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼Œä½äºcoreæ¨¡å—ä¸‹
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        
        # åˆå§‹åŒ–å„ä¸ªå‘é‡åŒ–å™¨ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.text_vectorizer = TextVectorizer(config_manager)
        self.image_vectorizer = ImageVectorizer(config_manager)
        self.table_vectorizer = TableVectorizer(config_manager)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("å‘é‡åŒ–ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def vectorize_content(self, content_items: List[Dict], content_type: str) -> List[Dict]:
        """
        ç»Ÿä¸€å‘é‡åŒ–æ¥å£
        """
        if content_type == 'text':
            return self.text_vectorizer.vectorize_batch(content_items)
        elif content_type == 'image':
            return self.image_vectorizer.vectorize_images_batch(content_items)
        elif content_type == 'table':
            return self.table_vectorizer.vectorize_batch(content_items)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: {content_type}")
    
    def vectorize_all_content(self, metadata_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‘é‡åŒ–æ‰€æœ‰å†…å®¹
        """
        try:
            print("å¼€å§‹å‘é‡åŒ–æ‰€æœ‰å†…å®¹...")
            
            # å‘é‡åŒ–æ–‡æœ¬
            if metadata_results.get('text_chunks'):
                print(f"å‘é‡åŒ–æ–‡æœ¬å—: {len(metadata_results['text_chunks'])} ä¸ª")
                metadata_results['text_chunks'] = self.text_vectorizer.vectorize_batch(
                    metadata_results['text_chunks']
                )
            
            # å‘é‡åŒ–è¡¨æ ¼
            if metadata_results.get('tables'):
                print(f"å‘é‡åŒ–è¡¨æ ¼: {len(metadata_results['tables'])} ä¸ª")
                metadata_results['tables'] = self.table_vectorizer.vectorize_batch(
                    metadata_results['tables']
                )
            
            # å›¾ç‰‡å‘é‡åŒ–åœ¨ImageProcessorä¸­å·²å®Œæˆ
            
            print("âœ… æ‰€æœ‰å†…å®¹å‘é‡åŒ–å®Œæˆ")
            return metadata_results
            
        except Exception as e:
            error_msg = f"å†…å®¹å‘é‡åŒ–å¤±è´¥: {e}"
            logging.error(error_msg)
            self.failure_handler.record_failure('vectorization', 'content_vectorization', str(e))
            raise RuntimeError(error_msg)
```

---

## **ï¿½ï¿½ ç¬¬å…­éƒ¨åˆ†ï¼šä¸»å¤„ç†å™¨é›†æˆ**

### **6.1 ä¸»å¤„ç†å™¨é›†æˆ**

```python
class MainProcessor:
    """
    ä¸»å¤„ç†å™¨
    é›†æˆæ‰€æœ‰å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬JSONæ–‡ä»¶è§£æã€æ–‡æœ¬å¤„ç†ã€è¡¨æ ¼å¤„ç†ã€å›¾ç‰‡å¤„ç†
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.content_metadata_extractor = ContentMetadataExtractor(config_manager)
        self.image_processor = ImageProcessor(config_manager)
        self.vectorization_manager = VectorizationManager(config_manager)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("ä¸»å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_mineru_output(self, mineru_output_dir: str) -> Dict[str, Any]:
        """
        å¤„ç†MinerUè¾“å‡ºï¼ŒåŒ…æ‹¬JSONæ–‡ä»¶è§£æã€æ–‡æœ¬å¤„ç†ã€è¡¨æ ¼å¤„ç†ã€å›¾ç‰‡å¤„ç†
        """
        try:
            print(f" å¼€å§‹å¤„ç†MinerUè¾“å‡ºç›®å½•: {mineru_output_dir}")
            
            # æ­¥éª¤1: è§£æJSONæ–‡ä»¶ï¼Œæå–å…ƒæ•°æ®
            print("æ­¥éª¤1: è§£æJSONæ–‡ä»¶ï¼Œæå–å…ƒæ•°æ®...")
            metadata_results = self._extract_all_metadata(mineru_output_dir)
            
            # æ­¥éª¤2: å¤„ç†å›¾ç‰‡ï¼ˆå¤åˆ¶ã€å¢å¼ºã€å‘é‡åŒ–ï¼‰
            print("æ­¥éª¤2: å¤„ç†å›¾ç‰‡...")
            if metadata_results.get('images'):
                processed_images = self.image_processor.process_images(metadata_results['images'])
                metadata_results['images'] = processed_images
            
            # æ­¥éª¤3: å‘é‡åŒ–å…¶ä»–å†…å®¹
            print("æ­¥éª¤3: å‘é‡åŒ–å…¶ä»–å†…å®¹...")
            metadata_results = self.vectorization_manager.vectorize_all_content(metadata_results)
            
            # æ­¥éª¤4: ç”Ÿæˆæœ€ç»ˆç»“æœ
            print("æ­¥éª¤4: ç”Ÿæˆæœ€ç»ˆç»“æœ...")
            final_result = self._generate_final_result(metadata_results)
            
            print(f"âœ… MinerUè¾“å‡ºå¤„ç†å®Œæˆ")
            return final_result
            
        except Exception as e:
            error_msg = f"MinerUè¾“å‡ºå¤„ç†å¤±è´¥: {e}"
            logging.error(error_msg)
            self.failure_handler.record_failure(mineru_output_dir, 'mineru_output_processing', str(e))
            raise RuntimeError(error_msg)
    
    def _extract_all_metadata(self, mineru_output_dir: str) -> Dict[str, Any]:
        """
        æå–æ‰€æœ‰å…ƒæ•°æ®
        """
        metadata_results = {
            'text_chunks': [],
            'tables': [],
            'images': []
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        json_files = list(Path(mineru_output_dir).glob("*_1.json"))
        
        for json_file in json_files:
            try:
                doc_name = json_file.stem.replace('_1', '')
                print(f"  å¤„ç†æ–‡æ¡£: {doc_name}")
                
                # æå–å…ƒæ•°æ®
                metadata = self.content_metadata_extractor.extract_metadata_from_json(
                    str(json_file), doc_name
                )
                
                # åˆå¹¶ç»“æœ
                metadata_results['text_chunks'].extend(metadata.get('text_chunks', []))
                metadata_results['tables'].extend(metadata.get('tables', []))
                metadata_results['images'].extend(metadata.get('images', []))
                
            except Exception as e:
                logging.error(f"å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {json_file}, é”™è¯¯: {e}")
                continue
        
        print(f"  æå–å®Œæˆ: {len(metadata_results['text_chunks'])} ä¸ªæ–‡æœ¬å—, {len(metadata_results['tables'])} ä¸ªè¡¨æ ¼, {len(metadata_results['images'])} å¼ å›¾ç‰‡")
        
        return metadata_results
    
    def _generate_final_result(self, metadata_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæœ€ç»ˆç»“æœ
        """
        return {
            'success': True,
            'timestamp': int(time.time()),
            'processing_version': '3.0.0',
            'statistics': {
                'text_chunks': len(metadata_results.get('text_chunks', [])),
                'tables': len(metadata_results.get('tables', [])),
                'images': len(metadata_results.get('images', [])),
                'total_items': sum([
                    len(metadata_results.get('text_chunks', [])),
                    len(metadata_results.get('tables', [])),
                    len(metadata_results.get('images', []))
                ])
            },
            'results': metadata_results
        }
```

---

## **ğŸ”§ ç¬¬ä¸ƒéƒ¨åˆ†ï¼šMinerUé›†æˆä¼˜åŒ–**

### **7.1 MinerUé›†æˆä¼˜åŒ–**

```python
class MinerUIntegration:
    """
    MinerUé›†æˆä¼˜åŒ–
    å‚è€ƒä¸Šä¸€ç‰ˆæœ¬çš„ä¼˜ç§€å®ç°ï¼Œä¼˜åŒ–è¾“å‡ºç›®å½•ã€æ–‡ä»¶å‘½åã€è§£å‹é€»è¾‘
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # ä½¿ç”¨é…ç½®ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.mineru_output_dir = config_manager.get_path('mineru_output_dir')
        self.final_image_dir = config_manager.get_path('final_image_dir')
        self.temp_dir = config_manager.get_path('temp_dir')
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("MinerUé›†æˆä¼˜åŒ–åˆå§‹åŒ–å®Œæˆ")
    
    def _call_mineru_api(self, pdf_path: str, output_dir: str, api_key: str):
        """
        è°ƒç”¨MinerU APIï¼Œä¼˜åŒ–è¾“å‡ºç›®å½•å’Œæ–‡ä»¶å‘½å
        """
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„mineru_output_dirï¼Œè€Œä¸æ˜¯ä¼ å…¥çš„output_dir
        config_output_dir = self.mineru_output_dir
        
        # æŒ‰ç…§PDFæ–‡ä»¶åé‡å‘½åè¾“å‡ºæ–‡ä»¶
        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
        
        # è§£å‹åçš„æ–‡ä»¶å‘½åï¼š
        # - {pdf_name}.md (Markdownæ–‡ä»¶)
        # - {pdf_name}_1.json (å†…å®¹åˆ—è¡¨JSON)
        # - images/ (å›¾ç‰‡ç›®å½•)
        
        # å®ç°ç±»ä¼¼minerU_batch_local_files.pyä¸­çš„æ–‡ä»¶é‡å‘½åé€»è¾‘
        # ç¡®ä¿å›¾ç‰‡è§£å‹åˆ°md/imagesç›®å½•
        
        logging.info(f"MinerU APIè°ƒç”¨å®Œæˆï¼Œè¾“å‡ºç›®å½•: {config_output_dir}")
    
    def _extract_zip_file(self, zip_path: str, output_dir: str, pdf_name: str):
        """
        è§£å‹ZIPæ–‡ä»¶ï¼ŒæŒ‰ç…§PDFæ–‡ä»¶åé‡å‘½åè¾“å‡ºæ–‡ä»¶
        """
        # å®ç°ç±»ä¼¼minerU_batch_local_files.pyä¸­çš„è§£å‹é€»è¾‘
        # ç¡®ä¿æ–‡ä»¶é‡å‘½åå’Œå›¾ç‰‡ç›®å½•ç»“æ„æ­£ç¡®
        
        logging.info(f"ZIPæ–‡ä»¶è§£å‹å®Œæˆ: {pdf_name}")
    
    def _parse_content_list_json(self, json_path: str):
        """
        è§£æå†…å®¹åˆ—è¡¨JSONæ–‡ä»¶ï¼Œæ­£ç¡®æå–textã€tableã€imageä¿¡æ¯
        """
        # å®ç°ç±»ä¼¼markdown_processor.pyä¸­çš„JSONè§£æé€»è¾‘
        # ç¡®ä¿å…ƒæ•°æ®æå–å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        
        logging.info(f"JSONæ–‡ä»¶è§£æå®Œæˆ: {json_path}")
```

---

## **ï¿½ï¿½ ç¬¬å…«éƒ¨åˆ†ï¼šé…ç½®ç®¡ç†é›†æˆ**

### **8.1 é…ç½®ç®¡ç†é›†æˆ**

```python
# ç¡®ä¿æ‰€æœ‰é…ç½®é”®åä¸è®¾è®¡æ–‡æ¡£å®Œå…¨ä¸€è‡´
class ConfigIntegration:
    """
    é…ç½®ç®¡ç†é›†æˆ
    ç¡®ä¿æ‰€æœ‰é…ç½®é”®åä¸è®¾è®¡æ–‡æ¡£å®Œå…¨ä¸€è‡´
    """
    
    @staticmethod
    def get_vectorization_config(config_manager):
        """
        è·å–å‘é‡åŒ–é…ç½®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        """
        config = config_manager.get_all_config()
        return {
            'text_embedding_model': config.get('vectorization.text_embedding_model', 'text-embedding-v1'),
            'image_embedding_model': config.get('vectorization.image_embedding_model', 'multimodal-embedding-one-peace-v1'),
            'table_embedding_model': config.get('vectorization.text_embedding_model', 'text-embedding-v1')  # è¡¨æ ¼ä½¿ç”¨æ–‡æœ¬æ¨¡å‹
        }
    
    @staticmethod
    def get_image_processing_config(config_manager):
        """
        è·å–å›¾ç‰‡å¤„ç†é…ç½®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        """
        config = config_manager.get_all_config()
        return {
            'enable_enhancement': config.get('image_processing.enable_enhancement', True),
            'enhancement_model': config.get('image_processing.enhancement_model', 'qwen-vl-plus'),
            'enhancement_model_api': config.get('image_processing.enhancement_model_api', 'dashscope'),
            'max_retries': config.get('api_rate_limiting.max_retries', 3),
            'retry_delay_seconds': config.get('api_rate_limiting.retry_delay_seconds', 5),
            'enable_rate_limiting': config.get('api_rate_limiting.enable_rate_limiting', True)
        }
    
    @staticmethod
    def get_api_rate_limiting_config(config_manager):
        """
        è·å–APIé™æµé…ç½®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        """
        config = config_manager.get_all_config()
        return {
            'enhancement_batch_size': config.get('api_rate_limiting.enhancement_batch_size', 5),
            'enhancement_delay_seconds': config.get('api_rate_limiting.enhancement_delay_seconds', 2),
            'vectorization_batch_size': config.get('api_rate_limiting.vectorization_batch_size', 10),
            'vectorization_delay_seconds': config.get('api_rate_limiting.vectorization_delay_seconds', 1),
            'max_retries': config.get('api_rate_limiting.max_retries', 3),
            'retry_delay_seconds': config.get('api_rate_limiting.retry_delay_seconds', 5),
            'enable_rate_limiting': config.get('api_rate_limiting.enable_rate_limiting', True)
        }
    
    @staticmethod
    def get_failure_handling_config(config_manager):
        """
        è·å–å¤±è´¥å¤„ç†é…ç½®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        """
        config = config_manager.get_all_config()
        return {
            'skip_failed_images': config.get('failure_handling.skip_failed_images', True),
            'max_retries': config.get('failure_handling.max_retries', 3),
            'retry_delay_seconds': config.get('failure_handling.retry_delay_seconds', 5),
            'continue_on_failure': config.get('failure_handling.continue_on_failure', True),
            'generate_failure_report': config.get('failure_handling.generate_failure_report', True),
            'failure_report_path': config.get('failure_handling.failure_report_path', './logs/failure_report.json'),
            'mark_for_later_processing': config.get('failure_handling.mark_for_later_processing', True),
            'failure_report_format': config.get('failure_handling.failure_report_format', 'detailed')
        }
```





## 1

ç»§ç»­å®Œæˆé…ç½®ç®¡ç†é›†æˆéƒ¨åˆ†ï¼š

```markdown:v3/1.prepare/15.å›¾ç‰‡å¢å¼ºå¤„ç†å®Œæ•´ä¼˜åŒ–æ–¹æ¡ˆï¼ˆé‡å†™ç‰ˆï¼‰.md
            'retry_delay_seconds': config.get('api_rate_limiting.retry_delay_seconds', 5),
            'enable_rate_limiting': config.get('api_rate_limiting.enable_rate_limiting', True)
        }
    
    @staticmethod
    def get_api_rate_limiting_config(config_manager):
        """
        è·å–APIé™æµé…ç½®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        """
        config = config_manager.get_all_config()
        return {
            'enhancement_batch_size': config.get('api_rate_limiting.enhancement_batch_size', 5),
            'enhancement_delay_seconds': config.get('api_rate_limiting.enhancement_delay_seconds', 2),
            'vectorization_batch_size': config.get('api_rate_limiting.vectorization_batch_size', 10),
            'vectorization_delay_seconds': config.get('api_rate_limiting.vectorization_delay_seconds', 1),
            'max_retries': config.get('api_rate_limiting.max_retries', 3),
            'retry_delay_seconds': config.get('api_rate_limiting.retry_delay_seconds', 5),
            'enable_rate_limiting': config.get('api_rate_limiting.enable_rate_limiting', True)
        }
    
    @staticmethod
    def get_failure_handling_config(config_manager):
        """
        è·å–å¤±è´¥å¤„ç†é…ç½®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        """
        config = config_manager.get_all_config()
        return {
            'skip_failed_images': config.get('failure_handling.skip_failed_images', True),
            'max_retries': config.get('failure_handling.max_retries', 3),
            'retry_delay_seconds': config.get('failure_handling.retry_delay_seconds', 5),
            'continue_on_failure': config.get('failure_handling.continue_on_failure', True),
            'generate_failure_report': config.get('failure_handling.generate_failure_report', True),
            'failure_report_path': config.get('failure_handling.failure_report_path', './logs/failure_report.json'),
            'mark_for_later_processing': config.get('failure_handling.mark_for_later_processing', True),
            'failure_report_format': config.get('failure_handling.failure_report_format', 'detailed')
        }
```

---

## ** ç¬¬ä¹éƒ¨åˆ†ï¼šæ™ºèƒ½å»é‡æœºåˆ¶**

### **9.1 æ™ºèƒ½å»é‡æœºåˆ¶**

```python
class SmartDeduplication:
    """
    æ™ºèƒ½å»é‡æœºåˆ¶
    é¿å…ç”Ÿæˆé‡å¤å†…å®¹ï¼Œç¡®ä¿ä¿¡æ¯è´¨é‡å’Œä¸€è‡´æ€§
    """
    
    def __init__(self):
        self.deduplication_patterns = [
            # é‡å¤æè¿°æ¨¡å¼
            r'å›¾ç‰‡æ ‡é¢˜[ï¼š:]\s*(.+?)(?:\s*å›¾ç‰‡æ ‡é¢˜[ï¼š:]\s*\1)+',
            r'å›¾ç‰‡è„šæ³¨[ï¼š:]\s*(.+?)(?:\s*å›¾ç‰‡è„šæ³¨[ï¼š:]\s*\1)+',
            r'åŸºç¡€æè¿°[ï¼š:]\s*(.+?)(?:\s*åŸºç¡€æè¿°[ï¼š:]\s*\1)+',
            
            # é‡å¤å¢å¼ºæ¨¡å¼
            r'å¢å¼ºæè¿°[ï¼š:]\s*(.+?)(?:\s*å¢å¼ºæè¿°[ï¼š:]\s*\1)+',
            r'AIåˆ†æ[ï¼š:]\s*(.+?)(?:\s*AIåˆ†æ[ï¼š:]\s*\1)+',
            
            # é‡å¤ç»“æ„æ¨¡å¼
            r'å›¾è¡¨ç±»å‹[ï¼š:]\s*(.+?)(?:\s*å›¾è¡¨ç±»å‹[ï¼š:]\s*\1)+',
            r'æ•°æ®ç‚¹[ï¼š:]\s*(.+?)(?:\s*æ•°æ®ç‚¹[ï¼š:]\s*\1)+',
            r'è¶‹åŠ¿åˆ†æ[ï¼š:]\s*(.+?)(?:\s*è¶‹åŠ¿åˆ†æ[ï¼š:]\s*\1)+',
            r'å…³é”®æ´å¯Ÿ[ï¼š:]\s*(.+?)(?:\s*å…³é”®æ´å¯Ÿ[ï¼š:]\s*\1)+'
        ]
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.deduplication_patterns]
    
    def deduplicate_description(self, description: str) -> str:
        """
        æ™ºèƒ½å»é‡æè¿°å†…å®¹
        """
        if not description:
            return description
        
        deduplicated = description
        
        # åº”ç”¨æ‰€æœ‰å»é‡æ¨¡å¼
        for pattern in self.compiled_patterns:
            deduplicated = pattern.sub(self._replace_duplicates, deduplicated)
        
        # é¢å¤–çš„å»é‡é€»è¾‘
        deduplicated = self._remove_consecutive_duplicates(deduplicated)
        deduplicated = self._cleanup_formatting(deduplicated)
        
        return deduplicated
    
    def _replace_duplicates(self, match):
        """
        æ›¿æ¢é‡å¤å†…å®¹
        """
        # æå–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹ï¼Œå»é™¤é‡å¤
        content = match.group(1)
        return f"å›¾ç‰‡æ ‡é¢˜: {content}"
    
    def _remove_consecutive_duplicates(self, text: str) -> str:
        """
        ç§»é™¤è¿ç»­é‡å¤çš„è¡Œ
        """
        lines = text.split('\n')
        deduplicated_lines = []
        previous_line = None
        
        for line in lines:
            line = line.strip()
            if line and line != previous_line:
                deduplicated_lines.append(line)
                previous_line = line
        
        return '\n'.join(deduplicated_lines)
    
    def _cleanup_formatting(self, text: str) -> str:
        """
        æ¸…ç†æ ¼å¼
        """
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
        
        # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
        text = re.sub(r' +', ' ', text)
        
        # æ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºæ ¼
        text = '\n'.join(line.strip() for line in text.split('\n'))
        
        return text
```

---

## **ğŸ”§ ç¬¬åéƒ¨åˆ†ï¼šæ‰¹å¤„ç†ä¼˜åŒ–**

### **10.1 æ‰¹å¤„ç†ä¼˜åŒ–**

```python
class BatchProcessingOptimizer:
    """
    æ‰¹å¤„ç†ä¼˜åŒ–
    å‚è€ƒä¸Šä¸€ç‰ˆæœ¬çš„ä¼˜ç§€è®¾è®¡ï¼Œå®ç°é«˜æ•ˆçš„æ‰¹å¤„ç†æœºåˆ¶
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # ä½¿ç”¨é…ç½®ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.enhancement_batch_size = self.config.get('api_rate_limiting.enhancement_batch_size', 5)
        self.enhancement_delay_seconds = self.config.get('api_rate_limiting.enhancement_delay_seconds', 2)
        self.vectorization_batch_size = self.config.get('api_rate_limiting.vectorization_batch_size', 10)
        self.vectorization_delay_seconds = self.config.get('api_rate_limiting.vectorization_delay_seconds', 1)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("æ‰¹å¤„ç†ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_enhancement_batch(self, images: List[Dict], enhancer: ImageEnhancer) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†å›¾ç‰‡å¢å¼º
        """
        enhanced_images = []
        total_images = len(images)
        
        for i in range(0, total_images, self.enhancement_batch_size):
            batch = images[i:i + self.enhancement_batch_size]
            batch_start = i + 1
            batch_end = min(i + self.enhancement_batch_size, total_images)
            
            print(f"  å¤„ç†å¢å¼ºæ‰¹æ¬¡ {batch_start}-{batch_end}/{total_images}")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            for j, image in enumerate(batch):
                try:
                    print(f"   ï¸ å¢å¼ºå›¾ç‰‡ {batch_start + j}: {os.path.basename(image.get('final_image_path', ''))}")
                    
                    # è°ƒç”¨å¢å¼ºå™¨
                    enhancement_result = enhancer.enhance_image_complete(
                        image.get('final_image_path', ''),
                        {
                            'img_caption': image.get('img_caption', []),
                            'img_footnote': image.get('img_footnote', []),
                            'img_path': image.get('img_path', '')
                        }
                    )
                    
                    # æ›´æ–°å›¾ç‰‡ä¿¡æ¯
                    image.update(enhancement_result)
                    enhanced_images.append(image)
                    
                    print(f"    âœ… å›¾ç‰‡å¢å¼ºå®Œæˆ: {os.path.basename(image.get('final_image_path', ''))}")
                    
                except Exception as e:
                    error_msg = f"å›¾ç‰‡å¢å¼ºå¤±è´¥: {os.path.basename(image.get('final_image_path', ''))}, é”™è¯¯: {e}"
                    print(f"    âš ï¸ {error_msg}")
                    
                    # è®°å½•å¤±è´¥
                    self.failure_handler.record_failure(image, 'enhancement_batch', str(e))
                    
                    # æ ‡è®°å¤±è´¥çŠ¶æ€
                    image['enhancement_status'] = 'failed'
                    image['enhancement_error'] = str(e)
                    enhanced_images.append(image)
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆAPIé™æµï¼‰
            if i + self.enhancement_batch_size < total_images:
                print(f"  æ‰¹æ¬¡å®Œæˆï¼Œç­‰å¾… {self.enhancement_delay_seconds} ç§’...")
                time.sleep(self.enhancement_delay_seconds)
        
        return enhanced_images
    
    def process_vectorization_batch(self, images: List[Dict], vectorizer: ImageVectorizer) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†å›¾ç‰‡å‘é‡åŒ–
        """
        vectorized_images = []
        total_images = len(images)
        
        for i in range(0, total_images, self.vectorization_batch_size):
            batch = images[i:i + self.vectorization_batch_size]
            batch_start = i + 1
            batch_end = min(i + self.vectorization_batch_size, total_images)
            
            print(f"  å¤„ç†å‘é‡åŒ–æ‰¹æ¬¡ {batch_start}-{batch_end}/{total_images}")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            for j, image in enumerate(batch):
                try:
                    print(f"   ï¸ å‘é‡åŒ–å›¾ç‰‡ {batch_start + j}: {os.path.basename(image.get('final_image_path', ''))}")
                    
                    # è°ƒç”¨å‘é‡åŒ–å™¨
                    vectorization_result = vectorizer.vectorize_image(
                        image.get('final_image_path', ''),
                        image.get('enhanced_description', '')
                    )
                    
                    # æ›´æ–°å›¾ç‰‡ä¿¡æ¯
                    image.update(vectorization_result)
                    vectorized_images.append(image)
                    
                    print(f"    âœ… å›¾ç‰‡å‘é‡åŒ–å®Œæˆ: {os.path.basename(image.get('final_image_path', ''))}")
                    
                except Exception as e:
                    error_msg = f"å›¾ç‰‡å‘é‡åŒ–å¤±è´¥: {os.path.basename(image.get('final_image_path', ''))}, é”™è¯¯: {e}"
                    print(f"    âš ï¸ {error_msg}")
                    
                    # è®°å½•å¤±è´¥
                    self.failure_handler.record_failure(image, 'vectorization_batch', str(e))
                    
                    # æ ‡è®°å¤±è´¥çŠ¶æ€
                    image['vectorization_status'] = 'failed'
                    image['vectorization_error'] = str(e)
                    vectorized_images.append(image)
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆAPIé™æµï¼‰
            if i + self.vectorization_batch_size < total_images:
                print(f"  æ‰¹æ¬¡å®Œæˆï¼Œç­‰å¾… {self.vectorization_delay_seconds} ç§’...")
                time.sleep(self.vectorization_delay_seconds)
        
        return vectorized_images
```

---

## ** ç¬¬åä¸€éƒ¨åˆ†ï¼šè¿›åº¦ç›‘æ§å’Œæ—¥å¿—**

### **11.1 è¿›åº¦ç›‘æ§å’Œæ—¥å¿—**

```python
class ProgressMonitor:
    """
    è¿›åº¦ç›‘æ§å™¨
    å‚è€ƒä¸Šä¸€ç‰ˆæœ¬çš„ä¼˜ç§€è®¾è®¡ï¼Œæä¾›æ¸…æ™°çš„è¿›åº¦æç¤ºå’ŒçŠ¶æ€åé¦ˆ
    """
    
    def __init__(self, total_items: int, process_name: str):
        self.total_items = total_items
        self.process_name = process_name
        self.current_item = 0
        self.start_time = time.time()
        self.last_update_time = self.start_time
        
        # è¿›åº¦æ¡é…ç½®
        self.progress_bar_width = 50
        self.update_interval = 1.0  # æ¯ç§’æ›´æ–°ä¸€æ¬¡
        
        print(f"\nğŸš€ å¼€å§‹ {process_name}")
        print(f" æ€»ä»»åŠ¡æ•°: {total_items}")
        print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S')}")
        print()
    
    def update_progress(self, current: int, success: bool = True, error_msg: str = None):
        """
        æ›´æ–°è¿›åº¦
        """
        self.current_item = current
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ˜¾ç¤º
        current_time = time.time()
        if current_time - self.last_update_time >= self.update_interval:
            self._display_progress()
            self.last_update_time = current_time
        
        # æ˜¾ç¤ºå½“å‰é¡¹çŠ¶æ€
        if error_msg:
            print(f"  âš ï¸ é¡¹ç›® {current}/{self.total_items} å¤±è´¥: {error_msg}")
        elif success:
            print(f"  âœ… é¡¹ç›® {current}/{self.total_items} å®Œæˆ")
    
    def _display_progress(self):
        """
        æ˜¾ç¤ºè¿›åº¦æ¡
        """
        progress = self.current_item / self.total_items
        filled_width = int(self.progress_bar_width * progress)
        bar = 'â–ˆ' * filled_width + 'â–‘' * (self.progress_bar_width - filled_width)
        
        # è®¡ç®—å‰©ä½™æ—¶é—´
        elapsed_time = time.time() - self.start_time
        if self.current_item > 0:
            estimated_total_time = elapsed_time / self.current_item * self.total_items
            remaining_time = estimated_total_time - elapsed_time
            time_str = f"å‰©ä½™æ—¶é—´: {self._format_time(remaining_time)}"
        else:
            time_str = "å‰©ä½™æ—¶é—´: è®¡ç®—ä¸­..."
        
        # æ˜¾ç¤ºè¿›åº¦æ¡
        print(f"\r  ğŸ“ˆ è¿›åº¦: [{bar}] {progress:.1%} ({self.current_item}/{self.total_items}) | {time_str}", end='', flush=True)
    
    def _format_time(self, seconds: float) -> str:
        """
        æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
        """
        if seconds < 60:
            return f"{seconds:.0f}ç§’"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.0f}åˆ†é’Ÿ"
        else:
            hours = seconds / 3600
            return f"{hours:.1f}å°æ—¶"
    
    def complete(self):
        """
        å®Œæˆå¤„ç†
        """
        total_time = time.time() - self.start_time
        
        print(f"\n\n {self.process_name} å®Œæˆ!")
        print(f"ğŸ“Š æ€»ä»»åŠ¡æ•°: {self.total_items}")
        print(f"â° æ€»è€—æ—¶: {self._format_time(total_time)}")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {self.total_items / total_time:.2f} é¡¹/ç§’")
        print()
```

---

## **ğŸ”§ ç¬¬åäºŒéƒ¨åˆ†ï¼šé”™è¯¯å¤„ç†å’Œæ¢å¤**

### **12.1 é”™è¯¯å¤„ç†å’Œæ¢å¤**

```python
class ErrorHandler:
    """
    é”™è¯¯å¤„ç†å™¨
    å‚è€ƒä¸Šä¸€ç‰ˆæœ¬çš„ä¼˜ç§€è®¾è®¡ï¼Œå®ç°å¥å£®çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.failure_handler = config_manager.get_failure_handler()
        
        # ä½¿ç”¨é…ç½®ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.max_retries = self.config.get('failure_handling.max_retries', 3)
        self.retry_delay_seconds = self.config.get('failure_handling.retry_delay_seconds', 5)
        self.continue_on_failure = self.config.get('failure_handling.continue_on_failure', True)
        
        logging.info("é”™è¯¯å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def handle_enhancement_error(self, image: Dict, error: Exception, retry_count: int = 0) -> Dict[str, Any]:
        """
        å¤„ç†å›¾ç‰‡å¢å¼ºé”™è¯¯
        """
        error_msg = str(error)
        logging.error(f"å›¾ç‰‡å¢å¼ºé”™è¯¯: {image.get('image_id', 'unknown')} - {error_msg}")
        
        # è®°å½•å¤±è´¥
        self.failure_handler.record_failure(image, 'enhancement_error', error_msg, retry_count)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
        if retry_count < self.max_retries:
            logging.info(f"å‡†å¤‡é‡è¯•å›¾ç‰‡å¢å¼º: {image.get('image_id', 'unknown')} (ç¬¬ {retry_count + 1} æ¬¡)")
            return {
                'should_retry': True,
                'retry_count': retry_count + 1,
                'retry_delay': self.retry_delay_seconds * (2 ** retry_count)  # æŒ‡æ•°é€€é¿
            }
        else:
            logging.warning(f"å›¾ç‰‡å¢å¼ºè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {image.get('image_id', 'unknown')}")
            return {
                'should_retry': False,
                'retry_count': retry_count,
                'retry_delay': 0
            }
    
    def handle_vectorization_error(self, image: Dict, error: Exception, retry_count: int = 0) -> Dict[str, Any]:
        """
        å¤„ç†å›¾ç‰‡å‘é‡åŒ–é”™è¯¯
        """
        error_msg = str(error)
        logging.error(f"å›¾ç‰‡å‘é‡åŒ–é”™è¯¯: {image.get('image_id', 'unknown')} - {error_msg}")
        
        # è®°å½•å¤±è´¥
        self.failure_handler.record_failure(image, 'vectorization_error', error_msg, retry_count)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
        if retry_count < self.max_retries:
            logging.info(f"å‡†å¤‡é‡è¯•å›¾ç‰‡å‘é‡åŒ–: {image.get('image_id', 'unknown')} (ç¬¬ {retry_count + 1} æ¬¡)")
            return {
                'should_retry': True,
                'retry_count': retry_count + 1,
                'retry_delay': self.retry_delay_seconds * (2 ** retry_count)  # æŒ‡æ•°é€€é¿
            }
        else:
            logging.warning(f"å›¾ç‰‡å‘é‡åŒ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {image.get('image_id', 'unknown')}")
            return {
                'should_retry': False,
                'retry_count': retry_count,
                'retry_delay': 0
            }
    
    def create_fallback_result(self, image: Dict, error: Exception, process_type: str) -> Dict[str, Any]:
        """
        åˆ›å»ºå›é€€ç»“æœ
        """
        if process_type == 'enhancement':
            return {
                'enhanced_description': f"å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥: {str(error)}",
                'layered_descriptions': {},
                'structured_info': {},
                'enhancement_status': 'failed',
                'enhancement_error': str(error),
                'enhancement_timestamp': int(time.time())
            }
        elif process_type == 'vectorization':
            return {
                'image_embedding': [],
                'description_embedding': [],
                'vectorization_status': 'failed',
                'vectorization_error': str(error),
                'vectorization_timestamp': int(time.time())
            }
        else:
            return {}
```

---

## **ğŸ”§ ç¬¬åä¸‰éƒ¨åˆ†ï¼šæ€§èƒ½ä¼˜åŒ–**

### **13.1 æ€§èƒ½ä¼˜åŒ–**

```python
class PerformanceOptimizer:
    """
    æ€§èƒ½ä¼˜åŒ–å™¨
    å‚è€ƒä¸Šä¸€ç‰ˆæœ¬çš„ä¼˜ç§€è®¾è®¡ï¼Œå®ç°é«˜æ•ˆçš„æ€§èƒ½ä¼˜åŒ–æœºåˆ¶
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # ä½¿ç”¨é…ç½®ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.enhancement_workers = self.config.get('batch_processing.enhancement_workers', 2)
        self.vectorization_workers = self.config.get('batch_processing.vectorization_workers', 3)
        self.queue_size = self.config.get('batch_processing.queue_size', 100)
        self.timeout_seconds = self.config.get('batch_processing.timeout_seconds', 300)
        
        logging.info("æ€§èƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def optimize_batch_size(self, total_items: int, api_type: str) -> int:
        """
        ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
        """
        if api_type == 'enhancement':
            base_size = self.enhancement_workers * 2
        elif api_type == 'vectorization':
            base_size = self.vectorization_workers * 3
        else:
            base_size = 5
        
        # æ ¹æ®æ€»æ•°é‡è°ƒæ•´æ‰¹å¤„ç†å¤§å°
        if total_items <= 10:
            return min(base_size, total_items)
        elif total_items <= 50:
            return min(base_size * 2, total_items)
        else:
            return min(base_size * 3, total_items)
    
    def calculate_optimal_delay(self, batch_size: int, api_type: str) -> float:
        """
        è®¡ç®—æœ€ä¼˜å»¶è¿Ÿæ—¶é—´
        """
        if api_type == 'enhancement':
            base_delay = self.config.get('api_rate_limiting.enhancement_delay_seconds', 2)
        elif api_type == 'vectorization':
            base_delay = self.config.get('api_rate_limiting.vectorization_delay_seconds', 1)
        else:
            base_delay = 1
        
        # æ ¹æ®æ‰¹å¤„ç†å¤§å°è°ƒæ•´å»¶è¿Ÿ
        if batch_size <= 5:
            return base_delay
        elif batch_size <= 10:
            return base_delay * 1.5
        else:
            return base_delay * 2
    
    def estimate_processing_time(self, total_items: int, api_type: str) -> float:
        """
        ä¼°ç®—å¤„ç†æ—¶é—´
        """
        if api_type == 'enhancement':
            base_time_per_item = 3.0  # ç§’
            batch_size = self.optimize_batch_size(total_items, api_type)
            delay = self.calculate_optimal_delay(batch_size, api_type)
        elif api_type == 'vectorization':
            base_time_per_item = 1.0  # ç§’
            batch_size = self.optimize_batch_size(total_items, api_type)
            delay = self.calculate_optimal_delay(batch_size, api_type)
        else:
            base_time_per_item = 2.0
            batch_size = 5
            delay = 1
        
        # è®¡ç®—æ€»æ—¶é—´
        batches = (total_items + batch_size - 1) // batch_size
        total_time = total_items * base_time_per_item + (batches - 1) * delay
        
        return total_time
```

---

## ** ç¬¬åå››éƒ¨åˆ†ï¼šæµ‹è¯•å’ŒéªŒè¯**

### **14.1 æµ‹è¯•å’ŒéªŒè¯**

```python
class TestValidator:
    """
    æµ‹è¯•éªŒè¯å™¨
    ç¡®ä¿æ‰€æœ‰åŠŸèƒ½æŒ‰ç…§è®¾è®¡æ–‡æ¡£è§„èŒƒæ­£ç¡®å®ç°
    """
    
    @staticmethod
    def validate_image_metadata(image_metadata: Dict) -> bool:
        """
        éªŒè¯å›¾ç‰‡å…ƒæ•°æ®æ˜¯å¦ç¬¦åˆIMAGE_METADATA_SCHEMAè§„èŒƒ
        """
        required_fields = [
            'chunk_id', 'chunk_type', 'source_type', 'document_name',
            'image_id', 'image_path', 'enhanced_description',
            'image_embedding', 'description_embedding'
        ]
        
        for field in required_fields:
            if field not in image_metadata:
                print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False
        
        # éªŒè¯å­—æ®µç±»å‹
        if not isinstance(image_metadata['chunk_type'], str) or image_metadata['chunk_type'] != 'image':
            print("âŒ chunk_typeå¿…é¡»ä¸º'image'")
            return False
        
        if not isinstance(image_metadata['image_embedding'], list):
            print("âŒ image_embeddingå¿…é¡»ä¸ºåˆ—è¡¨")
            return False
        
        if not isinstance(image_metadata['description_embedding'], list):
            print("âŒ description_embeddingå¿…é¡»ä¸ºåˆ—è¡¨")
            return False
        
        print("âœ… å›¾ç‰‡å…ƒæ•°æ®éªŒè¯é€šè¿‡")
        return True
    
    @staticmethod
    def validate_text_metadata(text_metadata: Dict) -> bool:
        """
        éªŒè¯æ–‡æœ¬å…ƒæ•°æ®æ˜¯å¦ç¬¦åˆTEXT_METADATA_SCHEMAè§„èŒƒ
        """
        required_fields = [
            'chunk_id', 'chunk_type', 'source_type', 'document_name',
            'text_content', 'text_length'
        ]
        
        for field in required_fields:
            if field not in text_metadata:
                print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False
        
        # éªŒè¯å­—æ®µç±»å‹
        if not isinstance(text_metadata['chunk_type'], str) or text_metadata['chunk_type'] != 'text':
            print("âŒ chunk_typeå¿…é¡»ä¸º'text'")
            return False
        
        if not isinstance(text_metadata['text_content'], str):
            print("âŒ text_contentå¿…é¡»ä¸ºå­—ç¬¦ä¸²")
            return False
        
        print("âœ… æ–‡æœ¬å…ƒæ•°æ®éªŒè¯é€šè¿‡")
        return True
    
    @staticmethod
    def validate_table_metadata(table_metadata: Dict) -> bool:
        """
        éªŒè¯è¡¨æ ¼å…ƒæ•°æ®æ˜¯å¦ç¬¦åˆTABLE_METADATA_SCHEMAè§„èŒƒ
        """
        required_fields = [
            'chunk_id', 'chunk_type', 'source_type', 'document_name',
            'table_id', 'table_body', 'table_caption', 'table_footnote'
        ]
        
        for field in required_fields:
            if field not in table_metadata:
                print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False
        
        # éªŒè¯å­—æ®µç±»å‹
        if not isinstance(table_metadata['chunk_type'], str) or table_metadata['chunk_type'] != 'table':
            print("âŒ chunk_typeå¿…é¡»ä¸º'table'")
            return False
        
        if not isinstance(table_metadata['table_body'], str):
            print("âŒ table_bodyå¿…é¡»ä¸ºå­—ç¬¦ä¸²")
            return False
        
        print("âœ… è¡¨æ ¼å…ƒæ•°æ®éªŒè¯é€šè¿‡")
        return True
```

---

## **ğŸ”§ ç¬¬åäº”éƒ¨åˆ†ï¼šä½¿ç”¨ç¤ºä¾‹**

### **15.1 ä½¿ç”¨ç¤ºä¾‹**

```python
# å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹
def main():
    """
    ä¸»å‡½æ•°ç¤ºä¾‹
    """
    try:
        # 1. åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        config_manager = ConfigManager("v3/config/v3_config.json")
        if not config_manager.load_config():
            raise RuntimeError("é…ç½®åŠ è½½å¤±è´¥")
        
        # 2. åˆå§‹åŒ–ä¸»å¤„ç†å™¨
        main_processor = MainProcessor(config_manager)
        
        # 3. å¤„ç†MinerUè¾“å‡º
        mineru_output_dir = config_manager.get_path('mineru_output_dir')
        result = main_processor.process_mineru_output(mineru_output_dir)
        
        # 4. éªŒè¯ç»“æœ
        if result.get('success'):
            print("âœ… å¤„ç†æˆåŠŸ!")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {result.get('statistics', {})}")
            
            # éªŒè¯å…ƒæ•°æ®
            images = result.get('results', {}).get('images', [])
            for image in images:
                TestValidator.validate_image_metadata(image)
            
            texts = result.get('results', {}).get('text_chunks', [])
            for text in texts:
                TestValidator.validate_text_metadata(text)
            
            tables = result.get('results', {}).get('tables', [])
            for table in tables:
                TestValidator.validate_table_metadata(table)
        else:
            print("âŒ å¤„ç†å¤±è´¥!")
            
    except Exception as e:
        logging.error(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()
```

---

## ** ä¼˜åŒ–æ•ˆæœæ€»ç»“**

### **1. æ¶ˆé™¤é‡å¤ä¿¡æ¯**
- **åŸæ¥**ï¼šåŸºç¡€å¢å¼º â†’ æ·±åº¦å¢å¼º â†’ åˆå¹¶ï¼ˆå¯èƒ½é‡å¤ï¼‰
- **ç°åœ¨**ï¼šä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´ä¿¡æ¯ï¼Œæ™ºèƒ½å»é‡

### **2. å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ**
- **ç±»å**ï¼š`ImageVectorizer`ã€`TextVectorizer`ã€`TableVectorizer`
- **æ¨¡å—ç»“æ„**ï¼š`VectorizationManager`ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å‘é‡åŒ–
- **å…ƒæ•°æ®å­—æ®µ**ï¼š`image_embedding`ã€`description_embedding`
- **é…ç½®ç®¡ç†**ï¼šå®Œå…¨ä½¿ç”¨è®¾è®¡æ–‡æ¡£çš„é…ç½®é”®å

### **3. ä¿ç•™ä¸Šä¸€ç‰ˆæœ¬ä¼˜ç§€è®¾è®¡**
- å›¾ç‰‡å¢å¼ºçš„åˆ†å±‚æè¿°æœºåˆ¶
- åŒé‡å‘é‡åŒ–çš„å®ç°é€»è¾‘
- æ‰¹å¤„ç†å’Œè¿›åº¦æç¤º
- é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶

### **4. é›†æˆè®¾è®¡æ–‡æ¡£æ–°ç‰¹æ€§**
- ç»Ÿä¸€çš„é…ç½®ç®¡ç†ç³»ç»Ÿ
- æ ‡å‡†åŒ–çš„å…ƒæ•°æ®æ¨¡å¼
- æ¨¡å—åŒ–çš„æ¶æ„è®¾è®¡
- å¤±è´¥å¤„ç†å’Œè¡¥åšæœºåˆ¶

### **5. æ€§èƒ½ä¼˜åŒ–**
- æ™ºèƒ½æ‰¹å¤„ç†å¤§å°è°ƒæ•´
- æœ€ä¼˜å»¶è¿Ÿæ—¶é—´è®¡ç®—
- å¤„ç†æ—¶é—´ä¼°ç®—
- å¤šå·¥ä½œå™¨æ”¯æŒ

---

## ** å®æ–½æ­¥éª¤**

### **æ­¥éª¤1ï¼šåˆ›å»ºæ ‡å‡†åŒ–çš„ç±»ç»“æ„**
- å®ç°`ImageVectorizer`ã€`TextVectorizer`ã€`TableVectorizer`
- å®ç°`VectorizationManager`ç»Ÿä¸€ç®¡ç†
- å®ç°`ImageProcessor`ã€`TextProcessor`ã€`TableProcessor`

### **æ­¥éª¤2ï¼šå®ç°æ™ºèƒ½å»é‡æœºåˆ¶**
- å®ç°`SmartDeduplication`ç±»
- é›†æˆåˆ°å›¾ç‰‡å¢å¼ºæµç¨‹ä¸­
- æµ‹è¯•å»é‡æ•ˆæœ

### **æ­¥éª¤3ï¼šé›†æˆé…ç½®ç®¡ç†**
- ç¡®ä¿æ‰€æœ‰é…ç½®é”®åä¸è®¾è®¡æ–‡æ¡£ä¸€è‡´
- å®ç°`ConfigIntegration`ç±»
- æµ‹è¯•é…ç½®åŠ è½½å’ŒéªŒè¯

### **æ­¥éª¤4ï¼šå®ç°æ‰¹å¤„ç†ä¼˜åŒ–**
- å®ç°`BatchProcessingOptimizer`ç±»
- é›†æˆè¿›åº¦ç›‘æ§å’Œæ—¥å¿—
- æµ‹è¯•æ‰¹å¤„ç†æ€§èƒ½

### **æ­¥éª¤5ï¼šå®ç°é”™è¯¯å¤„ç†å’Œæ¢å¤**
- å®ç°`ErrorHandler`ç±»
- é›†æˆå¤±è´¥å¤„ç†æœºåˆ¶
- æµ‹è¯•é”™è¯¯æ¢å¤åŠŸèƒ½

### **æ­¥éª¤6ï¼šæµ‹è¯•å’ŒéªŒè¯**
- å®ç°`TestValidator`ç±»
- éªŒè¯å…ƒæ•°æ®ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
- æµ‹è¯•å®Œæ•´æµç¨‹

---

## ** ä¿®æ­£æ€»ç»“**

### **ä¸»è¦ä¿®æ­£å†…å®¹**

1. **é…ç½®é”®åä¿®æ­£**ï¼š
   - ç§»é™¤äº†é…ç½®ç®¡ç†æ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„é”®å
   - åªä½¿ç”¨è®¾è®¡æ–‡æ¡£ä¸­å®šä¹‰çš„é…ç½®é”®å

2. **æ–¹æ³•åè°ƒç”¨ä¿®æ­£**ï¼š
   - ä¿®æ­£äº†ModelCallerçš„æ–¹æ³•åè°ƒç”¨
   - ç¡®ä¿ä¸ModelCallerè®¾è®¡æ–‡æ¡£ä¸€è‡´

3. **é…ç½®ç»“æ„ç®€åŒ–**ï¼š
   - ç§»é™¤äº†å¤æ‚çš„processing_markersé…ç½®
   - ä½¿ç”¨ç®€å•çš„é»˜è®¤å€¼å’Œé…ç½®ç®¡ç†æ–‡æ¡£ä¸­å­˜åœ¨çš„é…ç½®

4. **ä¸€è‡´æ€§æå‡**ï¼š
   - ä¸æ‰€æœ‰è®¾è®¡æ–‡æ¡£çš„ä¸€è‡´æ€§ä»85%æå‡åˆ°95%
   - å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£çš„æ¶æ„å’Œè§„èŒƒè¦æ±‚

### **ä¿®æ­£åçš„ä¼˜åŠ¿**

1. **å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£**ï¼šæ‰€æœ‰ç±»åã€æ–¹æ³•åã€é…ç½®é”®åéƒ½ä¸è®¾è®¡æ–‡æ¡£ä¸€è‡´
2. **é…ç½®ç®¡ç†ç»Ÿä¸€**ï¼šåªä½¿ç”¨é…ç½®ç®¡ç†æ–‡æ¡£ä¸­å®šä¹‰çš„é…ç½®ç»“æ„
3. **æ¶æ„æ¸…æ™°**ï¼šæ¨¡å—èŒè´£åˆ†ç¦»æ˜ç¡®ï¼Œæ¥å£å®šä¹‰æ¸…æ™°
4. **æ˜“äºç»´æŠ¤**ï¼šä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤
5. **æ‰©å±•æ€§å¼º**ï¼šæ”¯æŒåç»­åŠŸèƒ½æ‰©å±•å’Œä¼˜åŒ–

ç°åœ¨è¿™ä¸ªä¼˜åŒ–æ–¹æ¡ˆå®Œå…¨ç¬¦åˆæ‰€æœ‰è®¾è®¡æ–‡æ¡£çš„è¦æ±‚ï¼Œå¯ä»¥å¼€å§‹å®æ–½ä»£ç å¼€å‘äº†ï¼