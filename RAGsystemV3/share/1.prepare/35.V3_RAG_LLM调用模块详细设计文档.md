# M13-RAG_LLM调用模块详细设计文档

## 一、文档基础信息

| 模块名称 | M13-RAG_LLM调用模块 | 所属项目 | V3版本RAG系统 |
| -------- | ------------------- | -------- | ------------- |
| 文档版本 | V1.0 | 文档状态 | ☑ 草稿 □ 评审中 □ 已确认 □ 已归档 |
| 编写人   | AI助手 | 编写日期 | 2025年8月 |
| 关联文档 | 《V3版本向量数据库构建系统简要设计文档》《M12-RAG查询处理模块详细设计文档》 | | |

## 二、模块概述

### 1. 定位与目标

作为RAG系统的**核心智能问答模块**，RAG LLM调用器承担所有大模型调用职责，包括问题理解、上下文处理、答案生成、会话管理等，为RAG系统提供高质量、智能化的问答服务。

### 2. 设计原则

- **专门化设计**：专门为RAG系统设计，不复用V3向量化模块
- **职责单一**：专注于LLM调用和问答生成，不涉及向量化处理
- **配置统一**：通过V3配置管理系统统一管理配置参数
- **接口清晰**：提供简洁明了的API接口，便于其他模块调用

### 3. 依赖与交互

| 关联模块 | 交互方向 | 核心交互内容 |
| -------- | -------- | ------------ |
| V3配置管理系统 | 依赖 | 获取LLM模型配置、API密钥、系统参数 |
| RAG查询处理模块 | 被调用 | 接收查询请求，返回生成的答案 |
| RAG上下文管理器 | 依赖 | 获取优化后的上下文信息 |
| RAG提示词管理器 | 依赖 | 获取格式化的提示词模板 |

## 三、核心功能设计

### 1. 功能清单

| 功能ID | 功能名称 | 核心描述 | 操作角色 | 前置条件 |
| ------- | -------- | -------- | -------- | -------- |
| FM01 | LLM模型初始化 | 初始化DashScope LLM模型，配置模型参数 | 系统启动 | API密钥有效、模型配置完整 |
| FM02 | 智能问答生成 | 基于用户问题和上下文生成答案 | 用户查询 | 模型已初始化、上下文有效 |
| FM03 | 提示词管理 | 管理不同类型的RAG提示词模板 | 系统自动 | 提示词模板配置完整 |
| FM04 | 上下文优化 | 优化和格式化上下文信息 | 系统自动 | 上下文数据有效 |
| FM05 | 会话管理 | 管理多轮对话的上下文和状态 | 用户会话 | 会话配置启用 |
| FM06 | 错误处理 | 处理LLM调用过程中的各种异常 | 系统自动 | 错误处理配置完整 |
| FM07 | 性能监控 | 监控LLM调用的性能和响应时间 | 系统自动 | 监控配置启用 |

### 2. 关键功能流程（智能问答生成为例）

1. 接收用户问题和检索到的上下文；
2. 通过提示词管理器构建RAG提示词；
3. 通过上下文管理器优化上下文信息；
4. 调用DashScope LLM模型生成答案；
5. 处理LLM响应，格式化输出结果；
6. 记录调用日志和性能指标；
7. 返回生成的答案和相关信息。

## 四、核心函数设计与调用关系

### 1. 函数清单

| 函数名 | 功能描述 | 输入参数 | 返回结果 | 所属服务 |
| ------ | -------- | -------- | -------- | -------- |
| `__init__(config_manager)` | LLM调用器初始化 | 配置管理器实例 | 无 | RAGLLMCaller |
| `_initialize_llm()` | 初始化LLM模型 | 无 | 无 | RAGLLMCaller |
| `generate_answer(question, context)` | 生成RAG答案 | 问题、上下文 | 答案结果字典 | RAGLLMCaller |
| `_build_rag_prompt(question, context)` | 构建RAG提示词 | 问题、上下文 | 完整提示词字符串 | RAGLLMCaller |
| `get_prompt(prompt_type, **kwargs)` | 获取提示词模板 | 提示词类型、参数 | 格式化提示词 | RAGPromptManager |
| `optimize_context(context_list, question)` | 优化上下文信息 | 上下文列表、问题 | 优化后的上下文 | RAGContextManager |
| `format_context(context_data)` | 格式化上下文数据 | 上下文数据 | 格式化字符串 | RAGContextManager |

### 2. 关键调用流程

```
RAG查询处理模块 → 调用LLM生成答案
    ↓
RAGLLMCaller.generate_answer()
    ↓
RAGPromptManager.get_prompt() → 构建提示词
    ↓
RAGContextManager.optimize_context() → 优化上下文
    ↓
DashScope LLM模型调用 → 生成答案
    ↓
结果处理和格式化 → 返回答案
```

## 五、数据结构设计

### 1. 核心数据结构

#### LLM调用结果字典（llm_result）
```python
{
    'success': True,                    # 调用是否成功
    'answer': '生成的答案内容',          # LLM生成的答案
    'question': '用户问题',              # 原始问题
    'context': '上下文信息',             # 使用的上下文
    'model': 'qwen-turbo',              # 使用的模型名称
    'timestamp': 1640995200.0,          # 调用时间戳
    'processing_time': 2.5,             # 处理时间（秒）
    'token_usage': {                    # Token使用情况
        'input_tokens': 150,
        'output_tokens': 200,
        'total_tokens': 350
    }
}
```

#### 提示词模板字典（prompt_template）
```python
{
    'general_qa': {
        'system': '系统提示词',
        'template': '基于以下上下文信息回答问题：\n\n上下文：{context}\n\n问题：{question}'
    },
    'table_qa': {
        'system': '表格分析提示词',
        'template': '基于以下表格信息回答问题：\n\n表格内容：{context}\n\n问题：{question}'
    },
    'image_qa': {
        'system': '图像分析提示词',
        'template': '基于以下图像信息回答问题：\n\n图像描述：{context}\n\n问题：{question}'
    }
}
```

#### 上下文优化配置（context_config）
```python
{
    'max_context_length': 4000,         # 最大上下文长度
    'context_selection_strategy': 'relevance',  # 上下文选择策略
    'overlap_threshold': 0.3,           # 重叠阈值
    'quality_weights': {                # 质量权重
        'relevance': 0.6,
        'completeness': 0.3,
        'freshness': 0.1
    }
}
```

### 2. 核心数据表

#### LLM调用记录表（llm_call_logs）
| 字段名 | 数据类型 | 主键 | 说明 | 示例 |
| ------ | -------- | ---- | ---- | ---- |
| call_id | STRING | 是 | 调用唯一标识 | "call_20241218_001" |
| question | STRING | 否 | 用户问题 | "中芯国际的业绩如何？" |
| context_length | INTEGER | 否 | 上下文长度 | 3500 |
| model_name | STRING | 否 | 使用模型 | "qwen-turbo" |
| processing_time | FLOAT | 否 | 处理时间 | 2.5 |
| success | BOOLEAN | 否 | 是否成功 | true |
| timestamp | TIMESTAMP | 否 | 调用时间 | "2024-12-18 10:30:00" |

#### 提示词模板表（prompt_templates）
| 字段名 | 数据类型 | 主键 | 说明 | 示例 |
| ------ | -------- | ---- | ---- | ---- |
| template_id | STRING | 是 | 模板唯一标识 | "general_qa" |
| template_name | STRING | 否 | 模板名称 | "通用问答" |
| system_prompt | STRING | 否 | 系统提示词 | "你是一个专业的AI助手..." |
| prompt_template | STRING | 否 | 提示词模板 | "基于以下上下文..." |
| is_active | BOOLEAN | 否 | 是否启用 | true |
| created_time | TIMESTAMP | 否 | 创建时间 | "2024-12-18 10:00:00" |

## 六、核心接口设计

| 接口名 | 请求方式 | 请求地址 | 核心参数 | 返回结果 | 功能归属 |
| ------ | -------- | -------- | -------- | -------- | -------- |
| LLM答案生成接口 | POST | /api/v3/rag/llm/generate | question, context | 生成的答案 | FM02 |
| 提示词获取接口 | GET | /api/v3/rag/prompt/get | prompt_type, params | 提示词模板 | FM03 |
| 上下文优化接口 | POST | /api/v3/rag/context/optimize | context_list, question | 优化后的上下文 | FM04 |
| 会话管理接口 | POST | /api/v3/rag/conversation/manage | session_id, action | 会话状态 | FM05 |
| 性能监控接口 | GET | /api/v3/rag/llm/metrics | time_range | 性能指标 | FM07 |

## 七、关键实现细节

### 1. RAG LLM调用器核心实现

```python
class RAGLLMCaller:
    """RAG系统专用LLM调用器"""
    
    def __init__(self, config_manager):
        """
        初始化RAG LLM调用器
        
        :param config_manager: V3配置管理器实例
        """
        self.config_manager = config_manager
        
        # 获取RAG LLM配置
        self.rag_config = config_manager.get_rag_config('models.llm', {})
        
        # LLM模型参数
        self.model_name = self.rag_config.get('model_name', 'qwen-turbo')
        self.max_tokens = self.rag_config.get('max_tokens', 2048)
        self.temperature = self.rag_config.get('temperature', 0.7)
        self.system_prompt = self.rag_config.get('system_prompt', '')
        
        # 初始化子管理器
        self.prompt_manager = RAGPromptManager(config_manager)
        self.context_manager = RAGContextManager(config_manager)
        
        # 初始化LLM模型
        self._initialize_llm()
        
        logging.info("RAG LLM调用器初始化完成")
    
    def _initialize_llm(self):
        """初始化LLM模型"""
        try:
            # 获取API密钥（复用V3的环境变量管理）
            env_manager = self.config_manager.get_environment_manager()
            api_key = env_manager.get_required_var('DASHSCOPE_API_KEY')
            
            if not api_key:
                raise ValueError("未找到有效的DashScope API密钥")
            
            # 初始化DashScope LLM
            from langchain_community.llms import DashScopeLLM
            self.llm = DashScopeLLM(
                model_name=self.model_name,
                dashscope_api_key=api_key,
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            
            logging.info(f"RAG LLM模型初始化成功: {self.model_name}")
            
        except Exception as e:
            logging.error(f"RAG LLM模型初始化失败: {e}")
            raise
    
    def generate_answer(self, question: str, context: str, **kwargs) -> Dict[str, Any]:
        """
        生成RAG答案
        
        :param question: 用户问题
        :param context: 检索到的上下文
        :param kwargs: 其他参数
        :return: 生成结果字典
        """
        start_time = time.time()
        
        try:
            # 构建RAG提示词
            prompt = self._build_rag_prompt(question, context)
            
            # 调用LLM模型
            response = self.llm.invoke(prompt)
            
            # 计算处理时间
            processing_time = time.time() - start_time
            
            result = {
                'success': True,
                'answer': response,
                'question': question,
                'context': context,
                'model': self.model_name,
                'timestamp': time.time(),
                'processing_time': processing_time,
                'token_usage': self._estimate_token_usage(prompt, response)
            }
            
            logging.info(f"RAG答案生成成功，处理时间: {processing_time:.2f}秒")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"RAG答案生成失败: {str(e)}"
            logging.error(error_msg)
            
            return {
                'success': False,
                'error': str(e),
                'question': question,
                'context': context,
                'model': self.model_name,
                'timestamp': time.time(),
                'processing_time': processing_time
            }
    
    def _build_rag_prompt(self, question: str, context: str) -> str:
        """构建RAG提示词"""
        # 使用提示词管理器获取模板
        prompt_type = 'general_qa'  # 可以根据上下文类型动态选择
        
        return self.prompt_manager.get_prompt(prompt_type, context=context, question=question)
    
    def _estimate_token_usage(self, prompt: str, response: str) -> Dict[str, int]:
        """估算Token使用情况"""
        # 简单的字符数估算，实际可以使用更精确的tokenizer
        input_tokens = len(prompt) // 4  # 粗略估算
        output_tokens = len(response) // 4
        
        return {
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'total_tokens': input_tokens + output_tokens
        }
```

### 2. 提示词管理器实现

```python
class RAGPromptManager:
    """RAG提示词管理器"""
    
    def __init__(self, config_manager):
        """
        初始化提示词管理器
        
        :param config_manager: V3配置管理器实例
        """
        self.config_manager = config_manager
        self.prompt_templates = self._load_prompt_templates()
    
    def _load_prompt_templates(self):
        """加载提示词模板"""
        # 从配置文件加载，如果没有则使用默认模板
        config_templates = self.config_manager.get_rag_config('prompt_templates', {})
        
        if config_templates:
            return config_templates
        
        # 默认模板
        return {
            'general_qa': {
                'system': "你是一个专业的AI助手，能够基于提供的上下文信息生成准确、相关、完整的答案。",
                'template': "基于以下上下文信息回答问题：\n\n上下文：{context}\n\n问题：{question}"
            },
            'table_qa': {
                'system': "你是一个专业的表格数据分析助手，能够基于表格内容回答问题。",
                'template': "基于以下表格信息回答问题：\n\n表格内容：{context}\n\n问题：{question}"
            },
            'image_qa': {
                'system': "你是一个专业的图像分析助手，能够基于图像描述回答问题。",
                'template': "基于以下图像信息回答问题：\n\n图像描述：{context}\n\n问题：{question}"
            }
        }
    
    def get_prompt(self, prompt_type: str, **kwargs) -> str:
        """
        获取指定类型的提示词
        
        :param prompt_type: 提示词类型
        :param kwargs: 格式化参数
        :return: 格式化后的提示词
        """
        if prompt_type not in self.prompt_templates:
            prompt_type = 'general_qa'
        
        template = self.prompt_templates[prompt_type]
        system_prompt = template['system']
        prompt_content = template['template'].format(**kwargs)
        
        return f"{system_prompt}\n\n{prompt_content}"
```

### 3. 上下文管理器实现

```python
class RAGContextManager:
    """RAG上下文管理器"""
    
    def __init__(self, config_manager):
        """
        初始化上下文管理器
        
        :param config_manager: V3配置管理器实例
        """
        self.config_manager = config_manager
        self.max_context_length = self.config_manager.get_rag_config('query_processing.max_context_length', 4000)
    
    def optimize_context(self, context_list: List[str], question: str) -> str:
        """
        优化上下文，选择最相关的内容
        
        :param context_list: 上下文列表
        :param question: 用户问题
        :return: 优化后的上下文
        """
        if not context_list:
            return ""
        
        # 简单的上下文优化策略
        # 1. 合并所有上下文
        combined_context = "\n\n".join(context_list)
        
        # 2. 如果超出长度限制，进行截断
        if len(combined_context) > self.max_context_length:
            # 保留前面的内容，截断后面
            combined_context = combined_context[:self.max_context_length] + "..."
            logging.warning(f"上下文长度超出限制，已截断至{self.max_context_length}字符")
        
        return combined_context
    
    def format_context(self, context_data: Dict[str, Any]) -> str:
        """
        格式化上下文数据
        
        :param context_data: 上下文数据
        :return: 格式化后的上下文字符串
        """
        if isinstance(context_data, str):
            return context_data
        
        # 处理不同类型的上下文数据
        if 'content' in context_data:
            return context_data['content']
        elif 'text' in context_data:
            return context_data['text']
        elif 'description' in context_data:
            return context_data['description']
        else:
            return str(context_data)
```

## 八、非功能需求

- **性能**：LLM调用响应时间≤30秒，支持并发查询处理，单次调用Token消耗≤4000；
- **可靠性**：API调用失败重试机制，错误处理完善，支持降级策略；
- **安全性**：API密钥安全管理，输入输出内容过滤，防止敏感信息泄露。

## 九、风险与应对措施

| 潜在风险 | 应对措施 |
| -------- | -------- |
| LLM API调用失败 | 1. 实现重试机制；2. 错误日志记录；3. 降级策略（返回默认答案） |
| 上下文过长 | 1. 智能截断策略；2. 相关性排序；3. 长度限制配置 |
| 提示词注入攻击 | 1. 输入内容过滤；2. 提示词模板验证；3. 安全字符转义 |
| 性能瓶颈 | 1. 异步调用支持；2. 缓存机制；3. 并发控制 |

## 十、附件

- 附件1：LLM模型配置参数说明
- 附件2：提示词模板最佳实践
- 附件3：上下文优化策略详解
- 附件4：错误处理代码示例
- 附件5：性能监控指标说明

---

**文档版本历史**：
- V1.0 (2025-08-XX): 初始版本，基于RAG系统需求设计
