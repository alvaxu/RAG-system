{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3ad4f-9492-45c4-a693-570ecb300582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "å±äº InternVL 2.5ç³»åˆ—\n",
    "è§†é¢‘ç†è§£ä¸ç”Ÿæˆï¼šå¯ä»¥ç”¨äºè§†é¢‘å†…å®¹çš„åˆ†æã€æ€»ç»“å’Œç”Ÿæˆç›¸å…³çš„æ–‡æœ¬æè¿°ã€‚\n",
    "è§†è§‰é—®ç­”ï¼šèƒ½å¤Ÿå›ç­”ä¸å›¾åƒæˆ–è§†é¢‘å†…å®¹ç›¸å…³çš„é—®é¢˜ã€‚\n",
    "å¤šæ¨¡æ€å¯¹è¯ï¼šæ”¯æŒä¸ç”¨æˆ·è¿›è¡ŒåŒ…å«è§†è§‰ä¿¡æ¯çš„å¯¹è¯ã€‚\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0cfc4c-d3b1-47d0-ba82-8c4754ad5f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9942fcc12b6479ba30002bdec0b8931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from modelscope import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# æ¨¡å‹é…ç½®\n",
    "model_path = '/root/autodl-tmp/models/OpenGVLab/InternVideo2_5_Chat_8B'\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda().to(torch.bfloat16)\n",
    "\n",
    "# ImageNet æ•°æ®é›†çš„å‡å€¼å’Œæ ‡å‡†å·®\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"\n",
    "    æ„å»ºå›¾åƒè½¬æ¢pipeline\n",
    "    \n",
    "    å‚æ•°:\n",
    "        input_size: è¾“å…¥å›¾åƒå¤§å°\n",
    "    \n",
    "    è¿”å›:\n",
    "        transform: è½¬æ¢pipeline\n",
    "    \"\"\"\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img), \n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC), \n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"\n",
    "    å¯»æ‰¾æœ€æ¥è¿‘åŸå§‹å›¾åƒå®½é«˜æ¯”çš„ç›®æ ‡æ¯”ä¾‹\n",
    "    \n",
    "    å‚æ•°:\n",
    "        aspect_ratio: åŸå§‹å›¾åƒçš„å®½é«˜æ¯”\n",
    "        target_ratios: ç›®æ ‡æ¯”ä¾‹åˆ—è¡¨\n",
    "        width: åŸå§‹å›¾åƒå®½åº¦\n",
    "        height: åŸå§‹å›¾åƒé«˜åº¦\n",
    "        image_size: ç›®æ ‡å›¾åƒå¤§å°\n",
    "        \n",
    "    è¿”å›:\n",
    "        best_ratio: æœ€ä½³æ¯”ä¾‹\n",
    "    \"\"\"\n",
    "    best_ratio_diff = float(\"inf\")\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"\n",
    "    åŠ¨æ€é¢„å¤„ç†å›¾åƒï¼Œæ ¹æ®å®½é«˜æ¯”å°†å›¾åƒåˆ†å‰²æˆå¤šä¸ªå—\n",
    "    \n",
    "    å‚æ•°:\n",
    "        image: åŸå§‹å›¾åƒ\n",
    "        min_num: æœ€å°å—æ•°\n",
    "        max_num: æœ€å¤§å—æ•°\n",
    "        image_size: ç›®æ ‡å›¾åƒå¤§å°\n",
    "        use_thumbnail: æ˜¯å¦ä½¿ç”¨ç¼©ç•¥å›¾\n",
    "        \n",
    "    è¿”å›:\n",
    "        processed_images: å¤„ç†åçš„å›¾åƒåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # è®¡ç®—ç°æœ‰å›¾åƒå®½é«˜æ¯”\n",
    "    target_ratios = set((i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # å¯»æ‰¾æœ€æ¥è¿‘ç›®æ ‡çš„å®½é«˜æ¯”\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # è®¡ç®—ç›®æ ‡å®½åº¦å’Œé«˜åº¦\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # è°ƒæ•´å›¾åƒå¤§å°\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = ((i % (target_width // image_size)) * image_size, (i // (target_width // image_size)) * image_size, \n",
    "               ((i % (target_width // image_size)) + 1) * image_size, ((i // (target_width // image_size)) + 1) * image_size)\n",
    "        # åˆ†å‰²å›¾åƒ\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image, input_size=448, max_num=6):\n",
    "    \"\"\"\n",
    "    åŠ è½½å¹¶å¤„ç†å›¾åƒ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        image: è¾“å…¥å›¾åƒ\n",
    "        input_size: è¾“å…¥å¤§å°\n",
    "        max_num: æœ€å¤§å—æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        pixel_values: å¤„ç†åçš„å›¾åƒå¼ é‡\n",
    "    \"\"\"\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    \"\"\"\n",
    "    è·å–è§†é¢‘å¸§ç´¢å¼•\n",
    "    \n",
    "    å‚æ•°:\n",
    "        bound: æ—¶é—´è¾¹ç•Œ [å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´]\n",
    "        fps: è§†é¢‘å¸§ç‡\n",
    "        max_frame: æœ€å¤§å¸§æ•°\n",
    "        first_idx: ç¬¬ä¸€å¸§ç´¢å¼•\n",
    "        num_segments: åˆ†æ®µæ•°é‡\n",
    "        \n",
    "    è¿”å›:\n",
    "        frame_indices: å¸§ç´¢å¼•æ•°ç»„\n",
    "    \"\"\"\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([int(start_idx + (seg_size / 2) + np.round(seg_size * idx)) for idx in range(num_segments)])\n",
    "    return frame_indices\n",
    "\n",
    "def get_num_frames_by_duration(duration):\n",
    "    \"\"\"\n",
    "    æ ¹æ®è§†é¢‘æ—¶é•¿è®¡ç®—å¸§æ•°\n",
    "    \n",
    "    å‚æ•°:\n",
    "        duration: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰\n",
    "        \n",
    "    è¿”å›:\n",
    "        num_frames: è®¡ç®—å‡ºçš„å¸§æ•°\n",
    "    \"\"\"\n",
    "    local_num_frames = 4        \n",
    "    num_segments = int(duration // local_num_frames)\n",
    "    if num_segments == 0:\n",
    "        num_frames = local_num_frames\n",
    "    else:\n",
    "        num_frames = local_num_frames * num_segments\n",
    "    \n",
    "    num_frames = min(512, num_frames)\n",
    "    num_frames = max(128, num_frames)\n",
    "\n",
    "    return num_frames\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32, get_frame_by_duration = False):\n",
    "    \"\"\"\n",
    "    åŠ è½½å¹¶å¤„ç†è§†é¢‘\n",
    "    \n",
    "    å‚æ•°:\n",
    "        video_path: è§†é¢‘è·¯å¾„\n",
    "        bound: æ—¶é—´è¾¹ç•Œ\n",
    "        input_size: è¾“å…¥å¤§å°\n",
    "        max_num: æœ€å¤§å—æ•°\n",
    "        num_segments: åˆ†æ®µæ•°é‡\n",
    "        get_frame_by_duration: æ˜¯å¦æ ¹æ®æ—¶é•¿è·å–å¸§æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        pixel_values: å¤„ç†åçš„è§†é¢‘å¸§å¼ é‡\n",
    "        num_patches_list: æ¯å¸§çš„å—æ•°åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    if get_frame_by_duration:\n",
    "        duration = max_frame / fps\n",
    "        num_segments = get_num_frames_by_duration(duration)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert(\"RGB\")\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e992d5-6e66-469a-8f21-062c00967c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video clip features a close-up of a person's hands interacting with a vintage-looking, silver and white portable radio. The radio has two large speakers on the front, a cassette player on top, and a control panel with buttons and a dial. The person's hands are adjusting the radio, possibly tuning it or selecting a station. They are wearing a red long-sleeve shirt and blue denim shorts, suggesting a casual setting. The environment appears to be outdoors, with natural light indicating daytime. The background is blurred but hints at an urban setting with metal structures, possibly a park bench or outdoor seating area. The overall color palette is warm with the red shirt standing out against the cooler tones of the radio and the blue shorts.\n",
      "One person appears in the video.\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°è®¾ç½®\n",
    "max_num_frames = 512\n",
    "generation_config = dict(\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.1,\n",
    "    num_beams=1\n",
    ")\n",
    "video_path = \"test.mp4\"\n",
    "num_segments=128\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  # åŠ è½½è§†é¢‘å¹¶å¤„ç†\n",
    "  pixel_values, num_patches_list = load_video(video_path, num_segments=num_segments, max_num=1, get_frame_by_duration=False)\n",
    "  pixel_values = pixel_values.to(torch.bfloat16).to(model.device)\n",
    "  video_prefix = \"\".join([f\"Frame{i+1}: <image>\\n\" for i in range(len(num_patches_list))])\n",
    "  \n",
    "  # å•è½®å¯¹è¯ï¼šè§†é¢‘è¯¦ç»†æè¿°\n",
    "  question1 = \"Describe this video in detail.\"\n",
    "  question = video_prefix + question1\n",
    "  output1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "  print(output1)\n",
    "  \n",
    "  # å¤šè½®å¯¹è¯ï¼šè¯¢é—®è§†é¢‘ä¸­çš„äººæ•°\n",
    "  question2 = \"How many people appear in the video?\"\n",
    "  output2, chat_history = model.chat(tokenizer, pixel_values, question2, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\n",
    "  \n",
    "  print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d691952-7126-4dce-8ded-b61af1ab9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4528ec18-0b11-4bd9-9c86-446bf345a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½•éŸ³æœºæ˜¯æ¾ä¸‹çš„ã€‚\n",
      "è¿™ä¸ªäººæœ‰æ·±è‰²çš®è‚¤ã€‚\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  # å•è½®å¯¹è¯ï¼š\n",
    "  question1 = \"å½•éŸ³æœºæ˜¯ä»€ä¹ˆå“ç‰Œçš„ï¼Ÿ\"\n",
    "  question = video_prefix + question1\n",
    "  output1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "  print(output1)\n",
    "  \n",
    "  # å¤šè½®å¯¹è¯ï¼š\n",
    "  question2 = \"è¿™ä¸ªäººä»€ä¹ˆè‚¤è‰²ï¼Ÿ\"\n",
    "  output2, chat_history = model.chat(tokenizer, pixel_values, question2, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\n",
    "  \n",
    "  print(output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
