{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3ad4f-9492-45c4-a693-570ecb300582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "属于 InternVL 2.5系列\n",
    "视频理解与生成：可以用于视频内容的分析、总结和生成相关的文本描述。\n",
    "视觉问答：能够回答与图像或视频内容相关的问题。\n",
    "多模态对话：支持与用户进行包含视觉信息的对话。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0cfc4c-d3b1-47d0-ba82-8c4754ad5f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9942fcc12b6479ba30002bdec0b8931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from modelscope import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# 模型配置\n",
    "model_path = '/root/autodl-tmp/models/OpenGVLab/InternVideo2_5_Chat_8B'\n",
    "\n",
    "# 初始化分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda().to(torch.bfloat16)\n",
    "\n",
    "# ImageNet 数据集的均值和标准差\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"\n",
    "    构建图像转换pipeline\n",
    "    \n",
    "    参数:\n",
    "        input_size: 输入图像大小\n",
    "    \n",
    "    返回:\n",
    "        transform: 转换pipeline\n",
    "    \"\"\"\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img), \n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC), \n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"\n",
    "    寻找最接近原始图像宽高比的目标比例\n",
    "    \n",
    "    参数:\n",
    "        aspect_ratio: 原始图像的宽高比\n",
    "        target_ratios: 目标比例列表\n",
    "        width: 原始图像宽度\n",
    "        height: 原始图像高度\n",
    "        image_size: 目标图像大小\n",
    "        \n",
    "    返回:\n",
    "        best_ratio: 最佳比例\n",
    "    \"\"\"\n",
    "    best_ratio_diff = float(\"inf\")\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"\n",
    "    动态预处理图像，根据宽高比将图像分割成多个块\n",
    "    \n",
    "    参数:\n",
    "        image: 原始图像\n",
    "        min_num: 最小块数\n",
    "        max_num: 最大块数\n",
    "        image_size: 目标图像大小\n",
    "        use_thumbnail: 是否使用缩略图\n",
    "        \n",
    "    返回:\n",
    "        processed_images: 处理后的图像列表\n",
    "    \"\"\"\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # 计算现有图像宽高比\n",
    "    target_ratios = set((i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # 寻找最接近目标的宽高比\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # 计算目标宽度和高度\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # 调整图像大小\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = ((i % (target_width // image_size)) * image_size, (i // (target_width // image_size)) * image_size, \n",
    "               ((i % (target_width // image_size)) + 1) * image_size, ((i // (target_width // image_size)) + 1) * image_size)\n",
    "        # 分割图像\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image, input_size=448, max_num=6):\n",
    "    \"\"\"\n",
    "    加载并处理图像\n",
    "    \n",
    "    参数:\n",
    "        image: 输入图像\n",
    "        input_size: 输入大小\n",
    "        max_num: 最大块数\n",
    "        \n",
    "    返回:\n",
    "        pixel_values: 处理后的图像张量\n",
    "    \"\"\"\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    \"\"\"\n",
    "    获取视频帧索引\n",
    "    \n",
    "    参数:\n",
    "        bound: 时间边界 [开始时间, 结束时间]\n",
    "        fps: 视频帧率\n",
    "        max_frame: 最大帧数\n",
    "        first_idx: 第一帧索引\n",
    "        num_segments: 分段数量\n",
    "        \n",
    "    返回:\n",
    "        frame_indices: 帧索引数组\n",
    "    \"\"\"\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([int(start_idx + (seg_size / 2) + np.round(seg_size * idx)) for idx in range(num_segments)])\n",
    "    return frame_indices\n",
    "\n",
    "def get_num_frames_by_duration(duration):\n",
    "    \"\"\"\n",
    "    根据视频时长计算帧数\n",
    "    \n",
    "    参数:\n",
    "        duration: 视频时长（秒）\n",
    "        \n",
    "    返回:\n",
    "        num_frames: 计算出的帧数\n",
    "    \"\"\"\n",
    "    local_num_frames = 4        \n",
    "    num_segments = int(duration // local_num_frames)\n",
    "    if num_segments == 0:\n",
    "        num_frames = local_num_frames\n",
    "    else:\n",
    "        num_frames = local_num_frames * num_segments\n",
    "    \n",
    "    num_frames = min(512, num_frames)\n",
    "    num_frames = max(128, num_frames)\n",
    "\n",
    "    return num_frames\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32, get_frame_by_duration = False):\n",
    "    \"\"\"\n",
    "    加载并处理视频\n",
    "    \n",
    "    参数:\n",
    "        video_path: 视频路径\n",
    "        bound: 时间边界\n",
    "        input_size: 输入大小\n",
    "        max_num: 最大块数\n",
    "        num_segments: 分段数量\n",
    "        get_frame_by_duration: 是否根据时长获取帧数\n",
    "        \n",
    "    返回:\n",
    "        pixel_values: 处理后的视频帧张量\n",
    "        num_patches_list: 每帧的块数列表\n",
    "    \"\"\"\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    if get_frame_by_duration:\n",
    "        duration = max_frame / fps\n",
    "        num_segments = get_num_frames_by_duration(duration)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert(\"RGB\")\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e992d5-6e66-469a-8f21-062c00967c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video clip features a close-up of a person's hands interacting with a vintage-looking, silver and white portable radio. The radio has two large speakers on the front, a cassette player on top, and a control panel with buttons and a dial. The person's hands are adjusting the radio, possibly tuning it or selecting a station. They are wearing a red long-sleeve shirt and blue denim shorts, suggesting a casual setting. The environment appears to be outdoors, with natural light indicating daytime. The background is blurred but hints at an urban setting with metal structures, possibly a park bench or outdoor seating area. The overall color palette is warm with the red shirt standing out against the cooler tones of the radio and the blue shorts.\n",
      "One person appears in the video.\n"
     ]
    }
   ],
   "source": [
    "# 评估设置\n",
    "max_num_frames = 512\n",
    "generation_config = dict(\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.1,\n",
    "    num_beams=1\n",
    ")\n",
    "video_path = \"test.mp4\"\n",
    "num_segments=128\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  # 加载视频并处理\n",
    "  pixel_values, num_patches_list = load_video(video_path, num_segments=num_segments, max_num=1, get_frame_by_duration=False)\n",
    "  pixel_values = pixel_values.to(torch.bfloat16).to(model.device)\n",
    "  video_prefix = \"\".join([f\"Frame{i+1}: <image>\\n\" for i in range(len(num_patches_list))])\n",
    "  \n",
    "  # 单轮对话：视频详细描述\n",
    "  question1 = \"Describe this video in detail.\"\n",
    "  question = video_prefix + question1\n",
    "  output1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "  print(output1)\n",
    "  \n",
    "  # 多轮对话：询问视频中的人数\n",
    "  question2 = \"How many people appear in the video?\"\n",
    "  output2, chat_history = model.chat(tokenizer, pixel_values, question2, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\n",
    "  \n",
    "  print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d691952-7126-4dce-8ded-b61af1ab9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4528ec18-0b11-4bd9-9c86-446bf345a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "录音机是松下的。\n",
      "这个人有深色皮肤。\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  # 单轮对话：\n",
    "  question1 = \"录音机是什么品牌的？\"\n",
    "  question = video_prefix + question1\n",
    "  output1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "  print(output1)\n",
    "  \n",
    "  # 多轮对话：\n",
    "  question2 = \"这个人什么肤色？\"\n",
    "  output2, chat_history = model.chat(tokenizer, pixel_values, question2, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\n",
    "  \n",
    "  print(output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
