# 资金流入流出预测——进阶优化方案

---

## 一、核心思路转变：从宏观时序到微观个体

您当前的Prophet、周期因子及融合模型均属于在**聚合后**的整体数据上进行预测的**宏观时序模型**。这类模型非常适合捕捉全局的趋势、周期性和节假日效应，但在利用个体信息上存在天然瓶颈。为了突破当前性能极限，我们亟需引入新的信息维度。

**核心转变**：从预测"所有用户在某天的总申购/赎回量"转向预测"**单个用户在某天的申购/赎回量**"，然后将所有用户的预测结果加总，得到最终的每日预测总量。

**优势**：
1.  **利用更丰富的特征**：可以直接利用 `user_profile_table` 中的用户画像（性别、城市、星座）以及每个用户独立的历史交易数据。
2.  **更精细的模式捕捉**：不同城市、不同星座的用户在节假日或月初/月末的行为模式可能天差地别，个体模型能捕捉到这种差异。
3.  **更强的鲁棒性**：模型不易被少数行为异常的"大户"带偏，因为预测是基于所有个体的行为总和。

---

## 二、方案一：用户级梯度提升回归模型（重点推荐）

这是最具潜力的方案，预计能带来显著的性能提升。

### 2.1 目标与流程
1.  **数据处理**：为每个 `(user_id, date)` 组合构建一条训练样本。
2.  **特征工程**：围绕每个用户，在每个日期点上，构建丰富的个体特征。
3.  **模型训练**：使用 **LightGBM** 或 **XGBoost** 模型，分别对申购和赎回进行训练。
4.  **预测与聚合**：预测2014年9月每一天、每一个用户的申购和赎回量，然后按天求和得到最终结果。

### 2.2 特征工程详解（关键步骤）

这是决定模型成败的核心。建议创建一个独立的特征工程脚本。

**1. 用户静态画像特征（已实现）**
   - 直接从 `user_profile_table.csv` 获取：`sex`, `city`, `constellation`。
   - 缺失画像填充为-1或'Unknown'。
   - 采用LightGBM原生类别特征支持（即pandas的category类型），无需手动独热编码，模型可自动高效处理。

**2. 用户动态历史行为特征（最重要！）**
   - **滞后特征 (Lag Features) - [已实现]**:
     - 已实现用户前1、3、7、14、30天的申购/赎回额滞后项（如 `total_purchase_amt_lag7`），并在递归预测阶段通过动态递推，确保lag特征在预测区间内用历史预测值递推，彻底解决了lag特征断崖归零问题。
   - **滑动窗口统计特征 (Rolling Window Features) - [已实现]**:
     - 已实现用户在过去7、14、30天内的申购/赎回统计值，包括均值、总额、标准差。
   - **用户活跃度特征 (已实现)**:
     - 已实现如 `days_since_last_purchase`、`purchase_count_30d` 等特征。

**3. 时间与节假日特征 - [已实现]**
   我们已实现了一套丰富的时间周期特征，将时间信息显式地编码给模型学习。
   - **基础时间特征**: `weekday`, `day`, `month`, `is_month_start`, `is_month_end`。
   - **节假日特征**: 基于`chinese_calendar`库，我们实现了`is_holiday` (当天是否为法定节假日) 和 `is_next_workday` (是否为节后第一个工作日) 这两个关键特征。
   - **高级周期特征 (继承自4.0版本)**: 我们还从之前成功的模型中移植了更精细的周期特征，包括 `month_period` (月初/中/末), `is_quarter_end` (是否季末), `is_festival_eve` (是否节日前一天) 以及 `holiday_len` (当前所处假期的长度)。

**4. 外部经济环境特征 - [已实现]**
   - 已将所有利率和收益率指标以滞后一天的形式引入。

**5. 交互特征（部分实现）**
   - 已实现：
     - `city_is_holiday`（城市 × 节假日）
     - `sex_weekday`（性别 × 星期几）
   - 未实现：
     - `city_weekday`（城市 × 星期几）
     - `constellation_is_holiday`（星座 × 节假日）
   - 如需进一步提升模型表达力，建议后续补充上述未实现的交互特征。

### 2.3 样本构建与模型训练
- **样本**：以 `(user_id, ds)` 为一行，构建一个包含上述所有特征和目标值（`purchase`, `redeem`）的大数据框。
- **模型**：
  - **LightGBM**：速度快，内存占用低，性能强大，是处理这类问题的首选。
  - **训练策略**：为申购和赎回分别训练两个模型。
- **验证策略**：
  - 使用2014年3月-7月的数据作为训练集。
  - 使用2014年8月的数据作为验证集，模拟真实预测场景，用于调参和特征筛选。

### 2.4 如何处理新增用户与用户数量变化（冷启动问题）

您提出的这个问题非常关键，因为竞赛说明中明确提到"部分用户在2014年9月份第一次出现"。对于这些没有历史数据的新用户，我们不能直接应用依赖历史特征的模型。一个优雅的解决方案是**通过特征工程让同一个模型智能地处理新、老用户**。

**核心思想**：我们不建立两套模型，而是通过添加"标识特征"和合理的"缺失值填充"，让一个模型学会两种预测模式。

**具体实施方案**：

1.  **识别新用户**:
    - 在构建需要预测的9月份数据集时，通过比对训练集（3-8月）的用户列表，找出在9月首次出现的`user_id`。

2.  **创建关键标识特征**:
    - 新增一个核心的二元特征，例如 `is_new_user`。
    - 如果一个`user_id`是首次出现，则 `is_new_user` 记为 `1`，否则记为 `0`。这个特征是模型区分新老用户的最直接、最重要的信号。

3.  **处理历史特征缺失**:
    - 对于新用户，所有基于历史行为的特征（如滞后N天申购额、过去N天滑动窗口统计值等）都会是`NaN`（缺失值）。
    - 将这些`NaN`值统一用 `0` 进行填充。这在逻辑上是合理的，因为它代表"历史上无相关记录"。

4.  **模型的自适应学习机制**:
    - 像LightGBM这样的梯度提升树模型，在构建决策树时，会非常高效地利用 `is_new_user` 这个特征。
    - **对于老用户 (`is_new_user == 0`)**: 模型会正常地沿着决策树的分支，继续深入利用该用户丰富的历史行为特征（滞后值、滑动均值等）来进行精细化的预测。
    - **对于新用户 (`is_new_user == 1`)**: 模型会学习到走向另一条完全不同的决策路径。在这条路径上，由于所有用户的历史特征都被填充为0，模型会"放弃"这些无差别的信息，转而**完全依赖那些对新用户依然有效的共享特征**来进行预测，例如：
        - **时间与节假日特征**：当天是周几？是否节假日？是否月初/月末？
        - **外部经济环境特征**：当天的Shibor利率和余额宝收益率是多少？
        - **用户静态画像特征**（如果新用户有此信息）：用户的城市、性别、星座是什么？（如果也缺失，可填充为"未知"类别）
    - 最终，模型实质上是为新用户学习了一套"在当前宏观环境下，一个典型用户应该有怎样的申购/赎回行为"的预测模式，从而优雅地解决了冷启动问题。

通过这种方式，我们仅用一个模型就兼顾了两种截然不同的用户群体，大大简化了工程复杂度，同时保证了预测的有效性。

### 2.5 数据可用性及特征构建的稳健性

您提出的关于预测时数据可用性的问题非常关键。一个稳健的方案必须确保在预测未来（如9月）的任何一天时，所使用的全部特征在该时间点都是"已知"的。基于此原则，我们对特征构建进行如下重要补充：

**1. 关于用户画像数据 (`user_profile_table`)**
- **问题**：竞赛说明已明确，9月出现的新用户在画像表中没有记录。
- **解决方案**：在进行左连接时，新用户的`sex`, `city`, `constellation`等字段会自然产生`NaN`。我们不应丢弃或简单插值，而应将其视为一种特殊的、有意义的类别。例如，可以填充为-1或"Unknown"这样的字符串。这样，模型可以学习到"画像未知"的用户群体的行为模式。

**2. 关于宏观经济数据 (`mfd_*` 开头的文件)**
- **问题**：在预测9月1日时，我们不可能提前知道9月1日的利率和收益率。我们能获取到的最新数据截止到8月31日。
- **解决方案**：**所有外部宏观特征都必须使用滞后值（Lagged Features）**。这是时间序列建模的黄金法则。
    - **特征重新定义**：我们构建的特征不应该是`当日收益率`，而应该是`昨日收益率(lag=1)`、`3日前收益率(lag=3)`、`上周同日收益率(lag=7)`等。
    - **同理**：Shibor利率特征也必须是`昨日隔夜利率`、`昨日1周利率`等滞后形式。
    - **实现方式**：在特征工程中，当为日期`D`构建特征时，合并的宏观数据应取自日期`D-1`, `D-2`, ... `D-n`。
- **结论**：通过这种方式，模型学习的逻辑是"基于过去已知的宏观环境，预测今天的用户行为"，这确保了我们的模型在真实的预测场景中是完全可行的。

**重要澄清：为何必须使用滞后特征？**
竞赛说明中提及"14个月的收益率/利率表"，此描述是指主办方提供了与14个月用户行为数据（2013.07-2014.08）相匹配的宏观经济数据。这**不代表**在预测9月数据时，我们可以提前获取9月的利率。时间序列预测严格遵守"不能用未来信息预测过去"的原则。例如，在预测9月1日的申购量时，我们正站在8月31日的"时间点"上，能利用的最新的宏观数据就是8月31日的（即lag=1），而9月1日当天的数据在那个时间点是未知的。因此，使用滞后特征是唯一符合逻辑且正确的建模方式。

这些调整并不会削弱方案一的威力，反而使其更加严谨、鲁棒，并完全符合真实世界的时间序列预测约束。

### 2.6 如何处理周期性因素？

这是一个非常关键的问题，体现了机器学习模型与传统时序模型在处理周期性上的核心差异。

**核心思想**：不同于Prophet、SARIMA等模型拥有专门处理季节性的内置参数（如 `weekly_seasonality`），LightGBM这类树模型本身没有时间概念。因此，我们必须**将周期性信息编码为显式的特征**，让模型能够"读懂"和学习。

**具体实现方式**：

1.  **周内周期**：我们创建了 `weekday` 特征（值为0-6）。模型在训练时，会把它当作一个普通的类别特征。如果数据显示周一（`weekday=0`）和周五（`weekday=4`）的申购赎回模式与周二、三、四显著不同，模型就会自动学习到针对不同 `weekday` 值的决策规则。

2.  **节假日周期**：我们创建了 `is_holiday` 和 `is_next_workday` 这类二元特征。这使得模型可以学习到节假日期间交易量普遍下降，以及节后第一个工作日交易量可能反弹的复杂模式。

3.  **月内周期**：通过 `is_month_start` 和 `is_month_end` 等特征，模型可以捕捉到由工资发放、信用卡还款等社会经济活动引起的、在月初或月末的规律性资金流动。

**优势：更灵活、更数据驱动**
- 这种基于特征的方法，让模型可以从数据中学习任意复杂的周期模式，而不仅限于平滑的、符合某种数学函数的周期（如正弦波）。例如，如果每周三因特定原因出现交易低谷，模型可以精确捕捉到这一点，而这是Prophet的平滑季节性曲线难以做到的。
- 总结而言，我们是通过**特征工程**，将时间背后的周期性规律，转化为模型可以理解和利用的输入信息。

### 2.7 节假日特征的具体实现

在之前的章节中，我们多次提到了`is_holiday`和`is_next_workday`这两个关键的周期性特征。这里的具体实现方法是基于`chinese_calendar`这个强大的第三方库。

**实现步骤**:
1.  **引入库**：在Python脚本的开头引入`import chinese_calendar`。
2.  **生成`is_holiday`特征**：在生成了包含所有日期的`data`数据框后，我们可以通过`apply`方法或直接迭代，对`ds`（日期）列的每一天使用`chinese_calendar.is_holiday()`函数进行判断。该函数能自动识别中国的法定节假日（如国庆、春节）以及因调休而产生的周末上班日。我们将判断结果（True/False）转换为1/0，生成`is_holiday`列。
3.  **生成`is_next_workday`特征**：这个特征的含义是"节后的第一个工作日"，通常在这一天，被压抑的交易需求会集中释放。它的实现方法是：
    - 找到所有`is_holiday`从1变为0的日期点。这个日期点就是节后的第一个工作日。
    - 我们可以通过对`is_holiday`列进行`shift(1)`操作，然后与原列进行比较来高效地定位这些日期。

通过以上步骤，我们就能将复杂的节假日效应，转化为模型可以轻松学习的数值特征，从而极大地提升模型的预测精度。

---

### 当前未完成事项（2024-06-22）

1. **交互特征补充**
   - `city_weekday`（城市 × 星期几）尚未实现。
   - `constellation_is_holiday`（星座 × 节假日）尚未实现。
2. **更多交互特征探索**
   - 其他如"城市 × 月内周期"、"性别 × 节假日"等交互特征可进一步尝试。
3. **特征工程自动化与特征筛选**
   - 尚未实现特征自动筛选、特征重要性驱动的特征精简。
4. **异常值处理与归一化**
   - 针对极端用户/极端天的分箱、截断、归一化等尚未系统实施。
5. **特征工程脚本独立化**
   - 虽已在主脚本实现特征工程，但尚未完全独立为专门的特征工程脚本。
6. **模型调参自动化**
   - 混合概率、递归窗口长度、训练轮数等参数尚未系统性自动调优。
7. **模型融合与残差建模**
   - 方案二（残差修正模型）尚未落地。
8. **更丰富的用户活跃度特征**
   - 可进一步挖掘如"近N天最大/最小申购/赎回"、"活跃天数比例"等。
9. **特征重要性可视化与分析**
   - 目前仅保存特征重要性，尚未系统分析和可视化。
10. **大规模递归预测的分布式/并行优化**
    - 递归预测效率提升（如多进程/分布式）尚未实现。

> 以上为截至2024-06-22尚未完成的关键事项，建议后续按优先级逐步推进。

---

## 三、方案二：基于现有模型的残差修正模型

此方案可以作为对现有融合模型的补充和微调。

1.  **获取预测与残差**：
    - 运行您最优的融合模型（如`7.0.1_fusion_model.py`），得到在整个训练集（3月-8月）上的每日预测值。
    - 计算每日残差：`residual = true_value - predicted_value`。

2.  **残差建模**：
    - 将残差序列本身作为一个新的预测目标。
    - 使用一个相对简单的模型来预测每日的残差值，可选模型：
      - **ARIMA**：经典的残差分析模型。
      - **简单的XGBoost/LightGBM**：只使用时间、节假日和外部经济特征来预测残差。

3.  **生成最终预测**：
    - `Final_Prediction(day) = Fusion_Model_Prediction(day) + Residual_Model_Prediction(day)`

---

## 四、行动计划与建议（修正版）

1. 递归LightGBM方案已实现递归窗口扩展、递归训练/混合训练、冷启动处理、调试与正式模式切换、诊断工具链。
2. 后续优化建议：
   - 进一步丰富特征工程，尝试更多交互特征、异常值处理、归一化等。
   - 系统性调优混合概率、训练轮数、递归窗口长度等参数。
   - 持续跟踪模型在验证集和预测集上的递归表现，关注波动性和极端值。
3. 继续关注特征重要性分析，指导特征筛选和模型迭代。

---

## 五、关键问题与迭代历程（修正版）

### 5.1 预测归零问题与递归窗口修正
- 递归预测归零问题已通过lag特征动态递推彻底修正。
- 递归窗口扩展和递归训练/混合训练有效缓解了误差累积和预测锁死。

### 5.2 数据稀疏性与目标函数选择
- 目标变量稀疏问题已通过L2损失函数缓解。

### 5.3 训练与预测环境一致性
- 递归训练/混合训练让模型在训练阶段适应预测环境，提升泛化能力。

### 5.4 调试与正式模式切换
- 已支持调试模式（小样本、少轮数、快速诊断）与正式模式（全量训练、完整预测）一键切换。

### 5.5 递归预测性能瓶颈与向量化重构

#### 1. 问题现象
在递归预测逻辑跑通后，我们发现**全量数据下的递归预测过程极其缓慢**（耗时数小时甚至更长），无法满足快速迭代和验证的需求。

#### 2. 根本原因诊断：低效的循环操作
问题的根源在于`recursive_predict`函数中**逐用户、逐特征的嵌套慢循环**。
- **性能陷阱**: 在pandas中，对DataFrame进行行级或单元格级的循环读写，性能极差。原先的实现方式（`for ... for ... in ...`）会导致数千万次低效的读写操作，使计算时间呈指数级增长。
- **逻辑关注点**: 在迭代初期，我们为了**优先保证递归逻辑的正确性和可读性**，采用了这种直观但低效的实现方式，便于debug和验证。

#### 3. 解决方案：向量化重构（已完成）
当逻辑正确性得到保证后，性能优化就成为首要任务。我们对`recursive_predict`函数进行了**向量化重构**。
- **核心思想**: 将逐个处理改为**批量处理**。
- **具体实施**: 利用pandas的`map`、`.loc`索引赋值等高效的向量化操作，**一次性更新未来某一天所有用户的lag特征**，彻底取代了原先的慢速循环。
- **效果**: 性能提升了**上百倍甚至上千倍**，将原先数小时的运行时间缩短至几分钟，极大地提升了开发和验证效率。

---

## 六、递归LightGBM方案优先提升计划

- 递归窗口lag特征递推、递归训练/混合训练、冷启动、诊断工具链等已全部落地。
- 后续重点优化方向：
  1. 丰富特征工程，提升模型表达力。
  2. 系统性参数调优，提升递归预测稳定性和波动性。
  3. 持续跟踪特征重要性，指导特征筛选和模型迭代。

--- 

## 七、协作流程与沟通优化

为了提升协作效率，避免因长时间等待或信息不透明导致的中断，后续将遵循以下沟通与开发原则：

1.  **分阶段说明**：在提供一个多步骤、复杂解决方案的初期，会明确指出："**当前版本主要用于实现功能和验证逻辑，性能可能不是最优的。**"
2.  **性能预警**：当解决方案进入到可能涉及大规模数据处理或耗时操作的阶段时（例如从调试模式切换到全量数据模式），会主动提醒："**注意：在全量数据上运行时，此步骤可能会非常耗时。后续我们可以专门针对性能进行优化。**"
3.  **提供多种实现选项**：如果可能，会同时提供一个"逻辑直观但慢"的版本和一个"高效但复杂"的版本，并解释其优缺点，让你根据当前的需求（是想debug还是想快速出结果）进行选择。
4.  **主动询问与迭代**：在完成一个阶段后，会主动询问是否遇到了性能瓶颈，并准备好下一步的优化方案。

--- 