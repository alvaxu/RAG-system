{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8374106-6939-4d22-bf17-41ebf9a21b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch  # PyTorch深度学习库\n",
    "import torch.nn.functional as F  # PyTorch函数式接口，包含各种神经网络函数\n",
    "\n",
    "from torch import Tensor  # 导入Tensor类型，用于类型提示\n",
    "from modelscope import AutoTokenizer, AutoModel  # 从modelscope导入自动分词器和模型加载器\n",
    "\n",
    "\n",
    "# 定义最后一个token池化函数\n",
    "# 该函数从最后的隐藏状态中提取每个序列的最后一个有效token的表示\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    # 检查是否为左侧填充（即所有序列最后一个位置都有效）\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        # 如果是左侧填充，直接返回最后一个位置的隐藏状态\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        # 如果是右侧填充，计算每个序列的实际长度（减1是因为索引从0开始）\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        # 返回每个序列最后一个有效token的隐藏状态\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "# 定义获取详细指令的函数\n",
    "# 将任务描述和查询组合成特定格式的指令\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "\n",
    "# 每个查询都必须附带一个描述任务的简短指令\n",
    "# 定义任务描述：给定网络搜索查询，检索相关的回答段落\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "# 创建查询列表，每个查询都通过get_detailed_instruct函数添加了任务描述\n",
    "queries = [\n",
    "    get_detailed_instruct(task, 'how much protein should a female eat'),  # 女性应该摄入多少蛋白质\n",
    "    get_detailed_instruct(task, 'summit define')  # summit（顶峰）的定义\n",
    "]\n",
    "# 检索文档不需要添加指令\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",  # 关于女性蛋白质摄入量的文档\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"  # 关于summit定义的文档\n",
    "]\n",
    "# 将查询和文档合并为一个输入文本列表\n",
    "input_texts = queries + documents\n",
    "\n",
    "# 设置模型路径\n",
    "model_dir = \"C:\\\\Users\\\\Administrator\\\\AInewModels\\\\iic\\\\gte_Qwen2-1___5B-instruct\"\n",
    "# 加载分词器，trust_remote_code=True允许使用远程代码\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "# 加载模型\n",
    "model = AutoModel.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# 设置最大序列长度\n",
    "max_length = 8192\n",
    "\n",
    "# 对输入文本进行分词处理\n",
    "# padding=True：对较短的序列进行填充，使批次中所有序列长度一致\n",
    "# truncation=True：截断超过max_length的序列\n",
    "# return_tensors='pt'：返回PyTorch张量\n",
    "batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt')\n",
    "# 将分词后的输入传入模型，获取输出\n",
    "outputs = model(**batch_dict)\n",
    "# 使用last_token_pool函数从最后的隐藏状态中提取每个序列的表示\n",
    "embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# 对嵌入向量进行L2归一化，使其长度为1\n",
    "# p=2表示L2范数，dim=1表示在第1维（特征维度）上进行归一化\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "# 计算查询和文档之间的相似度分数\n",
    "# embeddings[:2]：查询的嵌入向量（前两个）\n",
    "# embeddings[2:]：文档的嵌入向量（后两个）\n",
    "# .T：转置操作\n",
    "# * 100：将相似度分数缩放到0-100的范围\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "# 打印相似度分数\n",
    "print(scores.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
