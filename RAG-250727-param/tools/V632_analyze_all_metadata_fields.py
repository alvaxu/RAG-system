'''
ç¨‹åºè¯´æ˜ï¼š
## 1. åˆ†æå‘é‡æ•°æ®åº“ä¸­æ‰€æœ‰ä¸‰ç§ç±»å‹æ–‡æ¡£çš„å…ƒæ•°æ®å­—æ®µ
## 2. å¯¹æ¯”ä¸åŒç±»å‹æ–‡æ¡£çš„å­—æ®µå·®å¼‚
## 3. è¯†åˆ«æ¯ç§ç±»å‹æ–‡æ¡£çš„ç‹¬ç‰¹å­—æ®µå’Œå…±åŒå­—æ®µ
'''

import pickle
import os
import sys
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.settings import Settings

def analyze_all_metadata_fields():
    """åˆ†ææ‰€æœ‰ç±»å‹æ–‡æ¡£çš„å…ƒæ•°æ®å­—æ®µ"""
    
    print("ğŸ” åˆ†æå‘é‡æ•°æ®åº“ä¸­æ‰€æœ‰ç±»å‹æ–‡æ¡£çš„å…ƒæ•°æ®å­—æ®µ")
    print("=" * 80)
    
    try:
        # åŠ è½½é…ç½®
        config = Settings.load_from_file('config.json')
        embeddings = DashScopeEmbeddings(dashscope_api_key=config.dashscope_api_key, model="text-embedding-v1")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        vector_db_path = "./central/vector_db"
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(vector_store.docstore._dict)}")
        
        # æŒ‰ç±»å‹åˆ†ç»„æ–‡æ¡£
        docs_by_type = defaultdict(list)
        
        for doc_id, doc in vector_store.docstore._dict.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            chunk_type = metadata.get('chunk_type', 'unknown')
            docs_by_type[chunk_type].append((doc_id, doc))
        
        # æ˜¾ç¤ºå„ç±»å‹æ–‡æ¡£æ•°é‡
        print(f"\nğŸ“Š å„ç±»å‹æ–‡æ¡£æ•°é‡:")
        for doc_type, docs in docs_by_type.items():
            emoji = {'image': 'ğŸ–¼ï¸', 'text': 'ğŸ“„', 'table': 'ğŸ“Š', 'unknown': 'â“'}.get(doc_type, 'ğŸ“„')
            print(f"  {emoji} {doc_type}: {len(docs)} ä¸ª")
        
        # åˆ†ææ¯ç§ç±»å‹çš„å­—æ®µ
        print(f"\nğŸ” å„ç±»å‹æ–‡æ¡£çš„å…ƒæ•°æ®å­—æ®µåˆ†æ:")
        print("=" * 80)
        
        all_fields_by_type = {}
        
        for doc_type, docs in docs_by_type.items():
            if not docs:
                continue
                
            emoji = {'image': 'ğŸ–¼ï¸', 'text': 'ğŸ“„', 'table': 'ğŸ“Š', 'unknown': 'â“'}.get(doc_type, 'ğŸ“„')
            print(f"\n{emoji} {doc_type.upper()} ç±»å‹æ–‡æ¡£:")
            print("-" * 60)
            
            # æ”¶é›†æ‰€æœ‰å­—æ®µ
            fields = defaultdict(set)
            field_values = defaultdict(list)
            
            for doc_id, doc in docs:
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                
                for field_name, field_value in metadata.items():
                    fields[field_name].add(type(field_value).__name__)
                    field_values[field_name].append(field_value)
            
            all_fields_by_type[doc_type] = dict(fields)
            
            # æ˜¾ç¤ºå­—æ®µç»Ÿè®¡
            print(f"  æ€»å­—æ®µæ•°: {len(fields)}")
            
            # æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„è¯¦ç»†ä¿¡æ¯
            for field_name, value_types in sorted(fields.items()):
                value_type_str = ", ".join(sorted(value_types))
                sample_values = field_values[field_name][:3]  # å–å‰3ä¸ªæ ·æœ¬
                
                print(f"    ğŸ“ {field_name}:")
                print(f"      ç±»å‹: {value_type_str}")
                print(f"      æ ·æœ¬å€¼: {sample_values}")
                
                # å¦‚æœæ˜¯ç‰¹æ®Šå­—æ®µï¼Œæ˜¾ç¤ºæ›´å¤šä¿¡æ¯
                if field_name in ['page_content', 'enhanced_description', 'content']:
                    non_empty_count = sum(1 for v in field_values[field_name] if v and str(v).strip())
                    print(f"      éç©ºå€¼æ•°é‡: {non_empty_count}/{len(field_values[field_name])}")
        
        # å¯¹æ¯”ä¸åŒç±»å‹çš„å­—æ®µå·®å¼‚
        print(f"\nğŸ” ä¸åŒç±»å‹æ–‡æ¡£çš„å­—æ®µå¯¹æ¯”:")
        print("=" * 80)
        
        # è·å–æ‰€æœ‰å­—æ®µ
        all_fields = set()
        for fields in all_fields_by_type.values():
            all_fields.update(fields.keys())
        
        print(f"  æ‰€æœ‰å­—æ®µæ€»æ•°: {len(all_fields)}")
        
        # åˆ†æå­—æ®µåˆ†å¸ƒ
        field_distribution = defaultdict(list)
        for field in all_fields:
            for doc_type, fields in all_fields_by_type.items():
                if field in fields:
                    field_distribution[field].append(doc_type)
        
        # æ˜¾ç¤ºå­—æ®µåˆ†å¸ƒ
        print(f"\nğŸ“Š å­—æ®µåˆ†å¸ƒæƒ…å†µ:")
        print("-" * 60)
        
        common_fields = []
        unique_fields = defaultdict(list)
        
        for field, types in field_distribution.items():
            if len(types) == len(all_fields_by_type):
                common_fields.append(field)
                print(f"  ğŸŒ {field}: æ‰€æœ‰ç±»å‹éƒ½æœ‰")
            else:
                unique_fields[tuple(types)].append(field)
                print(f"  ğŸ¯ {field}: ä»… {', '.join(types)} ç±»å‹æœ‰")
        
        # æ˜¾ç¤ºå…±åŒå­—æ®µå’Œç‹¬ç‰¹å­—æ®µ
        print(f"\nğŸ“‹ å­—æ®µåˆ†ç±»æ€»ç»“:")
        print("-" * 60)
        
        print(f"  ğŸŒ å…±åŒå­—æ®µ ({len(common_fields)}ä¸ª):")
        for field in sorted(common_fields):
            print(f"    - {field}")
        
        print(f"\n  ğŸ¯ ç‹¬ç‰¹å­—æ®µ:")
        for types, fields in unique_fields.items():
            type_str = ", ".join(types)
            print(f"    {type_str} ç±»å‹ç‹¬æœ‰ ({len(fields)}ä¸ª):")
            for field in sorted(fields):
                print(f"      - {field}")
        
        # æ€»ç»“
        print(f"\nğŸ“Š åˆ†ææ€»ç»“:")
        print("=" * 80)
        
        total_docs = sum(len(docs) for docs in docs_by_type.values())
        print(f"  æ€»æ–‡æ¡£æ•°: {total_docs}")
        print(f"  æ–‡æ¡£ç±»å‹æ•°: {len(docs_by_type)}")
        print(f"  æ€»å­—æ®µæ•°: {len(all_fields)}")
        print(f"  å…±åŒå­—æ®µæ•°: {len(common_fields)}")
        
        unique_field_count = sum(len(fields) for fields in unique_fields.values())
        print(f"  ç‹¬ç‰¹å­—æ®µæ•°: {unique_field_count}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analyze_all_metadata_fields()
