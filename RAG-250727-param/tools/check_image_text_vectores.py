#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š
## 1. æ£€æŸ¥FAISSç´¢å¼•ä¸­image_textæ–‡æ¡£çš„å‘é‡åŒ–çŠ¶æ€
## 2. åˆ†æimage_textæ–‡æ¡£æ˜¯å¦çœŸçš„æœ‰å¯¹åº”çš„å‘é‡
## 3. ç¡®è®¤å‘é‡åŒ–å­˜å‚¨çš„å®Œæ•´æ€§
"""

import sys
import os
import logging

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import Settings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings

# å¯¼å…¥ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—
from config.api_key_manager import get_dashscope_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_image_text_vectors():
    """æ£€æŸ¥FAISSç´¢å¼•ä¸­image_textæ–‡æ¡£çš„å‘é‡åŒ–çŠ¶æ€"""
    print("ğŸ” æ£€æŸ¥FAISSç´¢å¼•ä¸­image_textæ–‡æ¡£çš„å‘é‡åŒ–çŠ¶æ€")
    print("=" * 60)
    
    try:
        # åŠ è½½é…ç½®
        config = Settings.load_from_file('config.json')
        vector_db_path = config.vector_db_dir
        
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
        
        # è·å–APIå¯†é’¥
        config_key = config.dashscope_api_key
        api_key = get_dashscope_api_key(config_key)
        
        if not api_key:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return
        
        # åˆå§‹åŒ–embeddings
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        print("ğŸ“š æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥FAISSç´¢å¼•çŠ¶æ€
        print("\nğŸ” 1. æ£€æŸ¥FAISSç´¢å¼•çŠ¶æ€")
        print("-" * 40)
        
        # è·å–FAISSç´¢å¼•ä¿¡æ¯
        index = vector_store.index
        print(f"FAISSç´¢å¼•ç±»å‹: {type(index)}")
        print(f"FAISSç´¢å¼•ç»´åº¦: {index.d}")
        print(f"FAISSç´¢å¼•å‘é‡æ•°é‡: {index.ntotal}")
        
        # æ£€æŸ¥docstore
        docstore = vector_store.docstore
        docstore_dict = docstore._dict
        print(f"docstoreæ–‡æ¡£æ•°é‡: {len(docstore_dict)}")
        
        # 2. åˆ†æimage_textæ–‡æ¡£çŠ¶æ€
        print("\nğŸ” 2. åˆ†æimage_textæ–‡æ¡£çŠ¶æ€")
        print("-" * 40)
        
        image_text_docs = []
        image_docs = []
        text_docs = []
        table_docs = []
        
        for doc_id, doc in docstore_dict.items():
            if hasattr(doc, 'metadata') and doc.metadata:
                chunk_type = doc.metadata.get('chunk_type', '')
                if chunk_type == 'image_text':
                    image_text_docs.append((doc_id, doc))
                elif chunk_type == 'image':
                    image_docs.append((doc_id, doc))
                elif chunk_type == 'text':
                    text_docs.append((doc_id, doc))
                elif chunk_type == 'table':
                    table_docs.append((doc_id, doc))
        
        print(f"image_textæ–‡æ¡£æ•°é‡: {len(image_text_docs)}")
        print(f"imageæ–‡æ¡£æ•°é‡: {len(image_docs)}")
        print(f"textæ–‡æ¡£æ•°é‡: {len(text_docs)}")
        print(f"tableæ–‡æ¡£æ•°é‡: {len(table_docs)}")
        
        # 3. æ£€æŸ¥image_textæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        print("\nï¿½ï¿½ 3. æ£€æŸ¥image_textæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯")
        print("-" * 40)
        
        if image_text_docs:
            print("å‰5ä¸ªimage_textæ–‡æ¡£:")
            for i, (doc_id, doc) in enumerate(image_text_docs[:5]):
                metadata = doc.metadata
                print(f"  æ–‡æ¡£{i+1}: {doc_id}")
                print(f"    chunk_type: {metadata.get('chunk_type')}")
                print(f"    image_id: {metadata.get('image_id')}")
                print(f"    related_image_id: {metadata.get('related_image_id')}")
                print(f"    enhanced_descriptioné•¿åº¦: {len(metadata.get('enhanced_description', ''))}")
                print(f"    page_contenté•¿åº¦: {len(doc.page_content)}")
                print(f"    page_contentå‰100å­—ç¬¦: {doc.page_content[:100]}...")
                print()
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°image_textæ–‡æ¡£")
        
        # 4. æ£€æŸ¥å‘é‡ç´¢å¼•çš„å®Œæ•´æ€§
        print("\nğŸ” 4. æ£€æŸ¥å‘é‡ç´¢å¼•çš„å®Œæ•´æ€§")
        print("-" * 40)
        
        # æ£€æŸ¥FAISSç´¢å¼•ä¸­çš„å‘é‡æ•°é‡æ˜¯å¦ä¸docstoreåŒ¹é…
        if index.ntotal != len(docstore_dict):
            print(f"âš ï¸ å‘é‡æ•°é‡ä¸åŒ¹é…!")
            print(f"  FAISSç´¢å¼•å‘é‡æ•°é‡: {index.ntotal}")
            print(f"  docstoreæ–‡æ¡£æ•°é‡: {len(docstore_dict)}")
            print(f"  å·®å¼‚: {abs(index.ntotal - len(docstore_dict))}")
        else:
            print(f"âœ… å‘é‡æ•°é‡åŒ¹é…: {index.ntotal}")
        
        # 5. æµ‹è¯•image_textçš„å‘é‡æœç´¢
        print("\nï¿½ï¿½ 5. æµ‹è¯•image_textçš„å‘é‡æœç´¢")
        print("-" * 40)
        
        if image_text_docs:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªimage_textæ–‡æ¡£çš„enhanced_descriptionè¿›è¡Œæœç´¢
            test_doc = image_text_docs[0][1]
            test_query = test_doc.page_content[:100]  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºæŸ¥è¯¢
            
            print(f"æµ‹è¯•æŸ¥è¯¢: {test_query[:50]}...")
            
            try:
                # æ— filteræœç´¢
                results_no_filter = vector_store.similarity_search(test_query, k=5)
                print(f"æ— filteræœç´¢: {len(results_no_filter)} ä¸ªç»“æœ")
                
                # æœ‰filteræœç´¢
                results_with_filter = vector_store.similarity_search(test_query, k=5, filter={'chunk_type': 'image_text'})
                print(f"Filter {{'chunk_type': 'image_text'}}: {len(results_with_filter)} ä¸ªç»“æœ")
                
                # æ£€æŸ¥ç»“æœ
                if results_with_filter:
                    print("Filteræœç´¢ç»“æœ:")
                    for i, result in enumerate(results_with_filter):
                        chunk_type = result.metadata.get('chunk_type', 'N/A')
                        print(f"  ç»“æœ{i+1}: chunk_type={chunk_type}")
                else:
                    print("âš ï¸ Filteræœç´¢æ²¡æœ‰è¿”å›ç»“æœ")
                    
            except Exception as e:
                print(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}")
        else:
            print("âŒ æ²¡æœ‰image_textæ–‡æ¡£å¯ä»¥æµ‹è¯•")
        
        # 6. æ£€æŸ¥å‘é‡å­˜å‚¨çš„ä¿å­˜çŠ¶æ€
        print("\nğŸ” 6. æ£€æŸ¥å‘é‡å­˜å‚¨çš„ä¿å­˜çŠ¶æ€")
        print("-" * 40)
        
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ–‡ä»¶
        import glob
        faiss_files = glob.glob(os.path.join(vector_db_path, "*.faiss"))
        pkl_files = glob.glob(os.path.join(vector_db_path, "*.pkl"))
        
        print(f"FAISSç´¢å¼•æ–‡ä»¶: {len(faiss_files)} ä¸ª")
        for f in faiss_files:
            file_size = os.path.getsize(f) / (1024 * 1024)  # MB
            print(f"  {os.path.basename(f)}: {file_size:.2f} MB")
        
        print(f"å…ƒæ•°æ®æ–‡ä»¶: {len(pkl_files)} ä¸ª")
        for f in pkl_files:
            file_size = os.path.getsize(f) / (1024 * 1024)  # MB
            print(f"  {os.path.basename(f)}: {file_size:.2f} MB")
        
        print("\nâœ… image_textå‘é‡åŒ–çŠ¶æ€æ£€æŸ¥å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    check_image_text_vectors()