'''
ç¨‹åºè¯´æ˜ï¼š

## 1. æ·±åº¦è°ƒè¯•unified_pipelineå’Œsource_filter_engine
## 2. è¿½è¸ªmetadataå­—æ®µåœ¨pipelineä¸­çš„ä¼ é€’å’Œä¸¢å¤±æƒ…å†µ
## 3. åˆ†æLLMä¸Šä¸‹æ–‡æ„å»ºè¿‡ç¨‹
## 4. æ£€æŸ¥æºè¿‡æ»¤é€»è¾‘
'''

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.api_key_manager import get_dashscope_api_key
from v2.core.image_engine import ImageEngine
from v2.core.document_loader import DocumentLoader
from v2.config.v2_config import ImageEngineConfigV2
from v2.core.unified_pipeline import UnifiedPipeline
from v2.core.source_filter_engine import SourceFilterEngine
from v2.core.dashscope_llm_engine import DashScopeLLMEngine
import time


def debug_pipeline_deep():
    """æ·±åº¦è°ƒè¯•pipelineå¤„ç†æµç¨‹"""
    print("ğŸ” æ·±åº¦è°ƒè¯•pipelineå¤„ç†æµç¨‹")
    print("=" * 80)
    
    try:
        # 1. åˆå§‹åŒ–ç»„ä»¶
        print("ğŸ“¡ åˆå§‹åŒ–ç»„ä»¶...")
        config = ImageEngineConfigV2()
        
        api_key = get_dashscope_api_key()
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        vector_db_path = "../central/vector_db"
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        document_loader = DocumentLoader(vector_store=vector_store)
        image_engine = ImageEngine(
            config=config,
            vector_store=vector_store,
            document_loader=document_loader
        )
        print("âœ… ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. æµ‹è¯•æŸ¥è¯¢
        query = "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ"
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        print("-" * 60)
        
        # 3. ç¬¬ä¸€æ­¥ï¼šè·å–å®Œæ•´çš„äº”å±‚å¬å›ç»“æœ
        print("\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šè·å–å®Œæ•´çš„äº”å±‚å¬å›ç»“æœ")
        try:
            # æ£€æŸ¥image_engineçš„äº”å±‚å¬å›æ–¹æ³•
            if hasattr(image_engine, '_search_images_with_five_layer_recall'):
                print("  âœ… æ‰¾åˆ°äº”å±‚å¬å›æ–¹æ³•")
                
                # è°ƒç”¨äº”å±‚å¬å›
                all_results = image_engine._search_images_with_five_layer_recall(query)
                print(f"  äº”å±‚å¬å›ç»“æœæ•°é‡: {len(all_results)}")
                
                # åˆ†æç»“æœ
                if all_results:
                    print(f"\n  ç»“æœåˆ†æ:")
                    for i, result in enumerate(all_results[:3]):  # åªçœ‹å‰3ä¸ª
                        print(f"    ç»“æœ{i+1}:")
                        print(f"      åˆ†æ•°: {result.get('score', 'N/A'):.4f}")
                        print(f"      æœç´¢æ–¹æ³•: {result.get('search_method', 'N/A')}")
                        print(f"      æ¥æº: {result.get('source', 'N/A')}")
                        
                        # æ£€æŸ¥æˆ‘ä»¬çš„æ–°å¢å­—æ®µ
                        if 'document_name' in result:
                            print(f"      ğŸ“‹ document_name: {result.get('document_name', 'N/A')}")
                            print(f"      ğŸ“‹ page_number: {result.get('page_number', 'N/A')}")
                            print(f"      ğŸ“‹ chunk_type: {result.get('chunk_type', 'N/A')}")
                            print(f"      ğŸ“‹ llm_contexté•¿åº¦: {len(result.get('llm_context', ''))}")
                        else:
                            print(f"      âŒ ç¼ºå°‘æˆ‘ä»¬çš„æ–°å¢å­—æ®µ")
                        
                        # æ£€æŸ¥docå­—æ®µ
                        if 'doc' in result:
                            doc = result['doc']
                            print(f"      ğŸ“„ docç±»å‹: {type(doc)}")
                            print(f"      ğŸ“„ å†…å®¹é•¿åº¦: {len(doc.page_content)} å­—ç¬¦")
                        else:
                            print(f"      âŒ ç¼ºå°‘docå­—æ®µ")
                        print()
                        
            else:
                print("  âŒ æ²¡æœ‰æ‰¾åˆ°äº”å±‚å¬å›æ–¹æ³•")
                
        except Exception as e:
            print(f"  äº”å±‚å¬å›å¤±è´¥: {e}")
            import traceback
            print(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # 4. ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥unified_pipeline
        print(f"\nğŸ“Š ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥unified_pipeline")
        try:
            # åˆå§‹åŒ–unified_pipeline
            from v2.config.v2_config import LLMConfigV2
            llm_config = LLMConfigV2()
            llm_engine = DashScopeLLMEngine(api_key=api_key, config=llm_config)
            source_filter_engine = SourceFilterEngine(config=config)
            
            unified_pipeline = UnifiedPipeline(
                llm_engine=llm_engine,
                source_filter_engine=source_filter_engine,
                config=config
            )
            print("  âœ… unified_pipelineåˆå§‹åŒ–æˆåŠŸ")
            
            # æ£€æŸ¥processæ–¹æ³•
            if hasattr(unified_pipeline, 'process'):
                print("  âœ… æ‰¾åˆ°processæ–¹æ³•")
                import inspect
                sig = inspect.signature(unified_pipeline.process)
                print(f"  processæ–¹æ³•å‚æ•°: {sig}")
                
                # æŸ¥çœ‹processæ–¹æ³•çš„å®ç°
                source = inspect.getsource(unified_pipeline.process)
                print(f"  processæ–¹æ³•å®ç°é¢„è§ˆ:")
                lines = source.split('\n')
                for i, line in enumerate(lines[:20]):  # æ˜¾ç¤ºå‰20è¡Œ
                    print(f"    {i+1:2d}: {line}")
                if len(lines) > 20:
                    print(f"    ... (å…±{len(lines)}è¡Œ)")
                    
            else:
                print("  âŒ æ²¡æœ‰æ‰¾åˆ°processæ–¹æ³•")
                
        except Exception as e:
            print(f"  unified_pipelineæ£€æŸ¥å¤±è´¥: {e}")
            import traceback
            print(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # 5. ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥source_filter_engine
        print(f"\nğŸ“Š ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥source_filter_engine")
        try:
            source_filter_engine = SourceFilterEngine(config=config)
            print("  âœ… source_filter_engineåˆå§‹åŒ–æˆåŠŸ")
            
            # æ£€æŸ¥filter_sourcesæ–¹æ³•
            if hasattr(source_filter_engine, 'filter_sources'):
                print("  âœ… æ‰¾åˆ°filter_sourcesæ–¹æ³•")
                import inspect
                sig = inspect.signature(source_filter_engine.filter_sources)
                print(f"  filter_sourcesæ–¹æ³•å‚æ•°: {sig}")
                
                # æŸ¥çœ‹filter_sourcesæ–¹æ³•çš„å®ç°
                source = inspect.getsource(source_filter_engine.filter_sources)
                print(f"  filter_sourcesæ–¹æ³•å®ç°é¢„è§ˆ:")
                lines = source.split('\n')
                for i, line in enumerate(lines[:20]):  # æ˜¾ç¤ºå‰20è¡Œ
                    print(f"    {i+1:2d}: {line}")
                if len(lines) > 20:
                    print(f"    ... (å…±{len(lines)}è¡Œ)")
                    
            else:
                print("  âŒ æ²¡æœ‰æ‰¾åˆ°filter_sourcesæ–¹æ³•")
                
        except Exception as e:
            print(f"  source_filter_engineæ£€æŸ¥å¤±è´¥: {e}")
            import traceback
            print(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # 6. ç¬¬å››æ­¥ï¼šæ¨¡æ‹Ÿpipelineå¤„ç†è¿‡ç¨‹
        print(f"\nğŸ“Š ç¬¬å››æ­¥ï¼šæ¨¡æ‹Ÿpipelineå¤„ç†è¿‡ç¨‹")
        try:
            # è·å–å‘é‡æœç´¢ç»“æœä½œä¸ºè¾“å…¥
            vector_results = image_engine._vector_search(query, max_results=20)
            print(f"  è¾“å…¥ç»“æœæ•°é‡: {len(vector_results)}")
            
            if vector_results:
                # æ¨¡æ‹Ÿæºè¿‡æ»¤
                print(f"\n  æ¨¡æ‹Ÿæºè¿‡æ»¤è¿‡ç¨‹:")
                print(f"    è¾“å…¥: {len(vector_results)} ä¸ªç»“æœ")
                
                # æ£€æŸ¥æ¯ä¸ªç»“æœçš„åˆ†æ•°
                scores = [r.get('score', 0) for r in vector_results]
                print(f"    åˆ†æ•°èŒƒå›´: {min(scores):.4f} - {max(scores):.4f}")
                print(f"    å¹³å‡åˆ†æ•°: {sum(scores)/len(scores):.4f}")
                
                # æ£€æŸ¥æˆ‘ä»¬çš„å­—æ®µ
                fields_check = {
                    'document_name': 0,
                    'page_number': 0,
                    'chunk_type': 0,
                    'llm_context': 0
                }
                
                for result in vector_results:
                    for field in fields_check:
                        if result.get(field):
                            fields_check[field] += 1
                
                print(f"    å­—æ®µç»Ÿè®¡:")
                for field, count in fields_check.items():
                    print(f"      {field}: {count}/{len(vector_results)}")
                
                # æ¨¡æ‹ŸLLMä¸Šä¸‹æ–‡æ„å»º
                print(f"\n  æ¨¡æ‹ŸLLMä¸Šä¸‹æ–‡æ„å»º:")
                if vector_results:
                    first_result = vector_results[0]
                    if 'llm_context' in first_result:
                        llm_context = first_result['llm_context']
                        print(f"    ä½¿ç”¨æˆ‘ä»¬çš„llm_context:")
                        print(f"      é•¿åº¦: {len(llm_context)} å­—ç¬¦")
                        print(f"      å†…å®¹é¢„è§ˆ: {llm_context[:200]}...")
                    else:
                        print(f"    âŒ æ²¡æœ‰llm_contextå­—æ®µ")
                        
                        # å°è¯•ä»docæ„å»º
                        if 'doc' in first_result:
                            doc = first_result['doc']
                            print(f"    ä»docæ„å»ºä¸Šä¸‹æ–‡:")
                            print(f"      å†…å®¹: {doc.page_content}")
                            if hasattr(doc, 'metadata') and doc.metadata:
                                print(f"      å…ƒæ•°æ®: {doc.metadata}")
                
        except Exception as e:
            print(f"  æ¨¡æ‹Ÿpipelineå¤„ç†å¤±è´¥: {e}")
            import traceback
            print(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # 7. ç¬¬äº”æ­¥ï¼šé—®é¢˜åˆ†æ
        print("\n" + "=" * 80)
        print("ğŸ¯ æ·±åº¦é—®é¢˜åˆ†æ")
        print("=" * 80)
        
        print(f"\nå…³é”®å‘ç°:")
        print(f"1. å‘é‡æœç´¢å±‚é¢ï¼šâœ… æˆ‘ä»¬çš„metadataå­—æ®µæ­£ç¡®ä¼ é€’")
        print(f"2. äº”å±‚å¬å›å±‚é¢ï¼šâ“ éœ€è¦ç¡®è®¤æ˜¯å¦æ‰€æœ‰ç­–ç•¥éƒ½æœ‰metadata")
        print(f"3. unified_pipelineï¼šâ“ éœ€è¦ç¡®è®¤å¦‚ä½•å¤„ç†è¾“å…¥ç»“æœ")
        print(f"4. source_filter_engineï¼šâ“ éœ€è¦ç¡®è®¤è¿‡æ»¤é€»è¾‘")
        
        print(f"\nå¯èƒ½çš„é—®é¢˜ç‚¹:")
        print(f"1. äº”å±‚å¬å›ä¸­ï¼Œåªæœ‰_vector_searchæœ‰æˆ‘ä»¬çš„metadataï¼Œå…¶ä»–ç­–ç•¥æ²¡æœ‰")
        print(f"2. unified_pipelineå¯èƒ½é‡æ–°æ„å»ºLLMä¸Šä¸‹æ–‡ï¼Œå¿½ç•¥æˆ‘ä»¬çš„llm_context")
        print(f"3. source_filter_engineå¯èƒ½è¿‡æ»¤æ‰äº†åŒ…å«å›¾4ä¿¡æ¯çš„ç»“æœ")
        print(f"4. æœ€ç»ˆLLMä¸Šä¸‹æ–‡å¯èƒ½æ¥è‡ªå…¶ä»–å¬å›ç­–ç•¥çš„ç»“æœï¼Œè€Œä¸æ˜¯æˆ‘ä»¬çš„")
        
        print(f"\nå»ºè®®çš„è°ƒè¯•æ–¹å‘:")
        print(f"1. æ£€æŸ¥äº”å±‚å¬å›çš„å®Œæ•´ç»“æœï¼Œç¡®è®¤metadataå­—æ®µåˆ†å¸ƒ")
        print(f"2. æ£€æŸ¥unified_pipelineçš„LLMä¸Šä¸‹æ–‡æ„å»ºé€»è¾‘")
        print(f"3. æ£€æŸ¥source_filter_engineçš„è¿‡æ»¤è§„åˆ™")
        print(f"4. ç¡®è®¤æœ€ç»ˆä¼ é€’ç»™LLMçš„ä¸Šä¸‹æ–‡æ¥æº")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


if __name__ == "__main__":
    debug_pipeline_deep()
