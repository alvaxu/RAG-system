"""
ç¨‹åºè¯´æ˜ï¼š
## 1. æ£€æŸ¥æ–°å¢æ–‡æ¡£æ˜¯å¦åœ¨å‘é‡æ•°æ®åº“ä¸­
## 2. éªŒè¯æ–‡æ¡£æ£€ç´¢åŠŸèƒ½æ˜¯å¦æ­£å¸¸
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.settings import Settings

def check_new_doc_in_db():
    """æ£€æŸ¥æ–°å¢æ–‡æ¡£æ˜¯å¦åœ¨å‘é‡æ•°æ®åº“ä¸­"""
    print("ğŸ” æ£€æŸ¥æ–°å¢æ–‡æ¡£æ˜¯å¦åœ¨å‘é‡æ•°æ®åº“ä¸­...")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    settings = Settings.load_from_file("config.json")
    api_key = settings.dashscope_api_key
    
    if not api_key or api_key == 'ä½ çš„APIKEY':
        print("âŒ é”™è¯¯: æœªé…ç½®DashScope APIå¯†é’¥")
        return
    
    # åŠ è½½å‘é‡æ•°æ®åº“
    vector_db_path = "./central/vector_db"
    
    try:
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model="text-embedding-v1")
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(vector_store.docstore._dict)}")
        
        # æ£€æŸ¥æ–°å¢æ–‡æ¡£
        target_doc_name = "ã€ä¸Šæµ·è¯åˆ¸ã€‘ä¸­èŠ¯å›½é™…æ·±åº¦ç ”ç©¶æŠ¥å‘Šï¼šæ™¶åœ†åˆ¶é€ é¾™å¤´ï¼Œé¢†èˆªå›½äº§èŠ¯ç‰‡æ–°å¾ç¨‹"
        
        print(f"\nğŸ” æŸ¥æ‰¾æ–‡æ¡£: {target_doc_name}")
        print("-" * 60)
        
        found_docs = []
        for doc_id, doc in vector_store.docstore._dict.items():
            doc_name = doc.metadata.get('document_name', '')
            if target_doc_name in doc_name:
                found_docs.append({
                    'id': doc_id,
                    'content': doc.page_content[:100] + "...",
                    'metadata': doc.metadata
                })
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(found_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        if found_docs:
            print("\nğŸ“„ æ–‡æ¡£è¯¦æƒ…:")
            for i, doc in enumerate(found_docs[:5], 1):
                print(f"  æ–‡æ¡£ {i}:")
                print(f"    ID: {doc['id']}")
                print(f"    å†…å®¹: {doc['content']}")
                print(f"    ç±»å‹: {doc['metadata'].get('chunk_type', 'text')}")
                print(f"    é¡µç : {doc['metadata'].get('page_number', 'æœªçŸ¥')}")
                print()
        else:
            print("âŒ æœªæ‰¾åˆ°æ–°å¢æ–‡æ¡£")
        
        # æµ‹è¯•æ£€ç´¢åŠŸèƒ½
        print("\nğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
        print("-" * 40)
        
        test_queries = [
            "ä¸­èŠ¯å›½é™…äº§èƒ½åˆ©ç”¨ç‡",
            "æ™¶åœ†åˆ¶é€ ",
            "ä¸Šæµ·è¯åˆ¸",
            "æŠ•èµ„å»ºè®®"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æŸ¥è¯¢: {query}")
            try:
                results = vector_store.similarity_search(query, k=3)
                print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                
                for j, result in enumerate(results, 1):
                    doc_name = result.metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£')
                    chunk_type = result.metadata.get('chunk_type', 'text')
                    print(f"   ç»“æœ {j}: {doc_name} ({chunk_type})")
                    print(f"   å†…å®¹: {result.page_content[:80]}...")
                    
            except Exception as e:
                print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")

if __name__ == "__main__":
    check_new_doc_in_db() 