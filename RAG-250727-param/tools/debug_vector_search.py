#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š

## 1. ä¸“é—¨è°ƒè¯•å‘é‡æœç´¢é—®é¢˜
## 2. æµ‹è¯•ä¸åŒçš„è¿‡æ»¤æ¡ä»¶å’Œæœç´¢å‚æ•°
## 3. éªŒè¯å›¾ç‰‡æ–‡æ¡£çš„å‘é‡æœç´¢åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
from v2.core.document_loader import DocumentLoader
from core.vector_store import VectorStoreManager

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def debug_vector_search():
    """è°ƒè¯•å‘é‡æœç´¢åŠŸèƒ½"""
    print("ğŸ” å¼€å§‹è°ƒè¯•å‘é‡æœç´¢...")
    print("=" * 60)
    
    try:
        # 1. åŠ è½½å‘é‡æ•°æ®åº“
        print("ğŸ“š åŠ è½½å‘é‡æ•°æ®åº“...")
        vector_store_manager = VectorStoreManager()
        vector_store = vector_store_manager.load_vector_store("central/vector_db")
        
        if not vector_store:
            print("âŒ å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥")
            return
        
        print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(vector_store.docstore._dict)} ä¸ªæ–‡æ¡£")
        
        # 2. æ£€æŸ¥æ–‡æ¡£ç±»å‹åˆ†å¸ƒ
        print("\nğŸ“Š æ£€æŸ¥æ–‡æ¡£ç±»å‹åˆ†å¸ƒ...")
        type_counts = {}
        for doc_id, doc in vector_store.docstore._dict.items():
            chunk_type = doc.metadata.get('chunk_type', 'unknown')
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
        
        print("æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
        for doc_type, count in type_counts.items():
            print(f"  {doc_type}: {count}")
        
        # 3. æµ‹è¯•æ— è¿‡æ»¤æ¡ä»¶æœç´¢
        print("\nğŸ” æµ‹è¯•æ— è¿‡æ»¤æ¡ä»¶æœç´¢...")
        query = "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ"
        
        try:
            raw_results = vector_store.similarity_search(query, k=20)
            print(f"âœ… æ— è¿‡æ»¤æ¡ä»¶æœç´¢æˆåŠŸï¼Œè¿”å› {len(raw_results)} ä¸ªç»“æœ")
            
            # æ£€æŸ¥ç»“æœçš„ç±»å‹åˆ†å¸ƒ
            result_types = {}
            for doc in raw_results:
                chunk_type = doc.metadata.get('chunk_type', 'unknown')
                result_types[chunk_type] = result_types.get(chunk_type, 0) + 1
            
            print("æœç´¢ç»“æœç±»å‹åˆ†å¸ƒ:")
            for doc_type, count in result_types.items():
                print(f"  {doc_type}: {count}")
                
        except Exception as e:
            print(f"âŒ æ— è¿‡æ»¤æ¡ä»¶æœç´¢å¤±è´¥: {e}")
        
        # 4. æµ‹è¯•å¸¦è¿‡æ»¤æ¡ä»¶æœç´¢
        print("\nğŸ” æµ‹è¯•å¸¦è¿‡æ»¤æ¡ä»¶æœç´¢...")
        
        # æµ‹è¯•ä¸åŒçš„è¿‡æ»¤æ¡ä»¶
        filter_conditions = [
            {'chunk_type': 'image'},
            {'chunk_type': 'text'},
            {'chunk_type': 'table'},
            {'type': 'image'},  # é”™è¯¯çš„å­—æ®µå
            {'document_name': 'ã€ä¸Šæµ·è¯åˆ¸ã€‘ä¸­èŠ¯å›½é™…æ·±åº¦ç ”ç©¶æŠ¥å‘Šï¼šæ™¶åœ†åˆ¶é€ é¾™å¤´ï¼Œé¢†èˆªå›½äº§èŠ¯ç‰‡æ–°å¾ç¨‹'}
        ]
        
        for i, filter_cond in enumerate(filter_conditions, 1):
            try:
                print(f"\n  æµ‹è¯•è¿‡æ»¤æ¡ä»¶ {i}: {filter_cond}")
                filtered_results = vector_store.similarity_search(query, k=20, filter=filter_cond)
                print(f"    âœ… æœç´¢æˆåŠŸï¼Œè¿”å› {len(filtered_results)} ä¸ªç»“æœ")
                
                # æ£€æŸ¥è¿‡æ»¤åçš„ç»“æœç±»å‹
                if filtered_results:
                    first_doc = filtered_results[0]
                    print(f"    ç¬¬ä¸€ä¸ªç»“æœç±»å‹: {first_doc.metadata.get('chunk_type', 'unknown')}")
                    print(f"    ç¬¬ä¸€ä¸ªç»“æœæ–‡æ¡£å: {first_doc.metadata.get('document_name', 'unknown')}")
                
            except Exception as e:
                print(f"    âŒ æœç´¢å¤±è´¥: {e}")
        
        # 5. æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£çš„å…ƒæ•°æ®
        print("\nğŸ” æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£çš„å…ƒæ•°æ®...")
        image_docs = []
        for doc_id, doc in vector_store.docstore._dict.items():
            if doc.metadata.get('chunk_type') == 'image':
                image_docs.append(doc)
        
        print(f"æ‰¾åˆ° {len(image_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
        
        if image_docs:
            # æ˜¾ç¤ºå‰3ä¸ªå›¾ç‰‡æ–‡æ¡£çš„å…ƒæ•°æ®
            for i, doc in enumerate(image_docs[:3], 1):
                print(f"\n  å›¾ç‰‡æ–‡æ¡£ {i}:")
                print(f"    æ–‡æ¡£ID: {doc.metadata.get('image_id', 'unknown')}")
                print(f"    æ–‡æ¡£å: {doc.metadata.get('document_name', 'unknown')}")
                print(f"    é¡µç : {doc.metadata.get('page_number', 'unknown')}")
                print(f"    å›¾ç‰‡æ ‡é¢˜: {doc.metadata.get('img_caption', [])}")
                print(f"    å¢å¼ºæè¿°: {doc.metadata.get('enhanced_description', '')[:100]}...")
        
        # 6. æµ‹è¯•ç‰¹å®šå›¾ç‰‡æŸ¥è¯¢
        print("\nğŸ” æµ‹è¯•ç‰¹å®šå›¾ç‰‡æŸ¥è¯¢...")
        if image_docs:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå›¾ç‰‡æ–‡æ¡£çš„æ ‡é¢˜ä½œä¸ºæŸ¥è¯¢
            first_image_caption = image_docs[0].metadata.get('img_caption', [''])
            if first_image_caption:
                test_query = first_image_caption[0]
                print(f"æµ‹è¯•æŸ¥è¯¢: {test_query}")
                
                try:
                    # æ— è¿‡æ»¤æ¡ä»¶
                    raw_results = vector_store.similarity_search(test_query, k=10)
                    print(f"  æ— è¿‡æ»¤æ¡ä»¶: {len(raw_results)} ä¸ªç»“æœ")
                    
                    # å›¾ç‰‡è¿‡æ»¤æ¡ä»¶
                    filtered_results = vector_store.similarity_search(test_query, k=10, filter={'chunk_type': 'image'})
                    print(f"  å›¾ç‰‡è¿‡æ»¤æ¡ä»¶: {len(filtered_results)} ä¸ªç»“æœ")
                    
                except Exception as e:
                    print(f"  âŒ æµ‹è¯•æŸ¥è¯¢å¤±è´¥: {e}")
        
        print("\n" + "=" * 60)
        print("ğŸ¯ å‘é‡æœç´¢è°ƒè¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_vector_search()
