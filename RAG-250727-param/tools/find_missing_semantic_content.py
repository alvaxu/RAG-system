#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š
## 1. ä¸“é—¨ç”¨äºæ‰¾å‡ºç¼ºå°‘è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼æ–‡æ¡£
## 2. åˆ†æä¸ºä»€ä¹ˆæŸäº›è¡¨æ ¼æ²¡æœ‰è¯­ä¹‰åŒ–å†…å®¹
"""

import sys
import os
import json
from pathlib import Path

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import Settings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings

# å¯¼å…¥ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—
from config.api_key_manager import get_dashscope_api_key

def load_vector_store(vector_db_path):
    """åŠ è½½å‘é‡å­˜å‚¨"""
    try:
        config = Settings.load_from_file('config.json')
        
        # ä½¿ç”¨ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—è·å–APIå¯†é’¥
        config_key = config.dashscope_api_key
        api_key = get_dashscope_api_key(config_key)
        
        if not api_key:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return None
        
        # åˆå§‹åŒ–DashScope embeddings
        try:
            embedding_model = config.text_embedding_model
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®ï¼Œä½¿ç”¨é»˜è®¤embeddingæ¨¡å‹: {e}")
            embedding_model = 'text-embedding-v1'
        
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model=embedding_model)
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(vector_store.docstore._dict)} ä¸ªæ–‡æ¡£")
        return vector_store
    except Exception as e:
        print(f"âŒ åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
        return None

def analyze_semantic_content_missing(vector_store):
    """åˆ†æç¼ºå°‘è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼æ–‡æ¡£"""
    print("ğŸ” åˆ†æç¼ºå°‘è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼æ–‡æ¡£")
    print("=" * 60)
    
    if not vector_store or not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
        print("âŒ å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸")
        return None
    
    docstore = vector_store.docstore._dict
    table_docs = []
    
    # æ‰¾å‡ºæ‰€æœ‰è¡¨æ ¼æ–‡æ¡£
    for doc_id, doc in docstore.items():
        if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'table':
            table_docs.append((doc_id, doc))
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
    
    # åˆ†ææ¯ä¸ªè¡¨æ ¼æ–‡æ¡£çš„è¯­ä¹‰åŒ–å†…å®¹
    missing_semantic = []
    has_semantic = []
    
    for i, (doc_id, doc) in enumerate(table_docs):
        doc_info = {
            'index': i + 1,
            'doc_id': doc_id,
            'document_name': doc.metadata.get('document_name', 'N/A'),
            'page_number': doc.metadata.get('page_number', 'N/A'),
            'table_id': doc.metadata.get('table_id', 'N/A'),
            'table_type': doc.metadata.get('table_type', 'N/A'),
            'has_html': False,
            'has_semantic': False,
            'semantic_fields': [],
            'html_content_length': 0,
            'semantic_content_length': 0
        }
        
        # æ£€æŸ¥HTMLå†…å®¹
        if 'page_content' in doc.metadata and doc.metadata['page_content']:
            html_content = doc.metadata['page_content']
            doc_info['html_content_length'] = len(html_content)
            if '<table' in str(html_content).lower() or '<tr' in str(html_content).lower() or '<td' in str(html_content).lower():
                doc_info['has_html'] = True
        
        # æ£€æŸ¥è¯­ä¹‰åŒ–å†…å®¹
        semantic_fields = []
        semantic_content = ""
        
        # ä¸»è¦è¯­ä¹‰åŒ–å­—æ®µ - åªæ£€æŸ¥processed_table_content
        if 'processed_table_content' in doc.metadata and doc.metadata['processed_table_content']:
            semantic_fields.append('processed_table_content')
            semantic_content = doc.metadata['processed_table_content']
        
        # å¤‡ç”¨è¯­ä¹‰åŒ–å­—æ®µ - ä»…ä½œä¸ºå‚è€ƒï¼Œä¸å‚ä¸ä¸»è¦ç»Ÿè®¡
        alt_semantic_fields = []
        for alt_key in ['table_summary', 'table_title', 'related_text']:
            if alt_key in doc.metadata and doc.metadata[alt_key] and len(str(doc.metadata[alt_key])) > 0:
                alt_semantic_fields.append(alt_key)
        
        doc_info['semantic_fields'] = semantic_fields
        doc_info['alt_semantic_fields'] = alt_semantic_fields  # å¤‡ç”¨å­—æ®µ
        doc_info['semantic_content_length'] = len(semantic_content)
        
        # åªæœ‰processed_table_contentå­˜åœ¨æ‰ç®—æœ‰è¯­ä¹‰åŒ–å†…å®¹
        if semantic_fields:
            doc_info['has_semantic'] = True
            has_semantic.append(doc_info)
        else:
            missing_semantic.append(doc_info)
    
    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“Š è¯­ä¹‰åŒ–å†…å®¹ç»Ÿè®¡:")
    print(f"  æœ‰è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼: {len(has_semantic)}")
    print(f"  ç¼ºå°‘è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼: {len(missing_semantic)}")
    
    # æ˜¾ç¤ºç¼ºå°‘è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼è¯¦æƒ…
    if missing_semantic:
        print(f"\nâŒ ç¼ºå°‘è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼ ({len(missing_semantic)}ä¸ª):")
        print("=" * 60)
        for doc_info in missing_semantic:
            print(f"\nğŸ“„ è¡¨æ ¼ {doc_info['index']}:")
            print(f"  æ–‡æ¡£å: {doc_info['document_name']}")
            print(f"  é¡µç : {doc_info['page_number']}")
            print(f"  è¡¨æ ¼ID: {doc_info['table_id']}")
            print(f"  è¡¨æ ¼ç±»å‹: {doc_info['table_type']}")
            print(f"  æœ‰HTMLå†…å®¹: {'âœ…' if doc_info['has_html'] else 'âŒ'}")
            print(f"  HTMLå†…å®¹é•¿åº¦: {doc_info['html_content_length']}")
            print(f"  ä¸»è¦è¯­ä¹‰åŒ–å­—æ®µ: {doc_info['semantic_fields']}")
            print(f"  å¤‡ç”¨è¯­ä¹‰åŒ–å­—æ®µ: {doc_info['alt_semantic_fields']}")
            
            # æ˜¾ç¤ºå…ƒæ•°æ®ä¸­çš„æ‰€æœ‰å­—æ®µ
            doc = next(doc for doc_id, doc in table_docs if doc_id == doc_info['doc_id'])
            print(f"  æ‰€æœ‰å…ƒæ•°æ®å­—æ®µ: {list(doc.metadata.keys())}")
            
            # æ˜¾ç¤ºå…³é”®å­—æ®µçš„å€¼
            for field in ['processed_table_content', 'table_summary', 'table_title', 'related_text']:
                if field in doc.metadata:
                    value = doc.metadata[field]
                    if value is None:
                        print(f"    {field}: None")
                    elif value == "":
                        print(f"    {field}: (ç©ºå­—ç¬¦ä¸²)")
                    else:
                        print(f"    {field}: {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}")
                else:
                    print(f"    {field}: (å­—æ®µä¸å­˜åœ¨)")
    
    # æ˜¾ç¤ºæœ‰è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼ç»Ÿè®¡
    if has_semantic:
        print(f"\nâœ… æœ‰è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼ ({len(has_semantic)}ä¸ª):")
        print("=" * 60)
        
        # ç»Ÿè®¡è¯­ä¹‰åŒ–å­—æ®µçš„åˆ†å¸ƒ
        field_stats = {}
        for doc_info in has_semantic:
            for field in doc_info['semantic_fields']:
                field_stats[field] = field_stats.get(field, 0) + 1
        
        print("è¯­ä¹‰åŒ–å­—æ®µåˆ†å¸ƒ:")
        for field, count in sorted(field_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"  {field}: {count}ä¸ªè¡¨æ ¼")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæœ‰è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼
        print(f"\nå‰3ä¸ªæœ‰è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼ç¤ºä¾‹:")
        for i, doc_info in enumerate(has_semantic[:3]):
            print(f"\nğŸ“„ è¡¨æ ¼ {doc_info['index']}:")
            print(f"  æ–‡æ¡£å: {doc_info['document_name']}")
            print(f"  é¡µç : {doc_info['page_number']}")
            print(f"  è¡¨æ ¼ID: {doc_info['table_id']}")
            print(f"  ä¸»è¦è¯­ä¹‰åŒ–å­—æ®µ: {doc_info['semantic_fields']}")
            print(f"  å¤‡ç”¨è¯­ä¹‰åŒ–å­—æ®µ: {doc_info['alt_semantic_fields']}")
            print(f"  è¯­ä¹‰åŒ–å†…å®¹é•¿åº¦: {doc_info['semantic_content_length']}")
    
    return {
        'total_table_docs': len(table_docs),
        'has_semantic': len(has_semantic),
        'missing_semantic': len(missing_semantic),
        'missing_details': missing_semantic,
        'has_details': has_semantic
    }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æŸ¥æ‰¾ç¼ºå°‘è¯­ä¹‰åŒ–å†…å®¹çš„è¡¨æ ¼æ–‡æ¡£")
    print("=" * 60)
    
    try:
        config = Settings.load_from_file('config.json')
        vector_db_path = config.vector_db_dir
        
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
        if not os.path.exists(vector_db_path):
            possible_paths = [
                "./central/vector_db",
                "./vector_db",
                "./central/vector_db_test",
                "./vector_db_test"
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    vector_db_path = path
                    print(f"âœ… æ‰¾åˆ°å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
                    break
            else:
                print(f"âŒ å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•è¿‡çš„è·¯å¾„:")
                for path in possible_paths:
                    print(f"   - {path}")
                return
        
        # åŠ è½½å‘é‡å­˜å‚¨
        vector_store = load_vector_store(vector_db_path)
        if not vector_store:
            print("âŒ æ— æ³•åŠ è½½å‘é‡å­˜å‚¨")
            return
        
        # åˆ†æè¯­ä¹‰åŒ–å†…å®¹ç¼ºå¤±æƒ…å†µ
        analysis_result = analyze_semantic_content_missing(vector_store)
        
        # ä¿å­˜åˆ†æç»“æœ
        output_file = "semantic_content_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        print("\nâœ… è¯­ä¹‰åŒ–å†…å®¹ç¼ºå¤±åˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
