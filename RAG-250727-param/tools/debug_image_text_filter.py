'''
ç¨‹åºè¯´æ˜ï¼š
## 1. ä¸“é—¨è°ƒè¯•image_text filteræœç´¢é—®é¢˜
## 2. æ£€æŸ¥ä¸ºä»€ä¹ˆimage_textæ–‡æ¡£æ— æ³•é€šè¿‡filteræœç´¢åˆ°
## 3. åˆ†æfilteræœºåˆ¶å’Œæ–‡æ¡£ç±»å‹æ ‡è®°
'''

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.api_key_manager import get_dashscope_api_key


def debug_image_text_filter():
    """è°ƒè¯•image_text filteræœç´¢é—®é¢˜"""
    print("ğŸ” è°ƒè¯•image_text filteræœç´¢é—®é¢˜")
    print("=" * 80)
    
    try:
        # 1. åˆå§‹åŒ–
        api_key = get_dashscope_api_key()
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        vector_db_path = "../central/vector_db"
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        
        # 2. æ£€æŸ¥æ•°æ®åº“ä¸­çš„image_textæ–‡æ¡£
        print("\nğŸ“Š æ£€æŸ¥æ•°æ®åº“ä¸­çš„image_textæ–‡æ¡£")
        if hasattr(vector_store, 'docstore') and hasattr(vector_store.docstore, '_dict'):
            docs = vector_store.docstore._dict
            print(f"æ€»æ–‡æ¡£æ•°é‡: {len(docs)}")
            
            # ç»Ÿè®¡æ–‡æ¡£ç±»å‹
            chunk_types = {}
            image_text_docs = []
            
            for doc_id, doc in docs.items():
                chunk_type = doc.metadata.get('chunk_type', 'unknown')
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
                
                if chunk_type == 'image_text':
                    image_text_docs.append((doc_id, doc))
            
            print("æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
            for chunk_type, count in sorted(chunk_types.items()):
                print(f"  - {chunk_type}: {count} ä¸ª")
            
            print(f"\næ‰¾åˆ° {len(image_text_docs)} ä¸ªimage_textæ–‡æ¡£")
            
            # æ£€æŸ¥å‰å‡ ä¸ªimage_textæ–‡æ¡£çš„å†…å®¹
            if image_text_docs:
                print("\nğŸ“‹ å‰5ä¸ªimage_textæ–‡æ¡£å†…å®¹:")
                for i, (doc_id, doc) in enumerate(image_text_docs[:5]):
                    print(f"\n{i+1}. æ–‡æ¡£ID: {doc_id}")
                    print(f"   ç±»å‹: {doc.metadata.get('chunk_type')}")
                    print(f"   å†…å®¹: {doc.page_content[:200]}...")
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡å†…å®¹
                    if "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ" in doc.page_content:
                        print("   âœ… åŒ…å«ç›®æ ‡å†…å®¹ï¼")
                    else:
                        print("   âŒ ä¸åŒ…å«ç›®æ ‡å†…å®¹")
        
        # 3. æµ‹è¯•filteræœç´¢
        query = "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ"
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: '{query}'")
        
        # 3.1 ä¸ä½¿ç”¨filterçš„æœç´¢
        print("\nğŸ“‹ æ­¥éª¤1ï¼šä¸ä½¿ç”¨filterçš„æœç´¢ï¼ˆk=100ï¼‰")
        all_results = vector_store.similarity_search(query, k=100)
        print(f"æœç´¢åˆ° {len(all_results)} ä¸ªç»“æœ")
        
        # æŸ¥æ‰¾åŒ…å«ç›®æ ‡å†…å®¹çš„æ–‡æ¡£
        target_docs = []
        for i, doc in enumerate(all_results):
            if "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ" in doc.page_content:
                target_docs.append((i+1, doc))
        
        print(f"æ‰¾åˆ° {len(target_docs)} ä¸ªåŒ…å«ç›®æ ‡å†…å®¹çš„æ–‡æ¡£:")
        for rank, doc in target_docs:
            print(f"  æ’å{rank}: ç±»å‹={doc.metadata.get('chunk_type')}, å†…å®¹={doc.page_content[:100]}...")
        
        # 3.2 ä½¿ç”¨filteræœç´¢image_text
        print(f"\nğŸ“‹ æ­¥éª¤2ï¼šä½¿ç”¨filteræœç´¢image_textï¼ˆk=100ï¼‰")
        try:
            image_text_results = vector_store.similarity_search(
                query, 
                k=100,
                filter={'chunk_type': 'image_text'}
            )
            print(f"filteræœç´¢åˆ° {len(image_text_results)} ä¸ªimage_textç»“æœ")
            
            # æ£€æŸ¥filterç»“æœä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å†…å®¹
            filter_target_found = False
            for i, doc in enumerate(image_text_results):
                if "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ" in doc.page_content:
                    filter_target_found = True
                    print(f"âœ… åœ¨filterç»“æœä¸­æ‰¾åˆ°ç›®æ ‡æ–‡æ¡£ï¼ä½ç½®: ç¬¬{i+1}ä½")
                    print(f"   å†…å®¹: {doc.page_content[:200]}...")
                    break
            
            if not filter_target_found:
                print("âŒ åœ¨filterç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°ç›®æ ‡æ–‡æ¡£")
                
        except Exception as e:
            print(f"âŒ filteræœç´¢å¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # 4. æµ‹è¯•ä¸åŒçš„filteræ¡ä»¶
        print(f"\nğŸ“‹ æ­¥éª¤3ï¼šæµ‹è¯•ä¸åŒçš„filteræ¡ä»¶")
        
        # 4.1 æœç´¢textç±»å‹
        try:
            text_results = vector_store.similarity_search(
                query, 
                k=50,
                filter={'chunk_type': 'text'}
            )
            print(f"filteræœç´¢textç±»å‹: {len(text_results)} ä¸ªç»“æœ")
            
            for i, doc in enumerate(text_results):
                if "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ" in doc.page_content:
                    print(f"âœ… åœ¨textç±»å‹ä¸­æ‰¾åˆ°ç›®æ ‡æ–‡æ¡£ï¼ä½ç½®: ç¬¬{i+1}ä½")
                    break
                    
        except Exception as e:
            print(f"âŒ text filteræœç´¢å¤±è´¥: {e}")
        
        # 4.2 æœç´¢æ‰€æœ‰ç±»å‹
        try:
            all_types_results = vector_store.similarity_search(
                query, 
                k=50,
                filter={'chunk_type': ['image_text', 'text', 'image']}
            )
            print(f"filteræœç´¢æ‰€æœ‰ç›¸å…³ç±»å‹: {len(all_types_results)} ä¸ªç»“æœ")
            
        except Exception as e:
            print(f"âŒ å¤šç±»å‹filteræœç´¢å¤±è´¥: {e}")
        
        # 5. åˆ†æé—®é¢˜
        print("\n" + "=" * 80)
        print("ğŸ¯ é—®é¢˜åˆ†æ")
        print("=" * 80)
        
        if target_docs:
            print("âœ… ç›®æ ‡æ–‡æ¡£ç¡®å®å­˜åœ¨äºæ•°æ®åº“ä¸­")
            print("âŒ ä½†æ˜¯æ— æ³•é€šè¿‡image_text filteræœç´¢åˆ°")
            print("\nå¯èƒ½çš„åŸå› :")
            print("1. æ–‡æ¡£ç±»å‹æ ‡è®°é”™è¯¯ - åº”è¯¥æ ‡è®°ä¸ºimage_textä½†å®é™…æ˜¯text")
            print("2. Filteræœºåˆ¶æœ‰é—®é¢˜ - FAISSçš„filteråŠŸèƒ½å¯èƒ½ä¸å·¥ä½œ")
            print("3. æŸ¥è¯¢å†…å®¹åŒ¹é…é—®é¢˜ - è™½ç„¶åŒ…å«å…³é”®è¯ä½†ç›¸ä¼¼åº¦ä¸å¤Ÿ")
            
            print("\nå»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("1. æ£€æŸ¥æ–‡æ¡£ç±»å‹æ ‡è®°ï¼Œç¡®ä¿å›¾è¡¨ç›¸å…³å†…å®¹æ ‡è®°ä¸ºimage_text")
            print("2. åœ¨image_engineä¸­æ·»åŠ å¯¹textç±»å‹æ–‡æ¡£çš„æœç´¢")
            print("3. æˆ–è€…æ‰©å¤§æœç´¢èŒƒå›´ï¼Œä¸ä½¿ç”¨filter")
        else:
            print("âŒ ç›®æ ‡æ–‡æ¡£ä¸å­˜åœ¨äºæ•°æ®åº“ä¸­")
            
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


if __name__ == "__main__":
    debug_image_text_filter()
