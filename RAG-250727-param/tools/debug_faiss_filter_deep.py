'''
ç¨‹åºè¯´æ˜ï¼š
## 1. æ·±å…¥è°ƒè¯•FAISS filteræœºåˆ¶
## 2. æ£€æŸ¥ä¸ºä»€ä¹ˆfilterå¯¹image_textç±»å‹ä¸å·¥ä½œ
## 3. åˆ†æFAISSç´¢å¼•é…ç½®å’Œfilteræ”¯æŒ
'''

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.api_key_manager import get_dashscope_api_key


def debug_faiss_filter_deep():
    """æ·±å…¥è°ƒè¯•FAISS filteræœºåˆ¶"""
    print("ğŸ” æ·±å…¥è°ƒè¯•FAISS filteræœºåˆ¶")
    print("=" * 80)
    
    try:
        # 1. åˆå§‹åŒ–
        api_key = get_dashscope_api_key()
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        vector_db_path = "../central/vector_db"
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        
        # 2. æ£€æŸ¥FAISSç´¢å¼•ä¿¡æ¯
        print("\nğŸ“Š æ£€æŸ¥FAISSç´¢å¼•ä¿¡æ¯")
        if hasattr(vector_store, 'index'):
            faiss_index = vector_store.index
            print(f"FAISSç´¢å¼•ç±»å‹: {type(faiss_index).__name__}")
            print(f"ç´¢å¼•ç»´åº¦: {faiss_index.d}")
            print(f"ç´¢å¼•å¤§å°: {faiss_index.ntotal}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰filterç›¸å…³å±æ€§
            print("\nğŸ” æ£€æŸ¥filterç›¸å…³å±æ€§:")
            for attr in dir(faiss_index):
                if 'filter' in attr.lower() or 'metadata' in attr.lower():
                    try:
                        value = getattr(faiss_index, attr)
                        print(f"  {attr}: {value}")
                    except:
                        print(f"  {attr}: <æ— æ³•è·å–>")
        else:
            print("âŒ æ— æ³•è·å–FAISSç´¢å¼•ä¿¡æ¯")
        
        # 3. æ£€æŸ¥æ–‡æ¡£å…ƒæ•°æ®ç»“æ„
        print(f"\nğŸ“‹ æ£€æŸ¥æ–‡æ¡£å…ƒæ•°æ®ç»“æ„")
        if hasattr(vector_store, 'docstore') and hasattr(vector_store.docstore, '_dict'):
            docs = vector_store.docstore._dict
            sample_docs = list(docs.values())[:3]  # å–å‰3ä¸ªæ–‡æ¡£ä½œä¸ºæ ·æœ¬
            
            for i, doc in enumerate(sample_docs):
                print(f"\næ–‡æ¡£ {i+1}:")
                print(f"  ç±»å‹: {type(doc)}")
                print(f"  å±æ€§: {dir(doc)}")
                if hasattr(doc, 'metadata'):
                    print(f"  å…ƒæ•°æ®: {doc.metadata}")
                    print(f"  å…ƒæ•°æ®ç±»å‹: {type(doc.metadata)}")
                    if doc.metadata:
                        for key, value in doc.metadata.items():
                            print(f"    {key}: {value} (ç±»å‹: {type(value)})")
                else:
                    print("  æ— metadataå±æ€§")
        
        # 4. æµ‹è¯•ä¸åŒçš„filterè¯­æ³•
        print(f"\nğŸ§ª æµ‹è¯•ä¸åŒçš„filterè¯­æ³•")
        query = "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ"
        
        # 4.1 æµ‹è¯•åŸºæœ¬filter
        print("\nğŸ“‹ æµ‹è¯•1ï¼šåŸºæœ¬filterè¯­æ³•")
        try:
            results1 = vector_store.similarity_search(
                query, k=10, filter={'chunk_type': 'image_text'}
            )
            print(f"  ç»“æœæ•°é‡: {len(results1)}")
        except Exception as e:
            print(f"  å¤±è´¥: {e}")
        
        # 4.2 æµ‹è¯•å­—ç¬¦ä¸²filter
        print("\nğŸ“‹ æµ‹è¯•2ï¼šå­—ç¬¦ä¸²filterè¯­æ³•")
        try:
            results2 = vector_store.similarity_search(
                query, k=10, filter="chunk_type == 'image_text'"
            )
            print(f"  ç»“æœæ•°é‡: {len(results2)}")
        except Exception as e:
            print(f"  å¤±è´¥: {e}")
        
        # 4.3 æµ‹è¯•åˆ—è¡¨filter
        print("\nğŸ“‹ æµ‹è¯•3ï¼šåˆ—è¡¨filterè¯­æ³•")
        try:
            results3 = vector_store.similarity_search(
                query, k=10, filter={'chunk_type': ['image_text']}
            )
            print(f"  ç»“æœæ•°é‡: {len(results3)}")
        except Exception as e:
            print(f"  å¤±è´¥: {e}")
        
        # 4.4 æµ‹è¯•æ— filter
        print("\nğŸ“‹ æµ‹è¯•4ï¼šæ— filter")
        try:
            results4 = vector_store.similarity_search(query, k=10)
            print(f"  ç»“æœæ•°é‡: {len(results4)}")
            
            # æ£€æŸ¥è¿”å›çš„æ–‡æ¡£ç±»å‹
            image_text_count = 0
            for doc in results4:
                if hasattr(doc, 'metadata') and doc.metadata:
                    chunk_type = doc.metadata.get('chunk_type')
                    if chunk_type == 'image_text':
                        image_text_count += 1
            
            print(f"  å…¶ä¸­image_textç±»å‹: {image_text_count} ä¸ª")
            
        except Exception as e:
            print(f"  å¤±è´¥: {e}")
        
        # 5. æ£€æŸ¥LangChainç‰ˆæœ¬å’Œé…ç½®
        print(f"\nğŸ“¦ æ£€æŸ¥LangChainç‰ˆæœ¬å’Œé…ç½®")
        try:
            import langchain_community
            print(f"LangChainç‰ˆæœ¬: {langchain_community.__version__}")
        except:
            print("æ— æ³•è·å–LangChainç‰ˆæœ¬")
        
        # 6. åˆ†æé—®é¢˜
        print("\n" + "=" * 80)
        print("ğŸ¯ é—®é¢˜åˆ†æ")
        print("=" * 80)
        
        print("\nå¯èƒ½çš„åŸå› :")
        print("1. FAISSç´¢å¼•ä¸æ”¯æŒfilteråŠŸèƒ½")
        print("2. Filterè¯­æ³•ä¸æ­£ç¡®")
        print("3. æ–‡æ¡£å…ƒæ•°æ®æ ¼å¼é—®é¢˜")
        print("4. LangChainç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜")
        print("5. å‘é‡æ•°æ®åº“æ„å»ºæ—¶çš„é…ç½®é—®é¢˜")
        
        print("\nå»ºè®®ä¸‹ä¸€æ­¥:")
        print("1. æ£€æŸ¥FAISSç´¢å¼•æ˜¯å¦æ”¯æŒfilter")
        print("2. å°è¯•ä¸åŒçš„filterè¯­æ³•")
        print("3. æ£€æŸ¥æ–‡æ¡£å…ƒæ•°æ®æ ¼å¼")
        print("4. è€ƒè™‘å‡çº§æˆ–é™çº§LangChainç‰ˆæœ¬")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


if __name__ == "__main__":
    debug_faiss_filter_deep()
