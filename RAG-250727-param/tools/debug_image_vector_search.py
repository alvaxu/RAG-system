#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š
## 1. æ·±åº¦è¯Šæ–­å›¾ç‰‡å‘é‡æœç´¢é—®é¢˜
## 2. ç¡®å®šæ˜¯å‘é‡åŒ–é—®é¢˜è¿˜æ˜¯æŸ¥è¯¢é€»è¾‘é—®é¢˜
## 3. é€æ­¥æ’æŸ¥æ¯ä¸ªå¯èƒ½çš„é—®é¢˜ç‚¹
"""

import sys
import os
import logging

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import Settings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings

# å¯¼å…¥ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—
from config.api_key_manager import get_dashscope_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def debug_image_vector_search():
    """æ·±åº¦è¯Šæ–­å›¾ç‰‡å‘é‡æœç´¢é—®é¢˜"""
    print("ğŸ” æ·±åº¦è¯Šæ–­å›¾ç‰‡å‘é‡æœç´¢é—®é¢˜")
    print("=" * 60)
    
    try:
        # åŠ è½½é…ç½®
        config = Settings.load_from_file('config.json')
        vector_db_path = config.vector_db_dir
        
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
        
        # è·å–APIå¯†é’¥
        config_key = config.dashscope_api_key
        api_key = get_dashscope_api_key(config_key)
        
        if not api_key:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return
        
        # åˆå§‹åŒ–embeddings
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        print("âœ… DashScope embeddingsåˆå§‹åŒ–æˆåŠŸ")
        
        # åŠ è½½å‘é‡å­˜å‚¨
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(vector_store.docstore._dict)} ä¸ªæ–‡æ¡£")
        
        # æ·±åº¦è¯Šæ–­å¼€å§‹
        print("\nğŸ” å¼€å§‹æ·±åº¦è¯Šæ–­...")
        
        # è¯Šæ–­1ï¼šæ£€æŸ¥æ–‡æ¡£åˆ†å¸ƒ
        print("\nğŸ“Š è¯Šæ–­1ï¼šæ£€æŸ¥æ–‡æ¡£åˆ†å¸ƒ")
        docstore = vector_store.docstore._dict
        chunk_type_count = {}
        doc_details = {}
        
        for doc_id, doc in docstore.items():
            if hasattr(doc, 'metadata') and doc.metadata:
                chunk_type = doc.metadata.get('chunk_type', 'unknown')
                chunk_type_count[chunk_type] = chunk_type_count.get(chunk_type, 0) + 1
                
                # è®°å½•å‰å‡ ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
                if chunk_type in ['image', 'image_text'] and chunk_type not in doc_details:
                    doc_details[chunk_type] = {
                        'doc_id': doc_id,
                        'doc': doc,
                        'metadata': doc.metadata
                    }
        
        print("ğŸ“‹ æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
        for chunk_type, count in sorted(chunk_type_count.items()):
            print(f"  {chunk_type}: {count} ä¸ª")
        
        # è¯Šæ–­2ï¼šæ£€æŸ¥image_textæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        print("\nğŸ“Š è¯Šæ–­2ï¼šæ£€æŸ¥image_textæ–‡æ¡£è¯¦ç»†ä¿¡æ¯")
        if 'image_text' in doc_details:
            doc_info = doc_details['image_text']
            doc = doc_info['doc']
            metadata = doc_info['metadata']
            
            print(f"ğŸ“„ æ–‡æ¡£ID: {doc_info['doc_id']}")
            print(f"ğŸ“„ æ–‡æ¡£ç±»å‹: {type(doc)}")
            print(f"ğŸ“„ æ˜¯å¦æœ‰page_content: {hasattr(doc, 'page_content')}")
            
            if hasattr(doc, 'page_content'):
                print(f"ğŸ“„ page_contenté•¿åº¦: {len(doc.page_content)}")
                print(f"ğŸ“„ page_contenté¢„è§ˆ: {doc.page_content[:200]}...")
            
            print(f"ğŸ“„ metadataå­—æ®µ: {list(metadata.keys())}")
            print(f"ğŸ“„ chunk_type: {metadata.get('chunk_type', 'N/A')}")
            print(f"ğŸ“„ enhanced_descriptioné•¿åº¦: {len(metadata.get('enhanced_description', ''))}")
            
            # æ£€æŸ¥å…³é”®å­—æ®µ
            key_fields = ['enhanced_description', 'image_id', 'document_name', 'page_number']
            for field in key_fields:
                value = metadata.get(field, 'N/A')
                if isinstance(value, str) and len(value) > 100:
                    print(f"ğŸ“„ {field}: {value[:100]}...")
                else:
                    print(f"ğŸ“„ {field}: {value}")
        
        # è¯Šæ–­3ï¼šæ£€æŸ¥imageæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        print("\nğŸ“Š è¯Šæ–­3ï¼šæ£€æŸ¥imageæ–‡æ¡£è¯¦ç»†ä¿¡æ¯")
        if 'image' in doc_details:
            doc_info = doc_details['image']
            doc = doc_info['doc']
            metadata = doc_info['metadata']
            
            print(f"ğŸ–¼ï¸ æ–‡æ¡£ID: {doc_info['doc_id']}")
            print(f"ğŸ–¼ï¸ æ–‡æ¡£ç±»å‹: {type(doc)}")
            print(f"ğŸ–¼ï¸ æ˜¯å¦æœ‰page_content: {hasattr(doc, 'page_content')}")
            
            if hasattr(doc, 'page_content'):
                print(f"ğŸ–¼ï¸ page_contenté•¿åº¦: {len(doc.page_content)}")
                print(f"ğŸ–¼ï¸ page_contenté¢„è§ˆ: {doc.page_content[:200]}...")
            
            print(f"ğŸ–¼ï¸ metadataå­—æ®µ: {list(metadata.keys())}")
            print(f"ğŸ–¼ï¸ chunk_type: {metadata.get('chunk_type', 'N/A')}")
            print(f"ğŸ–¼ï¸ img_caption: {metadata.get('img_caption', 'N/A')}")
            print(f"ğŸ–¼ï¸ image_id: {metadata.get('image_id', 'N/A')}")
        
        # è¯Šæ–­4ï¼šæµ‹è¯•ä¸åŒçš„æŸ¥è¯¢æ–¹å¼
        print("\nğŸ“Š è¯Šæ–­4ï¼šæµ‹è¯•ä¸åŒçš„æŸ¥è¯¢æ–¹å¼")
        test_queries = [
            "ä¸­èŠ¯å›½é™…å‡€åˆ©æ¶¦",
            "è‚¡ä»·ç›¸å¯¹èµ°åŠ¿",
            "å›¾ç‰‡",
            "å›¾è¡¨"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
            
            # æµ‹è¯•æ— filteræœç´¢
            try:
                results = vector_store.similarity_search(query, k=3)
                print(f"  æ— filter: è¿”å› {len(results)} ä¸ªç»“æœ")
                for i, doc in enumerate(results[:2]):
                    chunk_type = doc.metadata.get('chunk_type', 'unknown') if hasattr(doc, 'metadata') else 'unknown'
                    print(f"    ç»“æœ{i+1}: {chunk_type}")
            except Exception as e:
                print(f"  æ— filter: å¤±è´¥ - {e}")
            
            # æµ‹è¯•image_textæœç´¢
            try:
                image_text_results = vector_store.similarity_search(
                    query, 
                    k=3,
                    filter={'chunk_type': 'image_text'}
                )
                print(f"  image_text filter: è¿”å› {len(image_text_results)} ä¸ªç»“æœ")
            except Exception as e:
                print(f"  image_text filter: å¤±è´¥ - {e}")
            
            # æµ‹è¯•imageæœç´¢
            try:
                image_results = vector_store.similarity_search(
                    query, 
                    k=3,
                    filter={'chunk_type': 'image'}
                )
                print(f"  image filter: è¿”å› {len(image_results)} ä¸ªç»“æœ")
            except Exception as e:
                print(f"  image filter: å¤±è´¥ - {e}")
        
        # è¯Šæ–­5ï¼šæ£€æŸ¥å‘é‡ç´¢å¼•
        print("\nğŸ“Š è¯Šæ–­5ï¼šæ£€æŸ¥å‘é‡ç´¢å¼•")
        try:
            # æ£€æŸ¥FAISSç´¢å¼•
            faiss_index = vector_store.index
            print(f"ğŸ”¢ FAISSç´¢å¼•ç±»å‹: {type(faiss_index)}")
            print(f"ğŸ”¢ å‘é‡ç»´åº¦: {faiss_index.d}")
            print(f"ğŸ”¢ å‘é‡æ•°é‡: {faiss_index.ntotal}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡æ•°æ®
            if hasattr(faiss_index, 'get_xb'):
                try:
                    vectors = faiss_index.get_xb()
                    print(f"ğŸ”¢ å‘é‡æ•°æ®ç±»å‹: {type(vectors)}")
                    if hasattr(vectors, 'shape'):
                        print(f"ğŸ”¢ å‘é‡æ•°æ®å½¢çŠ¶: {vectors.shape}")
                        print(f"ğŸ”¢ å‰3ä¸ªå‘é‡çš„å‰5ç»´: {vectors[:3, :5]}")
                except Exception as e:
                    print(f"ğŸ”¢ æ— æ³•è·å–å‘é‡æ•°æ®: {e}")
            
        except Exception as e:
            print(f"âŒ æ£€æŸ¥å‘é‡ç´¢å¼•å¤±è´¥: {e}")
        
        # è¯Šæ–­6ï¼šæ£€æŸ¥filterå‚æ•°
        print("\nğŸ“Š è¯Šæ–­6ï¼šæ£€æŸ¥filterå‚æ•°")
        print("ğŸ” æµ‹è¯•ä¸åŒçš„filterç»„åˆ...")
        
        filter_tests = [
            {},  # æ— filter
            {'chunk_type': 'image_text'},  # ç²¾ç¡®åŒ¹é…
            {'chunk_type': 'image'},  # ç²¾ç¡®åŒ¹é…
            {'chunk_type': {'$in': ['image_text', 'image']}},  # åŒ…å«åŒ¹é…
        ]
        
        for i, filter_param in enumerate(filter_tests):
            try:
                results = vector_store.similarity_search("ä¸­èŠ¯å›½é™…", k=5, filter=filter_param)
                print(f"  Filter {i+1} {filter_param}: è¿”å› {len(results)} ä¸ªç»“æœ")
                
                # æ˜¾ç¤ºç»“æœçš„chunk_typeåˆ†å¸ƒ
                chunk_types = {}
                for doc in results:
                    chunk_type = doc.metadata.get('chunk_type', 'unknown') if hasattr(doc, 'metadata') else 'unknown'
                    chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
                
                for chunk_type, count in chunk_types.items():
                    print(f"    {chunk_type}: {count} ä¸ª")
                    
            except Exception as e:
                print(f"  Filter {i+1} {filter_param}: å¤±è´¥ - {e}")
        
        print("\nâœ… æ·±åº¦è¯Šæ–­å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¯Šæ–­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_image_vector_search()
