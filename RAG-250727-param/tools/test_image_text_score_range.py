#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š
## 1. ä¸“é—¨æµ‹è¯•image_textæœç´¢çš„scoreèŒƒå›´
## 2. ä½¿ç”¨similarity_search_with_scoreè·å–çœŸå®åˆ†æ•°
## 3. åˆ†æFAISSè¿”å›çš„scoreç±»å‹å’ŒèŒƒå›´
## 4. ä¸ºè°ƒæ•´é˜ˆå€¼æä¾›æ•°æ®æ”¯æŒ
"""

import sys
import os
import logging
import json
from typing import List, Dict, Any

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import Settings
from v2.config.v2_config import ImageEngineConfigV2
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.api_key_manager import get_dashscope_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_faiss_config(vector_store):
    """
    ä¸“é—¨åˆ†æFAISSæ˜¯å¦æ”¯æŒä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»¥åŠå¦‚ä½•è·å–ä½™å¼¦ç›¸ä¼¼åº¦å€¼
    
    :param vector_store: FAISSå‘é‡æ•°æ®åº“
    :return: é…ç½®åˆ†æç»“æœ
    """
    print("\nğŸ”§ FAISSä½™å¼¦ç›¸ä¼¼åº¦æ”¯æŒåˆ†æ")
    print("=" * 60)
    
    config_info = {}
    
    try:
        # æ£€æŸ¥FAISSç´¢å¼•ç±»å‹å’Œåº¦é‡æ–¹å¼
        if hasattr(vector_store, 'index'):
            faiss_index = vector_store.index
            config_info['index_type'] = type(faiss_index).__name__
            config_info['index_metric_type'] = getattr(faiss_index, 'metric_type', 'unknown')
            
            print(f"ğŸ“Š FAISSç´¢å¼•ä¿¡æ¯:")
            print(f"  - ç´¢å¼•ç±»å‹: {config_info['index_type']}")
            print(f"  - åº¦é‡ç±»å‹: {config_info['index_metric_type']}")
            
            # è§£é‡Šåº¦é‡ç±»å‹
            metric_explanations = {
                0: "L2è·ç¦» (è¶Šå°è¶Šç›¸ä¼¼)",
                1: "IPå†…ç§¯ (è¶Šå¤§è¶Šç›¸ä¼¼)", 
                2: "ä½™å¼¦ç›¸ä¼¼åº¦ (è¶Šå¤§è¶Šç›¸ä¼¼)"
            }
            
            if config_info['index_metric_type'] in metric_explanations:
                print(f"  - åº¦é‡å«ä¹‰: {metric_explanations[config_info['index_metric_type']]}")
            else:
                print(f"  - åº¦é‡å«ä¹‰: æœªçŸ¥ç±»å‹ {config_info['index_metric_type']}")
        
        # å…³é”®é—®é¢˜ï¼šèƒ½å¦è·å–ä½™å¼¦ç›¸ä¼¼åº¦ï¼Ÿ
        print(f"\nğŸ¯ æ ¸å¿ƒé—®é¢˜ï¼šèƒ½å¦è·å–ä½™å¼¦ç›¸ä¼¼åº¦å€¼ï¼Ÿ")
        print("-" * 40)
        
        if config_info.get('index_metric_type') == 2:
            print("âœ… å¥½æ¶ˆæ¯ï¼FAISSå½“å‰é…ç½®ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦")
            print("  - å¯ä»¥ç›´æ¥è·å–ä½™å¼¦ç›¸ä¼¼åº¦å€¼")
            print("  - åˆ†æ•°èŒƒå›´: [0,1]")
            print("  - é˜ˆå€¼è®¾ç½®ç®€å•: 0.3-0.8")
            
        elif config_info.get('index_metric_type') == 1:
            print("âŒ å½“å‰FAISSä½¿ç”¨IPå†…ç§¯ï¼Œæ— æ³•ç›´æ¥è·å–ä½™å¼¦ç›¸ä¼¼åº¦")
            print("  - éœ€è¦æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦")
            print("  - æˆ–è€…é‡å»ºFAISSç´¢å¼•ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦")
            
        elif config_info.get('index_metric_type') == 0:
            print("âŒ å½“å‰FAISSä½¿ç”¨L2è·ç¦»ï¼Œæ— æ³•ç›´æ¥è·å–ä½™å¼¦ç›¸ä¼¼åº¦")
            print("  - éœ€è¦æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦")
            print("  - æˆ–è€…é‡å»ºFAISSç´¢å¼•ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦")
            
        else:
            print("âš ï¸ æ— æ³•ç¡®å®šFAISSåº¦é‡ç±»å‹ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
        
        # å°è¯•æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        print(f"\nğŸ”„ å°è¯•æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦...")
        try:
            # è·å–ä¸€ä¸ªæµ‹è¯•æŸ¥è¯¢çš„å‘é‡
            test_query = "æµ‹è¯•æŸ¥è¯¢"
            test_vector = vector_store.embedding_function.embed_query(test_query)
            print(f"  - æµ‹è¯•æŸ¥è¯¢å‘é‡ç»´åº¦: {len(test_vector)}")
            
            # è·å–ä¸€ä¸ªæ–‡æ¡£çš„å‘é‡ï¼ˆä»FAISSç´¢å¼•ä¸­ï¼‰
            if hasattr(faiss_index, 'reconstruct'):
                try:
                    # å°è¯•é‡å»ºç¬¬ä¸€ä¸ªå‘é‡
                    first_vector = faiss_index.reconstruct(0)
                    print(f"  - ç¬¬ä¸€ä¸ªæ–‡æ¡£å‘é‡ç»´åº¦: {len(first_vector)}")
                    
                    # æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    import numpy as np
                    dot_product = np.dot(test_vector, first_vector)
                    norm_test = np.linalg.norm(test_vector)
                    norm_doc = np.linalg.norm(first_vector)
                    cosine_similarity = dot_product / (norm_test * norm_doc)
                    
                    print(f"  - æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_similarity:.4f}")
                    print(f"  - éªŒè¯: å€¼åœ¨[0,1]èŒƒå›´å†… âœ…")
                    
                    config_info['manual_cosine_success'] = True
                    config_info['manual_cosine_value'] = cosine_similarity
                    
                except Exception as e:
                    print(f"  - æ— æ³•é‡å»ºå‘é‡: {e}")
                    config_info['manual_cosine_success'] = False
            else:
                print("  - FAISSç´¢å¼•ä¸æ”¯æŒå‘é‡é‡å»º")
                config_info['manual_cosine_success'] = False
                
        except Exception as e:
            print(f"  - æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            config_info['manual_cosine_success'] = False
        
        # æ€»ç»“å’Œå»ºè®®
        print(f"\nğŸ’¡ æ€»ç»“å’Œå»ºè®®:")
        print("-" * 40)
        
        if config_info.get('index_metric_type') == 2:
            print("âœ… ç›´æ¥è·å–ä½™å¼¦ç›¸ä¼¼åº¦: å¯ä»¥")
            print("  - æ— éœ€é¢å¤–æ“ä½œï¼ŒFAISSç›´æ¥è¿”å›ä½™å¼¦ç›¸ä¼¼åº¦")
            
        elif config_info.get('manual_cosine_success'):
            print("âš ï¸ ç›´æ¥è·å–ä½™å¼¦ç›¸ä¼¼åº¦: ä¸å¯ä»¥ï¼Œä½†å¯ä»¥æ‰‹åŠ¨è®¡ç®—")
            print("  - å½“å‰FAISSä½¿ç”¨å…¶ä»–åº¦é‡æ–¹å¼")
            print("  - å¯ä»¥é€šè¿‡æ‰‹åŠ¨è®¡ç®—è·å¾—ä½™å¼¦ç›¸ä¼¼åº¦")
            print("  - å»ºè®®: è€ƒè™‘é‡å»ºFAISSç´¢å¼•ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦")
            
        else:
            print("âŒ è·å–ä½™å¼¦ç›¸ä¼¼åº¦: å›°éš¾")
            print("  - å½“å‰FAISSé…ç½®ä¸æ”¯æŒä½™å¼¦ç›¸ä¼¼åº¦")
            print("  - æ‰‹åŠ¨è®¡ç®—ä¹Ÿå¤±è´¥")
            print("  - å»ºè®®: é‡å»ºFAISSç´¢å¼•æˆ–ä½¿ç”¨å…¶ä»–æ–¹æ¡ˆ")
            
    except Exception as e:
        print(f"âŒ FAISSé…ç½®åˆ†æå¤±è´¥: {e}")
        config_info['error'] = str(e)
    
    return config_info

def analyze_score_range(vector_store, query: str, max_results: int = 20):
    """
    åˆ†æimage_textæœç´¢çš„scoreèŒƒå›´
    
    :param vector_store: FAISSå‘é‡æ•°æ®åº“
    :param query: æŸ¥è¯¢æ–‡æœ¬
    :param max_results: æœ€å¤§ç»“æœæ•°
    :return: åˆ†æç»“æœ
    """
    print(f"\nğŸ” åˆ†ææŸ¥è¯¢: {query}")
    print("=" * 60)
    
    results = {
        'query': query,
        'max_results': max_results,
        'search_k': max_results * 3,
        'filter_condition': {'chunk_type': 'image_text'},
        'candidates': [],
        'score_analysis': {},
        'metadata_analysis': {},
        'recommendations': []
    }
    
    try:
        # ä½¿ç”¨ä¸image_engineå®Œå…¨ç›¸åŒçš„æ–¹æ³•
        search_k = max(max_results * 3, 50)
        print(f"ğŸ“Š æœç´¢å‚æ•°:")
        print(f"  - æŸ¥è¯¢: {query}")
        print(f"  - æœ€å¤§ç»“æœæ•°: {max_results}")
        print(f"  - æœç´¢å€™é€‰æ•°: {search_k}")
        print(f"  - Filteræ¡ä»¶: {results['filter_condition']}")
        
        # ç­–ç•¥1ï¼šä½¿ç”¨FAISS filterç›´æ¥æœç´¢image_textç±»å‹æ–‡æ¡£
        print(f"\nğŸ“‹ ç­–ç•¥1ï¼šFAISS filteræœç´¢")
        print("-" * 40)
        
        try:
            # ä½¿ç”¨similarity_search_with_scoreè·å–çœŸå®åˆ†æ•°
            docs_and_scores = vector_store.similarity_search_with_score(
                query, 
                k=max_results * 2,
                filter={'chunk_type': 'image_text'}
            )
            
            print(f"âœ… Filteræœç´¢æˆåŠŸï¼Œè¿”å› {len(docs_and_scores)} ä¸ªå€™é€‰ç»“æœ")
            
            # åˆ†ææ¯ä¸ªå€™é€‰ç»“æœï¼ˆåŒ…å«çœŸå®åˆ†æ•°ï¼‰
            for i, (doc, score) in enumerate(docs_and_scores):
                doc_analysis = {
                    'index': i + 1,
                    'has_score_attr': True,  # ç°åœ¨æœ‰çœŸå®åˆ†æ•°
                    'score_value': score,    # ä½¿ç”¨çœŸå®åˆ†æ•°
                    'score_type': type(score),
                    'metadata': doc.metadata if hasattr(doc, 'metadata') else {},
                    'chunk_type': doc.metadata.get('chunk_type') if hasattr(doc, 'metadata') else None,
                    'content_preview': doc.page_content[:100] + "..." if hasattr(doc, 'page_content') else "N/A"
                }
                
                results['candidates'].append(doc_analysis)
                
                print(f"  æ–‡æ¡£ {i+1}:")
                print(f"    - æ˜¯å¦æœ‰scoreå±æ€§: {doc_analysis['has_score_attr']}")
                print(f"    - Scoreå€¼: {doc_analysis['score_value']}")
                print(f"    - Scoreç±»å‹: {doc_analysis['score_type']}")
                print(f"    - Chunkç±»å‹: {doc_analysis['chunk_type']}")
                print(f"    - å†…å®¹é¢„è§ˆ: {doc_analysis['content_preview']}")
                
                # åˆ†æmetadata
                if doc.metadata:
                    for key, value in doc.metadata.items():
                        if key not in results['metadata_analysis']:
                            results['metadata_analysis'][key] = []
                        if value not in results['metadata_analysis'][key]:
                            results['metadata_analysis'][key].append(value)
            
            # åˆ†æscoreèŒƒå›´
            scores = [c['score_value'] for c in results['candidates'] if c['score_value'] is not None]
            if scores:
                results['score_analysis'] = {
                    'count': len(scores),
                    'min_score': min(scores),
                    'max_score': max(scores),
                    'avg_score': sum(scores) / len(scores),
                    'score_range': f"{min(scores)} - {max(scores)}",
                    'score_types': list(set(type(s).__name__ for s in scores))
                }
                
                print(f"\nğŸ“Š Scoreåˆ†æç»“æœ:")
                print(f"  - æœ‰æ•ˆScoreæ•°é‡: {results['score_analysis']['count']}")
                print(f"  - ScoreèŒƒå›´: {results['score_analysis']['score_range']}")
                print(f"  - æœ€å°Score: {results['score_analysis']['min_score']}")
                print(f"  - æœ€å¤§Score: {results['score_analysis']['max_score']}")
                print(f"  - å¹³å‡Score: {results['score_analysis']['avg_score']:.4f}")
                print(f"  - Scoreç±»å‹: {results['score_analysis']['score_types']}")
                
                # ç”Ÿæˆå»ºè®® - é’ˆå¯¹IPå†…ç§¯åˆ†æ•°
                min_score = results['score_analysis']['min_score']
                max_score = results['score_analysis']['max_score']
                
                if min_score > 1000:  # IPå†…ç§¯ç‰¹å¾
                    # åŸºäºå½“å‰åˆ†æ•°èŒƒå›´è®¡ç®—åˆç†é˜ˆå€¼
                    score_range = max_score - min_score
                    conservative_threshold = min_score + score_range * 0.1  # ä¿å®ˆé˜ˆå€¼
                    moderate_threshold = min_score + score_range * 0.2      # ä¸­ç­‰é˜ˆå€¼
                    aggressive_threshold = min_score + score_range * 0.3    # æ¿€è¿›é˜ˆå€¼
                    
                    results['recommendations'].append(f"IPå†…ç§¯åˆ†æ•°èŒƒå›´: {min_score:.2f} - {max_score:.2f}")
                    results['recommendations'].append(f"å»ºè®®é˜ˆå€¼è®¾ç½®:")
                    results['recommendations'].append(f"  - ä¿å®ˆé˜ˆå€¼: {conservative_threshold:.2f}")
                    results['recommendations'].append(f"  - ä¸­ç­‰é˜ˆå€¼: {moderate_threshold:.2f}")
                    results['recommendations'].append(f"  - æ¿€è¿›é˜ˆå€¼: {aggressive_threshold:.2f}")
                    results['recommendations'].append("æ³¨æ„: å½“å‰ä½¿ç”¨IPå†…ç§¯ï¼Œå»ºè®®è€ƒè™‘åˆ‡æ¢åˆ°ä½™å¼¦ç›¸ä¼¼åº¦")
                    
                elif min_score < 0.05:  # ä½™å¼¦ç›¸ä¼¼åº¦ç‰¹å¾
                    results['recommendations'].append("å½“å‰é˜ˆå€¼0.05è¿‡ä½ï¼Œå»ºè®®æé«˜åˆ°0.3-0.5")
                elif min_score > 0.8:
                    results['recommendations'].append("å½“å‰é˜ˆå€¼0.05è¿‡ä½ï¼Œå»ºè®®æé«˜åˆ°0.7-0.8")
                else:
                    results['recommendations'].append(f"å»ºè®®é˜ˆå€¼è®¾ç½®ä¸º: {min_score:.2f} - {max_score:.2f}")
                    
            else:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„Scoreå€¼")
                results['recommendations'].append("æ‰€æœ‰æ–‡æ¡£éƒ½æ²¡æœ‰Scoreå±æ€§ï¼Œéœ€è¦æ£€æŸ¥FAISSé…ç½®")
                
        except Exception as e:
            print(f"âŒ Filteræœç´¢å¤±è´¥: {e}")
            results['recommendations'].append(f"Filteræœç´¢å¤±è´¥: {e}")
            
            # å°è¯•é™çº§æœç´¢ï¼ˆä¹Ÿä½¿ç”¨with_scoreï¼‰
            print(f"\nğŸ”„ å°è¯•é™çº§æœç´¢...")
            try:
                all_candidates_with_scores = vector_store.similarity_search_with_score(query, k=search_k)
                print(f"é™çº§æœç´¢è¿”å› {len(all_candidates_with_scores)} ä¸ªå€™é€‰ç»“æœ")
                
                # åè¿‡æ»¤ï¼šç­›é€‰å‡ºimage_textç±»å‹çš„æ–‡æ¡£ï¼Œå¹¶ä¿ç•™åˆ†æ•°
                image_text_candidates = []
                for doc, score in all_candidates_with_scores:
                    if (hasattr(doc, 'metadata') and doc.metadata and 
                        doc.metadata.get('chunk_type') == 'image_text'):
                        image_text_candidates.append((doc, score))
                
                print(f"åè¿‡æ»¤åæ‰¾åˆ° {len(image_text_candidates)} ä¸ªimage_textæ–‡æ¡£")
                
                if image_text_candidates:
                    results['recommendations'].append("å»ºè®®ä½¿ç”¨åè¿‡æ»¤ç­–ç•¥ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
                    # åˆ†æé™çº§æœç´¢çš„åˆ†æ•°
                    fallback_scores = [score for doc, score in image_text_candidates]
                    if fallback_scores:
                        print(f"é™çº§æœç´¢åˆ†æ•°èŒƒå›´: {min(fallback_scores):.4f} - {max(fallback_scores):.4f}")
                else:
                    results['recommendations'].append("åè¿‡æ»¤ä¹Ÿæ²¡æœ‰æ‰¾åˆ°image_textæ–‡æ¡£ï¼Œéœ€è¦æ£€æŸ¥æ•°æ®")
                    
            except Exception as fallback_error:
                print(f"é™çº§æœç´¢ä¹Ÿå¤±è´¥: {fallback_error}")
                results['recommendations'].append(f"é™çº§æœç´¢å¤±è´¥: {fallback_error}")
        
        # åˆ†æmetadataå­—æ®µ
        print(f"\nğŸ“‹ Metadataå­—æ®µåˆ†æ:")
        print("-" * 40)
        for field, values in results['metadata_analysis'].items():
            print(f"  {field}: {values[:5]}")  # åªæ˜¾ç¤ºå‰5ä¸ªå€¼
            if len(values) > 5:
                print(f"    ... è¿˜æœ‰ {len(values) - 5} ä¸ªå€¼")
        
        # ç”Ÿæˆæœ€ç»ˆå»ºè®®
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        print("-" * 40)
        for i, rec in enumerate(results['recommendations'], 1):
            print(f"  {i}. {rec}")
            
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        results['error'] = str(e)
    
    return results

def test_image_text_score_range():
    """æµ‹è¯•image_textæœç´¢çš„scoreèŒƒå›´"""
    print("ğŸ” æµ‹è¯•ImageTextæœç´¢çš„ScoreèŒƒå›´")
    print("=" * 80)
    
    try:
        # åŠ è½½é…ç½®
        config = Settings.load_from_file('../config.json')  # ä¿®å¤è·¯å¾„ï¼šä»toolsç›®å½•çœ‹ï¼Œéœ€è¦å›åˆ°ä¸Šçº§ç›®å½•
        
        # åˆ›å»ºImageEngineé…ç½®ï¼ˆä½¿ç”¨ä¸image_engineç›¸åŒçš„é»˜è®¤å€¼ï¼‰
        image_config = ImageEngineConfigV2(
            enabled=True,
            max_results=20,
            image_similarity_threshold=0.05,  # ä½¿ç”¨ç›¸åŒçš„é»˜è®¤å€¼
            enable_vector_search=True,
            enable_keyword_search=True,
            max_recall_results=150,
            use_new_pipeline=False,
            enable_enhanced_reranking=False
        )
        
        print("âœ… ImageEngineé…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"  - å‘é‡æœç´¢é˜ˆå€¼: {image_config.image_similarity_threshold}")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        print("\nğŸ“š æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
        
        # è·å–APIå¯†é’¥
        config_key = config.dashscope_api_key
        api_key = get_dashscope_api_key(config_key)
        
        if not api_key:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return
        
        # åˆå§‹åŒ–embeddings
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        
        # ä¿®å¤è·¯å¾„é—®é¢˜ï¼šä»toolsç›®å½•è¿è¡Œï¼Œéœ€è¦å›åˆ°ä¸Šçº§ç›®å½•
        vector_db_path = "../central/vector_db"
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {os.path.abspath(vector_db_path)}")
        
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        doc_count = len(vector_store.docstore._dict)
        print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {doc_count} ä¸ªæ–‡æ¡£")
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ–‡æ¡£ç±»å‹åˆ†å¸ƒ
        print("\nğŸ“Š æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ–‡æ¡£ç±»å‹åˆ†å¸ƒ...")
        chunk_types = {}
        for doc_id, doc in vector_store.docstore._dict.items():
            if hasattr(doc, 'metadata') and doc.metadata:
                chunk_type = doc.metadata.get('chunk_type', 'unknown')
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
        
        print("æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
        for chunk_type, count in sorted(chunk_types.items()):
            print(f"  - {chunk_type}: {count} ä¸ª")
        
        # åˆ†æFAISSé…ç½®
        faiss_config = analyze_faiss_config(vector_store)
        
        # æµ‹è¯•æŸ¥è¯¢ - å¢åŠ æ›´å¤šæ ·åŒ–çš„æŸ¥è¯¢æ¥æµ‹è¯•ScoreèŒƒå›´
        test_queries = [
            "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ",  # ä½¿ç”¨æ§åˆ¶å°è¾“å‡ºä¸­çš„å®é™…æŸ¥è¯¢
            "ä¸­èŠ¯å›½é™…å‡€åˆ©æ¶¦",
            "å›¾è¡¨æ•°æ®",
            "è´¢åŠ¡åˆ†æ",
            "äº§èƒ½åˆ©ç”¨ç‡",  # æ–°å¢ï¼šæµ‹è¯•æ›´å¤šç›¸å…³æŸ¥è¯¢
            "å­£åº¦æŠ¥å‘Š",
            "æ•°æ®è¶‹åŠ¿",
            "å›¾è¡¨åˆ†æ"
        ]
        
        all_results = []
        
        for test_query in test_queries:
            result = analyze_score_range(vector_store, test_query, 20)
            all_results.append(result)
        
        # ä¿å­˜åˆ†æç»“æœ
        output_file = "image_text_score_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        print(f"\nğŸ“‹ æ€»ç»“æŠ¥å‘Š:")
        print("=" * 60)
        
        total_candidates = sum(len(r['candidates']) for r in all_results)
        successful_searches = sum(1 for r in all_results if r['candidates'])
        
        print(f"  - æ€»æµ‹è¯•æŸ¥è¯¢æ•°: {len(test_queries)}")
        print(f"  - æˆåŠŸæœç´¢æŸ¥è¯¢æ•°: {successful_searches}")
        print(f"  - æ€»å€™é€‰ç»“æœæ•°: {total_candidates}")
        
        if all_results:
            # åˆ†ææ‰€æœ‰score
            all_scores = []
            for result in all_results:
                for candidate in result['candidates']:
                    if candidate['score_value'] is not None:
                        all_scores.append(candidate['score_value'])
            
            if all_scores:
                print(f"  - å…¨å±€ScoreèŒƒå›´: {min(all_scores)} - {max(all_scores)}")
                print(f"  - å…¨å±€å¹³å‡Score: {sum(all_scores) / len(all_scores):.4f}")
                
                # æœ€ç»ˆå»ºè®®
                if min(all_scores) < 0.05:
                    print(f"  - ğŸš¨ å½“å‰é˜ˆå€¼0.05è¿‡ä½ï¼Œå»ºè®®æé«˜åˆ°: {min(all_scores) + 0.1:.2f}")
                else:
                    print(f"  - âœ… å½“å‰é˜ˆå€¼è®¾ç½®åˆç†")
        
        print(f"\nğŸ¯ ä¸»è¦å‘ç°å’Œå»ºè®®:")
        print("-" * 40)
        
        # FAISSä½™å¼¦ç›¸ä¼¼åº¦æ”¯æŒæ€»ç»“
        if faiss_config:
            print("ğŸ”§ ä½™å¼¦ç›¸ä¼¼åº¦æ”¯æŒæ€»ç»“:")
            if 'index_metric_type' in faiss_config:
                metric_type = faiss_config['index_metric_type']
                if metric_type == 2:
                    print("  âœ… å¯ä»¥ç›´æ¥è·å–ä½™å¼¦ç›¸ä¼¼åº¦å€¼")
                    print("  - FAISSé…ç½®ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦åº¦é‡")
                    print("  - åˆ†æ•°èŒƒå›´: [0,1]ï¼Œé˜ˆå€¼è®¾ç½®ç®€å•")
                    print("  - æ— éœ€é¢å¤–æ“ä½œ")
                elif metric_type == 1:
                    print("  âŒ æ— æ³•ç›´æ¥è·å–ä½™å¼¦ç›¸ä¼¼åº¦å€¼")
                    print("  - FAISSé…ç½®ä½¿ç”¨IPå†…ç§¯åº¦é‡")
                    print("  - å½“å‰è§‚å¯Ÿå€¼: 6026.57 (IPå†…ç§¯)")
                    if faiss_config.get('manual_cosine_success'):
                        print("  - ä½†å¯ä»¥æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦")
                        print(f"  - æ‰‹åŠ¨è®¡ç®—ç¤ºä¾‹å€¼: {faiss_config.get('manual_cosine_value', 'N/A')}")
                    print("  - å»ºè®®: é‡å»ºFAISSç´¢å¼•ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦")
                elif metric_type == 0:
                    print("  âŒ æ— æ³•ç›´æ¥è·å–ä½™å¼¦ç›¸ä¼¼åº¦å€¼")
                    print("  - FAISSé…ç½®ä½¿ç”¨L2è·ç¦»åº¦é‡")
                    print("  - å»ºè®®: é‡å»ºFAISSç´¢å¼•ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦")
                else:
                    print(f"  âš ï¸ æ— æ³•ç¡®å®šåº¦é‡ç±»å‹ {metric_type}")
                    print("  - å»ºè®®: éœ€è¦è¿›ä¸€æ­¥åˆ†æFAISSé…ç½®")
            else:
                print("  âš ï¸ æ— æ³•è·å–FAISSåº¦é‡ç±»å‹ä¿¡æ¯")
                print("  - å»ºè®®: æ£€æŸ¥FAISSç´¢å¼•é…ç½®")
        
        # åˆ†æä¸»è¦é—®é¢˜
        if successful_searches == 0:
            print("  âŒ æ‰€æœ‰æŸ¥è¯¢éƒ½å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥:")
            print("    1. å‘é‡æ•°æ®åº“æ˜¯å¦æ­£ç¡®åŠ è½½")
            print("    2. Filteræ¡ä»¶æ˜¯å¦æ­£ç¡®")
            print("    3. æ˜¯å¦æœ‰image_textç±»å‹çš„æ–‡æ¡£")
        elif total_candidates == 0:
            print("  âš ï¸ æœç´¢æˆåŠŸä½†æ²¡æœ‰å€™é€‰ç»“æœï¼Œéœ€è¦æ£€æŸ¥:")
            print("    1. Scoreé˜ˆå€¼æ˜¯å¦è¿‡ä½")
            print("    2. Scoreè®¡ç®—æ˜¯å¦æ­£ç¡®")
        else:
            print("  âœ… æœç´¢åŸºæœ¬æ­£å¸¸ï¼Œä¸»è¦å…³æ³¨ScoreèŒƒå›´ä¼˜åŒ–")
        
        print(f"\nğŸ“– è¯¦ç»†åˆ†æç»“æœè¯·æŸ¥çœ‹: {output_file}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_image_text_score_range()
