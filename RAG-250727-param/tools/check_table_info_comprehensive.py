'''
ç¨‹åºè¯´æ˜ï¼š
## 1. å…¨é¢çš„è¡¨æ ¼ä¿¡æ¯æ£€æŸ¥ç¨‹åº
## 2. åˆ†æå‘é‡æ•°æ®åº“ä¸­çš„è¡¨æ ¼æ–‡æ¡£çŠ¶æ€
## 3. æ£€æŸ¥HTMLå†…å®¹ã€çº¯æ–‡æœ¬å†…å®¹çš„åˆ†å¸ƒ
## 4. æä¾›è¯¦ç»†çš„è¯Šæ–­æŠ¥å‘Š
## 5. å¸®åŠ©ç¡®è®¤ä¸»ç¨‹åºä¿®å¤æ˜¯å¦ç”Ÿæ•ˆ

## ä¸»è¦åŠŸèƒ½ï¼š
- æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­çš„è¡¨æ ¼æ–‡æ¡£æ•°é‡
- åˆ†æHTMLå†…å®¹å’Œçº¯æ–‡æœ¬å†…å®¹çš„åˆ†å¸ƒ
- æ£€æŸ¥å…ƒæ•°æ®å­—æ®µçš„å®Œæ•´æ€§
- æä¾›ä¿®å¤å»ºè®®
'''

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pickle

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class TableInfoChecker:
    """
    è¡¨æ ¼ä¿¡æ¯æ£€æŸ¥å™¨
    """
    
    def __init__(self, vector_db_path: str = None):
        """
        åˆå§‹åŒ–è¡¨æ ¼ä¿¡æ¯æ£€æŸ¥å™¨
        :param vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
        """
        if vector_db_path:
            self.vector_db_path = Path(vector_db_path)
        else:
            # ä½¿ç”¨é»˜è®¤è·¯å¾„
            self.vector_db_path = Path("central/vector_db")
        
        self.metadata_file = self.vector_db_path / "metadata.pkl"
        self.index_file = self.vector_db_path / "index.faiss"
        
    def check_vector_db_structure(self) -> Dict[str, Any]:
        """
        æ£€æŸ¥å‘é‡æ•°æ®åº“ç»“æ„
        :return: æ•°æ®åº“ç»“æ„ä¿¡æ¯
        """
        result = {
            'vector_db_exists': False,
            'metadata_exists': False,
            'index_exists': False,
            'metadata_count': 0,
            'index_count': 0
        }
        
        try:
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
            if self.vector_db_path.exists():
                result['vector_db_exists'] = True
                logger.info(f"âœ… å‘é‡æ•°æ®åº“ç›®å½•å­˜åœ¨: {self.vector_db_path}")
            else:
                logger.error(f"âŒ å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨: {self.vector_db_path}")
                return result
            
            # æ£€æŸ¥metadata.pklæ–‡ä»¶
            if self.metadata_file.exists():
                result['metadata_exists'] = True
                try:
                    with open(self.metadata_file, 'rb') as f:
                        metadata = pickle.load(f)
                    result['metadata_count'] = len(metadata) if metadata else 0
                    logger.info(f"âœ… metadata.pklæ–‡ä»¶å­˜åœ¨ï¼ŒåŒ…å« {result['metadata_count']} ä¸ªæ–‡æ¡£")
                except Exception as e:
                    logger.error(f"âŒ è¯»å–metadata.pklå¤±è´¥: {e}")
            else:
                logger.error(f"âŒ metadata.pklæ–‡ä»¶ä¸å­˜åœ¨: {self.metadata_file}")
            
            # æ£€æŸ¥index.faissæ–‡ä»¶
            if self.index_file.exists():
                result['index_exists'] = True
                try:
                    import faiss
                    index = faiss.read_index(str(self.index_file))
                    result['index_count'] = index.ntotal
                    logger.info(f"âœ… index.faissæ–‡ä»¶å­˜åœ¨ï¼ŒåŒ…å« {result['index_count']} ä¸ªå‘é‡")
                except Exception as e:
                    logger.error(f"âŒ è¯»å–index.faisså¤±è´¥: {e}")
            else:
                logger.error(f"âŒ index.faissæ–‡ä»¶ä¸å­˜åœ¨: {self.index_file}")
            
            return result
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥å‘é‡æ•°æ®åº“ç»“æ„å¤±è´¥: {e}")
            return result
    
    def analyze_table_documents(self) -> Dict[str, Any]:
        """
        åˆ†æè¡¨æ ¼æ–‡æ¡£
        :return: è¡¨æ ¼æ–‡æ¡£åˆ†æç»“æœ
        """
        result = {
            'total_documents': 0,
            'table_documents': 0,
            'text_documents': 0,
            'other_documents': 0,
            'table_details': [],
            'html_content_stats': {
                'with_html': 0,
                'without_html': 0,
                'html_content_lengths': []
            },
            'text_content_stats': {
                'with_text': 0,
                'without_text': 0,
                'text_content_lengths': []
            }
        }
        
        try:
            if not self.metadata_file.exists():
                logger.error("metadata.pklæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ†ææ–‡æ¡£")
                return result
            
            # åŠ è½½å…ƒæ•°æ®
            with open(self.metadata_file, 'rb') as f:
                metadata_list = pickle.load(f)
            
            if not metadata_list:
                logger.warning("metadata.pklæ–‡ä»¶ä¸ºç©º")
                return result
            
            result['total_documents'] = len(metadata_list)
            logger.info(f"å¼€å§‹åˆ†æ {result['total_documents']} ä¸ªæ–‡æ¡£...")
            
            for i, metadata in enumerate(metadata_list):
                try:
                    chunk_type = metadata.get('chunk_type', 'unknown')
                    
                    if chunk_type == 'table':
                        result['table_documents'] += 1
                        table_info = self._analyze_table_document(metadata, i)
                        result['table_details'].append(table_info)
                        
                        # ç»Ÿè®¡HTMLå†…å®¹
                        if table_info['has_html_content']:
                            result['html_content_stats']['with_html'] += 1
                            result['html_content_stats']['html_content_lengths'].append(table_info['html_content_length'])
                        else:
                            result['html_content_stats']['without_html'] += 1
                        
                        # ç»Ÿè®¡æ–‡æœ¬å†…å®¹
                        if table_info['has_text_content']:
                            result['text_content_stats']['with_text'] += 1
                            result['text_content_stats']['text_content_lengths'].append(table_info['text_content_length'])
                        else:
                            result['text_content_stats']['without_text'] += 1
                            
                    elif chunk_type == 'text':
                        result['text_documents'] += 1
                    else:
                        result['other_documents'] += 1
                        
                except Exception as e:
                    logger.warning(f"åˆ†ææ–‡æ¡£ {i} æ—¶å‡ºé”™: {e}")
                    continue
            
            return result
            
        except Exception as e:
            logger.error(f"åˆ†æè¡¨æ ¼æ–‡æ¡£å¤±è´¥: {e}")
            return result
    
    def _analyze_table_document(self, metadata: Dict[str, Any], doc_index: int) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªè¡¨æ ¼æ–‡æ¡£
        :param metadata: æ–‡æ¡£å…ƒæ•°æ®
        :param doc_index: æ–‡æ¡£ç´¢å¼•
        :return: è¡¨æ ¼æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
        """
        table_info = {
            'doc_index': doc_index,
            'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
            'page_number': metadata.get('page_number', 'N/A'),
            'table_id': metadata.get('table_id', 'unknown'),
            'table_type': metadata.get('table_type', 'æœªçŸ¥è¡¨æ ¼'),
            'has_html_content': False,
            'html_content_length': 0,
            'html_content_preview': '',
            'has_text_content': False,
            'text_content_length': 0,
            'text_content_preview': '',
            'metadata_fields': list(metadata.keys()),
            'issues': []
        }
        
        try:
            # æ£€æŸ¥HTMLå†…å®¹
            html_content = metadata.get('page_content', '')
            if html_content and isinstance(html_content, str) and len(html_content.strip()) > 0:
                table_info['has_html_content'] = True
                table_info['html_content_length'] = len(html_content)
                table_info['html_content_preview'] = html_content[:200] + '...' if len(html_content) > 200 else html_content
            else:
                table_info['issues'].append('ç¼ºå°‘HTMLå†…å®¹')
            
            # æ£€æŸ¥æ–‡æœ¬å†…å®¹
            text_content = metadata.get('processed_table_content', '')
            if text_content and isinstance(text_content, str) and len(text_content.strip()) > 0:
                table_info['has_text_content'] = True
                table_info['text_content_length'] = len(text_content)
                table_info['text_content_preview'] = text_content[:200] + '...' if len(text_content) > 200 else text_content
            else:
                table_info['issues'].append('ç¼ºå°‘æ–‡æœ¬å†…å®¹')
            
            # æ£€æŸ¥å…¶ä»–é‡è¦å­—æ®µ
            if not metadata.get('table_id'):
                table_info['issues'].append('ç¼ºå°‘table_id')
            if not metadata.get('table_type'):
                table_info['issues'].append('ç¼ºå°‘table_type')
            
        except Exception as e:
            table_info['issues'].append(f'åˆ†æå‡ºé”™: {e}')
        
        return table_info
    
    def generate_report(self, structure_info: Dict[str, Any], analysis_info: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š
        :param structure_info: æ•°æ®åº“ç»“æ„ä¿¡æ¯
        :param analysis_info: æ–‡æ¡£åˆ†æä¿¡æ¯
        :return: æ ¼å¼åŒ–çš„æŠ¥å‘Š
        """
        report = []
        report.append("=" * 80)
        report.append("ğŸ“Š è¡¨æ ¼ä¿¡æ¯æ£€æŸ¥æŠ¥å‘Š")
        report.append("=" * 80)
        
        # æ•°æ®åº“ç»“æ„ä¿¡æ¯
        report.append("\nğŸ” æ•°æ®åº“ç»“æ„æ£€æŸ¥:")
        report.append(f"  âœ… å‘é‡æ•°æ®åº“ç›®å½•: {'å­˜åœ¨' if structure_info['vector_db_exists'] else 'ä¸å­˜åœ¨'}")
        report.append(f"  âœ… metadata.pklæ–‡ä»¶: {'å­˜åœ¨' if structure_info['metadata_exists'] else 'ä¸å­˜åœ¨'}")
        report.append(f"  âœ… index.faissæ–‡ä»¶: {'å­˜åœ¨' if structure_info['index_exists'] else 'ä¸å­˜åœ¨'}")
        report.append(f"  ğŸ“Š å…ƒæ•°æ®æ–‡æ¡£æ•°é‡: {structure_info['metadata_count']}")
        report.append(f"  ğŸ“Š å‘é‡ç´¢å¼•æ•°é‡: {structure_info['index_count']}")
        
        # æ–‡æ¡£ç±»å‹ç»Ÿè®¡
        report.append("\nğŸ“‹ æ–‡æ¡£ç±»å‹ç»Ÿè®¡:")
        report.append(f"  ğŸ“Š æ€»æ–‡æ¡£æ•°é‡: {analysis_info['total_documents']}")
        report.append(f"  ğŸ“‹ è¡¨æ ¼æ–‡æ¡£æ•°é‡: {analysis_info['table_documents']}")
        report.append(f"  ğŸ“ æ–‡æœ¬æ–‡æ¡£æ•°é‡: {analysis_info['text_documents']}")
        report.append(f"  ğŸ” å…¶ä»–æ–‡æ¡£æ•°é‡: {analysis_info['other_documents']}")
        
        # HTMLå†…å®¹ç»Ÿè®¡
        report.append("\nğŸŒ HTMLå†…å®¹ç»Ÿè®¡:")
        report.append(f"  âœ… æœ‰HTMLå†…å®¹çš„è¡¨æ ¼: {analysis_info['html_content_stats']['with_html']}")
        report.append(f"  âŒ æ— HTMLå†…å®¹çš„è¡¨æ ¼: {analysis_info['html_content_stats']['without_html']}")
        
        if analysis_info['html_content_stats']['html_content_lengths']:
            avg_html_length = sum(analysis_info['html_content_stats']['html_content_lengths']) / len(analysis_info['html_content_stats']['html_content_lengths'])
            report.append(f"  ğŸ“ å¹³å‡HTMLå†…å®¹é•¿åº¦: {avg_html_length:.0f} å­—ç¬¦")
        
        # æ–‡æœ¬å†…å®¹ç»Ÿè®¡
        report.append("\nğŸ“ æ–‡æœ¬å†…å®¹ç»Ÿè®¡:")
        report.append(f"  âœ… æœ‰æ–‡æœ¬å†…å®¹çš„è¡¨æ ¼: {analysis_info['text_content_stats']['with_text']}")
        report.append(f"  âŒ æ— æ–‡æœ¬å†…å®¹çš„è¡¨æ ¼: {analysis_info['text_content_stats']['without_text']}")
        
        if analysis_info['text_content_stats']['text_content_lengths']:
            avg_text_length = sum(analysis_info['text_content_stats']['text_content_lengths']) / len(analysis_info['text_content_stats']['text_content_lengths'])
            report.append(f"  ğŸ“ å¹³å‡æ–‡æœ¬å†…å®¹é•¿åº¦: {avg_text_length:.0f} å­—ç¬¦")
        
        # é—®é¢˜åˆ†æ
        if analysis_info['table_details']:
            report.append("\nâš ï¸ é—®é¢˜åˆ†æ:")
            issues_count = {}
            for table_info in analysis_info['table_details']:
                for issue in table_info['issues']:
                    issues_count[issue] = issues_count.get(issue, 0) + 1
            
            for issue, count in issues_count.items():
                report.append(f"  âŒ {issue}: {count} ä¸ªè¡¨æ ¼")
        
        # ä¿®å¤å»ºè®®
        report.append("\nğŸ’¡ ä¿®å¤å»ºè®®:")
        if analysis_info['html_content_stats']['without_html'] > 0:
            report.append("  1. ğŸ”§ ä¸»ç¨‹åºå·²ä¿®å¤ï¼Œä½†ç°æœ‰æ•°æ®åº“ä¸­çš„è¡¨æ ¼æ–‡æ¡£ä»ç¼ºå°‘HTMLå†…å®¹")
            report.append("  2. ğŸš€ å»ºè®®é‡æ–°ç”Ÿæˆå‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨ä¿®å¤åçš„ä¸»ç¨‹åº")
            report.append("  3. ğŸ“ å‘½ä»¤: python V501_simplified_document_processor.py --mode markdown")
        else:
            report.append("  âœ… æ‰€æœ‰è¡¨æ ¼æ–‡æ¡£éƒ½æœ‰HTMLå†…å®¹ï¼Œæ— éœ€ä¿®å¤")
        
        report.append("\n" + "=" * 80)
        return "\n".join(report)
    
    def save_detailed_analysis(self, analysis_info: Dict[str, Any], output_file: str = "table_analysis_detailed.json"):
        """
        ä¿å­˜è¯¦ç»†åˆ†æç»“æœåˆ°JSONæ–‡ä»¶
        :param analysis_info: åˆ†æä¿¡æ¯
        :param output_file: è¾“å‡ºæ–‡ä»¶å
        """
        try:
            # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®ï¼ˆç§»é™¤è¿‡é•¿çš„å†…å®¹é¢„è§ˆï¼‰
            save_data = analysis_info.copy()
            for table_detail in save_data['table_details']:
                if len(table_detail['html_content_preview']) > 100:
                    table_detail['html_content_preview'] = table_detail['html_content_preview'][:100] + '...'
                if len(table_detail['text_content_preview']) > 100:
                    table_detail['text_content_preview'] = table_detail['text_content_preview'][:100] + '...'
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¯¦ç»†åˆ†æç»“æœå¤±è´¥: {e}")


def main():
    """
    ä¸»å‡½æ•°
    """
    import argparse
    
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='è¡¨æ ¼ä¿¡æ¯æ£€æŸ¥ç¨‹åº')
    parser.add_argument('--vector-db', type=str, help='å‘é‡æ•°æ®åº“è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    args = parser.parse_args()
    
    print("ğŸ” å¼€å§‹æ£€æŸ¥è¡¨æ ¼ä¿¡æ¯...")
    
    # åˆ›å»ºæ£€æŸ¥å™¨
    checker = TableInfoChecker(args.vector_db)
    
    # æ£€æŸ¥æ•°æ®åº“ç»“æ„
    print("\n1ï¸âƒ£ æ£€æŸ¥å‘é‡æ•°æ®åº“ç»“æ„...")
    structure_info = checker.check_vector_db_structure()
    
    if not structure_info['vector_db_exists']:
        print("âŒ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºå‘é‡æ•°æ®åº“")
        return
    
    # åˆ†æè¡¨æ ¼æ–‡æ¡£
    print("\n2ï¸âƒ£ åˆ†æè¡¨æ ¼æ–‡æ¡£...")
    analysis_info = checker.analyze_table_documents()
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n3ï¸âƒ£ ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š...")
    report = checker.generate_report(structure_info, analysis_info)
    print(report)
    
    # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
    if analysis_info['table_documents'] > 0:
        print("\n4ï¸âƒ£ ä¿å­˜è¯¦ç»†åˆ†æç»“æœ...")
        checker.save_detailed_analysis(analysis_info)
    
    print("\nâœ… è¡¨æ ¼ä¿¡æ¯æ£€æŸ¥å®Œæˆï¼")


if __name__ == "__main__":
    main()
