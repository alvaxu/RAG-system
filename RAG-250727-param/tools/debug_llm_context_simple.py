'''
ç¨‹åºè¯´æ˜ï¼š

## 1. ç®€åŒ–ç‰ˆLLMä¸Šä¸‹æ–‡è°ƒè¯•
## 2. é‡ç‚¹æ£€æŸ¥å‘é‡æœç´¢å’Œæ–‡æ¡£å†…å®¹
## 3. åˆ†æä¸ºä»€ä¹ˆLLMè¯´æ²¡æœ‰å›¾4ä¿¡æ¯
'''

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.api_key_manager import get_dashscope_api_key
from v2.core.image_engine import ImageEngine
from v2.core.document_loader import DocumentLoader
from v2.config.v2_config import ImageEngineConfigV2
import time


def debug_llm_context_simple():
    """ç®€åŒ–ç‰ˆLLMä¸Šä¸‹æ–‡è°ƒè¯•"""
    print("ğŸ” ç®€åŒ–ç‰ˆLLMä¸Šä¸‹æ–‡è°ƒè¯•")
    print("=" * 80)
    
    try:
        # 1. åˆå§‹åŒ–ç»„ä»¶
        print("ğŸ“¡ åˆå§‹åŒ–ç»„ä»¶...")
        config = ImageEngineConfigV2()
        
        api_key = get_dashscope_api_key()
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        vector_db_path = "../central/vector_db"
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        document_loader = DocumentLoader(vector_store=vector_store)
        image_engine = ImageEngine(
            config=config,
            vector_store=vector_store,
            document_loader=document_loader
        )
        print("âœ… ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. æµ‹è¯•æŸ¥è¯¢
        query = "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ"
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        print("-" * 60)
        
        # 3. æ£€æŸ¥å‘é‡æœç´¢ç»“æœ
        print("\nğŸ“Š æ£€æŸ¥å‘é‡æœç´¢è¯¦ç»†ç»“æœ")
        vector_results = image_engine._vector_search(query, max_results=50)
        print(f"  å‘é‡æœç´¢ç»“æœæ•°é‡: {len(vector_results)}")
        
        if vector_results:
            for i, result in enumerate(vector_results):
                print(f"\n  ç»“æœ {i+1}:")
                print(f"    åˆ†æ•°: {result.get('score', 'N/A'):.4f}")
                print(f"    æœç´¢æ–¹æ³•: {result.get('search_method', 'N/A')}")
                print(f"    æ¥æº: {result.get('source', 'N/A')}")
                print(f"    å±‚çº§: {result.get('layer', 'N/A')}")
                
                # æ£€æŸ¥æ–‡æ¡£ç±»å‹
                if hasattr(result['doc'], 'metadata') and result['doc'].metadata:
                    chunk_type = result['doc'].metadata.get('chunk_type', 'unknown')
                    document_name = result['doc'].metadata.get('document_name', 'unknown')
                    page_number = result['doc'].metadata.get('page_number', 'unknown')
                    print(f"    æ–‡æ¡£ç±»å‹: {chunk_type}")
                    print(f"    æ–‡æ¡£åç§°: {document_name}")
                    print(f"    é¡µç : {page_number}")
                
                # æ£€æŸ¥æ–‡æ¡£å†…å®¹
                content = result['doc'].page_content
                print(f"    å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                
                # é‡ç‚¹æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾4å®Œæ•´ä¿¡æ¯
                if 'å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ' in content:
                    print("    âœ… åŒ…å«å®Œæ•´çš„å›¾4æ ‡é¢˜!")
                elif 'å›¾4' in content and 'ä¸­èŠ¯å›½é™…' in content and 'å‡€åˆ©æ¶¦' in content:
                    print("    âš ï¸  åŒ…å«å›¾4ç›¸å…³ä¿¡æ¯ï¼Œä½†å¯èƒ½ä¸å®Œæ•´")
                else:
                    print("    âŒ ä¸åŒ…å«å›¾4ç›¸å…³ä¿¡æ¯")
                
                # æ˜¾ç¤ºå†…å®¹å‰300å­—ç¬¦
                print(f"    å†…å®¹é¢„è§ˆ: {content[:300]}...")
                
                # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªç»“æœï¼ˆæœ€ç›¸å…³çš„ï¼‰ï¼Œæ˜¾ç¤ºå®Œæ•´å†…å®¹
                if i == 0:
                    print(f"\n    â­ æœ€ç›¸å…³ç»“æœçš„å®Œæ•´å†…å®¹:")
                    print(f"    {content}")
        
        # 4. æ£€æŸ¥æ˜¯å¦æœ‰enhanced_description
        print(f"\nğŸ“Š æ£€æŸ¥enhanced_descriptionå­—æ®µ")
        for i, result in enumerate(vector_results[:3]):
            if hasattr(result['doc'], 'metadata') and result['doc'].metadata:
                enhanced_desc = result['doc'].metadata.get('enhanced_description', '')
                if enhanced_desc:
                    print(f"\n  ç»“æœ {i+1} çš„enhanced_description:")
                    print(f"    é•¿åº¦: {len(enhanced_desc)} å­—ç¬¦")
                    print(f"    å†…å®¹: {enhanced_desc[:500]}...")
                else:
                    print(f"\n  ç»“æœ {i+1} æ²¡æœ‰enhanced_descriptionå­—æ®µ")
        
        # 5. æ¨¡æ‹Ÿæ„å»ºLLMä¸Šä¸‹æ–‡
        print(f"\nğŸ“Š æ¨¡æ‹Ÿæ„å»ºLLMä¸Šä¸‹æ–‡")
        if vector_results:
            # å–å‰5ä¸ªç»“æœæ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, result in enumerate(vector_results[:5]):
                doc_content = result['doc'].page_content
                
                # æ£€æŸ¥æ˜¯å¦æœ‰enhanced_description
                enhanced_desc = ""
                if hasattr(result['doc'], 'metadata') and result['doc'].metadata:
                    enhanced_desc = result['doc'].metadata.get('enhanced_description', '')
                
                # æ„å»ºå®Œæ•´çš„æ–‡æ¡£å†…å®¹
                full_content = doc_content
                if enhanced_desc and enhanced_desc not in doc_content:
                    full_content = f"{doc_content}\n\nå¢å¼ºæè¿°: {enhanced_desc}"
                
                context_parts.append(f"ç›¸å…³æ–‡æ¡£{i+1}:\n{full_content}")
            
            full_context = "\n\n" + "="*50 + "\n\n".join(context_parts)
            
            print(f"  æ„å»ºçš„å®Œæ•´ä¸Šä¸‹æ–‡é•¿åº¦: {len(full_context)} å­—ç¬¦")
            print(f"  ä¸Šä¸‹æ–‡åŒ…å«å›¾4æ ‡é¢˜: {'å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ' in full_context}")
            print(f"  ä¸Šä¸‹æ–‡åŒ…å«å›¾4å…³é”®è¯: {'å›¾4' in full_context and 'ä¸­èŠ¯å›½é™…' in full_context and 'å‡€åˆ©æ¶¦' in full_context}")
            
            print(f"\n  å®Œæ•´ä¸Šä¸‹æ–‡å†…å®¹:")
            print(f"  {full_context}")
        
        # 6. åˆ†æé—®é¢˜
        print("\n" + "=" * 80)
        print("ğŸ¯ é—®é¢˜åˆ†æ")
        print("=" * 80)
        
        if vector_results:
            first_result = vector_results[0]
            content = first_result['doc'].page_content
            
            print(f"\næœ€ç›¸å…³æ–‡æ¡£åˆ†æ:")
            print(f"- æ–‡æ¡£ç±»å‹: {first_result['doc'].metadata.get('chunk_type', 'unknown')}")
            print(f"- åˆ†æ•°: {first_result.get('score', 'N/A')}")
            print(f"- å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"- åŒ…å«å›¾4æ ‡é¢˜: {'å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ' in content}")
            
            if 'å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ' in content:
                print(f"âœ… ç»“è®º: å‘é‡æœç´¢æˆåŠŸæ‰¾åˆ°äº†å›¾4ç›¸å…³æ–‡æ¡£")
                print(f"   é—®é¢˜å¯èƒ½å‡ºç°åœ¨åç»­çš„æºè¿‡æ»¤æˆ–LLMå¤„ç†ç¯èŠ‚")
            else:
                print(f"âŒ ç»“è®º: å‘é‡æœç´¢æ²¡æœ‰æ‰¾åˆ°åŒ…å«å®Œæ•´å›¾4ä¿¡æ¯çš„æ–‡æ¡£")
                print(f"   éœ€è¦æ£€æŸ¥æ–‡æ¡£ç´¢å¼•æˆ–å‘é‡åŒ–è¿‡ç¨‹")
        else:
            print(f"âŒ ç»“è®º: å‘é‡æœç´¢æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£")
            print(f"   éœ€è¦æ£€æŸ¥åè¿‡æ»¤å®ç°æˆ–æ–‡æ¡£ç´¢å¼•")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


if __name__ == "__main__":
    debug_llm_context_simple()
