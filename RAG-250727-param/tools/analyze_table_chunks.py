#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
ç¨‹åºè¯´æ˜ï¼š
## 1. ä¸“é—¨åˆ†æå‘é‡æ•°æ®åº“ä¸­çš„table chunkæ–‡æ¡£
## 2. åŸºäºç°æœ‰åˆ†æå·¥å…·ï¼Œä¸“é—¨æå–è¡¨æ ¼æ–‡æ¡£çš„å…ƒæ•°æ®å’Œå†…å®¹
## 3. åˆ†æè¡¨æ ¼æ–‡æ¡£çš„ç»“æ„å’Œå†…å®¹ç‰¹å¾
## 4. å¸®åŠ©ç†è§£ä¸ºä»€ä¹ˆè¯„åˆ†ç®—æ³•ç»™æ‰€æœ‰æ–‡æ¡£çš„åˆ†æ•°éƒ½è¿™ä¹ˆä½
'''

import sys
import os
import pickle
import json
from pathlib import Path
from collections import defaultdict

def analyze_table_chunks():
    """åˆ†æå‘é‡æ•°æ®åº“ä¸­çš„è¡¨æ ¼æ–‡æ¡£chunks"""
    print("=" * 60)
    print("åˆ†æå‘é‡æ•°æ®åº“ä¸­çš„è¡¨æ ¼æ–‡æ¡£chunks")
    print("=" * 60)
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ–‡ä»¶
    vector_db_dir = Path("central/vector_db")
    index_file = vector_db_dir / "index.pkl"
    metadata_file = vector_db_dir / "metadata.pkl"
    
    if not index_file.exists():
        print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}")
        return
    
    try:
        # æ–¹æ³•1ï¼šä»index.pklè¯»å–
        print("ğŸ” æ–¹æ³•1ï¼šä»index.pklè¯»å–è¡¨æ ¼æ–‡æ¡£...")
        table_docs_from_index = extract_table_docs_from_index(index_file)
        
        # æ–¹æ³•2ï¼šä»metadata.pklè¯»å–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        table_docs_from_metadata = []
        if metadata_file.exists():
            print("\nğŸ” æ–¹æ³•2ï¼šä»metadata.pklè¯»å–è¡¨æ ¼æ–‡æ¡£...")
            table_docs_from_metadata = extract_table_docs_from_metadata(metadata_file)
        
        # åˆå¹¶ç»“æœ
        all_table_docs = table_docs_from_index + table_docs_from_metadata
        
        if not all_table_docs:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•è¡¨æ ¼æ–‡æ¡£")
            return
        
        print(f"\nğŸ“Š æ‰¾åˆ° {len(all_table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
        
        # åˆ†æè¡¨æ ¼æ–‡æ¡£çš„è¯¦ç»†å†…å®¹
        analyze_table_documents(all_table_docs)
        
        # ä¿å­˜åˆ†æç»“æœ
        save_analysis_results(all_table_docs)
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

def extract_table_docs_from_index(index_file):
    """ä»index.pklä¸­æå–è¡¨æ ¼æ–‡æ¡£"""
    table_docs = []
    
    try:
        with open(index_file, 'rb') as f:
            index_data = pickle.load(f)
        
        print(f"  ç´¢å¼•æ–‡ä»¶ç±»å‹: {type(index_data)}")
        print(f"  ç´¢å¼•æ–‡ä»¶é•¿åº¦: {len(index_data)}")
        
        # æ£€æŸ¥ç¬¬1ä¸ªå…ƒç´ ï¼ˆInMemoryDocstoreï¼‰
        if len(index_data) >= 1:
            docstore = index_data[0]
            print(f"  ç¬¬1ä¸ªå…ƒç´ ç±»å‹: {type(docstore)}")
            
            if hasattr(docstore, '_dict'):
                docstore_dict = docstore._dict
                print(f"  docstore._dicté•¿åº¦: {len(docstore_dict)}")
                
                # ç­›é€‰è¡¨æ ¼æ–‡æ¡£
                for doc_id, doc in docstore_dict.items():
                    if hasattr(doc, 'metadata') and doc.metadata:
                        chunk_type = doc.metadata.get('chunk_type', '')
                        if chunk_type == 'table':
                            table_docs.append({
                                'source': 'index_pkl_docstore',
                                'doc_id': doc_id,
                                'doc': doc,
                                'metadata': doc.metadata,
                                'page_content': getattr(doc, 'page_content', '')
                            })
        
        # æ£€æŸ¥ç¬¬2ä¸ªå…ƒç´ ï¼ˆæ–‡æ¡£å­—å…¸ï¼‰
        if len(index_data) >= 2 and isinstance(index_data[1], dict):
            metadata_dict = index_data[1]
            print(f"  ç¬¬2ä¸ªå…ƒç´ ç±»å‹: {type(metadata_dict)}")
            print(f"  ç¬¬2ä¸ªå…ƒç´ é•¿åº¦: {len(metadata_dict)}")
            
            # ç­›é€‰è¡¨æ ¼æ–‡æ¡£
            for doc_id, doc in metadata_dict.items():
                if isinstance(doc, dict) and doc.get('chunk_type') == 'table':
                    table_docs.append({
                        'source': 'index_pkl_metadata',
                        'doc_id': doc_id,
                        'doc': doc,
                        'metadata': doc,
                        'page_content': doc.get('page_content', '')
                    })
        
        print(f"  ä»index.pklæ‰¾åˆ° {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
        
    except Exception as e:
        print(f"  ä»index.pklè¯»å–å¤±è´¥: {e}")
    
    return table_docs

def extract_table_docs_from_metadata(metadata_file):
    """ä»metadata.pklä¸­æå–è¡¨æ ¼æ–‡æ¡£"""
    table_docs = []
    
    try:
        with open(metadata_file, 'rb') as f:
            metadata = pickle.load(f)
        
        print(f"  metadata.pklç±»å‹: {type(metadata)}")
        print(f"  metadata.pklé•¿åº¦: {len(metadata)}")
        
        # ç­›é€‰è¡¨æ ¼æ–‡æ¡£
        for i, item in enumerate(metadata):
            if isinstance(item, dict) and item.get('chunk_type') == 'table':
                table_docs.append({
                    'source': 'metadata_pkl',
                    'doc_id': i,
                    'doc': item,
                    'metadata': item,
                    'page_content': item.get('page_content', '')
                })
        
        print(f"  ä»metadata.pklæ‰¾åˆ° {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
        
    except Exception as e:
        print(f"  ä»metadata.pklè¯»å–å¤±è´¥: {e}")
    
    return table_docs

def analyze_table_documents(table_docs):
    """åˆ†æè¡¨æ ¼æ–‡æ¡£çš„è¯¦ç»†å†…å®¹"""
    print(f"\nğŸ” åˆ†æ {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£çš„è¯¦ç»†å†…å®¹...")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_docs': len(table_docs),
        'metadata_fields': set(),
        'table_types': defaultdict(int),
        'document_names': set(),
        'content_lengths': [],
        'has_columns': 0,
        'has_table_type': 0,
        'has_financial_keywords': 0,
        'has_time_keywords': 0,
        'has_smic_keywords': 0
    }
    
    # åˆ†æå‰10ä¸ªæ–‡æ¡£çš„è¯¦ç»†å†…å®¹
    for i, doc_info in enumerate(table_docs[:10]):
        print(f"\n{'='*50}")
        print(f"ğŸ“„ è¡¨æ ¼æ–‡æ¡£ {i+1}")
        print(f"{'='*50}")
        
        doc = doc_info['doc']
        metadata = doc_info['metadata']
        content = doc_info['page_content']
        source = doc_info['source']
        
        print(f"æ¥æº: {source}")
        print(f"æ–‡æ¡£ID: {doc_info['doc_id']}")
        
        # åˆ†æå…ƒæ•°æ®
        print(f"\nğŸ“‹ å…ƒæ•°æ®åˆ†æ:")
        print(f"  å…ƒæ•°æ®å­—æ®µ: {list(metadata.keys())}")
        stats['metadata_fields'].update(metadata.keys())
        
        # æ£€æŸ¥å…³é”®å­—æ®µ
        key_fields = [
            'chunk_type', 'document_name', 'page_number', 'chunk_index',
            'table_id', 'table_type', 'columns', 'table_row_count', 'table_column_count',
            'source', 'title'
        ]
        
        for field in key_fields:
            value = metadata.get(field, 'NOT_FOUND')
            print(f"  {field}: {value}")
            
            # ç»Ÿè®¡ç‰¹å®šå­—æ®µ
            if field == 'table_type' and value != 'NOT_FOUND':
                stats['table_types'][value] += 1
                stats['has_table_type'] += 1
            elif field == 'document_name' and value != 'NOT_FOUND':
                stats['document_names'].add(value)
            elif field == 'columns' and value != 'NOT_FOUND':
                stats['has_columns'] += 1
        
        # åˆ†æå†…å®¹
        if content:
            print(f"\nğŸ“ å†…å®¹åˆ†æ:")
            print(f"  å†…å®¹é•¿åº¦: {len(content)}")
            stats['content_lengths'].append(len(content))
            
            # æ˜¾ç¤ºå‰300å­—ç¬¦
            content_preview = content[:300] + "..." if len(content) > 300 else content
            print(f"  å†…å®¹é¢„è§ˆ: {content_preview}")
            
            # åˆ†æå†…å®¹ç‰¹å¾
            analyze_content_features(content, stats)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šå…³é”®è¯
            check_keywords_in_content(content, stats)
        else:
            print(f"\nâŒ æ²¡æœ‰é¡µé¢å†…å®¹")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    display_statistics(stats)
    
    # åˆ†æå‰©ä½™æ–‡æ¡£çš„å…ƒæ•°æ®
    if len(table_docs) > 10:
        print(f"\nğŸ” åˆ†æå‰©ä½™ {len(table_docs) - 10} ä¸ªæ–‡æ¡£çš„å…ƒæ•°æ®...")
        for i, doc_info in enumerate(table_docs[10:]):
            metadata = doc_info['metadata']
            stats['metadata_fields'].update(metadata.keys())
            
            # ç»Ÿè®¡ç‰¹å®šå­—æ®µ
            if metadata.get('table_type'):
                stats['table_types'][metadata['table_type']] += 1
                stats['has_table_type'] += 1
            if metadata.get('document_name'):
                stats['document_names'].add(metadata['document_name'])
            if metadata.get('columns'):
                stats['has_columns'] += 1
        
        print(f"  å®Œæˆå‰©ä½™æ–‡æ¡£åˆ†æ")

def analyze_content_features(content, stats):
    """åˆ†æå†…å®¹ç‰¹å¾"""
    lines = content.split('\n')
    print(f"  æ€»è¡Œæ•°: {len(lines)}")
    
    # æ˜¾ç¤ºå‰5è¡Œ
    print(f"  å‰5è¡Œå†…å®¹:")
    for j, line in enumerate(lines[:5]):
        print(f"    {j+1:2d}: {line[:100]}")
    
    # æ£€æŸ¥è¡¨æ ¼ç‰¹å¾
    table_indicators = ['|', '\t', 'è¡¨æ ¼', 'è¡¨', 'è¡Œ', 'åˆ—', 'æ•°æ®', 'ç»Ÿè®¡']
    found_indicators = []
    for indicator in table_indicators:
        if indicator in content:
            found_indicators.append(indicator)
    
    print(f"  è¡¨æ ¼ç‰¹å¾: {found_indicators if found_indicators else 'æ— '}")

def check_keywords_in_content(content, stats):
    """æ£€æŸ¥å†…å®¹ä¸­çš„å…³é”®è¯"""
    # è´¢åŠ¡å…³é”®è¯
    financial_keywords = ['æ”¶å…¥', 'æ”¯å‡º', 'åˆ©æ¶¦', 'æˆæœ¬', 'è´¹ç”¨', 'è¥æ”¶', 'æ”¶ç›Š', 'é‡‘é¢', 'æ€»é¢']
    found_financial = [kw for kw in financial_keywords if kw in content]
    if found_financial:
        stats['has_financial_keywords'] += 1
        print(f"  è´¢åŠ¡å…³é”®è¯: {found_financial}")
    
    # æ—¶é—´å…³é”®è¯
    time_keywords = ['2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', 'å¹´', 'æœˆ']
    found_time = [kw for kw in time_keywords if kw in content]
    if found_time:
        stats['has_time_keywords'] += 1
        print(f"  æ—¶é—´å…³é”®è¯: {found_time}")
    
    # ä¸­èŠ¯å›½é™…å…³é”®è¯
    smic_keywords = ['ä¸­èŠ¯å›½é™…', 'SMIC', 'ä¸­èŠ¯', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“']
    found_smic = [kw for kw in smic_keywords if kw in content]
    if found_smic:
        stats['has_smic_keywords'] += 1
        print(f"  ä¸­èŠ¯å›½é™…å…³é”®è¯: {found_smic}")

def display_statistics(stats):
    """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š è¡¨æ ¼æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)
    
    print(f"æ€»æ–‡æ¡£æ•°: {stats['total_docs']}")
    print(f"æœ‰è¡¨æ ¼ç±»å‹çš„æ–‡æ¡£: {stats['has_table_type']}")
    print(f"æœ‰åˆ—ä¿¡æ¯çš„æ–‡æ¡£: {stats['has_columns']}")
    print(f"æœ‰è´¢åŠ¡å…³é”®è¯çš„æ–‡æ¡£: {stats['has_financial_keywords']}")
    print(f"æœ‰æ—¶é—´å…³é”®è¯çš„æ–‡æ¡£: {stats['has_time_keywords']}")
    print(f"æœ‰ä¸­èŠ¯å›½é™…å…³é”®è¯çš„æ–‡æ¡£: {stats['has_smic_keywords']}")
    
    print(f"\nå…ƒæ•°æ®å­—æ®µæ€»æ•°: {len(stats['metadata_fields'])}")
    print(f"å…ƒæ•°æ®å­—æ®µ: {sorted(list(stats['metadata_fields']))}")
    
    if stats['table_types']:
        print(f"\nè¡¨æ ¼ç±»å‹åˆ†å¸ƒ:")
        for table_type, count in sorted(stats['table_types'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {table_type}: {count}")
    
    if stats['document_names']:
        print(f"\næ–‡æ¡£åç§° (å…±{len(stats['document_names'])}ä¸ª):")
        for doc_name in sorted(list(stats['document_names']))[:10]:
            print(f"  {doc_name}")
        if len(stats['document_names']) > 10:
            print(f"  ... è¿˜æœ‰ {len(stats['document_names']) - 10} ä¸ª")
    
    if stats['content_lengths']:
        avg_length = sum(stats['content_lengths']) / len(stats['content_lengths'])
        print(f"\nå†…å®¹é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {avg_length:.1f}")
        print(f"  æœ€çŸ­é•¿åº¦: {min(stats['content_lengths'])}")
        print(f"  æœ€é•¿é•¿åº¦: {max(stats['content_lengths'])}")

def save_analysis_results(table_docs):
    """ä¿å­˜åˆ†æç»“æœ"""
    try:
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„æ•°æ®
        results = {
            'total_table_docs': len(table_docs),
            'analysis_timestamp': str(Path().cwd()),
            'sample_docs': []
        }
        
        # ä¿å­˜å‰5ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        for i, doc_info in enumerate(table_docs[:5]):
            sample_doc = {
                'index': i,
                'source': doc_info['source'],
                'doc_id': str(doc_info['doc_id']),
                'metadata': doc_info['metadata'],
                'content_preview': doc_info['page_content'][:500] if doc_info['page_content'] else ''
            }
            results['sample_docs'].append(sample_doc)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = "table_chunks_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åˆ†æå‘é‡æ•°æ®åº“ä¸­çš„è¡¨æ ¼æ–‡æ¡£chunks")
    
    analyze_table_chunks()
    
    print("\n" + "=" * 60)
    print("åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
