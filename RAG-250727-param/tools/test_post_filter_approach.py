'''
ç¨‹åºè¯´æ˜ï¼š

## 1. æµ‹è¯•åè¿‡æ»¤æ–¹æ¡ˆ
## 2. éªŒè¯å…ˆæœç´¢åè¿‡æ»¤çš„æ•ˆæœ
## 3. å¯¹æ¯”ä¸åŒé˜ˆå€¼è®¾ç½®çš„å½±å“
'''

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.api_key_manager import get_dashscope_api_key
import time


def calculate_content_relevance(query: str, content: str) -> float:
    """
    è®¡ç®—æŸ¥è¯¢ä¸å†…å®¹çš„ç›¸å…³æ€§åˆ†æ•°
    
    :param query: æŸ¥è¯¢æ–‡æœ¬
    :param content: æ–‡æ¡£å†…å®¹
    :return: ç›¸å…³æ€§åˆ†æ•° (0.0-1.0)
    """
    try:
        query_lower = query.lower()
        content_lower = content.lower()
        
        # å®Œå…¨åŒ¹é…
        if query_lower in content_lower:
            return 0.8
        
        # åˆ†è¯åŒ¹é…
        query_words = [word for word in query_lower.split() if len(word) > 1]
        if not query_words:
            return 0.0
        
        content_words = content_lower.split()
        matched_words = 0
        total_score = 0.0
        
        for query_word in query_words:
            if query_word in content_words:
                matched_words += 1
                word_count = content_lower.count(query_word)
                word_score = min(word_count / len(content_words), 0.3)
                total_score += word_score
        
        match_rate = matched_words / len(query_words) if query_words else 0
        final_score = (match_rate * 0.7 + total_score * 0.3)
        return min(final_score, 1.0)
        
    except Exception as e:
        print(f"è®¡ç®—å†…å®¹ç›¸å…³æ€§å¤±è´¥: {e}")
        return 0.0


def test_post_filter_approach():
    """æµ‹è¯•åè¿‡æ»¤æ–¹æ¡ˆ"""
    print("ğŸ§ª æµ‹è¯•åè¿‡æ»¤æ–¹æ¡ˆ")
    print("=" * 80)
    
    try:
        # 1. åˆå§‹åŒ–
        print("ğŸ“¡ åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
        api_key = get_dashscope_api_key()
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        vector_db_path = "../central/vector_db"
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        
        # 2. æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ",
            "ä¸­èŠ¯å›½é™…",
            "èŠ¯ç‰‡åˆ¶é€ ",
            "æ™¶åœ†ä»£å·¥"
        ]
        
        # 3. æµ‹è¯•ä¸åŒçš„æœç´¢ç­–ç•¥
        for query in test_queries:
            print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
            print("-" * 60)
            
            # ç­–ç•¥1: ä½¿ç”¨filter (åº”è¯¥å¤±è´¥)
            print("ğŸ“‹ ç­–ç•¥1: ä½¿ç”¨filter (é¢„æœŸå¤±è´¥)")
            try:
                start_time = time.time()
                results_with_filter = vector_store.similarity_search(
                    query, k=20, filter={'chunk_type': 'image_text'}
                )
                filter_time = time.time() - start_time
                print(f"  ç»“æœæ•°é‡: {len(results_with_filter)}")
                print(f"  è€—æ—¶: {filter_time:.3f}ç§’")
            except Exception as e:
                print(f"  å¤±è´¥: {e}")
            
            # ç­–ç•¥2: åè¿‡æ»¤æ–¹æ¡ˆ
            print("\nğŸ“‹ ç­–ç•¥2: åè¿‡æ»¤æ–¹æ¡ˆ")
            try:
                start_time = time.time()
                # å…ˆè¿›è¡Œæ— filterçš„å‘é‡æœç´¢ï¼Œå¢åŠ kå€¼ä»¥æ‰¾åˆ°æ›´å¤šæ–‡æ¡£
                all_results = vector_store.similarity_search(query, k=200)
                search_time = time.time() - start_time
                
                # åè¿‡æ»¤
                filter_start = time.time()
                image_text_candidates = []
                other_candidates = []
                
                for doc in all_results:
                    if hasattr(doc, 'metadata') and doc.metadata:
                        chunk_type = doc.metadata.get('chunk_type')
                        if chunk_type == 'image_text':
                            # è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°ï¼Œä½†ä¸ç›´æ¥èµ‹å€¼ç»™doc.score
                            relevance_score = calculate_content_relevance(query, doc.page_content)
                            # åˆ›å»ºä¸€ä¸ªåŒ…å«åˆ†æ•°çš„å…ƒç»„
                            doc_with_score = (doc, relevance_score)
                            image_text_candidates.append(doc_with_score)
                        else:
                            other_candidates.append(doc)
                
                # æŒ‰åˆ†æ•°æ’åº
                image_text_candidates.sort(key=lambda x: x[1], reverse=True)
                
                filter_time = time.time() - filter_start
                total_time = time.time() - start_time
                
                print(f"  æ€»æœç´¢ç»“æœ: {len(all_results)}")
                print(f"  image_textç±»å‹: {len(image_text_candidates)}")
                print(f"  å…¶ä»–ç±»å‹: {len(other_candidates)}")
                print(f"  å‘é‡æœç´¢è€—æ—¶: {search_time:.3f}ç§’")
                print(f"  åè¿‡æ»¤è€—æ—¶: {filter_time:.3f}ç§’")
                print(f"  æ€»è€—æ—¶: {total_time:.3f}ç§’")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªimage_textç»“æœ
                if image_text_candidates:
                    print(f"\n  ğŸ“Š image_textç»“æœ (å‰5ä¸ª):")
                    for i, (doc, score) in enumerate(image_text_candidates[:5]):
                        print(f"    {i+1}. åˆ†æ•°: {score:.4f}")
                        print(f"       å†…å®¹: {doc.page_content[:100]}...")
                        if hasattr(doc, 'metadata') and doc.metadata:
                            print(f"       å…ƒæ•°æ®: {doc.metadata}")
                        print()
                else:
                    print(f"\n  âš ï¸  åœ¨{len(all_results)}ä¸ªç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°image_textç±»å‹æ–‡æ¡£")
                    # æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£çš„ç±»å‹åˆ†å¸ƒ
                    type_distribution = {}
                    for doc in all_results:
                        if hasattr(doc, 'metadata') and doc.metadata:
                            chunk_type = doc.metadata.get('chunk_type', 'unknown')
                            type_distribution[chunk_type] = type_distribution.get(chunk_type, 0) + 1
                    
                    print(f"  æ–‡æ¡£ç±»å‹åˆ†å¸ƒ: {type_distribution}")
                
            except Exception as e:
                print(f"  å¤±è´¥: {e}")
                import traceback
                print(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # 4. æ€§èƒ½åˆ†æ
        print("\n" + "=" * 80)
        print("ğŸ“Š æ€§èƒ½åˆ†æ")
        print("=" * 80)
        
        print("\nåè¿‡æ»¤æ–¹æ¡ˆçš„ä¼˜åŠ¿:")
        print("âœ… èƒ½è·å–åˆ°æ‰€æœ‰ç›¸å…³æ–‡æ¡£")
        print("âœ… å¯ä»¥è‡ªå®šä¹‰ç›¸å…³æ€§è®¡ç®—é€»è¾‘")
        print("âœ… æ”¯æŒå¤æ‚çš„è¿‡æ»¤æ¡ä»¶")
        print("âœ… ä¸éœ€è¦é‡å»ºå‘é‡æ•°æ®åº“")
        
        print("\nåè¿‡æ»¤æ–¹æ¡ˆçš„åŠ£åŠ¿:")
        print("âŒ éœ€è¦å¤„ç†æ›´å¤šæ–‡æ¡£")
        print("âŒ è¿‡æ»¤æ—¶é—´ä¼šå¢åŠ ")
        print("âŒ å†…å­˜ä½¿ç”¨å¯èƒ½å¢åŠ ")
        
        print("\nå»ºè®®:")
        print("1. è®¾ç½®åˆç†çš„kå€¼ (å¦‚50-100)ï¼Œå¹³è¡¡å¬å›ç‡å’Œæ€§èƒ½")
        print("2. ä¼˜åŒ–ç›¸å…³æ€§è®¡ç®—å‡½æ•°ï¼Œæé«˜è®¡ç®—æ•ˆç‡")
        print("3. è€ƒè™‘ç¼“å­˜è¿‡æ»¤ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


if __name__ == "__main__":
    test_post_filter_approach()
