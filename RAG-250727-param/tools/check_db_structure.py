'''
ç¨‹åºè¯´æ˜ï¼š
## 1. æ£€æŸ¥å‘é‡æ•°æ®åº“ç»“æ„çš„å¢å¼ºç‰ˆè„šæœ¬
## 2. åŸºäºview_image_descriptions.pyçš„æ•°æ®åº“è¿æ¥æ–¹æ³•
## 3. åˆ†ææ•°æ®åº“ä¸­çš„å­—æ®µå’Œå†…å®¹ç±»å‹
## 4. æŠ½æ ·æå–ä¸åŒç±»å‹æ–‡æ¡£ï¼ˆå›¾ç‰‡ã€æ–‡æœ¬ã€è¡¨æ ¼ï¼‰çš„å­˜æ”¾æƒ…å†µ
## 5. æä¾›è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯å’Œæ ·æœ¬å±•ç¤º
'''

import sys
import os
import json
import logging
from pathlib import Path

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import Settings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings

# å¯¼å…¥ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—
from config.api_key_manager import get_dashscope_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_vector_store(vector_db_path):
    """åŠ è½½å‘é‡å­˜å‚¨"""
    try:
        config = Settings.load_from_file('config.json')
        
        # ä½¿ç”¨ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—è·å–APIå¯†é’¥
        config_key = config.dashscope_api_key
        api_key = get_dashscope_api_key(config_key)
        
        if not api_key:
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return None
        
        # åˆå§‹åŒ–DashScope embeddings
        try:
            from config.settings import Settings
            config = Settings.load_from_file('../config.json')
            embedding_model = config.text_embedding_model
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®ï¼Œä½¿ç”¨é»˜è®¤embeddingæ¨¡å‹: {e}")
            embedding_model = 'text-embedding-v1'
        
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model=embedding_model)
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        logger.info(f"å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(vector_store.docstore._dict)} ä¸ªæ–‡æ¡£")
        return vector_store
    except Exception as e:
        logger.error(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
        return None

def analyze_database_structure(vector_store):
    """åˆ†ææ•°æ®åº“ç»“æ„"""
    print("ğŸ“Š å‘é‡æ•°æ®åº“ç»“æ„åˆ†æ")
    print("=" * 60)
    
    if not vector_store or not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
        print("âŒ å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸")
        return None
    
    docstore = vector_store.docstore._dict
    total_docs = len(docstore)
    print(f"ğŸ“š æ€»æ–‡æ¡£æ•°: {total_docs}")
    
    if total_docs == 0:
        print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
        return None
    
    # åˆ†ææ–‡æ¡£ç±»å‹åˆ†å¸ƒ
    chunk_types = {}
    document_names = set()
    all_fields = set()
    
    # æ”¶é›†æ‰€æœ‰å­—æ®µå’Œç»Ÿè®¡ä¿¡æ¯
    for doc_id, doc in docstore.items():
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        
        # ç»Ÿè®¡åˆ†å—ç±»å‹
        chunk_type = metadata.get('chunk_type', 'unknown')
        if chunk_type not in chunk_types:
            chunk_types[chunk_type] = 0
        chunk_types[chunk_type] += 1
        
        # ç»Ÿè®¡æ–‡æ¡£åç§°
        doc_name = metadata.get('document_name', 'unknown')
        document_names.add(doc_name)
        
        # æ”¶é›†æ‰€æœ‰å­—æ®µ
        all_fields.update(metadata.keys())
    
    print(f"\nğŸ“Š åˆ†å—ç±»å‹åˆ†å¸ƒ:")
    for chunk_type, count in chunk_types.items():
        print(f"  {chunk_type}: {count}")
    
    print(f"\nğŸ“š æ–‡æ¡£ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æ¡£æ•°: {total_docs}")
    print(f"  å”¯ä¸€æ–‡æ¡£å: {len(document_names)}")
    print(f"  æ–‡æ¡£åç§°: {sorted(list(document_names))[:5]}...")
    
    print(f"\nğŸ“‹ å­—æ®µç»Ÿè®¡:")
    print(f"  æ€»å­—æ®µæ•°: {len(all_fields)}")
    print(f"  æ‰€æœ‰å­—æ®µ: {sorted(list(all_fields))}")
    
    return {
        'total_docs': total_docs,
        'chunk_types': chunk_types,
        'document_names': list(document_names),
        'all_fields': list(all_fields)
    }

def sample_documents_by_type(vector_store, chunk_type: str, sample_size: int = 3):
    """æŒ‰ç±»å‹æŠ½æ ·æå–æ–‡æ¡£"""
    print(f"\nğŸ” æŠ½æ ·æå– {chunk_type} ç±»å‹æ–‡æ¡£ (æœ€å¤š {sample_size} ä¸ª):")
    print("-" * 60)
    
    samples = []
    count = 0
    
    for doc_id, doc in vector_store.docstore._dict.items():
        if count >= sample_size:
            break
            
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        if metadata.get('chunk_type') == chunk_type:
            count += 1
            
            sample_info = {
                'doc_id': doc_id,
                'content_preview': str(doc.page_content)[:200] + "..." if len(str(doc.page_content)) > 200 else str(doc.page_content),
                'metadata': metadata
            }
            samples.append(sample_info)
            
            print(f"\nğŸ“„ æ ·æœ¬ {count}:")
            print(f"  ğŸ†” æ–‡æ¡£ID: {doc_id}")
            print(f"  ğŸ“„ æ–‡æ¡£åç§°: {metadata.get('document_name', 'N/A')}")
            print(f"  ğŸ“– é¡µç : {metadata.get('page_number', 'N/A')}")
            print(f"  ğŸ“ åˆ†å—ç´¢å¼•: {metadata.get('chunk_index', 'N/A')}")
            
            # æ ¹æ®ç±»å‹æ˜¾ç¤ºç‰¹å®šå­—æ®µ
            if chunk_type == 'image':
                print(f"  ğŸ–¼ï¸ å›¾ç‰‡ID: {metadata.get('image_id', 'N/A')}")
                print(f"  ğŸ·ï¸ å›¾ç‰‡ç±»å‹: {metadata.get('image_type', 'N/A')}")
                print(f"  ğŸ“‹ å›¾ç‰‡æ ‡é¢˜: {metadata.get('img_caption', 'N/A')}")
                print(f"  ğŸ¯ å¢å¼ºæè¿°: {metadata.get('enhanced_description', 'N/A')[:100]}...")
                
            elif chunk_type == 'table':
                print(f"  ğŸ“Š è¡¨æ ¼ID: {metadata.get('table_id', 'N/A')}")
                print(f"  ğŸ·ï¸ è¡¨æ ¼ç±»å‹: {metadata.get('table_type', 'N/A')}")
                
            elif chunk_type == 'text':
                print(f"  ğŸ“ å†…å®¹é¢„è§ˆ: {sample_info['content_preview']}")
            
            print(f"  ğŸ”§ æ‰€æœ‰å…ƒæ•°æ®å­—æ®µ: {list(metadata.keys())}")
    
    if not samples:
        print(f"  âŒ æœªæ‰¾åˆ° {chunk_type} ç±»å‹çš„æ–‡æ¡£")
    
    return samples

def analyze_field_content(vector_store, field_name: str):
    """åˆ†æç‰¹å®šå­—æ®µçš„å†…å®¹åˆ†å¸ƒ"""
    print(f"\nğŸ” åˆ†æå­—æ®µ '{field_name}' çš„å†…å®¹åˆ†å¸ƒ:")
    print("-" * 60)
    
    field_values = {}
    field_types = {}
    
    for doc_id, doc in vector_store.docstore._dict.items():
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        value = metadata.get(field_name)
        
        if value is not None:
            # ç»Ÿè®¡å€¼åˆ†å¸ƒ
            if value not in field_values:
                field_values[value] = 0
            field_values[value] += 1
            
            # ç»Ÿè®¡ç±»å‹åˆ†å¸ƒ
            value_type = type(value).__name__
            if value_type not in field_types:
                field_types[value_type] = 0
            field_types[value_type] += 1
    
    if not field_values:
        print(f"  âŒ å­—æ®µ '{field_name}' åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­éƒ½ä¸å­˜åœ¨")
        return
    
    print(f"  ğŸ“Š å€¼åˆ†å¸ƒ (å…± {len(field_values)} ä¸ªå”¯ä¸€å€¼):")
    for value, count in sorted(field_values.items(), key=lambda x: x[1], reverse=True)[:10]:
        if isinstance(value, str) and len(value) > 50:
            display_value = value[:50] + "..."
        else:
            display_value = str(value)
        print(f"    '{display_value}': {count} æ¬¡")
    
    print(f"\n  ğŸ”§ ç±»å‹åˆ†å¸ƒ:")
    for value_type, count in field_types.items():
        print(f"    {value_type}: {count} æ¬¡")

def show_detailed_statistics(vector_store, structure_info):
    """æ˜¾ç¤ºè¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)
    
    # æŒ‰æ–‡æ¡£åç§°ç»Ÿè®¡
    doc_stats = {}
    for doc_id, doc in vector_store.docstore._dict.items():
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        doc_name = metadata.get('document_name', 'unknown')
        chunk_type = metadata.get('chunk_type', 'unknown')
        
        if doc_name not in doc_stats:
            doc_stats[doc_name] = {'total': 0, 'types': {}}
        
        doc_stats[doc_name]['total'] += 1
        if chunk_type not in doc_stats[doc_name]['types']:
            doc_stats[doc_name]['types'][chunk_type] = 0
        doc_stats[doc_name]['types'][chunk_type] += 1
    
    print("ğŸ“„ æŒ‰æ–‡æ¡£ç»Ÿè®¡:")
    for doc_name, stats in sorted(doc_stats.items()):
        print(f"  {doc_name}:")
        print(f"    æ€»åˆ†å—æ•°: {stats['total']}")
        for chunk_type, count in stats['types'].items():
            print(f"    - {chunk_type}: {count}")
    
    # å­—æ®µä½¿ç”¨ç‡ç»Ÿè®¡
    field_usage = {}
    total_docs = structure_info['total_docs']
    
    for doc_id, doc in vector_store.docstore._dict.items():
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        for field in metadata.keys():
            if field not in field_usage:
                field_usage[field] = 0
            field_usage[field] += 1
    
    print(f"\nğŸ“‹ å­—æ®µä½¿ç”¨ç‡ç»Ÿè®¡:")
    for field, usage_count in sorted(field_usage.items(), key=lambda x: x[1], reverse=True):
        usage_rate = (usage_count / total_docs) * 100
        print(f"  {field}: {usage_count}/{total_docs} ({usage_rate:.1f}%)")

def save_analysis_results(structure_info, output_file: str = "database_analysis.json"):
    """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
    try:
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„æ•°æ®
        serializable_info = {
            'total_docs': structure_info['total_docs'],
            'chunk_types': structure_info['chunk_types'],
            'document_names': structure_info['document_names'],
            'all_fields': structure_info['all_fields'],
            'analysis_timestamp': str(Path().cwd()),
            'analysis_summary': f"æ•°æ®åº“åŒ…å« {structure_info['total_docs']} ä¸ªæ–‡æ¡£ï¼Œ{len(structure_info['chunk_types'])} ç§åˆ†å—ç±»å‹"
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_info, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å‘é‡æ•°æ®åº“ç»“æ„åˆ†æå·¥å…·")
    print("=" * 60)
    
    try:
        config = Settings.load_from_file('config.json')
        vector_db_path = config.vector_db_dir
        
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
        if not os.path.exists(vector_db_path):
            possible_paths = [
                "./central/vector_db",
                "./vector_db",
                "./central/vector_db_test",
                "./vector_db_test"
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    vector_db_path = path
                    print(f"âœ… æ‰¾åˆ°å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
                    break
            else:
                print(f"âŒ å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•è¿‡çš„è·¯å¾„:")
                for path in possible_paths:
                    print(f"   - {path}")
                return
        
        # åŠ è½½å‘é‡å­˜å‚¨
        vector_store = load_vector_store(vector_db_path)
        if not vector_store:
            print("âŒ æ— æ³•åŠ è½½å‘é‡å­˜å‚¨")
            return
        
        # åˆ†ææ•°æ®åº“ç»“æ„
        structure_info = analyze_database_structure(vector_store)
        if not structure_info:
            return
        
        # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
        show_detailed_statistics(vector_store, structure_info)
        
        # æŠ½æ ·æå–ä¸åŒç±»å‹çš„æ–‡æ¡£
        print(f"\nğŸ” æŠ½æ ·åˆ†æä¸åŒç±»å‹æ–‡æ¡£:")
        for chunk_type in structure_info['chunk_types'].keys():
            sample_documents_by_type(vector_store, chunk_type, sample_size=2)
        
        # åˆ†æé‡è¦å­—æ®µ
        important_fields = ['chunk_type', 'document_name', 'page_number', 'chunk_index']
        for field in important_fields:
            if field in structure_info['all_fields']:
                analyze_field_content(vector_store, field)
        
        # ä¿å­˜åˆ†æç»“æœ
        save_choice = input("\næ˜¯å¦ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶? (y/n): ").strip().lower()
        if save_choice == 'y':
            output_file = "database_analysis.json"
            save_analysis_results(structure_info, output_file)
        
        print("\nâœ… æ•°æ®åº“ç»“æ„åˆ†æå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
