'''
ç¨‹åºè¯´æ˜ï¼š
## 1. æµ‹è¯•image_textæ–‡æ¡£çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢
## 2. æ¨¡æ‹Ÿtext_engineçš„å®ç°æ–¹å¼ï¼Œä½¿ç”¨similarity_searchæ–¹æ³•
## 3. æµ‹è¯•ä¸åŒé˜ˆå€¼è®¾ç½®å¯¹å¬å›ç»“æœçš„å½±å“
## 4. ä¸ºimage_engineçš„é˜ˆå€¼è®¾ç½®æä¾›å‚è€ƒ
'''

import os
import sys
import json
import numpy as np
from typing import Dict, Any, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.api_key_manager import get_dashscope_api_key


def test_image_text_similarity_search():
    """æµ‹è¯•image_textæ–‡æ¡£çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
    print("ğŸ” å¼€å§‹æµ‹è¯•image_textæ–‡æ¡£çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢")
    print("=" * 80)
    
    try:
        # 1. è·å–APIå¯†é’¥
        print("ğŸ”‘ è·å–APIå¯†é’¥...")
        api_key = get_dashscope_api_key()
        if not api_key:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return
        
        print("âœ… APIå¯†é’¥è·å–æˆåŠŸ")
        
        # 2. åˆå§‹åŒ–embeddings
        print("\nğŸš€ åˆå§‹åŒ–embeddings...")
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        print("âœ… embeddingsåˆå§‹åŒ–æˆåŠŸ")
        
        # 3. åŠ è½½å‘é‡æ•°æ®åº“
        print("\nğŸ“š åŠ è½½å‘é‡æ•°æ®åº“...")
        vector_db_path = "../central/vector_db"
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {os.path.abspath(vector_db_path)}")
        
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        
        # 4. åˆ†ææ•°æ®åº“ç»“æ„
        print("\nğŸ“Š åˆ†ææ•°æ®åº“ç»“æ„...")
        if hasattr(vector_store, 'docstore') and hasattr(vector_store.docstore, '_dict'):
            docs = vector_store.docstore._dict
            print(f"ğŸ“„ æ€»æ–‡æ¡£æ•°é‡: {len(docs)}")
            
            # ç»Ÿè®¡æ–‡æ¡£ç±»å‹
            chunk_types = {}
            for doc_id, doc in docs.items():
                chunk_type = doc.metadata.get('chunk_type', 'unknown')
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
            
            print("æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
            for chunk_type, count in sorted(chunk_types.items()):
                print(f"  - {chunk_type}: {count} ä¸ª")
            
            # æ£€æŸ¥image_textæ–‡æ¡£
            image_text_count = chunk_types.get('image_text', 0)
            if image_text_count == 0:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°image_textç±»å‹çš„æ–‡æ¡£")
                return
            else:
                print(f"âœ… æ‰¾åˆ° {image_text_count} ä¸ªimage_textæ–‡æ¡£")
        else:
            print("âŒ æ— æ³•è·å–æ–‡æ¡£ä¿¡æ¯")
            return
        
        # 5. æµ‹è¯•æŸ¥è¯¢
        print("\nğŸ” å¼€å§‹æµ‹è¯•æŸ¥è¯¢...")
        test_queries = [
            "å›¾4ï¼šä¸­èŠ¯å›½é™…å½’æ¯å‡€åˆ©æ¶¦æƒ…å†µæ¦‚è§ˆ",  # æ·»åŠ å…·ä½“çš„å›¾è¡¨æŸ¥è¯¢
            "å›¾è¡¨æ•°æ®",
            "ä¸­èŠ¯å›½é™…å‡€åˆ©æ¶¦",
            "äº§èƒ½åˆ©ç”¨ç‡",
            "å­£åº¦æŠ¥å‘Š",
            "è´¢åŠ¡åˆ†æ",
            "æ•°æ®è¶‹åŠ¿"
        ]
        
        # æµ‹è¯•ä¸åŒé˜ˆå€¼
        test_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]
        
        for query in test_queries:
            print(f"\nğŸ“ æµ‹è¯•æŸ¥è¯¢: '{query}'")
            print("-" * 50)
            
            # ä½¿ç”¨similarity_searchæ–¹æ³•ï¼ˆæ¨¡æ‹Ÿtext_engineï¼‰
            try:
                # è·å–æ›´å¤šç»“æœç”¨äºåˆ†æ
                vector_results = vector_store.similarity_search(query, k=50)
                print(f"ğŸ” åŸå§‹æœç´¢ç»“æœ: {len(vector_results)} ä¸ª")
                
                if vector_results:
                    # åˆ†ææ¯ä¸ªç»“æœçš„ç›¸ä¼¼åº¦åˆ†æ•°
                    scores = []
                    image_text_results = []
                    
                    for i, doc in enumerate(vector_results):
                        # è·å–æ–‡æ¡£ç±»å‹
                        chunk_type = doc.metadata.get('chunk_type', 'unknown')
                        
                        # è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°ï¼ˆæ¨¡æ‹Ÿtext_engineçš„_calculate_content_relevanceï¼‰
                        content_score = calculate_content_relevance(query, doc.page_content)
                        
                        # è®°å½•image_textç±»å‹çš„ç»“æœ
                        if chunk_type == 'image_text':
                            image_text_results.append({
                                'rank': i + 1,
                                'content': doc.page_content[:100] + "...",
                                'chunk_type': chunk_type,
                                'content_score': content_score,
                                'metadata': doc.metadata
                            })
                        
                        scores.append(content_score)
                    
                    # åˆ†æåˆ†æ•°åˆ†å¸ƒ
                    if scores:
                        min_score = min(scores)
                        max_score = max(scores)
                        avg_score = sum(scores) / len(scores)
                        
                        print(f"ğŸ“Š åˆ†æ•°ç»Ÿè®¡:")
                        print(f"  - æœ€å°åˆ†æ•°: {min_score:.4f}")
                        print(f"  - æœ€å¤§åˆ†æ•°: {max_score:.4f}")
                        print(f"  - å¹³å‡åˆ†æ•°: {avg_score:.4f}")
                        
                        # æµ‹è¯•ä¸åŒé˜ˆå€¼
                        print(f"ğŸ¯ é˜ˆå€¼æµ‹è¯•ç»“æœ:")
                        for threshold in test_thresholds:
                            above_threshold = sum(1 for score in scores if score >= threshold)
                            print(f"  - é˜ˆå€¼ {threshold}: {above_threshold}/{len(scores)} ä¸ªç»“æœ")
                        
                        # æ˜¾ç¤ºimage_textç»“æœ
                        if image_text_results:
                            print(f"\nğŸ–¼ï¸ Image_textç±»å‹ç»“æœ (å…±{len(image_text_results)}ä¸ª):")
                            for result in image_text_results[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                                print(f"  {result['rank']}. åˆ†æ•°: {result['content_score']:.4f}")
                                print(f"     å†…å®¹: {result['content']}")
                                print(f"     ç±»å‹: {result['chunk_type']}")
                        else:
                            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°image_textç±»å‹çš„ç»“æœ")
                    
                else:
                    print("âŒ æœç´¢è¿”å›0ä¸ªç»“æœ")
                    
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
                import traceback
                print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # 6. æ€»ç»“å’Œå»ºè®®
        print("\n" + "=" * 80)
        print("ğŸ¯ æµ‹è¯•æ€»ç»“å’Œå»ºè®®")
        print("=" * 80)
        
        print("\nğŸ“‹ ä¸»è¦å‘ç°:")
        print("1. âœ… æˆåŠŸä½¿ç”¨similarity_searchæ–¹æ³•è¿›è¡Œå‘é‡æœç´¢")
        print("2. âœ… èƒ½å¤Ÿè·å–åˆ°image_textç±»å‹çš„æ–‡æ¡£")
        print("3. âœ… å¯ä»¥è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°")
        
        print("\nğŸ’¡ é˜ˆå€¼è®¾ç½®å»ºè®®:")
        print("1. åŸºäºåˆ†æ•°åˆ†å¸ƒè®¾ç½®åˆç†é˜ˆå€¼")
        print("2. è€ƒè™‘image_textæ–‡æ¡£çš„ç‰¹æ®Šæ€§")
        print("3. å¹³è¡¡å¬å›ç‡å’Œç²¾ç¡®ç‡")
        
        print("\nğŸ”§ å®æ–½å»ºè®®:")
        print("1. åœ¨image_engineä¸­ä½¿ç”¨similarity_searchæ–¹æ³•")
        print("2. å‚è€ƒtext_engineçš„å®ç°æ–¹å¼")
        print("3. æ ¹æ®æµ‹è¯•ç»“æœè°ƒæ•´é˜ˆå€¼è®¾ç½®")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


def calculate_content_relevance(query: str, content: str) -> float:
    """
    è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°ï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼‰
    
    :param query: æŸ¥è¯¢æ–‡æœ¬
    :param content: æ–‡æ¡£å†…å®¹
    :return: ç›¸å…³æ€§åˆ†æ•° [0, 1]
    """
    try:
        # é¢„å¤„ç†ï¼šè½¬æ¢ä¸ºå°å†™å¹¶åˆ†è¯
        query_lower = query.lower()
        content_lower = content.lower()
        
        # æ–¹æ³•1ï¼šç›´æ¥å­—ç¬¦ä¸²åŒ…å«åŒ¹é…
        if query_lower in content_lower:
            return 0.8  # å®Œå…¨åŒ…å«ç»™é«˜åˆ†
        
        # æ–¹æ³•2ï¼šåˆ†è¯åŒ¹é…
        query_words = [word for word in query_lower.split() if len(word) > 1]  # è¿‡æ»¤å•å­—ç¬¦
        if not query_words:
            return 0.0
        
        content_words = content_lower.split()
        
        # è®¡ç®—åŒ¹é…è¯æ•°
        matched_words = 0
        total_score = 0.0
        
        for query_word in query_words:
            if query_word in content_words:
                matched_words += 1
                # è®¡ç®—è¯é¢‘åˆ†æ•°
                word_count = content_lower.count(query_word)
                word_score = min(word_count / len(content_words), 0.3)  # é™åˆ¶å•ä¸ªè¯çš„æœ€å¤§åˆ†æ•°
                total_score += word_score
        
        # è®¡ç®—åŒ¹é…ç‡
        match_rate = matched_words / len(query_words) if query_words else 0
        
        # ç»¼åˆåˆ†æ•°ï¼šåŒ¹é…ç‡ + è¯é¢‘åˆ†æ•°
        final_score = (match_rate * 0.7 + total_score * 0.3)
        
        return min(final_score, 1.0)
        
    except Exception:
        return 0.0


if __name__ == "__main__":
    test_image_text_similarity_search()
