#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š
## 1. è¯Šæ–­FAISS filterä¸å·¥ä½œçš„æ ¹æœ¬åŸå› 
## 2. æ£€æŸ¥å‘é‡æ•°æ®åº“ç»“æ„å’Œchunk_typeå­—æ®µ
## 3. æµ‹è¯•ä¸åŒçš„filterè¯­æ³•å’ŒFAISSæ”¯æŒæƒ…å†µ
## 4. åˆ†æå‘é‡åŒ–æ¨¡å‹å’Œæ•°æ®ç±»å‹
"""

import sys
import os
import logging

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import Settings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings

# å¯¼å…¥ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—
from config.api_key_manager import get_dashscope_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def debug_faiss_filter():
    """è¯Šæ–­FAISS filterä¸å·¥ä½œçš„æ ¹æœ¬åŸå› """
    print("ğŸ” è¯Šæ–­FAISS filterä¸å·¥ä½œçš„æ ¹æœ¬åŸå› ")
    print("=" * 60)
    
    try:
        # åŠ è½½é…ç½®
        config = Settings.load_from_file('config.json')
        vector_db_path = config.vector_db_dir
        
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
        
        # è·å–APIå¯†é’¥
        config_key = config.dashscope_api_key
        api_key = get_dashscope_api_key(config_key)
        
        if not api_key:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return
        
        # åˆå§‹åŒ–embeddings
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model='text-embedding-v1')
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        print("ğŸ“š æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        
        # 1. æ£€æŸ¥FAISSç‰ˆæœ¬å’Œé…ç½®
        print("\nğŸ” 1. æ£€æŸ¥FAISSç‰ˆæœ¬å’Œé…ç½®")
        print("-" * 40)
        try:
            import faiss
            print(f"FAISSç‰ˆæœ¬: {faiss.__version__}")
            print(f"FAISSç¼–è¯‘ä¿¡æ¯: {faiss.get_compile_options()}")
        except Exception as e:
            print(f"âŒ è·å–FAISSä¿¡æ¯å¤±è´¥: {e}")
        
        # 2. åˆ†æå‘é‡æ•°æ®åº“ç»“æ„
        print("\nğŸ” 2. åˆ†æå‘é‡æ•°æ®åº“ç»“æ„")
        print("-" * 40)
        
        docstore = vector_store.docstore
        docstore_dict = docstore._dict
        print(f"æ€»æ–‡æ¡£æ•°é‡: {len(docstore_dict)}")
        
        # ç»Ÿè®¡chunk_typeåˆ†å¸ƒ
        chunk_type_stats = {}
        metadata_fields = set()
        
        for doc_id, doc in docstore_dict.items():
            if hasattr(doc, 'metadata') and doc.metadata:
                chunk_type = doc.metadata.get('chunk_type', 'unknown')
                chunk_type_stats[chunk_type] = chunk_type_stats.get(chunk_type, 0) + 1
                
                # æ”¶é›†æ‰€æœ‰metadataå­—æ®µ
                metadata_fields.update(doc.metadata.keys())
                
                # æ£€æŸ¥å‰å‡ ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
                if len(chunk_type_stats) <= 5:
                    print(f"æ–‡æ¡£ {doc_id}: chunk_type={chunk_type}")
                    print(f"  metadataå­—æ®µ: {list(doc.metadata.keys())}")
                    if 'chunk_type' in doc.metadata:
                        print(f"  chunk_typeå€¼: '{doc.metadata['chunk_type']}' (ç±»å‹: {type(doc.metadata['chunk_type'])})")
        
        print(f"\nchunk_typeåˆ†å¸ƒ:")
        for chunk_type, count in sorted(chunk_type_stats.items()):
            print(f"  {chunk_type}: {count} ä¸ª")
        
        print(f"\næ‰€æœ‰metadataå­—æ®µ: {sorted(metadata_fields)}")
        
        # 3. æµ‹è¯•ä¸åŒçš„filterè¯­æ³•
        print("\nğŸ” 3. æµ‹è¯•ä¸åŒçš„filterè¯­æ³•")
        print("-" * 40)
        
        test_query = "ä¸­èŠ¯å›½é™…å‡€åˆ©æ¶¦"
        test_filters = [
            None,  # æ— filter
            {'chunk_type': 'image'},
            {'chunk_type': 'image_text'},
            {'chunk_type': 'text'},
            {'chunk_type': 'table'},
            {'metadata.chunk_type': 'image'},
            {'metadata.chunk_type': 'image_text'},
            {'chunk_type': 1},  # æ•°å€¼åŒ–æµ‹è¯•
            {'chunk_type': 'image', 'document_name': 'ä¸­èŠ¯å›½é™…'},
            {'chunk_type': 'image_text', 'document_name': 'ä¸­èŠ¯å›½é™…'},
        ]
        
        for filter_dict in test_filters:
            try:
                if filter_dict is None:
                    results = vector_store.similarity_search(test_query, k=5)
                    print(f"æ— filter: {len(results)} ä¸ªç»“æœ")
                else:
                    results = vector_store.similarity_search(test_query, k=5, filter=filter_dict)
                    print(f"Filter {filter_dict}: {len(results)} ä¸ªç»“æœ")
                    
                    # æ˜¾ç¤ºç»“æœçš„chunk_type
                    if results:
                        chunk_types = [r.metadata.get('chunk_type', 'N/A') for r in results]
                        print(f"  ç»“æœchunk_type: {chunk_types}")
                        
            except Exception as e:
                print(f"Filter {filter_dict}: é”™è¯¯ - {e}")
        
        # 4. åˆ†æå‘é‡åŒ–æ¨¡å‹
        print("\nğŸ” 4. åˆ†æå‘é‡åŒ–æ¨¡å‹")
        print("-" * 40)
        
        # æ£€æŸ¥imageå’Œimage_textæ–‡æ¡£çš„å‘é‡åŒ–ä¿¡æ¯
        image_docs = []
        image_text_docs = []
        
        for doc_id, doc in docstore_dict.items():
            if hasattr(doc, 'metadata') and doc.metadata:
                chunk_type = doc.metadata.get('chunk_type', '')
                if chunk_type == 'image':
                    image_docs.append(doc)
                elif chunk_type == 'image_text':
                    image_text_docs.append(doc)
        
        print(f"imageæ–‡æ¡£æ•°é‡: {len(image_docs)}")
        print(f"image_textæ–‡æ¡£æ•°é‡: {len(image_text_docs)}")
        
        # æ£€æŸ¥å‰å‡ ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        if image_docs:
            print(f"\nç¬¬ä¸€ä¸ªimageæ–‡æ¡£:")
            doc = image_docs[0]
            metadata = doc.metadata
            print(f"  chunk_type: {metadata.get('chunk_type')}")
            print(f"  image_id: {metadata.get('image_id')}")
            print(f"  enhanced_description: {metadata.get('enhanced_description', '')[:100]}...")
            print(f"  semantic_features: {'æœ‰' if 'semantic_features' in metadata else 'æ— '}")
        
        if image_text_docs:
            print(f"\nç¬¬ä¸€ä¸ªimage_textæ–‡æ¡£:")
            doc = image_text_docs[0]
            metadata = doc.metadata
            print(f"  chunk_type: {metadata.get('chunk_type')}")
            print(f"  image_id: {metadata.get('image_id')}")
            print(f"  enhanced_description: {metadata.get('enhanced_description', '')[:100]}...")
            print(f"  semantic_features: {'æœ‰' if 'semantic_features' in metadata else 'æ— '}")
        
        # 5. æµ‹è¯•å‘é‡æœç´¢çš„ç›¸ä¼¼åº¦åˆ†æ•°
        print("\nğŸ” 5. æµ‹è¯•å‘é‡æœç´¢çš„ç›¸ä¼¼åº¦åˆ†æ•°")
        print("-" * 40)
        
        try:
            # ä½¿ç”¨similarity_search_with_scoreè·å–åˆ†æ•°
            results_with_score = vector_store.similarity_search_with_score(test_query, k=10)
            print(f"å‰10ä¸ªç»“æœçš„ç›¸ä¼¼åº¦åˆ†æ•°:")
            
            for i, (doc, score) in enumerate(results_with_score):
                chunk_type = doc.metadata.get('chunk_type', 'N/A') if hasattr(doc, 'metadata') else 'N/A'
                print(f"  ç»“æœ{i+1}: chunk_type={chunk_type}, score={score:.4f}")
                
                if i >= 4:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    break
                    
        except Exception as e:
            print(f"âŒ è·å–ç›¸ä¼¼åº¦åˆ†æ•°å¤±è´¥: {e}")
        
        print("\nâœ… FAISS filterè¯Šæ–­å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¯Šæ–­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_faiss_filter()
