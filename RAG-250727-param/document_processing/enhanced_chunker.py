'''
ç¨‹åºè¯´æ˜ï¼š
## 1. ç‹¬ç«‹çš„å¢å¼ºæ–‡æ¡£åˆ†å—å™¨æ¨¡å—
## 2. ä»V310_enhanced_document_chunker.pyä¸­æå–æ ¸å¿ƒåŠŸèƒ½
## 3. ç»Ÿä¸€æ–‡æ¡£åˆ†å—æ¥å£
## 4. æ”¯æŒæ–‡æœ¬å’Œè¡¨æ ¼çš„åˆ†å—å¤„ç†
'''

import os
import json
import re
from pathlib import Path
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass
from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging

# å¯¼å…¥è¡¨æ ¼å¤„ç†å™¨
from .table_processor import ConfigurableTableProcessor as TableProcessor, ConfigurableTableChunkGenerator as TableChunkGenerator

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


@dataclass
class EnhancedDocumentChunk:
    """
    å¢å¼ºç‰ˆæ–‡æ¡£åˆ†å—æ•°æ®ç»“æ„
    :param content: åˆ†å—å†…å®¹
    :param document_name: æ–‡æ¡£åç§°
    :param page_number: é¡µç 
    :param chunk_index: åˆ†å—ç´¢å¼•
    :param chunk_type: åˆ†å—ç±»å‹ï¼ˆtext/tableï¼‰
    :param table_id: è¡¨æ ¼IDï¼ˆå¦‚æœæ˜¯è¡¨æ ¼åˆ†å—ï¼‰
    :param table_type: è¡¨æ ¼ç±»å‹ï¼ˆå¦‚æœæ˜¯è¡¨æ ¼åˆ†å—ï¼‰
    :param table_title: è¡¨æ ¼æ ‡é¢˜ï¼ˆå¦‚æœæ˜¯è¡¨æ ¼åˆ†å—ï¼‰
    :param table_summary: è¡¨æ ¼æ‘˜è¦ï¼ˆå¦‚æœæ˜¯è¡¨æ ¼åˆ†å—ï¼‰
    :param table_headers: è¡¨æ ¼åˆ—æ ‡é¢˜ï¼ˆå¦‚æœæ˜¯è¡¨æ ¼åˆ†å—ï¼‰
    :param related_text: ç›¸å…³æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæ˜¯è¡¨æ ¼åˆ†å—ï¼‰
    :param processed_table_content: å¤„ç†åçš„è¡¨æ ¼å†…å®¹ï¼ˆå¦‚æœæ˜¯è¡¨æ ¼åˆ†å—ï¼‰
    :param table_row_count: è¡¨æ ¼è¡Œæ•°ï¼ˆå¦‚æœæ˜¯è¡¨æ ¼åˆ†å—ï¼‰
    :param table_column_count: è¡¨æ ¼åˆ—æ•°ï¼ˆå¦‚æœæ˜¯è¡¨æ ¼åˆ†å—ï¼‰
    """
    content: str
    document_name: str
    page_number: int
    chunk_index: int
    chunk_type: str = "text"
    table_id: str = None
    table_type: str = None
    table_title: str = None
    table_summary: str = None
    table_headers: List[str] = None
    related_text: str = None
    processed_table_content: str = None
    table_row_count: int = None
    table_column_count: int = None


class EnhancedDocumentLoader:
    """
    å¢å¼ºç‰ˆæ–‡æ¡£åŠ è½½å™¨ç±»ï¼Œç”¨äºåŠ è½½markdownå’ŒJSONæ–‡ä»¶
    """
    
    def __init__(self, md_dir: str):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆæ–‡æ¡£åŠ è½½å™¨
        :param md_dir: markdownæ–‡ä»¶ç›®å½•è·¯å¾„
        """
        self.md_dir = Path(md_dir)
        
    def load_documents(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½æ‰€æœ‰æ–‡æ¡£çš„markdownå’ŒJSONæ–‡ä»¶
        :return: åŒ…å«æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®çš„å­—å…¸åˆ—è¡¨
        """
        documents = []
        
        # éå†æ‰€æœ‰markdownæ–‡ä»¶
        for md_file in self.md_dir.glob("*.md"):
            doc_name = md_file.stem  # è·å–ä¸å¸¦æ‰©å±•åçš„æ–‡ä»¶å
            
            # æŸ¥æ‰¾å¯¹åº”çš„JSONæ–‡ä»¶
            json_file = md_file.with_name(f"{doc_name}_1.json")
            
            if not json_file.exists():
                print(f"è­¦å‘Š: æ‰¾ä¸åˆ° {doc_name} å¯¹åº”çš„JSONæ–‡ä»¶")
                continue
                
            # è¯»å–markdownå†…å®¹
            with open(md_file, 'r', encoding='utf-8') as f:
                md_content = f.read()
                
            # è¯»å–JSONå…ƒæ•°æ®
            with open(json_file, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
                
            documents.append({
                'name': doc_name,
                'md_content': md_content,
                'json_data': json_data
            })
            
        return documents


class EnhancedSemanticChunker:
    """
    å¢å¼ºç‰ˆè¯­ä¹‰åˆ†å—å™¨ç±»ï¼Œç”¨äºå¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œè¯­ä¹‰åˆ†å—å¤„ç†ï¼ŒåŒ…æ‹¬è¡¨æ ¼å¤„ç†
    """
    
    def __init__(self, chunk_size: int = 600, chunk_overlap: int = 100):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆè¯­ä¹‰åˆ†å—å™¨
        :param chunk_size: åˆ†å—å¤§å°
        :param chunk_overlap: åˆ†å—é‡å å¤§å°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        # DashScope APIé™åˆ¶ï¼šæœ€å¤§2048å­—ç¬¦
        self.max_chunk_length = 2048
        self.text_splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", ".", "!", "?", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""],
            chunk_size=min(chunk_size, self.max_chunk_length),  # ç¡®ä¿ä¸è¶…è¿‡APIé™åˆ¶
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
    
    def chunk_document(self, document: Dict[str, Any]) -> List[EnhancedDocumentChunk]:
        """
        å¯¹å•ä¸ªæ–‡æ¡£è¿›è¡Œåˆ†å—å¤„ç†
        :param document: æ–‡æ¡£æ•°æ®
        :return: æ–‡æ¡£åˆ†å—åˆ—è¡¨
        """
        chunks = []
        doc_name = document['name']
        md_content = document['md_content']
        json_data = document['json_data']
        
        # æå–æ–‡æœ¬å†…å®¹
        text_content = self._extract_text_content(json_data)
        
        # æå–è¡¨æ ¼å†…å®¹
        table_content = self._extract_table_content(json_data)
        
        # å¤„ç†æ–‡æœ¬åˆ†å—
        text_chunks = self._process_text_chunks(text_content, doc_name)
        chunks.extend(text_chunks)
        
        # å¤„ç†è¡¨æ ¼åˆ†å—
        table_chunks = self._process_table_chunks(table_content, doc_name)
        chunks.extend(table_chunks)
        
        return chunks
    
    def _extract_text_content(self, json_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æå–æ–‡æœ¬å†…å®¹
        :param json_data: JSONæ•°æ®
        :return: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        """
        text_content = []
        for item in json_data:
            if item.get("type") == "text":
                text_content.append(item)
        return text_content
    
    def _extract_table_content(self, json_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æå–è¡¨æ ¼å†…å®¹
        :param json_data: JSONæ•°æ®
        :return: è¡¨æ ¼å†…å®¹åˆ—è¡¨
        """
        table_content = []
        for item in json_data:
            if item.get("type") == "table":
                table_content.append(item)
        return table_content
    
    def _process_text_chunks(self, text_content: List[Dict[str, Any]], doc_name: str) -> List[EnhancedDocumentChunk]:
        """
        å¤„ç†æ–‡æœ¬åˆ†å—
        :param text_content: æ–‡æœ¬å†…å®¹
        :param doc_name: æ–‡æ¡£åç§°
        :return: æ–‡æœ¬åˆ†å—åˆ—è¡¨
        """
        chunks = []
        chunk_index_offset = 0  # ä½¿ç”¨åç§»é‡ç®¡ç†ç´¢å¼•ï¼ˆä¸è€ä»£ç ä¸€è‡´ï¼‰
        
        for item in text_content:
            text = item.get("text", "")
            # å°†0ç´¢å¼•çš„page_idxè½¬æ¢ä¸º1ç´¢å¼•çš„é¡µç ï¼ˆä¸è€ä»£ç ä¸€è‡´ï¼‰
            page_idx = item.get("page_idx", 0)
            page_number = page_idx + 1
            
            # ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨è¿›è¡Œåˆ†å—
            text_chunks = self.text_splitter.split_text(text)
            
            for i, chunk_text in enumerate(text_chunks):
                if chunk_text.strip():  # è·³è¿‡ç©ºåˆ†å—
                    # éªŒè¯å¹¶æˆªæ–­åˆ†å—å†…å®¹
                    validated_content = self._validate_and_truncate_chunk(chunk_text, "æ–‡æœ¬")
                    
                    chunk = EnhancedDocumentChunk(
                        content=validated_content,
                        document_name=doc_name,
                        page_number=page_number,
                        chunk_index=chunk_index_offset + i,
                        chunk_type="text"
                    )
                    chunks.append(chunk)
            
            chunk_index_offset += len(text_chunks)
        
        return chunks
    
    def _process_table_chunks(self, table_content: List[Dict[str, Any]], doc_name: str) -> List[EnhancedDocumentChunk]:
        """
        å¤„ç†è¡¨æ ¼åˆ†å—
        :param table_content: è¡¨æ ¼å†…å®¹
        :param doc_name: æ–‡æ¡£åç§°
        :return: è¡¨æ ¼åˆ†å—åˆ—è¡¨
        """
        chunks = []
        chunk_index_offset = 0  # ä½¿ç”¨åç§»é‡ç®¡ç†ç´¢å¼•ï¼ˆä¸è€ä»£ç ä¸€è‡´ï¼‰
        
        # åˆå§‹åŒ–è¡¨æ ¼å¤„ç†å™¨å’Œåˆ†å—ç”Ÿæˆå™¨
        table_processor = TableProcessor({})
        chunk_generator = TableChunkGenerator({})
        
        # æ”¶é›†æ‰€æœ‰è¡¨æ ¼ä¿¡æ¯ï¼Œç”¨äºæ‰¹é‡å¤„ç†
        all_table_infos = []
        
        for item in table_content:
            # å°†0ç´¢å¼•çš„page_idxè½¬æ¢ä¸º1ç´¢å¼•çš„é¡µç ï¼ˆä¸è€ä»£ç ä¸€è‡´ï¼‰
            page_idx = item.get("page_idx", 0)
            page_number = page_idx + 1
            table_body = item.get("table_body", "")
            table_id = item.get("id", f"table_{hash(table_body) % 1000000}")
            table_type = item.get("table_type", "æ•°æ®è¡¨æ ¼")
            
            try:
                # è§£æè¡¨æ ¼å†…å®¹
                table_info = table_processor.parse_html_table(table_body, table_type)
                logger.info(f"è¡¨æ ¼ {table_id} è§£æå®Œæˆï¼Œè¡Œæ•°: {table_info.row_count}, åˆ—æ•°: {table_info.column_count}")
                
                # ä»…ä¸ºåŸå§‹è¡¨ç”Ÿæˆä¸€ä¸ªåˆ†å—ï¼ˆä½¿ç”¨åŸå§‹HTMLï¼‰ï¼Œä¸åœ¨æ­¤å¤„åšé¢„æ‹†åˆ†
                chunk = EnhancedDocumentChunk(
                    content=table_info.html_content,  # ä½¿ç”¨åŸå§‹HTMLå†…å®¹
                    document_name=doc_name,
                    page_number=page_number,
                    chunk_index=chunk_index_offset,
                    chunk_type="table",
                    table_id=table_info.table_id,
                    table_type=table_info.table_type
                )
                chunks.append(chunk)
                chunk_index_offset += 1
                            
            except Exception as e:
                logger.error(f"å¤„ç†è¡¨æ ¼æ—¶å‡ºé”™: {e}")
                # å¦‚æœè¡¨æ ¼è§£æå¤±è´¥ï¼Œå°†åŸå§‹HTMLä½œä¸ºæ–‡æœ¬å¤„ç†ï¼ˆä¸è€ä»£ç ä¸€è‡´ï¼‰
                # éªŒè¯å¹¶æˆªæ–­å†…å®¹
                fallback_content = f"è¡¨æ ¼å†…å®¹ï¼ˆè§£æå¤±è´¥ï¼‰ï¼š {table_body}"
                validated_content = self._validate_and_truncate_chunk(fallback_content, "è¡¨æ ¼")
                
                chunk = EnhancedDocumentChunk(
                    content=validated_content,
                    document_name=doc_name,
                    page_number=page_number,
                    chunk_index=chunk_index_offset,
                    chunk_type="text"
                )
                chunks.append(chunk)
                chunk_index_offset += 1
                continue
        
        # è°ƒç”¨æ–°çš„è¡¨æ ¼å¤„ç†é€»è¾‘ï¼Œæ·»åŠ å®Œæ•´çš„å…ƒæ•°æ®å­—æ®µ
        try:
            # å°† EnhancedDocumentChunk è½¬æ¢ä¸º Document å¯¹è±¡
            from langchain.docstore.document import Document
            document_chunks = []
            
            for chunk in chunks:
                if chunk.chunk_type == "table":
                    # åˆ›å»ºåŒ…å«å®Œæ•´å…ƒæ•°æ®çš„ Document å¯¹è±¡
                    doc = Document(
                        page_content=chunk.content,
                        metadata={
                            'chunk_index': chunk.chunk_index,
                            'chunk_type': chunk.chunk_type,
                            'document_name': chunk.document_name,
                            'page_number': chunk.page_number,
                            'table_id': chunk.table_id,
                            'table_type': chunk.table_type,
                            # æ·»åŠ æ–°çš„å…ƒæ•°æ®å­—æ®µ
                            'table_title': f"è¡¨æ ¼ {chunk.table_id}",
                            'table_summary': f"è¡¨æ ¼ç±»å‹: {chunk.table_type}ï¼Œè¡Œæ•°: {len(chunk.content.split('\n')) if chunk.content else 0}",
                            'table_headers': [],  # è¿™é‡Œå¯ä»¥ä»è¡¨æ ¼å†…å®¹ä¸­æå–
                            'related_text': chunk.content,
                            'processed_table_content': chunk.content,
                            'table_row_count': len(chunk.content.split('\n')) if chunk.content else 0,
                            'table_column_count': 0,  # å¯ä»¥ä»è¡¨æ ¼å†…å®¹ä¸­æå–
                            'page_content': chunk.content  # ç¡®ä¿åŸå§‹å†…å®¹è¢«ä¿å­˜
                        }
                    )
                    document_chunks.append(doc)
                else:
                    # éè¡¨æ ¼åˆ†å—ä¿æŒåŸæ ·
                    doc = Document(
                        page_content=chunk.content,
                        metadata={
                            'chunk_index': chunk.chunk_index,
                            'chunk_type': chunk.chunk_type,
                            'document_name': chunk.document_name,
                            'page_number': chunk.page_number,
                            'table_id': chunk.table_id if hasattr(chunk, 'table_id') else None,
                            'table_type': chunk.table_type if hasattr(chunk, 'table_type') else None
                        }
                    )
                    document_chunks.append(doc)
            
            # è°ƒç”¨æ–°çš„è¡¨æ ¼å¤„ç†é€»è¾‘
            enhanced_chunks = table_processor.process_tables(document_chunks)
            if enhanced_chunks:
                logger.info(f"è¡¨æ ¼å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº† {len(enhanced_chunks)} ä¸ªå¢å¼ºåˆ†å—")
                # å°†å¢å¼ºåçš„ Document å¯¹è±¡è½¬æ¢å› EnhancedDocumentChunk å¯¹è±¡
                enhanced_table_chunks = []
                for doc in enhanced_chunks:
                    if doc.metadata.get('chunk_type') == 'table':
                        chunk = EnhancedDocumentChunk(
                            content=doc.page_content,  # è¿™é‡Œåº”è¯¥æ˜¯åŸå§‹HTMLå†…å®¹
                            document_name=doc.metadata.get('document_name', doc_name),
                            page_number=doc.metadata.get('page_number', 1),
                            chunk_index=doc.metadata.get('chunk_index', 0),
                            chunk_type="table",
                            table_id=doc.metadata.get('table_id', 'unknown'),
                            table_type=doc.metadata.get('table_type', 'æœªçŸ¥è¡¨æ ¼'),
                            table_title=doc.metadata.get('table_title', ''),
                            table_summary=doc.metadata.get('table_summary', ''),
                            table_headers=doc.metadata.get('table_headers', []),
                            related_text=doc.metadata.get('related_text', ''),
                            processed_table_content=doc.metadata.get('processed_table_content', ''),
                            table_row_count=doc.metadata.get('table_row_count', 0),
                            table_column_count=doc.metadata.get('table_column_count', 0)
                        )
                        enhanced_table_chunks.append(chunk)
                    else:
                        chunk = EnhancedDocumentChunk(
                            content=doc.page_content,
                            document_name=doc.metadata.get('document_name', doc_name),
                            page_number=doc.metadata.get('page_number', 1),
                            chunk_index=doc.metadata.get('chunk_index', 0),
                            chunk_type=doc.metadata.get('chunk_type', 'text')
                        )
                        enhanced_table_chunks.append(chunk)
                
                # ç”¨å¢å¼ºåçš„è¡¨æ ¼åˆ†å—æ›¿æ¢åŸå§‹è¡¨æ ¼åˆ†å—
                chunks = [chunk for chunk in chunks if chunk.chunk_type != 'table' or '_sub_' in chunk.table_id]
                chunks.extend(enhanced_table_chunks)
                logger.info(f"è¡¨æ ¼å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç”Ÿæˆäº† {len(chunks)} ä¸ªåˆ†å—")
            else:
                logger.warning("è¡¨æ ¼å¤„ç†è¿”å›ç©ºç»“æœï¼Œä½¿ç”¨åŸå§‹åˆ†å—")
                
        except Exception as e:
            logger.error(f"è°ƒç”¨æ–°çš„è¡¨æ ¼å¤„ç†é€»è¾‘æ—¶å‡ºé”™: {e}")
            logger.info("ç»§ç»­ä½¿ç”¨åŸå§‹è¡¨æ ¼å¤„ç†é€»è¾‘")
        
        return chunks
    
    def chunk_documents(self, documents: List[Dict[str, Any]]) -> List[EnhancedDocumentChunk]:
        """
        å¯¹å¤šä¸ªæ–‡æ¡£è¿›è¡Œåˆ†å—å¤„ç†
        :param documents: æ–‡æ¡£åˆ—è¡¨
        :return: æ‰€æœ‰æ–‡æ¡£çš„åˆ†å—åˆ—è¡¨
        """
        all_chunks = []
        for document in documents:
            chunks = self.chunk_document(document)
            all_chunks.extend(chunks)
        return all_chunks

    # ä»¥ä¸‹ä¸¤ä¸ªå‡½æ•°éƒ½æ˜¯ä»è€ä»£ç ç§»æ¤è¿‡æ¥çš„ï¼Œä½†åœ¨å½“å‰å®ç°ä¸­éƒ½æ²¡æœ‰è¢«ä½¿ç”¨
    # ç®€åŒ–å®ç°ï¼šå½“å‰çš„åˆ†å—å®ç°ç›´æ¥ä»JSONæ•°æ®ä¸­è·å–page_idxï¼Œç„¶åè½¬æ¢ä¸ºé¡µç 
    # ä¸éœ€è¦å¤æ‚æ˜ å°„ï¼šå› ä¸ºJSONæ•°æ®å·²ç»åŒ…å«äº†æ¯ä¸ªæ–‡æœ¬å—å’Œè¡¨æ ¼å—çš„é¡µç ä¿¡æ¯
    # def _build_page_mapping(self, text_with_pages: List[Dict[str, Any]]) -> Dict[Tuple[int, int], int]:
    #     """
    #     æ„å»ºæ–‡æœ¬ä½ç½®åˆ°é¡µç çš„æ˜ å°„ï¼ˆä»è€ä»£ç ç§»æ¤ï¼‰
    #     :param text_with_pages: åŒ…å«é¡µç ä¿¡æ¯çš„æ–‡æœ¬åˆ—è¡¨
    #     :return: ä½ç½®åˆ°é¡µç çš„æ˜ å°„å­—å…¸
    #     """
    #     page_mapping = {}
    #     current_pos = 0
        
    #     for item in text_with_pages:
    #         text = item.get('text', '')
    #         page = item.get('page', 0)
    #         text_length = len(text)
            
    #         # ä¸ºæ–‡æœ¬çš„æ¯ä¸ªå­—ç¬¦ä½ç½®åˆ†é…é¡µç 
    #         for i in range(text_length):
    #             page_mapping[(current_pos + i, current_pos + i + 1)] = page
            
    #         current_pos += text_length
        
    #     return page_mapping
    
    # def _find_most_frequent_page(self, chunk: str, full_text: str, page_mapping: Dict[Tuple[int, int], int]) -> int:
    #     """
    #     æ‰¾åˆ°åˆ†å—ä¸­å‡ºç°æœ€é¢‘ç¹çš„é¡µç ï¼ˆä»è€ä»£ç ç§»æ¤ï¼‰
    #     :param chunk: åˆ†å—å†…å®¹
    #     :param full_text: å®Œæ•´æ–‡æœ¬
    #     :param page_mapping: é¡µç æ˜ å°„
    #     :return: æœ€é¢‘ç¹çš„é¡µç 
    #     """
    #     chunk_start = full_text.find(chunk)
    #     if chunk_start == -1:
    #         return 0
        
    #     chunk_end = chunk_start + len(chunk)
    #     page_counts = {}
        
    #     # ç»Ÿè®¡åˆ†å—ä¸­æ¯ä¸ªå­—ç¬¦ä½ç½®å¯¹åº”çš„é¡µç 
    #     for pos in range(chunk_start, chunk_end):
    #         for (start, end), page in page_mapping.items():
    #             if start <= pos < end:
    #                 page_counts[page] = page_counts.get(page, 0) + 1
    #                 break
        
    #     # è¿”å›å‡ºç°æ¬¡æ•°æœ€å¤šçš„é¡µç 
    #     if page_counts:
    #         return max(page_counts.items(), key=lambda x: x[1])[0]
    #     return 0

    def _validate_and_truncate_chunk(self, content: str, content_type: str) -> str:
        """
        éªŒè¯å¹¶æ™ºèƒ½å¤„ç†åˆ†å—å†…å®¹ï¼Œé¿å…ç®€å•æˆªæ–­å¯¼è‡´çš„ä¿¡æ¯ä¸¢å¤±
        
        :param content: åŸå§‹å†…å®¹
        :param content_type: å†…å®¹ç±»å‹ï¼ˆæ–‡æœ¬/è¡¨æ ¼ï¼‰
        :return: å¤„ç†åçš„å†…å®¹
        """
        original_length = len(content)
        
        if original_length <= self.max_chunk_length:
            return content
        
        # è®°å½•å¤„ç†ä¿¡æ¯
        print(f"ğŸ“Š æ£€æµ‹åˆ°è¶…é•¿{content_type}å†…å®¹: {original_length}å­—ç¬¦ > {self.max_chunk_length}å­—ç¬¦é™åˆ¶")
        print(f"ğŸ”§ å¼€å§‹è¿›è¡Œæ™ºèƒ½å¤„ç†...")
        
        if content_type == "è¡¨æ ¼":
            result = self._smart_table_chunking(content)
        else:
            result = self._smart_text_chunking(content)
        
        # è®°å½•å¤„ç†ç»“æœ
        processed_length = len(result)
        print(f"âœ… {content_type}å¤„ç†å®Œæˆ: {original_length}å­—ç¬¦ â†’ {processed_length}å­—ç¬¦")
        
        return result
    
    def _smart_table_chunking(self, table_content: str) -> str:
        """
        æ™ºèƒ½è¡¨æ ¼åˆ†å—ï¼šå°†å¤§è¡¨æ ¼åˆ†è§£ä¸ºå¤šä¸ªé€»è¾‘ç›¸å…³çš„å­è¡¨æ ¼
        
        :param table_content: åŸå§‹è¡¨æ ¼å†…å®¹
        :return: å¤„ç†åçš„è¡¨æ ¼å†…å®¹
        """
        table_length = len(table_content)
        threshold = self.max_chunk_length * 1.5
        
        print(f"ğŸ“‹ è¡¨æ ¼é•¿åº¦åˆ†æ: {table_length}å­—ç¬¦")
        print(f"ğŸ” å¤„ç†ç­–ç•¥åˆ¤æ–­: é˜ˆå€¼ = {threshold}å­—ç¬¦")
        
        # å¦‚æœè¡¨æ ¼å†…å®¹ä¸æ˜¯ç‰¹åˆ«é•¿ï¼Œå°è¯•ä¼˜åŒ–æ ¼å¼
        if table_length <= threshold:
            print(f"ğŸ“ é€‰æ‹©æ ¼å¼ä¼˜åŒ–ç­–ç•¥ (â‰¤ {threshold}å­—ç¬¦)")
            return self._optimize_table_format(table_content)
        
        # å¯¹äºè¶…é•¿è¡¨æ ¼ï¼Œè¿›è¡Œæ™ºèƒ½åˆ†å—
        print(f"âœ‚ï¸ é€‰æ‹©æˆªæ–­å¤„ç†ç­–ç•¥ (> {threshold}å­—ç¬¦)")
        return self._split_large_table(table_content)
    
    def _smart_text_chunking(self, text_content: str) -> str:
        """
        æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼šåœ¨åˆé€‚çš„ä½ç½®æˆªæ–­ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        
        :param text_content: åŸå§‹æ–‡æœ¬å†…å®¹
        :return: å¤„ç†åçš„æ–‡æœ¬å†…å®¹
        """
        if len(text_content) <= self.max_chunk_length:
            return text_content
        
        # å°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦ç­‰ä½ç½®æˆªæ–­
        for separator in ["ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", "\n\n", "\n"]:
            last_sep_pos = text_content[:self.max_chunk_length].rfind(separator)
            if last_sep_pos > self.max_chunk_length * 0.8:  # åœ¨80%ä½ç½®ä¹‹åæ‰¾åˆ°åˆ†éš”ç¬¦
                truncated_text = text_content[:last_sep_pos + len(separator)]
                # æ·»åŠ æˆªæ–­æ ‡è®°
                return truncated_text + f"\n[å†…å®¹å·²æˆªæ–­ï¼ŒåŸå§‹é•¿åº¦: {len(text_content)} å­—ç¬¦]"
        
        # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„åˆ†éš”ç¬¦ï¼Œç›´æ¥æˆªæ–­å¹¶æ·»åŠ æ ‡è®°
        return text_content[:self.max_chunk_length] + f"\n[å†…å®¹å·²æˆªæ–­ï¼ŒåŸå§‹é•¿åº¦: {len(text_content)} å­—ç¬¦]"
    
    def _optimize_table_format(self, table_content: str) -> str:
        """
        ä¼˜åŒ–è¡¨æ ¼æ ¼å¼ï¼Œå»é™¤å†—ä½™ä¿¡æ¯ï¼Œä¿æŒæ ¸å¿ƒæ•°æ®
        
        :param table_content: åŸå§‹è¡¨æ ¼å†…å®¹
        :return: ä¼˜åŒ–åçš„è¡¨æ ¼å†…å®¹
        """
        original_length = len(table_content)
        print(f"ğŸ”§ å¼€å§‹è¡¨æ ¼æ ¼å¼ä¼˜åŒ–...")
        
        # ç®€åŒ–è¡¨æ ¼ç»“æ„è¯´æ˜
        lines = table_content.split('\n')
        optimized_lines = []
        optimized_count = 0
        
        for line in lines:
            # ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œç®€åŒ–å†—é•¿çš„æè¿°
            if line.startswith('è¡¨æ ¼ç±»å‹:') or line.startswith('è¡¨æ ¼ID:'):
                optimized_lines.append(line)
            elif line.startswith('è¡Œæ•°:') or line.startswith('åˆ—æ•°:'):
                optimized_lines.append(line)
            elif line.startswith('åˆ—æ ‡é¢˜ï¼ˆå­—æ®µå®šä¹‰ï¼‰:'):
                # ç®€åŒ–åˆ—æ ‡é¢˜æè¿°
                if len(line) > 100:
                    optimized_lines.append(line[:100] + "...")
                    optimized_count += 1
                else:
                    optimized_lines.append(line)
            elif line.startswith('æ•°æ®è®°å½•:'):
                optimized_lines.append(line)
            elif line.startswith('  è®°å½•'):
                # ä¿ç•™æ•°æ®è®°å½•ï¼Œä½†é™åˆ¶é•¿åº¦
                if len(line) > 150:
                    optimized_lines.append(line[:150] + "...")
                    optimized_count += 1
                else:
                    optimized_lines.append(line)
            else:
                # å…¶ä»–è¡Œç›´æ¥ä¿ç•™
                optimized_lines.append(line)
        
        result = '\n'.join(optimized_lines)
        result_length = len(result)
        
        print(f"ğŸ“ æ ¼å¼ä¼˜åŒ–å®Œæˆ: {original_length}å­—ç¬¦ â†’ {result_length}å­—ç¬¦")
        if optimized_count > 0:
            print(f"âœ¨ ä¼˜åŒ–äº† {optimized_count} è¡Œå†…å®¹")
        
        # æ·»åŠ ä¼˜åŒ–æ ‡è®°
        if result_length < original_length:
            result += f"\n[è¡¨æ ¼æ ¼å¼å·²ä¼˜åŒ–ï¼ŒåŸå§‹é•¿åº¦: {original_length} å­—ç¬¦]"
        
        return result
    
    def _split_large_table(self, table_content: str) -> str:
        """
        å°†å¤§è¡¨æ ¼åˆ†å‰²ä¸ºå¤šä¸ªå­è¡¨æ ¼
        
        :param table_content: åŸå§‹è¡¨æ ¼å†…å®¹
        :return: åˆ†å‰²åçš„è¡¨æ ¼å†…å®¹
        """
        original_length = len(table_content)
        print(f"âœ‚ï¸ å¼€å§‹å¤§è¡¨æ ¼æˆªæ–­å¤„ç†...")
        
        lines = table_content.split('\n')
        
        # æå–è¡¨æ ¼å¤´éƒ¨ä¿¡æ¯
        header_lines = []
        data_lines = []
        in_data_section = False
        
        for line in lines:
            if line.startswith('æ•°æ®è®°å½•:'):
                in_data_section = True
                header_lines.append(line)
                continue
            
            if in_data_section:
                data_lines.append(line)
            else:
                header_lines.append(line)
        
        print(f"ğŸ“Š è¡¨æ ¼ç»“æ„åˆ†æ: å¤´éƒ¨{len(header_lines)}è¡Œï¼Œæ•°æ®{len(data_lines)}è¡Œ")
        
        # åˆå§‹åŒ–æˆªæ–­æ ‡è®°å’Œå¤„ç†ä¿¡æ¯
        truncation_mark = ""
        processing_info = f"\n[è¡¨æ ¼å·²è¿›è¡Œæ™ºèƒ½æˆªæ–­å¤„ç†ï¼ŒåŸå§‹é•¿åº¦: {original_length} å­—ç¬¦]"
        
        # å¦‚æœæ•°æ®è¡Œå¤ªå¤šï¼Œåªä¿ç•™å‰å‡ è¡Œå’Œåå‡ è¡Œ
        if len(data_lines) > 20:
            total_data_rows = len(data_lines)
            kept_rows = 15  # å‰10è¡Œ + å5è¡Œ
            truncated_rows = total_data_rows - kept_rows
            
            print(f"âš ï¸ æ•°æ®è¡Œè¿‡å¤š({total_data_rows}è¡Œ)ï¼Œè¿›è¡Œæˆªæ–­å¤„ç†:")
            print(f"   - ä¿ç•™å‰10è¡Œæ•°æ®")
            print(f"   - ä¿ç•™å5è¡Œæ•°æ®")
            print(f"   - æˆªæ–­ä¸­é—´{truncated_rows}è¡Œæ•°æ®")
            
            # ä¿ç•™å‰10è¡Œå’Œå5è¡Œ
            selected_data = data_lines[:10] + [f'  ... (ä¸­é—´{truncated_rows}è¡Œæ•°æ®çœç•¥) ...'] + data_lines[-5:]
            data_lines = selected_data
            
            # è®¾ç½®æˆªæ–­å¤„ç†æ ‡è®°
            truncation_mark = f"\n[è¡¨æ ¼æ•°æ®è¡Œå·²æˆªæ–­å¤„ç†: åŸå§‹{total_data_rows}è¡Œ â†’ ä¿ç•™{kept_rows}è¡Œï¼Œæˆªæ–­{truncated_rows}è¡Œ]"
        else:
            print(f"âœ… æ•°æ®è¡Œæ•°é‡é€‚ä¸­({len(data_lines)}è¡Œ)ï¼Œæ— éœ€è¡Œæ•°æˆªæ–­")
        
        # é‡æ–°ç»„åˆ
        result_lines = header_lines + data_lines
        result_content = '\n'.join(result_lines)
        
        # æ€»æ˜¯æ·»åŠ å¤„ç†ä¿¡æ¯æ ‡è®°
        result_content += processing_info
        
        # å¦‚æœæœ‰è¡Œæ•°æˆªæ–­ï¼Œä¹Ÿæ·»åŠ è¡Œæ•°æˆªæ–­æ ‡è®°
        if truncation_mark:
            result_content += truncation_mark
        
        result_length = len(result_content)
        print(f"ğŸ“‹ æˆªæ–­å¤„ç†å®Œæˆ: {original_length}å­—ç¬¦ â†’ {result_length}å­—ç¬¦")
        
        # å¦‚æœä»ç„¶è¶…é•¿ï¼Œè¿›è¡Œæœ€ç»ˆæˆªæ–­
        if result_length > self.max_chunk_length:
            print(f"âš ï¸ å¤„ç†åä»è¶…é•¿ï¼Œè¿›è¡Œæœ€ç»ˆæˆªæ–­...")
            final_length = self.max_chunk_length
            
            # è®¡ç®—å¯ä»¥ä¿ç•™çš„æ ‡è®°é•¿åº¦
            # ä¿ç•™å¤„ç†ä¿¡æ¯æ ‡è®°å’Œè¡Œæ•°æˆªæ–­æ ‡è®°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            marks_to_keep = processing_info
            if truncation_mark:
                marks_to_keep += truncation_mark
            
            marks_length = len(marks_to_keep)
            available_length = final_length - marks_length
            
            # æˆªæ–­ä¸»è¦å†…å®¹ï¼Œä¿ç•™æ ‡è®°
            if available_length > 0:
                result_content = result_content[:available_length] + marks_to_keep
                # æ·»åŠ æœ€ç»ˆæˆªæ–­æ ‡è®°
                result_content += f"\n[è¡¨æ ¼å†…å®¹å·²æˆªæ–­å¤„ç†ï¼ŒåŸå§‹é•¿åº¦: {original_length} å­—ç¬¦]"
            else:
                # å¦‚æœæ ‡è®°å¤ªé•¿ï¼Œåªä¿ç•™æœ€ç»ˆæˆªæ–­æ ‡è®°
                result_content = result_content[:final_length] + f"\n[è¡¨æ ¼å†…å®¹å·²æˆªæ–­å¤„ç†ï¼ŒåŸå§‹é•¿åº¦: {original_length} å­—ç¬¦]"
            
            print(f"ğŸ”š æœ€ç»ˆå¤„ç†å®Œæˆ: {original_length}å­—ç¬¦ â†’ {len(result_content)}å­—ç¬¦")
        
        return result_content 

def process_documents_with_tables(md_dir: str = None, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[EnhancedDocumentChunk]:
    """
    å¤„ç†æ–‡æ¡£å¹¶åŒ…å«è¡¨æ ¼çš„å®Œæ•´æµç¨‹
    :param md_dir: markdownæ–‡ä»¶ç›®å½•
    :param chunk_size: åˆ†å—å¤§å°
    :param chunk_overlap: åˆ†å—é‡å å¤§å°
    :return: å¢å¼ºæ–‡æ¡£åˆ†å—åˆ—è¡¨
    """
    if not md_dir:
        print("é”™è¯¯: æœªæŒ‡å®šmarkdownæ–‡ä»¶ç›®å½•")
        return []
    
    try:
        # åŠ è½½æ–‡æ¡£
        loader = EnhancedDocumentLoader(md_dir)
        documents = loader.load_documents()
        
        if not documents:
            print(f"è­¦å‘Š: åœ¨ç›®å½• {md_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£")
            return []
        
        # åˆ†å—å¤„ç†
        chunker = EnhancedSemanticChunker(chunk_size, chunk_overlap)
        chunks = chunker.chunk_documents(documents)
        
        print(f"æˆåŠŸå¤„ç† {len(documents)} ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
        
        # ç»Ÿè®¡åˆ†å—ç±»å‹
        text_chunks = [c for c in chunks if c.chunk_type == "text"]
        table_chunks = [c for c in chunks if c.chunk_type == "table"]
        
        print(f"  - æ–‡æœ¬åˆ†å—: {len(text_chunks)} ä¸ª")
        print(f"  - è¡¨æ ¼åˆ†å—: {len(table_chunks)} ä¸ª")
        
        return chunks
        
    except Exception as e:
        print(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
        return [] 