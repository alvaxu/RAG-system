"""
V3ç‰ˆæœ¬ä¸»å¤„ç†å™¨

V3ç‰ˆæœ¬å‘é‡æ•°æ®åº“æ„å»ºç³»ç»Ÿçš„æ ¸å¿ƒæ§åˆ¶å™¨ï¼Œè´Ÿè´£ç»Ÿä¸€ç®¡ç†æ•´ä¸ªæ–‡æ¡£å¤„ç†æµç¨‹ã€‚
"""

import os
import json
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path

from config.config_manager import ConfigManager
from utils.document_type_detector import DocumentTypeDetector
from utils.model_caller import ModelCaller
from .content_processor import ContentProcessor
from .vectorization_manager import VectorizationManager
from .metadata_manager import MetadataManager
from .vector_store_manager import VectorStoreManager

class V3MainProcessor:
    """
    V3ç‰ˆæœ¬ä¸»å¤„ç†å™¨

    åŠŸèƒ½ï¼š
    - ç»Ÿä¸€çš„ç¨‹åºå…¥å£å’Œæµç¨‹æ§åˆ¶
    - æ™ºèƒ½æ¨¡å¼é€‰æ‹©ï¼ˆæ–°å»º vs å¢é‡ï¼‰
    - ä¸æ‰€æœ‰å­æ¨¡å—çš„æ·±åº¦é›†æˆ
    - é…ç½®ç®¡ç†å’ŒéªŒè¯
    - å¤±è´¥å¤„ç†å’ŒçŠ¶æ€è·Ÿè¸ª
    """

    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–ä¸»å¤„ç†å™¨

        :param config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        # 1. åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager(config_path)

        # 2. åŠ è½½é…ç½®
        if not self.config_manager.load_config():
            raise RuntimeError("é…ç½®åŠ è½½å¤±è´¥")

        # 3. åˆå§‹åŒ–å­æ¨¡å—
        self._initialize_modules()

        # 4. éªŒè¯ç¯å¢ƒ
        self._validate_environment()

        logging.info("V3MainProcessoråˆå§‹åŒ–å®Œæˆ")

    def _initialize_modules(self):
        """åˆå§‹åŒ–æ‰€æœ‰å­æ¨¡å—"""
        try:
            # æ–‡æ¡£ç±»å‹æ£€æµ‹å™¨
            self.document_type_detector = DocumentTypeDetector(self.config_manager)

            # AIæ¨¡å‹è°ƒç”¨å™¨
            self.model_caller = ModelCaller(self.config_manager)

            # å†…å®¹å¤„ç†å™¨
            self.content_processor = ContentProcessor(self.config_manager, self.model_caller)

            # å‘é‡åŒ–ç®¡ç†å™¨
            self.vectorization_manager = VectorizationManager(self.config_manager, self.model_caller)

            # å…ƒæ•°æ®ç®¡ç†å™¨
            self.metadata_manager = MetadataManager(self.config_manager)

            # å‘é‡å­˜å‚¨ç®¡ç†å™¨
            self.vector_store_manager = VectorStoreManager(self.config_manager)

            # å¤±è´¥å¤„ç†å™¨ï¼ˆé€šè¿‡é…ç½®ç®¡ç†å™¨è·å–ï¼‰
            self.failure_handler = self.config_manager.get_failure_handler()

            logging.info("æ‰€æœ‰å­æ¨¡å—åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logging.error(f"å­æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _validate_environment(self):
        """éªŒè¯ç¯å¢ƒé…ç½®"""
        try:
            # éªŒè¯ç¯å¢ƒå˜é‡
            from config.environment_manager import environment_manager
            if not environment_manager.setup_environment():
                environment_manager.print_environment_setup_guide()
                raise RuntimeError("ç¯å¢ƒå˜é‡éªŒè¯å¤±è´¥")

            # éªŒè¯è·¯å¾„é…ç½®
            paths = self.config_manager.get('paths', {})
            for path_key, path_value in paths.items():
                if path_value and not os.path.exists(path_value):
                    os.makedirs(path_value, exist_ok=True)
                    logging.info(f"åˆ›å»ºç›®å½•: {path_value}")

            logging.info("ç¯å¢ƒéªŒè¯å®Œæˆ")

        except Exception as e:
            logging.error(f"ç¯å¢ƒéªŒè¯å¤±è´¥: {e}")
            raise

    def process_documents(self, input_type: str = None, input_path: str = None,
                         output_path: str = None) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£ï¼ˆå¸¦é»˜è®¤å€¼æ”¯æŒï¼‰

        :param input_type: è¾“å…¥ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼'pdf'
        :param input_path: è¾“å…¥è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        :param output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        :return: å¤„ç†ç»“æœ
        """
        try:
            print("=" * 50)
            print("      V3ç‰ˆæœ¬å‘é‡æ•°æ®åº“æ„å»ºç³»ç»Ÿ")
            print("=" * 50)

            # 1. ä½¿ç”¨DocumentTypeDetectoréªŒè¯è¾“å…¥ç±»å‹å’Œè·¯å¾„
            validation_result = self.document_type_detector.validate_input_type(
                input_type, input_path, output_path
            )

            if not validation_result['valid']:
                error_msg = f"è¾“å…¥éªŒè¯å¤±è´¥: {validation_result.get('message', 'æœªçŸ¥é”™è¯¯')}"
                print(f"âŒ {error_msg}")
                return {
                    'success': False,
                    'error': error_msg,
                    'validation_result': validation_result
                }

            # 2. æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
            self._display_processing_info(validation_result)

            # 3. æ ¹æ®è¾“å…¥ç±»å‹é€‰æ‹©å¤„ç†æµç¨‹
            if validation_result['input_type'] == 'pdf':
                result = self._process_from_pdf(validation_result)
            elif validation_result['input_type'] == 'mineru_output':
                result = self._process_from_mineru_output(validation_result)
            else:
                error_msg = f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {validation_result['input_type']}"
                print(f"âŒ {error_msg}")
                return {
                    'success': False,
                    'error': error_msg
                }

            # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_result = self._generate_final_report(result)
            return final_result

        except Exception as e:
            error_msg = f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            logging.error(error_msg, exc_info=True)

            # è®°å½•å¤±è´¥ä¿¡æ¯
            if hasattr(self, 'failure_handler') and self.failure_handler:
                self.failure_handler.record_failure(
                    image_info={'operation': 'document_processing'},
                    error_type='processing_error',
                    error_message=str(e)
                )

            return {
                'success': False,
                'error': error_msg,
                'exception': str(e)
            }

    def _display_processing_info(self, validation_result: Dict[str, Any]):
        """æ˜¾ç¤ºå¤„ç†ä¿¡æ¯"""
        print("\nğŸ“‹ å¤„ç†ä¿¡æ¯:")
        print(f"   è¾“å…¥ç±»å‹: {validation_result['input_type']}")
        print(f"   è¾“å…¥è·¯å¾„: {validation_result['input_path']}")
        print(f"   è¾“å‡ºè·¯å¾„: {validation_result['output_path']}")
        print(f"   æ˜¯å¦éœ€è¦minerU: {'æ˜¯' if validation_result['needs_mineru'] else 'å¦'}")

        if validation_result.get('file_count'):
            print(f"   æ–‡ä»¶æ•°é‡: {validation_result['file_count']}")

        if validation_result.get('file_size'):
            size_mb = validation_result['file_size'] / (1024 * 1024)
            print(f"   æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")

        print(f"   æè¿°: {validation_result['description']}")

    def _process_from_pdf(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä»PDFå¼€å§‹å¤„ç†"""
        print("\nğŸš€ ä»PDFå¼€å§‹å¤„ç†")
        print("   æ³¨æ„: ä»PDFå¼€å§‹å¤„ç†éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ŒåŒ…å«minerUè§£ææ­¥éª¤")

        # 1. æ£€æŸ¥ç›®æ ‡å‘é‡æ•°æ®åº“çŠ¶æ€
        target_vector_db = validation_result['output_path']
        db_exists = self._check_vector_db_exists(target_vector_db)

        # 2. æ™ºèƒ½é€‰æ‹©æ¨¡å¼
        if db_exists:
            print("   ğŸ“Š æ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼")
            result = self._incremental_process(validation_result, target_vector_db)
        else:
            print("   ğŸ†• æœªæ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨æ–°å»ºæ¨¡å¼")
            result = self._new_process(validation_result, target_vector_db)

        return result

    def _process_from_mineru_output(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä»minerUè¾“å‡ºå¼€å§‹å¤„ç†"""
        print("\nâš¡ ä»minerUè¾“å‡ºå¼€å§‹å¤„ç†")
        print("   æ³¨æ„: ä»minerUè¾“å‡ºå¼€å§‹å¤„ç†ï¼Œè·³è¿‡è§£ææ­¥éª¤ï¼Œé€Ÿåº¦è¾ƒå¿«")

        # 1. æ£€æŸ¥ç›®æ ‡å‘é‡æ•°æ®åº“çŠ¶æ€
        target_vector_db = validation_result['output_path']
        db_exists = self._check_vector_db_exists(target_vector_db)

        # 2. æ™ºèƒ½é€‰æ‹©æ¨¡å¼
        if db_exists:
            print("   ğŸ“Š æ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼")
            result = self._incremental_process(validation_result, target_vector_db)
        else:
            print("   ğŸ†• æœªæ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨æ–°å»ºæ¨¡å¼")
            result = self._new_process(validation_result, target_vector_db)

        return result

    def _check_vector_db_exists(self, target_vector_db: str) -> bool:
        """æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨"""
        index_file = os.path.join(target_vector_db, 'index.faiss')
        metadata_file = os.path.join(target_vector_db, 'metadata.pkl')

        exists = os.path.exists(index_file) and os.path.exists(metadata_file)

        if exists:
            logging.info(f"æ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“: {target_vector_db}")
        else:
            logging.info(f"æœªæ£€æµ‹åˆ°å‘é‡æ•°æ®åº“: {target_vector_db}")

        return exists

    def _new_process(self, validation_result: Dict[str, Any], target_vector_db: str) -> Dict[str, Any]:
        """æ–°å»ºæ¨¡å¼å¤„ç†"""
        try:
            print("\nğŸ—ï¸  å¼€å§‹æ–°å»ºæ¨¡å¼å¤„ç†...")

            # 1. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            success = self.vector_store_manager.create_vector_store(
                dimension=1536,  # é»˜è®¤å‘é‡ç»´åº¦
                index_type='faiss'
            )

            if not success:
                raise RuntimeError("å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")

            # 2. å¤„ç†æ–‡æ¡£å†…å®¹
            processing_result = self._process_documents_new(validation_result)

            # 3. å­˜å‚¨ç»“æœ
            storage_result = self._store_results(processing_result, target_vector_db)

            # 4. ç”ŸæˆæŠ¥å‘Š
            result = {
                'success': True,
                'mode': 'new',
                'processing_result': processing_result,
                'storage_result': storage_result,
                'target_vector_db': target_vector_db
            }

            print("âœ… æ–°å»ºæ¨¡å¼å¤„ç†å®Œæˆ")
            return result

        except Exception as e:
            logging.error(f"æ–°å»ºæ¨¡å¼å¤„ç†å¤±è´¥: {e}")
            raise

    def _incremental_process(self, validation_result: Dict[str, Any], target_vector_db: str) -> Dict[str, Any]:
        """å¢é‡æ¨¡å¼å¤„ç†"""
        try:
            print("\nğŸ”„ å¼€å§‹å¢é‡æ¨¡å¼å¤„ç†...")

            # 1. åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“
            # è¿™é‡Œéœ€è¦å®ç°åŠ è½½é€»è¾‘

            # 2. å¤„ç†æ–°å¢æ–‡æ¡£å†…å®¹
            processing_result = self._process_documents_incremental(validation_result)

            # 3. æ›´æ–°å‘é‡æ•°æ®åº“
            storage_result = self._update_results(processing_result, target_vector_db)

            # 4. ç”ŸæˆæŠ¥å‘Š
            result = {
                'success': True,
                'mode': 'incremental',
                'processing_result': processing_result,
                'storage_result': storage_result,
                'target_vector_db': target_vector_db
            }

            print("âœ… å¢é‡æ¨¡å¼å¤„ç†å®Œæˆ")
            return result

        except Exception as e:
            logging.error(f"å¢é‡æ¨¡å¼å¤„ç†å¤±è´¥: {e}")
            raise

    def _process_documents_new(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ–°å»ºæ¨¡å¼æ–‡æ¡£å¤„ç†"""
        try:
            print("   ğŸ“„ å¤„ç†æ–‡æ¡£å†…å®¹...")
            
            # è·å–è¾“å…¥è·¯å¾„
            input_path = validation_result.get('input_path', './document/orig_pdf')
            output_path = validation_result.get('output_path', './document/md')
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_path, exist_ok=True)
            
            # è·å–PDFæ–‡ä»¶åˆ—è¡¨
            pdf_files = self._get_pdf_files(input_path)
            if not pdf_files:
                return {
                    'processed_items': [],
                    'total_files': 0,
                    'status': 'success',
                    'message': 'æœªæ‰¾åˆ°PDFæ–‡ä»¶'
                }
            
            print(f"   å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
            
            # å¤„ç†æ¯ä¸ªPDFæ–‡ä»¶
            processed_items = []
            total_size = 0
            
            for i, pdf_file in enumerate(pdf_files):
                print(f"   å¤„ç†ç¬¬ {i+1}/{len(pdf_files)} ä¸ªæ–‡ä»¶: {os.path.basename(pdf_file)}")
                
                try:
                    # è§£æPDF
                    parse_result = self._parse_pdf_with_mineru(pdf_file, output_path)
                    
                    if parse_result.get('success'):
                        # å‘é‡åŒ–å†…å®¹
                        vectorization_result = self._vectorize_parsed_content(parse_result['parsed_content'])
                        
                        # åˆå¹¶ç»“æœ
                        item_result = {
                            'pdf_path': pdf_file,
                            'parse_result': parse_result,
                            'vectorization_result': vectorization_result,
                            'file_size': parse_result.get('file_size', 0),
                            'status': 'success'
                        }
                        
                        processed_items.append(item_result)
                        total_size += item_result['file_size']
                        
                        print(f"     âœ… å¤„ç†æˆåŠŸ")
                    else:
                        print(f"     âŒ å¤„ç†å¤±è´¥: {parse_result.get('error')}")
                        
                        # è®°å½•å¤±è´¥é¡¹
                        processed_items.append({
                            'pdf_path': pdf_file,
                            'status': 'failed',
                            'error': parse_result.get('error'),
                            'file_size': 0
                        })
                
                except Exception as e:
                    logging.error(f"å¤„ç†PDFæ–‡ä»¶å¤±è´¥: {pdf_file}, é”™è¯¯: {e}")
                    print(f"     âŒ å¤„ç†å¼‚å¸¸: {e}")
                    
                    processed_items.append({
                        'pdf_path': pdf_file,
                        'status': 'failed',
                        'error': str(e),
                        'file_size': 0
                    })
            
            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for item in processed_items if item.get('status') == 'success')
            failed_count = len(processed_items) - success_count
            
            print(f"   å¤„ç†å®Œæˆ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
            
            return {
                'processed_items': processed_items,
                'total_files': len(pdf_files),
                'success_count': success_count,
                'failed_count': failed_count,
                'total_size': total_size,
                'status': 'success'
            }
            
        except Exception as e:
            logging.error(f"æ–°å»ºæ¨¡å¼æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return {
                'processed_items': [],
                'total_files': 0,
                'status': 'failed',
                'error': str(e)
            }

    def _process_documents_incremental(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """å¢é‡æ¨¡å¼æ–‡æ¡£å¤„ç†"""
        # å ä½ç¬¦å®ç°
        print("   ğŸ“„ å¤„ç†æ–°å¢æ–‡æ¡£å†…å®¹...")

        return {
            'processed_items': [],
            'new_files': validation_result.get('file_count', 0),
            'status': 'success'
        }

    def _store_results(self, processing_result: Dict[str, Any], target_vector_db: str) -> Dict[str, Any]:
        """å­˜å‚¨å¤„ç†ç»“æœ"""
        try:
            print("   ğŸ’¾ å­˜å‚¨å¤„ç†ç»“æœ...")
            
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            os.makedirs(target_vector_db, exist_ok=True)
            
            # è·å–å¤„ç†çš„é¡¹ç›®
            processed_items = processing_result.get('processed_items', [])
            if not processed_items:
                print("     æ²¡æœ‰éœ€è¦å­˜å‚¨çš„å†…å®¹")
                return {
                    'stored_items': 0,
                    'storage_path': target_vector_db,
                    'status': 'success',
                    'message': 'æ²¡æœ‰éœ€è¦å­˜å‚¨çš„å†…å®¹'
                }
            
            # ç»Ÿè®¡å‘é‡æ•°é‡
            total_vectors = 0
            total_metadata = 0
            
            # æ”¶é›†æ‰€æœ‰å‘é‡å’Œå…ƒæ•°æ®
            all_vectors = []
            all_metadata = []
            
            for item in processed_items:
                if item.get('status') == 'success':
                    vectorization_result = item.get('vectorization_result', {})
                    
                    # æ”¶é›†æ–‡æœ¬å‘é‡
                    text_vectors = vectorization_result.get('text_vectors', [])
                    for tv in text_vectors:
                        if tv.get('status') == 'success':
                            all_vectors.append(tv['vector'])
                            all_metadata.append({
                                'type': 'text',
                                'source': item.get('pdf_path'),
                                'chunk': tv['chunk'],
                                'vector_type': 'text_embedding'
                            })
                            total_vectors += 1
                    
                    # æ”¶é›†å›¾åƒå‘é‡
                    image_vectors = vectorization_result.get('image_vectors', [])
                    for iv in image_vectors:
                        if iv.get('status') == 'success':
                            all_vectors.append(iv['vector'])
                            all_metadata.append({
                                'type': 'image',
                                'source': item.get('pdf_path'),
                                'image': iv['image'],
                                'enhanced_description': iv.get('enhanced_description'),
                                'vector_type': 'visual_embedding'
                            })
                            total_vectors += 1
                    
                    # æ”¶é›†è¡¨æ ¼å‘é‡
                    table_vectors = vectorization_result.get('table_vectors', [])
                    for tv in table_vectors:
                        if tv.get('status') == 'success':
                            all_vectors.append(tv['vector'])
                            all_metadata.append({
                                'type': 'table',
                                'source': item.get('pdf_path'),
                                'table': tv['table'],
                                'vector_type': 'text_embedding'
                            })
                            total_vectors += 1
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            if all_vectors:
                print(f"     å­˜å‚¨ {len(all_vectors)} ä¸ªå‘é‡åˆ°æ•°æ®åº“...")
                
                # ä½¿ç”¨å‘é‡å­˜å‚¨ç®¡ç†å™¨å­˜å‚¨
                success = self.vector_store_manager.add_vectors(all_vectors, all_metadata)
                
                if success:
                    print(f"     âœ… å‘é‡å­˜å‚¨æˆåŠŸ")
                    
                    # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
                    metadata_file = os.path.join(target_vector_db, 'processing_metadata.json')
                    metadata_summary = {
                        'total_vectors': total_vectors,
                        'total_files': len(processed_items),
                        'success_count': sum(1 for item in processed_items if item.get('status') == 'success'),
                        'failed_count': sum(1 for item in processed_items if item.get('status') == 'failed'),
                        'vector_types': {
                            'text': sum(1 for m in all_metadata if m['type'] == 'text'),
                            'image': sum(1 for m in all_metadata if m['type'] == 'image'),
                            'table': sum(1 for m in all_metadata if m['type'] == 'table')
                        },
                        'processing_timestamp': int(time.time()),
                        'items': processed_items
                    }
                    
                    with open(metadata_file, 'w', encoding='utf-8') as f:
                        json.dump(metadata_summary, f, ensure_ascii=False, indent=2)
                    
                    print(f"     ğŸ“„ å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {metadata_file}")
                    
                    return {
                        'stored_items': total_vectors,
                        'storage_path': target_vector_db,
                        'metadata_file': metadata_file,
                        'status': 'success'
                    }
                else:
                    print(f"     âŒ å‘é‡å­˜å‚¨å¤±è´¥")
                    return {
                        'stored_items': 0,
                        'storage_path': target_vector_db,
                        'status': 'failed',
                        'error': 'å‘é‡å­˜å‚¨å¤±è´¥'
                    }
            else:
                print("     æ²¡æœ‰æœ‰æ•ˆçš„å‘é‡éœ€è¦å­˜å‚¨")
                return {
                    'stored_items': 0,
                    'storage_path': target_vector_db,
                    'status': 'success',
                    'message': 'æ²¡æœ‰æœ‰æ•ˆçš„å‘é‡éœ€è¦å­˜å‚¨'
                }
                
        except Exception as e:
            logging.error(f"å­˜å‚¨å¤„ç†ç»“æœå¤±è´¥: {e}")
            print(f"     âŒ å­˜å‚¨å¤±è´¥: {e}")
            return {
                'stored_items': 0,
                'storage_path': target_vector_db,
                'status': 'failed',
                'error': str(e)
            }

    def _update_results(self, processing_result: Dict[str, Any], target_vector_db: str) -> Dict[str, Any]:
        """æ›´æ–°å¤„ç†ç»“æœ"""
        # å ä½ç¬¦å®ç°
        print("   ğŸ”„ æ›´æ–°å‘é‡æ•°æ®åº“...")

        return {
            'updated_items': 0,
            'storage_path': target_vector_db,
            'status': 'success'
        }

    def _generate_final_report(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        report = {
            'success': result.get('success', False),
            'mode': result.get('mode', 'unknown'),
            'timestamp': None,  # å°†åœ¨ä¸‹é¢è®¾ç½®
            'system_info': self._get_system_info(),
            'processing_stats': self._get_processing_stats(result)
        }

        # è®¾ç½®æ—¶é—´æˆ³
        import time
        report['timestamp'] = int(time.time())

        # æ·»åŠ æ¨¡å¼ç‰¹å®šçš„ä¿¡æ¯
        if result.get('mode') == 'new':
            report['database_status'] = 'created'
        elif result.get('mode') == 'incremental':
            report['database_status'] = 'updated'

        # æ·»åŠ å¤±è´¥å¤„ç†ç»Ÿè®¡
        if hasattr(self, 'failure_handler') and self.failure_handler:
            report['failure_stats'] = self.failure_handler.get_failure_summary()

        return report

    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import platform

        return {
            'platform': platform.platform(),
            'python_version': platform.python_version(),
            'system': platform.system(),
            'processor': platform.processor()
        }

    def _get_processing_stats(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'mode': result.get('mode', 'unknown'),
            'success': result.get('success', False)
        }

        # æ·»åŠ å¤„ç†ç»“æœç»Ÿè®¡
        processing_result = result.get('processing_result', {})
        if processing_result:
            stats.update({
                'total_files': processing_result.get('total_files', 0),
                'processed_items': len(processing_result.get('processed_items', []))
            })

        # æ·»åŠ å­˜å‚¨ç»“æœç»Ÿè®¡
        storage_result = result.get('storage_result', {})
        if storage_result:
            stats.update({
                'stored_items': storage_result.get('stored_items', 0),
                'storage_path': storage_result.get('storage_path', '')
            })

        return stats

    def _get_pdf_files(self, input_path: str) -> List[str]:
        """è·å–PDFæ–‡ä»¶åˆ—è¡¨"""
        try:
            if not os.path.exists(input_path):
                return []
            
            pdf_files = []
            for file in os.listdir(input_path):
                if file.lower().endswith('.pdf'):
                    pdf_files.append(os.path.join(input_path, file))
            
            return sorted(pdf_files)
            
        except Exception as e:
            logging.error(f"è·å–PDFæ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def _parse_pdf_with_mineru(self, pdf_path: str, output_dir: str) -> Dict[str, Any]:
        """ä½¿ç”¨minerUè§£æPDF"""
        try:
            # å¯¼å…¥minerUé›†æˆæ¨¡å—
            from utils.mineru_integration import MinerUIntegration
            
            # åˆ›å»ºminerUé›†æˆå®ä¾‹
            mineru = MinerUIntegration(self.config_manager)
            
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„mineru_output_dirï¼Œè€Œä¸æ˜¯ä¼ å…¥çš„output_dir
            mineru_output_dir = self.config_manager.get_path('mineru_output_dir')
            
            # è§£æPDFåˆ°æ­£ç¡®çš„MinerUè¾“å‡ºç›®å½•
            result = mineru.parse_pdf_document(pdf_path, mineru_output_dir)
            
            return result
            
        except Exception as e:
            logging.error(f"minerUè§£æPDFå¤±è´¥: {pdf_path}, é”™è¯¯: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def _vectorize_parsed_content(self, parsed_content: Dict[str, Any]) -> Dict[str, Any]:
        """å‘é‡åŒ–è§£æåçš„å†…å®¹"""
        try:
            vectorization_result = {
                'text_vectors': [],
                'image_vectors': [],
                'table_vectors': [],
                'metadata': {}
            }
            
            # å‘é‡åŒ–æ–‡æœ¬å—
            text_chunks = parsed_content.get('text_chunks', [])
            for chunk in text_chunks:
                try:
                    vector = self.model_caller.call_text_embedding(chunk['content'])
                    vectorization_result['text_vectors'].append({
                        'chunk': chunk,
                        'vector': vector,
                        'status': 'success'
                    })
                except Exception as e:
                    logging.error(f"æ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {e}")
                    vectorization_result['text_vectors'].append({
                        'chunk': chunk,
                        'status': 'failed',
                        'error': str(e)
                    })
            
            # å‘é‡åŒ–å›¾åƒ
            images = parsed_content.get('images', [])
            for image in images:
                try:
                    # å›¾åƒå¢å¼ºæè¿°
                    enhanced_description = self.model_caller.call_image_enhancement(
                        image.get('image_path', '')
                    )
                    
                    # å›¾åƒå‘é‡åŒ–
                    image_vector = self.model_caller.call_visual_embedding(
                        image.get('image_path', '')
                    )
                    
                    vectorization_result['image_vectors'].append({
                        'image': image,
                        'enhanced_description': enhanced_description,
                        'vector': image_vector,
                        'status': 'success'
                    })
                except Exception as e:
                    logging.error(f"å›¾åƒå‘é‡åŒ–å¤±è´¥: {e}")
                    vectorization_result['image_vectors'].append({
                        'image': image,
                        'status': 'failed',
                        'error': str(e)
                    })
            
            # å‘é‡åŒ–è¡¨æ ¼
            tables = parsed_content.get('tables', [])
            for table in tables:
                try:
                    # è¡¨æ ¼å†…å®¹å‘é‡åŒ–
                    table_text = table.get('text_content', '')
                    if table_text:
                        vector = self.model_caller.call_text_embedding(table_text)
                        vectorization_result['table_vectors'].append({
                            'table': table,
                            'vector': vector,
                            'status': 'success'
                        })
                    else:
                        vectorization_result['table_vectors'].append({
                            'table': table,
                            'status': 'failed',
                            'error': 'è¡¨æ ¼å†…å®¹ä¸ºç©º'
                        })
                except Exception as e:
                    logging.error(f"è¡¨æ ¼å‘é‡åŒ–å¤±è´¥: {e}")
                    vectorization_result['table_vectors'].append({
                        'table': table,
                        'status': 'failed',
                        'error': str(e)
                    })
            
            # æ·»åŠ å…ƒæ•°æ®
            vectorization_result['metadata'] = parsed_content.get('metadata', {})
            
            return vectorization_result
            
        except Exception as e:
            logging.error(f"å†…å®¹å‘é‡åŒ–å¤±è´¥: {e}")
            return {
                'text_vectors': [],
                'image_vectors': [],
                'table_vectors': [],
                'metadata': {},
                'error': str(e)
            }


if __name__ == "__main__":
    # æµ‹è¯•V3MainProcessor
    try:
        processor = V3MainProcessor()

        # æµ‹è¯•é»˜è®¤å¤„ç†
        result = processor.process_documents()
        print(f"\nå¤„ç†ç»“æœ: {'æˆåŠŸ' if result.get('success') else 'å¤±è´¥'}")

        if not result.get('success'):
            print(f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    except Exception as e:
        print(f"V3MainProcessoræµ‹è¯•å¤±è´¥: {e}")
