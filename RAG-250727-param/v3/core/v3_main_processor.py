"""
V3ç‰ˆæœ¬ä¸»å¤„ç†å™¨

V3ç‰ˆæœ¬å‘é‡æ•°æ®åº“æ„å»ºç³»ç»Ÿçš„æ ¸å¿ƒæ§åˆ¶å™¨ï¼Œè´Ÿè´£ç»Ÿä¸€ç®¡ç†æ•´ä¸ªæ–‡æ¡£å¤„ç†æµç¨‹ã€‚
å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼Œä½äºcoreæ¨¡å—ä¸‹ã€‚
"""

import os
import json
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path

from config.config_manager import ConfigManager
from utils.document_type_detector import DocumentTypeDetector
from .model_caller import LangChainModelCaller as ModelCaller
from .content_processor import ContentProcessor
from .vectorization_manager import VectorizationManager
from .metadata_manager import MetadataManager
from .vector_store_manager import LangChainVectorStoreManager as VectorStoreManager

class V3MainProcessor:
    """
    V3ç‰ˆæœ¬ä¸»å¤„ç†å™¨

    åŠŸèƒ½ï¼š
    - ç»Ÿä¸€çš„ç¨‹åºå…¥å£å’Œæµç¨‹æ§åˆ¶
    - æ™ºèƒ½æ¨¡å¼é€‰æ‹©ï¼ˆæ–°å»º vs å¢é‡ï¼‰
    - ä¸æ‰€æœ‰å­æ¨¡å—çš„æ·±åº¦é›†æˆ
    - é…ç½®ç®¡ç†å’ŒéªŒè¯
    - å¤±è´¥å¤„ç†å’ŒçŠ¶æ€è·Ÿè¸ª
    - å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
    """

    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–ä¸»å¤„ç†å™¨

        :param config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        # 1. åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager(config_path)

        # 2. åŠ è½½é…ç½®
        if not self.config_manager.load_config():
            raise RuntimeError("é…ç½®åŠ è½½å¤±è´¥")

        # 3. åˆå§‹åŒ–å­æ¨¡å—
        self._initialize_modules()

        # 4. éªŒè¯ç¯å¢ƒ
        self._validate_environment()
        
        # 5. ç³»ç»ŸçŠ¶æ€
        self.system_status = {
            'initialized': True,
            'initialization_time': int(time.time()),
            'version': '3.0.0',
            'status': 'ready'
        }

        logging.info("V3MainProcessoråˆå§‹åŒ–å®Œæˆ")

    def _initialize_modules(self):
        """åˆå§‹åŒ–æ‰€æœ‰å­æ¨¡å—"""
        try:
            # æ–‡æ¡£ç±»å‹æ£€æµ‹å™¨
            self.document_type_detector = DocumentTypeDetector(self.config_manager)

            # AIæ¨¡å‹è°ƒç”¨å™¨
            self.model_caller = ModelCaller(self.config_manager)

            # å†…å®¹å¤„ç†å™¨
            self.content_processor = ContentProcessor(self.config_manager)

            # å‘é‡åŒ–ç®¡ç†å™¨
            self.vectorization_manager = VectorizationManager(self.config_manager)

            # å…ƒæ•°æ®ç®¡ç†å™¨
            self.metadata_manager = MetadataManager(self.config_manager)

            # å‘é‡å­˜å‚¨ç®¡ç†å™¨
            self.vector_store_manager = VectorStoreManager(self.config_manager)

            # å¤±è´¥å¤„ç†å™¨ï¼ˆé€šè¿‡é…ç½®ç®¡ç†å™¨è·å–ï¼‰
            self.failure_handler = self.config_manager.get_failure_handler()

            logging.info("æ‰€æœ‰å­æ¨¡å—åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logging.error(f"å­æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _validate_environment(self):
        """éªŒè¯ç¯å¢ƒé…ç½®"""
        try:
            # éªŒè¯ç¯å¢ƒå˜é‡
            from config.environment_manager import environment_manager
            if not environment_manager.setup_environment():
                environment_manager.print_environment_setup_guide()
                raise RuntimeError("ç¯å¢ƒå˜é‡éªŒè¯å¤±è´¥")

            # éªŒè¯è·¯å¾„é…ç½®
            paths = self.config_manager.get('paths', {})
            for path_key, path_value in paths.items():
                if path_value and not os.path.exists(path_value):
                    os.makedirs(path_value, exist_ok=True)
                    logging.info(f"åˆ›å»ºç›®å½•: {path_value}")

            # éªŒè¯APIå¯†é’¥ï¼ˆé€šè¿‡EnvironmentManagerè·å–ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£ï¼‰
            env_manager = self.config_manager.get_environment_manager()
            required_keys = ['DASHSCOPE_API_KEY', 'MINERU_API_KEY']
            for key in required_keys:
                if not env_manager.get_required_var(key):
                    logging.warning(f"ç¼ºå°‘APIå¯†é’¥: {key}")
                else:
                    logging.info(f"APIå¯†é’¥å·²è®¾ç½®: {key}")

            logging.info("ç¯å¢ƒéªŒè¯å®Œæˆ")

        except Exception as e:
            logging.error(f"ç¯å¢ƒéªŒè¯å¤±è´¥: {e}")
            raise

    def process_documents(self, input_type: str = None, input_path: str = None,
                         output_path: str = None) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£ï¼ˆå¸¦é»˜è®¤å€¼æ”¯æŒï¼‰

        :param input_type: è¾“å…¥ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼'pdf'
        :param input_path: è¾“å…¥è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        :param output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        :return: å¤„ç†ç»“æœ
        """
        try:
            print("=" * 50)
            print("      V3ç‰ˆæœ¬å‘é‡æ•°æ®åº“æ„å»ºç³»ç»Ÿ")
            print("=" * 50)

            # 1. ä½¿ç”¨DocumentTypeDetectoréªŒè¯è¾“å…¥ç±»å‹å’Œè·¯å¾„
            validation_result = self.document_type_detector.validate_input_type(
                input_type, input_path, output_path
            )

            if not validation_result['valid']:
                error_msg = f"è¾“å…¥éªŒè¯å¤±è´¥: {validation_result.get('message', 'æœªçŸ¥é”™è¯¯')}"
                print(f"âŒ {error_msg}")
                return {
                    'success': False,
                    'error': error_msg,
                    'validation_result': validation_result
                }

            # 2. æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
            self._display_processing_info(validation_result)

            # 3. æ ¹æ®è¾“å…¥ç±»å‹é€‰æ‹©å¤„ç†æµç¨‹
            if validation_result['input_type'] == 'pdf':
                result = self._process_from_pdf(validation_result)
            elif validation_result['input_type'] == 'mineru_output':
                result = self._process_from_mineru_output(validation_result)
            else:
                error_msg = f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {validation_result['input_type']}"
                print(f"âŒ {error_msg}")
                return {
                    'success': False,
                    'error': error_msg
                }

            # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_result = self._generate_final_report(result)
            
            # ç¡®ä¿è¿”å›å®é™…ä½¿ç”¨çš„æ•°æ®åº“è·¯å¾„
            final_result['storage_path'] = validation_result['output_path']
            
            return final_result

        except Exception as e:
            error_msg = f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            logging.error(error_msg, exc_info=True)

            # è®°å½•å¤±è´¥ä¿¡æ¯
            if hasattr(self, 'failure_handler') and self.failure_handler:
                self.failure_handler.record_failure(
                    image_info={'operation': 'document_processing'},
                    error_type='processing_error',
                    error_message=str(e)
                )

            return {
                'success': False,
                'error': error_msg,
                'exception': str(e)
            }

    def _display_processing_info(self, validation_result: Dict[str, Any]):
        """æ˜¾ç¤ºå¤„ç†ä¿¡æ¯"""
        print("\nğŸ“‹ å¤„ç†ä¿¡æ¯:")
        print(f"   è¾“å…¥ç±»å‹: {validation_result['input_type']}")
        print(f"   è¾“å…¥è·¯å¾„: {validation_result['input_path']}")
        print(f"   è¾“å‡ºè·¯å¾„: {validation_result['output_path']}")
        print(f"   æ˜¯å¦éœ€è¦minerU: {'æ˜¯' if validation_result['needs_mineru'] else 'å¦'}")

        if validation_result.get('file_count'):
            print(f"   æ–‡ä»¶æ•°é‡: {validation_result['file_count']}")

        if validation_result.get('file_size'):
            size_mb = validation_result['file_size'] / (1024 * 1024)
            print(f"   æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")

        print(f"   æè¿°: {validation_result['description']}")

    def _process_from_pdf(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä»PDFå¼€å§‹å¤„ç†"""
        print("\nğŸš€ ä»PDFå¼€å§‹å¤„ç†")
        print("   æ³¨æ„: ä»PDFå¼€å§‹å¤„ç†éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ŒåŒ…å«minerUè§£ææ­¥éª¤")

        # 1. æ£€æŸ¥ç›®æ ‡å‘é‡æ•°æ®åº“çŠ¶æ€
        target_vector_db = validation_result['output_path']
        db_exists = self._check_vector_db_exists(target_vector_db)

        # 2. æ™ºèƒ½é€‰æ‹©æ¨¡å¼
        if db_exists:
            print("   ğŸ“Š æ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼")
            result = self._incremental_process(validation_result, target_vector_db)
        else:
            print("   ğŸ†• æœªæ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨æ–°å»ºæ¨¡å¼")
            result = self._new_process(validation_result, target_vector_db)

        return result

    def _process_from_mineru_output(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä»minerUè¾“å‡ºå¼€å§‹å¤„ç†"""
        print("\nâš¡ ä»minerUè¾“å‡ºå¼€å§‹å¤„ç†")
        print("   æ³¨æ„: ä»minerUè¾“å‡ºå¼€å§‹å¤„ç†ï¼Œè·³è¿‡è§£ææ­¥éª¤ï¼Œé€Ÿåº¦è¾ƒå¿«")

        # 1. æ£€æŸ¥ç›®æ ‡å‘é‡æ•°æ®åº“çŠ¶æ€
        target_vector_db = validation_result['output_path']
        db_exists = self._check_vector_db_exists(target_vector_db)

        # 2. æ™ºèƒ½é€‰æ‹©æ¨¡å¼
        if db_exists:
            print("   ğŸ“Š æ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼")
            result = self._incremental_process(validation_result, target_vector_db)
        else:
            print("   ğŸ†• æœªæ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨æ–°å»ºæ¨¡å¼")
            result = self._new_process(validation_result, target_vector_db)

        return result

    def _check_vector_db_exists(self, target_vector_db: str) -> bool:
        """æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨"""
        # ç›´æ¥ä½¿ç”¨æ–‡ä»¶æ£€æµ‹æ–¹æ³•ï¼Œé¿å…FAISS.load_localçš„è·¯å¾„é—®é¢˜
        # V3ç‰ˆæœ¬çš„æ–‡ä»¶ç»“æ„ï¼šcentral/vector_db/langchain_faiss_index/
        index_file = os.path.join(target_vector_db, 'langchain_faiss_index', 'index.faiss')
        index_pkl_file = os.path.join(target_vector_db, 'langchain_faiss_index', 'index.pkl')
        
        exists = os.path.exists(index_file) and os.path.exists(index_pkl_file)
        
        if exists:
            logging.info(f"æ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“: {target_vector_db}")
        else:
            logging.info(f"æœªæ£€æµ‹åˆ°å‘é‡æ•°æ®åº“: {target_vector_db}")
        
        return exists

    def _get_existing_document_names(self) -> List[str]:
        """è·å–ç°æœ‰æ•°æ®åº“ä¸­çš„æ–‡æ¡£ååˆ—è¡¨"""
        try:
            if not self.vector_store_manager.vector_store:
                return []
            
            docstore = self.vector_store_manager.vector_store.docstore._dict
            document_names = set()
            
            for doc_id, doc in docstore.items():
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                doc_name = metadata.get('document_name', '')
                if doc_name and doc_name != 'unknown':
                    document_names.add(doc_name)
            
            return list(document_names)
            
        except Exception as e:
            logging.error(f"è·å–ç°æœ‰æ–‡æ¡£åå¤±è´¥: {e}")
            return []

    def _new_process(self, validation_result: Dict[str, Any], target_vector_db: str) -> Dict[str, Any]:
        """æ–°å»ºæ¨¡å¼å¤„ç†"""
        try:
            print("\nğŸ—ï¸  å¼€å§‹æ–°å»ºæ¨¡å¼å¤„ç†...")

            # 1. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            success = self.vector_store_manager.create_vector_store(
                dimension=1536  # é»˜è®¤å‘é‡ç»´åº¦
            )

            if not success:
                raise RuntimeError("å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")

            # 2. å¤„ç†æ–‡æ¡£å†…å®¹
            processing_result = self._process_documents_new(validation_result)

            # 3. å­˜å‚¨ç»“æœ
            storage_result = self._store_results(processing_result, target_vector_db)

            # 4. ç”ŸæˆæŠ¥å‘Š
            result = {
                'success': True,
                'mode': 'new',
                'processing_result': processing_result,
                'storage_result': storage_result,
                'target_vector_db': target_vector_db
            }

            print("âœ… æ–°å»ºæ¨¡å¼å¤„ç†å®Œæˆ")
            return result

        except Exception as e:
            logging.error(f"æ–°å»ºæ¨¡å¼å¤„ç†å¤±è´¥: {e}")
            raise

    def _incremental_process(self, validation_result: Dict[str, Any], target_vector_db: str) -> Dict[str, Any]:
        """å¢é‡æ¨¡å¼å¤„ç†"""
        try:
            print("\nğŸ”„ å¼€å§‹å¢é‡æ¨¡å¼å¤„ç†...")

            # 1. åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“
            print("   ğŸ“Š åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
            load_success = self.vector_store_manager.load(target_vector_db)
            if not load_success:
                raise RuntimeError("æ— æ³•åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“")
            
            # è·å–ç°æœ‰æ–‡æ¡£åˆ—è¡¨ï¼Œç”¨äºå»é‡
            existing_docs = self._get_existing_document_names()
            print(f"   ğŸ“š ç°æœ‰æ–‡æ¡£: {existing_docs}")
            
            # 2. å¤„ç†æ–°å¢æ–‡æ¡£å†…å®¹
            print("   ğŸ“„ å¢é‡æ¨¡å¼ï¼šå¤„ç†æ–°å¢æ–‡æ¡£å†…å®¹...")
            processing_result = self._process_documents_incremental(validation_result, existing_docs)

            # 3. æ›´æ–°å‘é‡æ•°æ®åº“
            storage_result = self._update_results(processing_result, target_vector_db)

            # 4. ç”ŸæˆæŠ¥å‘Š
            result = {
                'success': True,
                'mode': 'incremental',
                'processing_result': processing_result,
                'storage_result': storage_result,
                'target_vector_db': target_vector_db,
                'storage_path': target_vector_db  # æ·»åŠ storage_pathå­—æ®µ
            }

            print("âœ… å¢é‡æ¨¡å¼å¤„ç†å®Œæˆ")
            return result

        except Exception as e:
            logging.error(f"å¢é‡æ¨¡å¼å¤„ç†å¤±è´¥: {e}")
            raise

    def _process_documents_new(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ–°å»ºæ¨¡å¼æ–‡æ¡£å¤„ç†"""
        try:
            print("   ğŸ“„ å¤„ç†æ–‡æ¡£å†…å®¹...")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºvalidation_resultçš„å†…å®¹
            print(f"   ğŸ” è°ƒè¯•ä¿¡æ¯: validation_result.keys() = {list(validation_result.keys())}")
            print(f"   ğŸ” è°ƒè¯•ä¿¡æ¯: file_list = {validation_result.get('file_list', [])}")
            print(f"   ğŸ” è°ƒè¯•ä¿¡æ¯: file_count = {validation_result.get('file_count', 0)}")
            print(f"   ğŸ” è°ƒè¯•ä¿¡æ¯: input_type = {validation_result.get('input_type', 'unknown')}")
            
            # è·å–æ–‡ä»¶åˆ—è¡¨å’Œè¾“å…¥ç±»å‹
            files = validation_result.get('file_list', [])
            input_type = validation_result.get('input_type', 'pdf')
            
            processed_items = []
            
            if input_type == 'pdf':
                # å¤„ç†PDFæ–‡ä»¶
                for pdf_file in files:
                    try:
                        print(f"     å¤„ç†PDFæ–‡ä»¶: {os.path.basename(pdf_file)}")
                        
                        # 1. ä½¿ç”¨MinerUè§£æPDF
                        mineru_result = self._call_mineru_api(pdf_file)
                        if not mineru_result.get('success'):
                            print(f"     âš ï¸  MinerUè§£æå¤±è´¥: {pdf_file}")
                            continue
                        
                        # 2. è·å–å¯¹åº”çš„JSONæ–‡ä»¶è·¯å¾„ï¼ˆä»MinerUè¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾ï¼‰
                        mineru_output_dir = self.config_manager.get_path('mineru_output_dir')
                        json_file = self._find_json_file_for_pdf(pdf_file, mineru_output_dir)
                        if not json_file:
                            print(f"     âš ï¸  æœªæ‰¾åˆ°å¯¹åº”çš„JSONæ–‡ä»¶: {pdf_file}")
                            continue
                        
                        # 3. ä½¿ç”¨ContentProcessorå¤„ç†æ–‡æ¡£å†…å®¹
                        doc_name = os.path.splitext(os.path.basename(pdf_file))[0]
                        content_result = self.content_processor.process_document_content(json_file, doc_name)
                        
                        # 4. ä½¿ç”¨VectorizationManagerè¿›è¡Œå‘é‡åŒ–
                        vectorization_result = self.vectorization_manager.vectorize_all_content(content_result)
                        
                        # 5. æ„å»ºå¤„ç†ç»“æœ
                        processed_item = {
                            'pdf_path': pdf_file,
                            'json_path': json_file,
                            'doc_name': doc_name,
                            'content_result': content_result,
                            'vectorization_result': vectorization_result,
                            'status': 'success',
                            'processing_timestamp': int(time.time())
                        }
                        
                        processed_items.append(processed_item)
                        print(f"     âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {doc_name}")
                        
                    except Exception as e:
                        error_msg = f"å¤„ç†PDFæ–‡ä»¶å¤±è´¥: {pdf_file}, é”™è¯¯: {e}"
                        print(f"     âŒ {error_msg}")
                        logging.error(error_msg)
                        
                        # è®°å½•å¤±è´¥ä¿¡æ¯
                        self.failure_handler.record_failure(pdf_file, 'pdf_processing', str(e))
                        
                        # æ·»åŠ å¤±è´¥é¡¹
                        processed_items.append({
                            'pdf_path': pdf_file,
                            'status': 'failed',
                            'error': str(e),
                            'processing_timestamp': int(time.time())
                        })
                        
            elif input_type == 'mineru_output':
                # ç›´æ¥å¤„ç†MinerUè¾“å‡ºæ–‡ä»¶ï¼ˆJSON/MDï¼‰ï¼Œè·³è¿‡MinerUè§£ææ­¥éª¤
                print(f"     âš¡ è·³è¿‡MinerUè§£æï¼Œç›´æ¥å¤„ç† {len(files)} ä¸ªæ–‡ä»¶")
                
                for file_path in files:
                    try:
                        print(f"     å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
                        
                        # 1. ç›´æ¥ä½¿ç”¨ContentProcessorå¤„ç†æ–‡æ¡£å†…å®¹
                        doc_name = os.path.splitext(os.path.basename(file_path))[0]
                        content_result = self.content_processor.process_document_content(file_path, doc_name)
                        
                        # 2. ä½¿ç”¨VectorizationManagerè¿›è¡Œå‘é‡åŒ–
                        vectorization_result = self.vectorization_manager.vectorize_all_content(content_result)
                        
                        # 3. æ„å»ºå¤„ç†ç»“æœ
                        processed_item = {
                            'file_path': file_path,
                            'doc_name': doc_name,
                            'content_result': content_result,
                            'vectorization_result': vectorization_result,
                            'status': 'success',
                            'processing_timestamp': int(time.time())
                        }
                        
                        processed_items.append(processed_item)
                        print(f"     âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {doc_name}")
                        
                    except Exception as e:
                        error_msg = f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}"
                        print(f"     âŒ {error_msg}")
                        logging.error(error_msg)
                        
                        # è®°å½•å¤±è´¥ä¿¡æ¯
                        self.failure_handler.record_failure(file_path, 'file_processing', str(e))
                        
                        # æ·»åŠ å¤±è´¥é¡¹
                        processed_items.append({
                            'file_path': file_path,
                            'status': 'failed',
                            'error': str(e),
                            'processing_timestamp': int(time.time())
                        })
            
            # ç»Ÿè®¡å¤„ç†ç»“æœ
            successful_items = [item for item in processed_items if item.get('status') == 'success']
            failed_items = [item for item in processed_items if item.get('status') == 'failed']
            
            result = {
                'processed_items': processed_items,
                'total_files': len(files),
                'successful_files': len(successful_items),
                'failed_files': len(failed_items),
                'status': 'success' if successful_items else 'failed',
                'processing_timestamp': int(time.time())
            }
            
            print(f"   ğŸ“Š æ–‡æ¡£å¤„ç†å®Œæˆ: æˆåŠŸ {len(successful_items)} ä¸ªï¼Œå¤±è´¥ {len(failed_items)} ä¸ª")
            return result
            
        except Exception as e:
            error_msg = f"æ–°å»ºæ¨¡å¼æ–‡æ¡£å¤„ç†å¤±è´¥: {e}"
            logging.error(error_msg)
            self.failure_handler.record_failure('document_processing', 'new_mode', str(e))
            
            return {
                'processed_items': [],
                'total_files': 0,
                'status': 'failed',
                'error': str(e)
            }
    
    def _find_json_file_for_pdf(self, pdf_file: str, mineru_output_dir: str) -> Optional[str]:
        """
        ä¸ºPDFæ–‡ä»¶æ‰¾åˆ°å¯¹åº”çš„JSONæ–‡ä»¶
        
        :param pdf_file: PDFæ–‡ä»¶è·¯å¾„
        :param mineru_output_dir: MinerUè¾“å‡ºç›®å½•
        :return: JSONæ–‡ä»¶è·¯å¾„æˆ–None
        """
        pdf_name = os.path.splitext(os.path.basename(pdf_file))[0]
        
        # åœ¨MinerUè¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾å¯¹åº”çš„JSONæ–‡ä»¶
        for item in Path(mineru_output_dir).iterdir():
            if item.is_file() and item.suffix.lower() == '.json':
                if pdf_name in item.name:
                    return str(item)
        
        return None

    def _process_documents_incremental(self, validation_result: Dict[str, Any], existing_docs: List[str] = None) -> Dict[str, Any]:
        """
        å¢é‡æ¨¡å¼æ–‡æ¡£å¤„ç†
        
        åŠŸèƒ½ï¼š
        - æ£€æµ‹æ–°å¢çš„æ–‡æ¡£
        - åªå¤„ç†æ–°å¢å†…å®¹
        - å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“
        - ä¿æŒç°æœ‰æ•°æ®å®Œæ•´æ€§
        """
        try:
            print("   ğŸ“„ å¢é‡æ¨¡å¼ï¼šå¤„ç†æ–°å¢æ–‡æ¡£å†…å®¹...")
            
            # è·å–æ–‡ä»¶åˆ—è¡¨å’Œè¾“å…¥ç±»å‹
            files = validation_result.get('file_list', [])
            input_type = validation_result.get('input_type', 'pdf')
            
            if not files:
                print("     æ²¡æœ‰æ–‡ä»¶éœ€è¦å¤„ç†")
                return {
                    'processed_items': [],
                    'new_files': 0,
                    'incremental_updates': 0,
                    'status': 'success',
                    'message': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦å¤„ç†'
                }
            
            # æ£€æµ‹æ–°å¢æ–‡æ¡£ï¼ˆä¸åœ¨ç°æœ‰æ•°æ®åº“ä¸­çš„æ–‡æ¡£ï¼‰
            new_files = []
            for file_path in files:
                file_name = os.path.basename(file_path)
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åœ¨ç°æœ‰æ–‡æ¡£ä¸­
                is_new = True
                for existing_doc in (existing_docs or []):
                    if existing_doc in file_name or file_name in existing_doc:
                        is_new = False
                        break
                
                if is_new:
                    new_files.append({
                        'path': file_path,
                        'name': file_name,
                        'type': input_type
                    })
            
            if not new_files:
                print("     æ²¡æœ‰æ–°å¢æ–‡æ¡£éœ€è¦å¤„ç†")
                return {
                    'processed_items': [],
                    'new_files': 0,
                    'incremental_updates': 0,
                    'status': 'success',
                    'message': 'æ²¡æœ‰æ–°å¢æ–‡æ¡£'
                }
            
            print(f"     æ£€æµ‹åˆ° {len(new_files)} ä¸ªæ–°å¢æ–‡æ¡£")
            
            # å¢é‡å¤„ç†æ–°å¢æ–‡æ¡£
            processed_items = []
            successful_items = []
            failed_items = []
            
            for file_info in new_files:
                try:
                    print(f"     ğŸ”„ å¤„ç†æ–°å¢æ–‡æ¡£: {file_info.get('name', 'unknown')}")
                    
                    # ä½¿ç”¨æ–°å»ºæ¨¡å¼çš„å¤„ç†é€»è¾‘æ¥å¤„ç†å•ä¸ªæ–‡æ¡£
                    # è¿™æ ·å¯ä»¥å¤ç”¨ç°æœ‰çš„å¤„ç†æµç¨‹
                    single_validation = {
                        'file_list': [file_info['path']],
                        'input_type': file_info['type'],
                        'file_count': 1
                    }
                    
                    item_result = self._process_documents_new(single_validation)
                    
                    # ä¿®å¤ï¼šæ£€æŸ¥ 'status' å­—æ®µè€Œä¸æ˜¯ 'success' å­—æ®µ
                    if item_result.get('status') == 'success':
                        successful_items.append({
                            'file_info': file_info,
                            'status': 'success',
                            'result': item_result,
                            'processing_timestamp': int(time.time())
                        })
                        processed_items.append(item_result)
                        print(f"       âœ… å¤„ç†æˆåŠŸ")
                    else:
                        failed_items.append({
                            'file_info': file_info,
                            'status': 'failed',
                            'error': 'å¤„ç†å¤±è´¥',
                            'processing_timestamp': int(time.time())
                        })
                        print(f"       âŒ å¤„ç†å¤±è´¥")
                        
                except Exception as e:
                    error_msg = f"å¤„ç†æ–°å¢æ–‡æ¡£å¤±è´¥: {file_info.get('name', 'unknown')}, é”™è¯¯: {e}"
                    logging.error(error_msg)
                    
                    failed_item = {
                        'file_info': file_info,
                        'status': 'failed',
                        'error': str(e),
                        'processing_timestamp': int(time.time())
                    }
                    failed_items.append(failed_item)
                    processed_items.append(failed_item)
                    
                    # è®°å½•å¤±è´¥
                    self.failure_handler.record_failure(file_info, 'incremental_processing', str(e))
            
            # æ›´æ–°å‘é‡æ•°æ®åº“ï¼ˆè¿™é‡Œæš‚æ—¶è¿”å›0ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨å¤„ç†æ—¶å·²ç»æ›´æ–°äº†ï¼‰
            incremental_updates = len(successful_items)
            
            # ç”Ÿæˆç»“æœ
            result = {
                'mode': 'incremental',
                'processed_items': processed_items,
                'successful_items': successful_items,
                'failed_items': failed_items,
                'new_files': len(new_files),
                'incremental_updates': incremental_updates,
                'total_vectors_added': sum(item.get('vector_count', 0) for item in successful_items),
                'status': 'success' if successful_items else 'failed',
                'processing_timestamp': int(time.time())
            }
            
            print(f"   ğŸ“Š å¢é‡å¤„ç†å®Œæˆ: æˆåŠŸ {len(successful_items)} ä¸ªï¼Œå¤±è´¥ {len(failed_items)} ä¸ª")
            print(f"     æ–°å¢å‘é‡: {result['total_vectors_added']} ä¸ª")
            
            return result
            
        except Exception as e:
            error_msg = f"å¢é‡æ¨¡å¼æ–‡æ¡£å¤„ç†å¤±è´¥: {e}"
            logging.error(error_msg)
            self.failure_handler.record_failure('document_processing', 'incremental_mode', str(e))
            
            return {
                'processed_items': [],
                'new_files': 0,
                'incremental_updates': 0,
                'status': 'failed',
                'error': str(e)
            }

    def _load_existing_vector_db(self, vector_db_path: str) -> bool:
        """
        åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“
        
        :param vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
        :return: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            if not vector_db_path or not os.path.exists(vector_db_path):
                logging.warning(f"å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {vector_db_path}")
                return False
            
            # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶
            index_dir = os.path.join(vector_db_path, 'index')
            metadata_dir = os.path.join(vector_db_path, 'metadata')
            
            if not os.path.exists(index_dir) or not os.path.exists(metadata_dir):
                logging.warning(f"å‘é‡æ•°æ®åº“ç»“æ„ä¸å®Œæ•´: {vector_db_path}")
                return False
            
            # å°è¯•åŠ è½½ç°æœ‰æ•°æ®åº“
            if self.vector_store_manager.load_existing_database(vector_db_path):
                logging.info(f"æˆåŠŸåŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“: {vector_db_path}")
                return True
            else:
                logging.warning(f"åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“å¤±è´¥: {vector_db_path}")
                return False
                
        except Exception as e:
            logging.error(f"åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def _process_single_document_incremental(self, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¢é‡å¤„ç†å•ä¸ªæ–‡æ¡£
        
        :param file_info: æ–‡ä»¶ä¿¡æ¯
        :return: å¤„ç†ç»“æœ
        """
        try:
            file_path = file_info.get('path', '')
            file_type = file_info.get('type', '')
            file_name = file_info.get('name', '')
            
            if not file_path or not os.path.exists(file_path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
            if file_type == 'pdf':
                return self._process_pdf_incremental(file_path, file_name)
            elif file_type == 'json':
                return self._process_json_incremental(file_path, file_name)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
                
        except Exception as e:
            logging.error(f"å¢é‡å¤„ç†å•ä¸ªæ–‡æ¡£å¤±è´¥: {e}")
            return {
                'file_info': file_info,
                'status': 'failed',
                'error': str(e),
                'processing_timestamp': int(time.time())
            }

    def _process_pdf_incremental(self, pdf_path: str, pdf_name: str) -> Dict[str, Any]:
        """
        å¢é‡å¤„ç†PDFæ–‡æ¡£
        
        :param pdf_path: PDFæ–‡ä»¶è·¯å¾„
        :param pdf_name: PDFæ–‡ä»¶å
        :return: å¤„ç†ç»“æœ
        """
        try:
            print(f"       ğŸ“„ å¤„ç†PDF: {pdf_name}")
            
            # 1. è°ƒç”¨MinerU APIå¤„ç†PDF
            mineru_result = self._call_mineru_api(pdf_path)
            if not mineru_result.get('success'):
                raise RuntimeError(f"MinerUå¤„ç†å¤±è´¥: {mineru_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # 2. ä»MinerUè¾“å‡ºå¤„ç†å†…å®¹
            json_path = mineru_result.get('json_path', '')
            if not json_path or not os.path.exists(json_path):
                raise FileNotFoundError(f"MinerUè¾“å‡ºJSONæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            
            # 3. å¤„ç†æ–‡æ¡£å†…å®¹
            content_result = self.content_processor.process_document_content(json_path, pdf_name)
            
            # 4. å¢é‡å‘é‡åŒ–
            vectorization_result = self._vectorize_content_incremental(content_result)
            
            # 5. ç”Ÿæˆç»“æœ
            result = {
                'file_info': {
                    'path': pdf_path,
                    'name': pdf_name,
                    'type': 'pdf'
                },
                'status': 'success',
                'mineru_result': mineru_result,
                'content_result': content_result,
                'vectorization_result': vectorization_result,
                'vector_count': self._count_vectors(vectorization_result),
                'processing_timestamp': int(time.time())
            }
            
            return result
            
        except Exception as e:
            logging.error(f"å¢é‡å¤„ç†PDFå¤±è´¥: {pdf_path}, é”™è¯¯: {e}")
            return {
                'file_info': {
                    'path': pdf_path,
                    'name': pdf_name,
                    'type': 'pdf'
                },
                'status': 'failed',
                'error': str(e),
                'processing_timestamp': int(time.time())
            }

    def _process_json_incremental(self, json_path: str, json_name: str) -> Dict[str, Any]:
        """
        å¢é‡å¤„ç†JSONæ–‡æ¡£
        
        :param json_path: JSONæ–‡ä»¶è·¯å¾„
        :param json_name: JSONæ–‡ä»¶å
        :return: å¤„ç†ç»“æœ
        """
        try:
            print(f"       ğŸ“‹ å¤„ç†JSON: {json_name}")
            
            # 1. å¤„ç†æ–‡æ¡£å†…å®¹
            content_result = self.content_processor.process_document_content(json_path, json_name)
            
            # 2. å¢é‡å‘é‡åŒ–
            vectorization_result = self._vectorize_content_incremental(content_result)
            
            # 3. ç”Ÿæˆç»“æœ
            result = {
                'file_info': {
                    'path': json_path,
                    'name': json_name,
                    'type': 'json'
                },
                'status': 'success',
                'content_result': content_result,
                'vectorization_result': vectorization_result,
                'vector_count': self._count_vectors(vectorization_result),
                'processing_timestamp': int(time.time())
            }
            
            return result
            
        except Exception as e:
            logging.error(f"å¢é‡å¤„ç†JSONå¤±è´¥: {json_path}, é”™è¯¯: {e}")
            return {
                'file_info': {
                    'path': json_path,
                    'name': json_name,
                    'type': 'json'
                },
                'status': 'failed',
                'error': str(e),
                'processing_timestamp': int(time.time())
            }

    def _vectorize_content_incremental(self, content_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¢é‡å‘é‡åŒ–å†…å®¹
        
        :param content_result: å†…å®¹å¤„ç†ç»“æœ
        :return: å‘é‡åŒ–ç»“æœ
        """
        try:
            # åªå‘é‡åŒ–æ–°å¢å†…å®¹ï¼Œé¿å…é‡å¤å¤„ç†
            vectorization_result = {
                'text_vectors': [],
                'image_vectors': [],
                'table_vectors': [],
                'incremental_mode': True
            }
            
            # å‘é‡åŒ–æ–‡æœ¬ï¼ˆåªå¤„ç†æ–°æ–‡æœ¬ï¼‰
            if content_result.get('text_chunks'):
                text_vectors = self.vectorization_manager.vectorize_content(
                    content_result['text_chunks'], 'text'
                )
                vectorization_result['text_vectors'] = text_vectors
            
            # å‘é‡åŒ–è¡¨æ ¼ï¼ˆåªå¤„ç†æ–°è¡¨æ ¼ï¼‰
            if content_result.get('tables'):
                table_vectors = self.vectorization_manager.vectorize_content(
                    content_result['tables'], 'table'
                )
                vectorization_result['table_vectors'] = table_vectors
            
            # å›¾ç‰‡å‘é‡åŒ–åœ¨ImageProcessorä¸­å·²å®Œæˆï¼Œè¿™é‡Œæ”¶é›†ç»“æœ
            if content_result.get('images'):
                image_vectors = []
                for image in content_result['images']:
                    if image.get('image_embedding') and image.get('description_embedding'):
                        # ä¿®å¤ï¼šå¤åˆ¶å®Œæ•´çš„å›¾ç‰‡å¯¹è±¡ï¼Œä¿ç•™æ‰€æœ‰metadataï¼Œä¸æ–°å»ºæ¨¡å¼ä¿æŒä¸€è‡´
                        image_vector = image.copy()  # ä¿ç•™å®Œæ•´çš„metadata
                        
                        # æ›´æ–°å‘é‡åŒ–ç›¸å…³å­—æ®µ
                        image_vector.update({
                            'status': 'success',
                            'vectorization_status': 'success',
                            'embedding_model': 'multimodal-embedding-one-peace-v1',
                            'vectorization_timestamp': int(time.time())
                        })
                        
                        image_vectors.append(image_vector)
                vectorization_result['image_vectors'] = image_vectors
            
            return vectorization_result
            
        except Exception as e:
            logging.error(f"å¢é‡å‘é‡åŒ–å¤±è´¥: {e}")
            return {
                'text_vectors': [],
                'image_vectors': [],
                'table_vectors': [],
                'incremental_mode': True,
                'status': 'failed',
                'error': str(e)
            }

    def _update_vector_database_incremental(self, successful_items: List[Dict[str, Any]]) -> int:
        """
        å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“
        
        :param successful_items: æˆåŠŸå¤„ç†çš„é¡¹ç›®åˆ—è¡¨
        :return: æ›´æ–°çš„å‘é‡æ•°é‡
        """
        try:
            print("     ğŸ”„ å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“...")
            
            total_vectors_added = 0
            
            for item in successful_items:
                vectorization_result = item.get('vectorization_result', {})
                
                # æ”¶é›†æ‰€æœ‰å‘é‡
                all_vectors = []
                all_metadata = []
                
                # æ–‡æœ¬å‘é‡
                for tv in vectorization_result.get('text_vectors', []):
                    if tv.get('vectorization_status') == 'success':
                        all_vectors.append(tv.get('vector', []))
                        
                        # ä»två¯¹è±¡ä¸­æå–metadataï¼Œä¸æ–°å»ºæ¨¡å¼ä¿æŒä¸€è‡´
                        metadata = tv.get('metadata', {})
                        all_metadata.append({
                            'type': 'text',
                            'chunk_type': 'text',
                            'source': item.get('file_info', {}).get('name', '') or metadata.get('document_name', ''),
                            'document_name': metadata.get('document_name', ''),
                            'page_number': metadata.get('page_number', 1),
                            'chunk_id': metadata.get('chunk_id', ''),
                            'text': metadata.get('text', ''),
                            'text_length': metadata.get('text_length', 0),
                            'text_level': metadata.get('text_level', 0),
                            'chunk_size': metadata.get('chunk_size', 0),
                            'chunk_overlap': metadata.get('chunk_overlap', 0),
                            'chunk_position': metadata.get('chunk_position', {}),
                            'related_images': metadata.get('related_images', []),
                            'related_tables': metadata.get('related_tables', []),
                            'text_embedding': tv.get('vector', []),  # ä¿å­˜å®é™…çš„text embeddingå‘é‡
                            'text_embedding_model': tv.get('text_embedding_model', 'text-embedding-v1'),  # ä¿å­˜embeddingæ¨¡å‹ä¿¡æ¯
                            'vector_type': 'text_embedding',
                            'vectorization_status': 'success',
                            'vectorization_timestamp': int(time.time()),
                            'incremental_update': True,
                            'update_timestamp': int(time.time())
                        })
                        total_vectors_added += 1
                
                # å›¾åƒå‘é‡
                for iv in vectorization_result.get('image_vectors', []):
                    if iv.get('vectorization_status') == 'success':
                        all_vectors.append(iv.get('image_embedding', []))
                        
                        # ä»ivå¯¹è±¡ä¸­æå–metadataï¼Œä¸æ–°å»ºæ¨¡å¼ä¿æŒä¸€è‡´
                        metadata = iv.get('metadata', {})
                        all_metadata.append({
                            'type': 'image',
                            'chunk_type': 'image',
                            'source': item.get('file_info', {}).get('name', '') or iv.get('document_name', ''),
                            'document_name': iv.get('document_name', ''),
                            'page_number': metadata.get('page_number', 1),
                            'chunk_id': metadata.get('chunk_id', ''),
                            'image_id': metadata.get('image_id', ''),
                            'image_path': metadata.get('image_path', ''),
                            'image_filename': metadata.get('image_filename', ''),
                            'image_type': metadata.get('image_type', 'general'),
                            'image_format': metadata.get('image_format', 'UNKNOWN'),
                            'image_dimensions': metadata.get('image_dimensions', {}),
                            'basic_description': metadata.get('basic_description', ''),
                            'enhanced_description': metadata.get('enhanced_description', ''),
                            'layered_descriptions': metadata.get('layered_descriptions', {}),
                            'structured_info': metadata.get('structured_info', {}),
                            'img_caption': metadata.get('img_caption', []),
                            'img_footnote': metadata.get('img_footnote', []),
                            'enhancement_enabled': metadata.get('enhancement_enabled', True),
                            'enhancement_model': metadata.get('enhancement_model', ''),
                            'enhancement_status': metadata.get('enhancement_status', 'success'),
                            'enhancement_timestamp': metadata.get('enhancement_timestamp'),
                            'image_embedding': iv.get('image_embedding', []),
                            'description_embedding': iv.get('description_embedding', []),
                            'image_embedding_model': iv.get('embedding_model', ''),
                            'description_embedding_model': iv.get('embedding_model', ''),
                            'related_text_chunks': metadata.get('related_text_chunks', []),
                            'related_table_chunks': metadata.get('related_table_chunks', []),
                            'parent_document_id': metadata.get('parent_document_id', ''),
                            'copy_status': metadata.get('copy_status', 'success'),
                            'vectorization_status': 'success',
                            'vector_type': 'visual_embedding',
                            'vectorization_timestamp': int(time.time()),
                            'incremental_update': True,
                            'update_timestamp': int(time.time())
                        })
                        total_vectors_added += 1
                
                # è¡¨æ ¼å‘é‡
                for tv in vectorization_result.get('table_vectors', []):
                    if tv.get('vectorization_status') == 'success':
                        all_vectors.append(tv.get('vector', []))
                        
                        # ä»två¯¹è±¡ä¸­æå–metadataï¼Œä¸æ–°å»ºæ¨¡å¼ä¿æŒä¸€è‡´
                        metadata = tv.get('metadata', {})
                        all_metadata.append({
                            'type': 'table',
                            'chunk_type': 'table',
                            'source': item.get('file_info', {}).get('name', '') or metadata.get('document_name', ''),
                            'document_name': metadata.get('document_name', ''),
                            'page_number': metadata.get('page_number', 1),
                            'chunk_id': metadata.get('chunk_id', ''),
                            'table_id': metadata.get('table_id', ''),
                            'table_type': metadata.get('table_type', 'data_table'),
                            'table_title': metadata.get('table_title', ''),
                            'table_summary': metadata.get('table_summary', ''),
                            'table_headers': metadata.get('table_headers', []),
                            'table_row_count': metadata.get('table_rows', 0),
                            'table_column_count': metadata.get('table_columns', 0),
                            'table_body': metadata.get('table_body', ''),
                            'table_content': metadata.get('table_content', ''),
                            'table_caption': metadata.get('table_caption', []),
                            'table_footnote': metadata.get('table_footnote', []),
                            'is_subtable': metadata.get('is_subtable', False),
                            'parent_table_id': metadata.get('parent_table_id'),
                            'subtable_index': metadata.get('subtable_index'),
                            'chunk_start_row': metadata.get('chunk_start_row', 0),
                            'chunk_end_row': metadata.get('chunk_end_row', 0),
                            'related_text': metadata.get('related_text', ''),
                            'related_images': metadata.get('related_images', []),
                            'related_text_chunks': metadata.get('related_text_chunks', []),
                            'table_context': metadata.get('table_context', ''),
                            'table_embedding': tv.get('vector', []),  # ä¿å­˜å®é™…çš„table embeddingå‘é‡
                            'table_embedding_model': tv.get('table_embedding_model', 'text-embedding-v1'),  # ä¿å­˜embeddingæ¨¡å‹ä¿¡æ¯
                            'vector_type': 'text_embedding',
                            'vectorization_status': 'success',
                            'vectorization_timestamp': int(time.time()),
                            'incremental_update': True,
                            'update_timestamp': int(time.time())
                        })
                        total_vectors_added += 1
            
            # æ‰¹é‡æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            if all_vectors and all_metadata:
                success = self.vector_store_manager.add_vectors(all_vectors, all_metadata)
                if success:
                    print(f"       âœ… æˆåŠŸæ·»åŠ  {total_vectors_added} ä¸ªå‘é‡åˆ°æ•°æ®åº“")
                else:
                    print(f"       âŒ æ·»åŠ å‘é‡åˆ°æ•°æ®åº“å¤±è´¥")
                    total_vectors_added = 0
            
            return total_vectors_added
            
        except Exception as e:
            logging.error(f"å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return 0

    def _count_vectors(self, vectorization_result: Dict[str, Any]) -> int:
        """
        è®¡ç®—å‘é‡æ•°é‡
        
        :param vectorization_result: å‘é‡åŒ–ç»“æœ
        :return: å‘é‡æ€»æ•°
        """
        try:
            count = 0
            
            # æ–‡æœ¬å‘é‡
            count += len([v for v in vectorization_result.get('text_vectors', []) if v.get('status') == 'success'])
            
            # å›¾åƒå‘é‡
            count += len([v for v in vectorization_result.get('image_vectors', []) if v.get('status') == 'success'])
            
            # è¡¨æ ¼å‘é‡
            count += len([v for v in vectorization_result.get('table_vectors', []) if v.get('status') == 'success'])
            
            return count
            
        except Exception as e:
            logging.error(f"è®¡ç®—å‘é‡æ•°é‡å¤±è´¥: {e}")
            return 0

    def _store_results(self, processing_result: Dict[str, Any], target_vector_db: str) -> Dict[str, Any]:
        """å­˜å‚¨å¤„ç†ç»“æœ"""
        try:
            print("   ğŸ’¾ å­˜å‚¨å¤„ç†ç»“æœ...")
            
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            os.makedirs(target_vector_db, exist_ok=True)
            
            # è·å–å¤„ç†çš„é¡¹ç›®
            processed_items = processing_result.get('processed_items', [])
            if not processed_items:
                print("     æ²¡æœ‰éœ€è¦å­˜å‚¨çš„å†…å®¹")
                return {
                    'stored_items': 0,
                    'storage_path': target_vector_db,
                    'status': 'success',
                    'message': 'æ²¡æœ‰éœ€è¦å­˜å‚¨çš„å†…å®¹'
                }
            
            # ç»Ÿè®¡å‘é‡æ•°é‡
            total_vectors = 0
            total_metadata = 0
            
            # æ”¶é›†æ‰€æœ‰å‘é‡å’Œå…ƒæ•°æ®
            all_vectors = []
            all_metadata = []
            
            for item in processed_items:
                if item.get('status') == 'success':
                    vectorization_result = item.get('vectorization_result', {})
                    
                    # æ”¶é›†æ–‡æœ¬å‘é‡
                    text_vectors = vectorization_result.get('text_vectors', [])
                    for tv in text_vectors:
                        if tv.get('vectorization_status') == 'success':
                            all_vectors.append(tv['vector'])
                            # ä¿å­˜å®Œæ•´çš„æ–‡æœ¬å…ƒæ•°æ®ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
                            metadata = tv.get('metadata', {})
                            logging.info(f"æ–‡æœ¬å‘é‡tvçš„metadataå­—æ®µ: {'metadata' in tv}")
                            if 'metadata' in tv:
                                logging.info(f"æ–‡æœ¬metadataå†…å®¹: {tv['metadata']}")

                            text_metadata = {
                                'type': 'text',
                                'chunk_type': 'text',
                                'source': item.get('pdf_path'),
                                'document_name': metadata.get('document_name', ''),
                                'page_number': metadata.get('page_number', 1),
                                'chunk_id': metadata.get('chunk_id', ''),
                                'text': metadata.get('text', ''),
                                'text_length': metadata.get('text_length', 0),
                                'text_level': metadata.get('text_level', 0),
                                'chunk_size': metadata.get('chunk_size', 0),
                                'chunk_overlap': metadata.get('chunk_overlap', 0),
                                'chunk_position': metadata.get('chunk_position', {}),
                                'related_images': metadata.get('related_images', []),
                                'related_tables': metadata.get('related_tables', []),
                                'text_embedding': tv['vector'],  # ä¿å­˜å®é™…çš„text embeddingå‘é‡
                                'text_embedding_model': tv.get('text_embedding_model', 'text-embedding-v1'),  # ä¿å­˜embeddingæ¨¡å‹ä¿¡æ¯
                                'vector_type': 'text_embedding',
                                'vectorization_status': 'success',
                                'vectorization_timestamp': int(time.time())
                            }
                            all_metadata.append(text_metadata)
                            total_vectors += 1
                    
                    # æ”¶é›†å›¾åƒå‘é‡
                    image_vectors = vectorization_result.get('image_vectors', [])
                    for iv in image_vectors:
                        if iv.get('vectorization_status') == 'success':
                            all_vectors.append(iv['image_embedding'])
                            # ä¿å­˜å®Œæ•´çš„å›¾ç‰‡å…ƒæ•°æ®ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
                            logging.info(f"å›¾åƒå‘é‡ivçš„ç»“æ„: {list(iv.keys())}")
                            logging.info(f"å›¾åƒå‘é‡ivçš„metadataå­—æ®µ: {'metadata' in iv}")
                            if 'metadata' in iv:
                                logging.info(f"å›¾åƒmetadataå†…å®¹: {iv['metadata']}")

                            # è·å–metadata
                            metadata = iv.get('metadata', {})
                            image_metadata = {
                                'type': 'image',
                                'chunk_type': 'image',
                                'source': item.get('pdf_path'),
                                'document_name': metadata.get('document_name', ''),
                                'page_number': metadata.get('page_number', 1),
                                'chunk_id': metadata.get('chunk_id', ''),
                                'image_id': metadata.get('image_id', ''),
                                'image_path': metadata.get('image_path', ''),
                                'image_filename': metadata.get('image_filename', ''),
                                'image_type': metadata.get('image_type', 'general'),
                                'image_format': metadata.get('image_format', 'UNKNOWN'),
                                'image_dimensions': metadata.get('image_dimensions', {}),
                                'basic_description': metadata.get('basic_description', ''),
                                'enhanced_description': metadata.get('enhanced_description', ''),
                                'layered_descriptions': metadata.get('layered_descriptions', {}),
                                'structured_info': metadata.get('structured_info', {}),
                                'img_caption': metadata.get('img_caption', []),
                                'img_footnote': metadata.get('img_footnote', []),
                                'enhancement_enabled': metadata.get('enhancement_enabled', True),
                                'enhancement_model': metadata.get('enhancement_model', ''),
                                'enhancement_status': metadata.get('enhancement_status', 'success'),
                                'enhancement_timestamp': metadata.get('enhancement_timestamp'),
                                'image_embedding': iv.get('image_embedding', []),
                                'description_embedding': iv.get('description_embedding', []),
                                'image_embedding_model': iv.get('embedding_model', ''),
                                'description_embedding_model': iv.get('embedding_model', ''),
                                'related_text_chunks': metadata.get('related_text_chunks', []),
                                'related_table_chunks': metadata.get('related_table_chunks', []),
                                'parent_document_id': metadata.get('parent_document_id', ''),
                                'copy_status': metadata.get('copy_status', 'success'),
                                'vectorization_status': 'success',
                                'vector_type': 'visual_embedding',
                                'vectorization_timestamp': int(time.time())
                            }
                            all_metadata.append(image_metadata)
                            total_vectors += 1
                    
                    # æ”¶é›†è¡¨æ ¼å‘é‡
                    table_vectors = vectorization_result.get('table_vectors', [])
                    for tv in table_vectors:
                        if tv.get('vectorization_status') == 'success':
                            all_vectors.append(tv['vector'])
                            # ä¿å­˜å®Œæ•´çš„è¡¨æ ¼å…ƒæ•°æ®ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
                            metadata = tv.get('metadata', {})
                            logging.info(f"è¡¨æ ¼å‘é‡tvçš„metadataå­—æ®µ: {'metadata' in tv}")
                            if 'metadata' in tv:
                                logging.info(f"è¡¨æ ¼metadataå†…å®¹: {tv['metadata']}")

                            table_metadata = {
                                'type': 'table',
                                'chunk_type': 'table',
                                'source': item.get('pdf_path'),
                                'document_name': metadata.get('document_name', ''),
                                'page_number': metadata.get('page_number', 1),
                                'chunk_id': metadata.get('chunk_id', ''),
                                'table_id': metadata.get('table_id', ''),
                                'table_type': metadata.get('table_type', 'data_table'),
                                'table_title': metadata.get('table_title', ''),
                                'table_summary': metadata.get('table_summary', ''),
                                'table_headers': metadata.get('table_headers', []),
                                'table_row_count': metadata.get('table_rows', 0),
                                'table_column_count': metadata.get('table_columns', 0),
                                'table_body': metadata.get('table_body', ''),
                                'table_content': metadata.get('table_content', ''),
                                'table_caption': metadata.get('table_caption', []),
                                'table_footnote': metadata.get('table_footnote', []),
                                'is_subtable': metadata.get('is_subtable', False),
                                'parent_table_id': metadata.get('parent_table_id'),
                                'subtable_index': metadata.get('subtable_index'),
                                'chunk_start_row': metadata.get('chunk_start_row', 0),
                                'chunk_end_row': metadata.get('chunk_end_row', 0),
                                'related_text': metadata.get('related_text', ''),
                                'related_images': metadata.get('related_images', []),
                                'related_text_chunks': metadata.get('related_text_chunks', []),
                                'table_context': metadata.get('table_context', ''),
                                'table_embedding': tv['vector'],  # ä¿å­˜å®é™…çš„table embeddingå‘é‡
                                'table_embedding_model': tv.get('table_embedding_model', 'text-embedding-v1'),  # ä¿å­˜embeddingæ¨¡å‹ä¿¡æ¯
                                'vector_type': 'text_embedding',
                                'vectorization_status': 'success',
                                'vectorization_timestamp': int(time.time())
                            }
                            all_metadata.append(table_metadata)
                            total_vectors += 1
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            if all_vectors:
                print(f"     å­˜å‚¨ {len(all_vectors)} ä¸ªå‘é‡åˆ°æ•°æ®åº“...")
                
                # ä½¿ç”¨å‘é‡å­˜å‚¨ç®¡ç†å™¨å­˜å‚¨
                success = self.vector_store_manager.add_vectors(all_vectors, all_metadata)
                
                if success:
                    print(f"     âœ… å‘é‡å­˜å‚¨æˆåŠŸ")
                    
                    # ä¿å­˜å‘é‡æ•°æ®åº“åˆ°ç£ç›˜
                    print(f"     ğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“åˆ°ç£ç›˜...")
                    if self.vector_store_manager.save():
                        print(f"     âœ… å‘é‡æ•°æ®åº“ä¿å­˜æˆåŠŸ")
                    else:
                        print(f"     âš ï¸  å‘é‡æ•°æ®åº“ä¿å­˜å¤±è´¥")
                    
                    # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
                    logs_dir = self.config_manager.get_path('logs_dir')
                    metadata_file = os.path.join(logs_dir, 'processing_metadata.json')
                    metadata_summary = {
                        'total_vectors': total_vectors,
                        'total_files': len(processed_items),
                        'success_count': sum(1 for item in processed_items if item.get('status') == 'success'),
                        'failed_count': sum(1 for item in processed_items if item.get('status') == 'failed'),
                        'vector_types': {
                            'text': sum(1 for m in all_metadata if m['type'] == 'text'),
                            'image': sum(1 for m in all_metadata if m['type'] == 'image'),
                            'table': sum(1 for m in all_metadata if m['type'] == 'table')
                        },
                        'processing_timestamp': int(time.time()),
                        'vector_db_path': target_vector_db,  # æ·»åŠ å‘é‡æ•°æ®åº“è·¯å¾„ä¿¡æ¯
                        'items': processed_items
                    }
                    
                    with open(metadata_file, 'w', encoding='utf-8') as f:
                        json.dump(metadata_summary, f, ensure_ascii=False, indent=2)
                    
                    print(f"     ğŸ“„ å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {metadata_file}")
                    
                    return {
                        'stored_items': total_vectors,
                        'storage_path': target_vector_db,
                        'metadata_file': metadata_file,
                        'status': 'success'
                    }
                else:
                    print(f"     âŒ å‘é‡å­˜å‚¨å¤±è´¥")
                    return {
                        'stored_items': 0,
                        'storage_path': target_vector_db,
                        'status': 'failed',
                        'error': 'å‘é‡å­˜å‚¨å¤±è´¥'
                    }
            else:
                print("     æ²¡æœ‰æœ‰æ•ˆçš„å‘é‡éœ€è¦å­˜å‚¨")
                return {
                    'stored_items': 0,
                    'storage_path': target_vector_db,
                    'status': 'success',
                    'message': 'æ²¡æœ‰æœ‰æ•ˆçš„å‘é‡éœ€è¦å­˜å‚¨'
                }
                
        except Exception as e:
            logging.error(f"å­˜å‚¨å¤„ç†ç»“æœå¤±è´¥: {e}")
            print(f"     âŒ å­˜å‚¨å¤±è´¥: {e}")
            return {
                'stored_items': 0,
                'storage_path': target_vector_db,
                'status': 'failed',
                'error': str(e)
            }

    def _process_vectorization_result(self, item: Dict[str, Any], updated_vectors: List, updated_metadata: List, 
                                    text_updates: int, image_updates: int, table_updates: int) -> None:
        """
        å¤„ç†å‘é‡åŒ–ç»“æœï¼Œæå–å‘é‡å’Œå…ƒæ•°æ®
        
        :param item: å¤„ç†é¡¹ç›®
        :param updated_vectors: æ›´æ–°çš„å‘é‡åˆ—è¡¨
        :param updated_metadata: æ›´æ–°çš„å…ƒæ•°æ®åˆ—è¡¨
        :param text_updates: æ–‡æœ¬æ›´æ–°è®¡æ•°
        :param image_updates: å›¾åƒæ›´æ–°è®¡æ•°
        :param table_updates: è¡¨æ ¼æ›´æ–°è®¡æ•°
        """
        try:
            # è·å–å‘é‡åŒ–ç»“æœ
            vectorization_result = item.get('vectorization_result', {})
            
            # æ›´æ–°æ–‡æœ¬å‘é‡
            text_vectors = vectorization_result.get('text_vectors', [])
            for tv in text_vectors:
                if tv.get('vectorization_status') == 'success':  # ä¿®å¤ï¼šä½¿ç”¨ vectorization_status ä¸æ–°å»ºæ¨¡å¼ä¸€è‡´
                    # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼Œå‚è€ƒæ–°å»ºæ¨¡å¼çš„å®ç°
                    vector_data = tv.get('vector', [])
                    if vector_data:  # ç¡®ä¿å‘é‡æ•°æ®å­˜åœ¨
                        updated_vectors.append(vector_data)
                        
                        # ä»två¯¹è±¡ä¸­æå–metadataï¼Œä¸æ–°å»ºæ¨¡å¼ä¿æŒä¸€è‡´
                        metadata = tv.get('metadata', {})
                        updated_metadata.append({
                            'type': 'text',
                            'chunk_type': 'text',
                            'source': item.get('file_info', {}).get('name', '') or metadata.get('document_name', ''),
                            'document_name': metadata.get('document_name', ''),
                            'page_number': metadata.get('page_number', 1),
                            'chunk_id': metadata.get('chunk_id', ''),
                            'text': metadata.get('text', ''),
                            'text_length': metadata.get('text_length', 0),
                            'text_level': metadata.get('text_level', 0),
                            'chunk_size': metadata.get('chunk_size', 0),
                            'chunk_overlap': metadata.get('chunk_overlap', 0),
                            'chunk_position': metadata.get('chunk_position', {}),
                            'related_images': metadata.get('related_images', []),
                            'related_tables': metadata.get('related_tables', []),
                            'text_embedding': vector_data,  # æ·»åŠ text_embeddingå­—æ®µ
                            'text_embedding_model': tv.get('text_embedding_model', 'text-embedding-v1'),  # æ·»åŠ æ¨¡å‹ä¿¡æ¯
                            'vector_type': 'text_embedding',
                            'vectorization_status': 'success',
                            'vectorization_timestamp': int(time.time()),
                            'update_type': 'content_update',
                            'update_timestamp': int(time.time()),
                            'incremental_update': True
                        })
                        text_updates += 1
            
            # æ›´æ–°å›¾åƒå‘é‡
            image_vectors = vectorization_result.get('image_vectors', [])
            for iv in image_vectors:
                if iv.get('vectorization_status') == 'success':  # ä¿®å¤ï¼šä½¿ç”¨ vectorization_status ä¸æ–°å»ºæ¨¡å¼ä¸€è‡´
                    # ä¿®å¤ï¼šå›¾åƒå‘é‡ä½¿ç”¨ 'image_embedding' å­—æ®µï¼Œå‚è€ƒæ–°å»ºæ¨¡å¼
                    vector_data = iv.get('image_embedding', [])
                    if vector_data:  # ç¡®ä¿å‘é‡æ•°æ®å­˜åœ¨
                        updated_vectors.append(vector_data)
                        
                        # ä»ivå¯¹è±¡ä¸­æå–metadataï¼Œä¸æ–°å»ºæ¨¡å¼ä¿æŒä¸€è‡´
                        metadata = iv.get('metadata', {})
                        updated_metadata.append({
                            'type': 'image',
                            'chunk_type': 'image',
                            'source': item.get('file_info', {}).get('name', '') or metadata.get('document_name', ''),
                            'document_name': metadata.get('document_name', ''),
                            'page_number': metadata.get('page_number', 1),
                            'chunk_id': metadata.get('chunk_id', ''),
                            'image_id': metadata.get('image_id', ''),
                            'image_path': metadata.get('image_path', ''),
                            'image_filename': metadata.get('image_filename', ''),
                            'image_type': metadata.get('image_type', 'general'),
                            'image_format': metadata.get('image_format', 'UNKNOWN'),
                            'image_dimensions': metadata.get('image_dimensions', {}),
                            'basic_description': metadata.get('basic_description', ''),
                            'enhanced_description': metadata.get('enhanced_description', ''),
                            'layered_descriptions': metadata.get('layered_descriptions', {}),
                            'structured_info': metadata.get('structured_info', {}),
                            'img_caption': metadata.get('img_caption', []),
                            'img_footnote': metadata.get('img_footnote', []),
                            'enhancement_enabled': metadata.get('enhancement_enabled', True),
                            'enhancement_model': metadata.get('enhancement_model', ''),
                            'enhancement_status': metadata.get('enhancement_status', 'success'),
                            'enhancement_timestamp': metadata.get('enhancement_timestamp'),
                            'image_embedding': iv.get('image_embedding', []),
                            'description_embedding': iv.get('description_embedding', []),
                            'image_embedding_model': iv.get('embedding_model', ''),
                            'description_embedding_model': iv.get('embedding_model', ''),
                            'related_text_chunks': metadata.get('related_text_chunks', []),
                            'related_table_chunks': metadata.get('related_table_chunks', []),
                            'parent_document_id': metadata.get('parent_document_id', ''),
                            'copy_status': metadata.get('copy_status', 'success'),
                            'vectorization_status': 'success',
                            'vector_type': 'visual_embedding',
                            'vectorization_timestamp': int(time.time()),
                            'update_type': 'content_update',
                            'update_timestamp': int(time.time()),
                            'incremental_update': True
                        })
                        image_updates += 1
            
            # æ›´æ–°è¡¨æ ¼å‘é‡
            table_vectors = vectorization_result.get('table_vectors', [])
            for tv in table_vectors:
                if tv.get('vectorization_status') == 'success':  # ä¿®å¤ï¼šä½¿ç”¨ vectorization_status ä¸æ–°å»ºæ¨¡å¼ä¸€è‡´
                    # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼Œå‚è€ƒæ–°å»ºæ¨¡å¼çš„å®ç°
                    vector_data = tv.get('vector', [])
                    if vector_data:  # ç¡®ä¿å‘é‡æ•°æ®å­˜åœ¨
                        updated_vectors.append(vector_data)
                        
                        # ä»två¯¹è±¡ä¸­æå–metadataï¼Œä¸æ–°å»ºæ¨¡å¼ä¿æŒä¸€è‡´
                        metadata = tv.get('metadata', {})
                        updated_metadata.append({
                            'type': 'table',
                            'chunk_type': 'table',
                            'source': item.get('file_info', {}).get('name', '') or metadata.get('document_name', ''),
                            'document_name': metadata.get('document_name', ''),
                            'page_number': metadata.get('page_number', 1),
                            'chunk_id': metadata.get('chunk_id', ''),
                            'table_id': metadata.get('table_id', ''),
                            'table_type': metadata.get('table_type', 'data_table'),
                            'table_title': metadata.get('table_title', ''),
                            'table_summary': metadata.get('table_summary', ''),
                            'table_headers': metadata.get('table_headers', []),
                            'table_row_count': metadata.get('table_rows', 0),
                            'table_column_count': metadata.get('table_columns', 0),
                            'table_body': metadata.get('table_body', ''),
                            'table_content': metadata.get('table_content', ''),
                            'table_caption': metadata.get('table_caption', []),
                            'table_footnote': metadata.get('table_footnote', []),
                            'is_subtable': metadata.get('is_subtable', False),
                            'parent_table_id': metadata.get('parent_table_id'),
                            'subtable_index': metadata.get('subtable_index'),
                            'chunk_start_row': metadata.get('chunk_start_row', 0),
                            'chunk_end_row': metadata.get('chunk_end_row', 0),
                            'related_text': metadata.get('related_text', ''),
                            'related_images': metadata.get('related_images', []),
                            'related_text_chunks': metadata.get('related_text_chunks', []),
                            'table_context': metadata.get('table_context', ''),
                            'table_embedding': vector_data,  # æ·»åŠ table_embeddingå­—æ®µ
                            'table_embedding_model': tv.get('table_embedding_model', 'text-embedding-v1'),  # æ·»åŠ æ¨¡å‹ä¿¡æ¯
                            'vector_type': 'text_embedding',
                            'vectorization_status': 'success',
                            'vectorization_timestamp': int(time.time()),
                            'update_type': 'content_update',
                            'update_timestamp': int(time.time()),
                            'incremental_update': True
                        })
                        table_updates += 1
                    
        except Exception as e:
            logging.error(f"å¤„ç†å‘é‡åŒ–ç»“æœå¤±è´¥: {e}")
            raise

    def _update_results(self, processing_result: Dict[str, Any], target_vector_db: str) -> Dict[str, Any]:
        """
        æ›´æ–°å¤„ç†ç»“æœ
        
        åŠŸèƒ½ï¼š
        - æ›´æ–°å‘é‡æ•°æ®åº“ä¸­çš„ç°æœ‰å†…å®¹
        - å¤„ç†å†…å®¹æ›´æ–°å’Œä¿®æ”¹
        - ç»´æŠ¤æ•°æ®ä¸€è‡´æ€§
        - æä¾›è¯¦ç»†çš„æ›´æ–°ç»Ÿè®¡
        """
        try:
            print("   ğŸ”„ æ›´æ–°å‘é‡æ•°æ®åº“...")
            
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            os.makedirs(target_vector_db, exist_ok=True)
            
            # è·å–å¤„ç†çš„é¡¹ç›®
            processed_items = processing_result.get('processed_items', [])
            if not processed_items:
                print("     æ²¡æœ‰éœ€è¦æ›´æ–°çš„å†…å®¹")
                return {
                    'updated_items': 0,
                    'storage_path': target_vector_db,
                    'status': 'success',
                    'message': 'æ²¡æœ‰éœ€è¦æ›´æ–°çš„å†…å®¹'
                }
            
            # ç»Ÿè®¡æ›´æ–°ä¿¡æ¯
            total_updated = 0
            text_updates = 0
            image_updates = 0
            table_updates = 0
            failed_updates = 0
            
            # æ”¶é›†éœ€è¦æ›´æ–°çš„å‘é‡å’Œå…ƒæ•°æ®
            updated_vectors = []
            updated_metadata = []
            
            for item in processed_items:
                if item.get('status') == 'success':
                    try:
                        # å¤„ç†æ•°æ®ç»“æ„åµŒå¥—é—®é¢˜
                        # å¢é‡æ¨¡å¼è¿”å›çš„æ˜¯åµŒå¥—ç»“æ„ï¼Œæ–°å»ºæ¨¡å¼è¿”å›çš„æ˜¯ç›´æ¥ç»“æ„
                        if 'processed_items' in item:
                            # å¤„ç†åµŒå¥—ç»“æ„ï¼ˆå¢é‡æ¨¡å¼ï¼‰
                            print(f"     æ£€æµ‹åˆ°å¢é‡æ¨¡å¼æ•°æ®ç»“æ„ï¼Œå¤„ç†åµŒå¥—é¡¹...")
                            for sub_item in item['processed_items']:
                                if sub_item.get('status') == 'success':
                                    self._process_vectorization_result(
                                        sub_item, updated_vectors, updated_metadata,
                                        text_updates, image_updates, table_updates
                                    )
                                    total_updated += 1
                        else:
                            # å¤„ç†ç›´æ¥ç»“æ„ï¼ˆæ–°å»ºæ¨¡å¼ï¼‰
                            self._process_vectorization_result(
                                item, updated_vectors, updated_metadata,
                                text_updates, image_updates, table_updates
                            )
                            total_updated += 1
                        
                    except Exception as e:
                        logging.error(f"å¤„ç†é¡¹ç›®æ›´æ–°å¤±è´¥: {e}")
                        failed_updates += 1
                        continue
                else:
                    failed_updates += 1
            
            # æ‰§è¡Œæ•°æ®åº“æ›´æ–°
            if updated_vectors and updated_metadata:
                print(f"     å‡†å¤‡æ›´æ–° {len(updated_vectors)} ä¸ªå‘é‡...")
                
                # ä½¿ç”¨å‘é‡å­˜å‚¨ç®¡ç†å™¨æ›´æ–°
                update_success = self.vector_store_manager.update_vectors(
                    updated_vectors, updated_metadata
                )
                
                if update_success:
                    print(f"     âœ… æˆåŠŸæ›´æ–° {len(updated_vectors)} ä¸ªå‘é‡")
                else:
                    print(f"     âŒ å‘é‡æ•°æ®åº“æ›´æ–°å¤±è´¥")
                    return {
                        'updated_items': 0,
                        'storage_path': target_vector_db,
                        'status': 'failed',
                        'error': 'å‘é‡æ•°æ®åº“æ›´æ–°å¤±è´¥'
                    }
            
            # æ›´æ–°å…ƒæ•°æ®ç®¡ç†å™¨
            self._update_metadata_manager(updated_metadata)
            
            # ç”Ÿæˆæ›´æ–°ç»“æœ
            result = {
                'updated_items': total_updated,
                'storage_path': target_vector_db,
                'status': 'success',
                'update_statistics': {
                    'total_vectors_updated': len(updated_vectors),
                    'text_updates': text_updates,
                    'image_updates': image_updates,
                    'table_updates': table_updates,
                    'failed_updates': failed_updates
                },
                'update_timestamp': int(time.time()),
                'update_type': 'content_update'
            }
            
            print(f"   ğŸ“Š æ›´æ–°å®Œæˆ: æˆåŠŸæ›´æ–° {total_updated} ä¸ªé¡¹ç›®ï¼Œ{len(updated_vectors)} ä¸ªå‘é‡")
            return result
            
        except Exception as e:
            error_msg = f"æ›´æ–°å¤„ç†ç»“æœå¤±è´¥: {e}"
            logging.error(error_msg)
            self.failure_handler.record_failure('update_results', 'content_update', str(e))
            
            return {
                'updated_items': 0,
                'storage_path': target_vector_db,
                'status': 'failed',
                'error': str(e)
            }

    def _update_metadata_manager(self, updated_metadata: List[Dict[str, Any]]):
        """
        æ›´æ–°å…ƒæ•°æ®ç®¡ç†å™¨
        
        :param updated_metadata: æ›´æ–°çš„å…ƒæ•°æ®åˆ—è¡¨
        """
        try:
            if not updated_metadata:
                return
            
            # æ›´æ–°å…ƒæ•°æ®ç®¡ç†å™¨ä¸­çš„ç›¸å…³è®°å½•
            for metadata in updated_metadata:
                chunk_id = metadata.get('chunk_id') or metadata.get('image_id') or metadata.get('table_id')
                if chunk_id:
                    # æŸ¥æ‰¾ç°æœ‰å…ƒæ•°æ®å¹¶æ›´æ–°
                    existing_metadata = self.metadata_manager.get_metadata_by_id(chunk_id)
                    if existing_metadata:
                        # æ›´æ–°å…ƒæ•°æ®
                        updates = {
                            'updated_timestamp': int(time.time()),
                            'vectorization_status': 'updated',
                            'update_type': metadata.get('update_type', 'content_update'),
                            'update_timestamp': metadata.get('update_timestamp')
                        }
                        self.metadata_manager.update_metadata(chunk_id, updates)
            
            logging.info(f"å…ƒæ•°æ®ç®¡ç†å™¨æ›´æ–°å®Œæˆ: {len(updated_metadata)} æ¡è®°å½•")
            
        except Exception as e:
            logging.error(f"æ›´æ–°å…ƒæ•°æ®ç®¡ç†å™¨å¤±è´¥: {e}")

    def _generate_final_report(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        report = {
            'success': result.get('success', False),
            'mode': result.get('mode', 'unknown'),
            'timestamp': None,  # å°†åœ¨ä¸‹é¢è®¾ç½®
            'system_info': self._get_system_info(),
            'processing_stats': self._get_processing_stats(result)
        }
        
        # ä¿ç•™åŸå§‹çš„storage_pathå­—æ®µ
        if 'storage_path' in result:
            report['storage_path'] = result['storage_path']
        elif 'target_vector_db' in result:
            report['storage_path'] = result['target_vector_db']

        # è®¾ç½®æ—¶é—´æˆ³
        import time
        report['timestamp'] = int(time.time())

        # æ·»åŠ æ¨¡å¼ç‰¹å®šçš„ä¿¡æ¯
        if result.get('mode') == 'new':
            report['database_status'] = 'created'
        elif result.get('mode') == 'incremental':
            report['database_status'] = 'updated'

        # æ·»åŠ å¤±è´¥å¤„ç†ç»Ÿè®¡
        if hasattr(self, 'failure_handler') and self.failure_handler:
            report['failure_stats'] = self.failure_handler.get_failure_summary()

        return report

    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import platform

        return {
            'platform': platform.platform(),
            'python_version': platform.python_version(),
            'system': platform.system(),
            'processor': platform.processor()
        }

    def _get_processing_stats(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'mode': result.get('mode', 'unknown'),
            'success': result.get('success', False)
        }

        # æ·»åŠ å¤„ç†ç»“æœç»Ÿè®¡
        processing_result = result.get('processing_result', {})
        if processing_result:
            stats.update({
                'total_files': processing_result.get('total_files', 0),
                'processed_items': len(processing_result.get('processed_items', []))
            })

        # æ·»åŠ å­˜å‚¨ç»“æœç»Ÿè®¡
        storage_result = result.get('storage_result', {})
        if storage_result:
            stats.update({
                'stored_items': storage_result.get('stored_items', 0),
                'storage_path': storage_result.get('storage_path', '')
            })

        return stats

    def _get_pdf_files(self, input_path: str) -> List[str]:
        """è·å–PDFæ–‡ä»¶åˆ—è¡¨"""
        try:
            if not os.path.exists(input_path):
                return []
            
            pdf_files = []
            for file in os.listdir(input_path):
                if file.lower().endswith('.pdf'):
                    pdf_files.append(os.path.join(input_path, file))
            
            return sorted(pdf_files)
            
        except Exception as e:
            logging.error(f"è·å–PDFæ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def _parse_pdf_with_mineru(self, pdf_path: str, output_dir: str) -> Dict[str, Any]:
        """ä½¿ç”¨minerUè§£æPDF"""
        try:
            # å¯¼å…¥minerUé›†æˆæ¨¡å—
            from utils.mineru_integration import MinerUIntegration
            
            # åˆ›å»ºminerUé›†æˆå®ä¾‹
            mineru = MinerUIntegration(self.config_manager)
            
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„mineru_output_dirï¼Œè€Œä¸æ˜¯ä¼ å…¥çš„output_dir
            mineru_output_dir = self.config_manager.get_path('mineru_output_dir')
            
            # è§£æPDFåˆ°æ­£ç¡®çš„MinerUè¾“å‡ºç›®å½•
            result = mineru.parse_pdf_document(pdf_path, mineru_output_dir)
            
            return result
            
        except Exception as e:
            logging.error(f"minerUè§£æPDFå¤±è´¥: {pdf_path}, é”™è¯¯: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    # æ³¨æ„ï¼š_vectorize_parsed_contentæ–¹æ³•å·²è¢«åˆ é™¤
    # å›¾ç‰‡å‘é‡åŒ–ç°åœ¨é€šè¿‡ImageProcessor -> ImageVectorizerç»Ÿä¸€å¤„ç†
    # æ–‡æœ¬å’Œè¡¨æ ¼å‘é‡åŒ–é€šè¿‡VectorizationManagerç»Ÿä¸€å¤„ç†

    def _call_mineru_api(self, pdf_path: str) -> Dict[str, Any]:
        """
        è°ƒç”¨MinerU APIå¤„ç†PDFæ–‡æ¡£
        
        :param pdf_path: PDFæ–‡ä»¶è·¯å¾„
        :return: MinerUå¤„ç†ç»“æœ
        """
        try:
            if not pdf_path or not os.path.exists(pdf_path):
                raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            
            # è·å–MinerUé…ç½®
            mineru_config = self.config_manager.get('mineru', {})
            api_key = mineru_config.get('api_key') or os.getenv('MINERU_API_KEY')
            output_dir = mineru_config.get('output_dir', './document/md')
            
            if not api_key:
                raise RuntimeError("ç¼ºå°‘MinerU APIå¯†é’¥")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # è°ƒç”¨MinerUé›†æˆæ¨¡å—
            from utils.mineru_integration import MinerUIntegration
            mineru = MinerUIntegration(self.config_manager)
            
            # è§£æPDFåˆ°æ­£ç¡®çš„MinerUè¾“å‡ºç›®å½•
            result = mineru.parse_pdf_document(pdf_path, output_dir)
            
            return result
            
        except Exception as e:
            logging.error(f"minerUè§£æPDFå¤±è´¥: {pdf_path}, é”™è¯¯: {e}")
            return {
                'success': False,
                'error': str(e)
            }


if __name__ == "__main__":
    # æµ‹è¯•V3MainProcessor
    try:
        processor = V3MainProcessor()

        # æµ‹è¯•é»˜è®¤å¤„ç†
        result = processor.process_documents()
        print(f"\nå¤„ç†ç»“æœ: {'æˆåŠŸ' if result.get('success') else 'å¤±è´¥'}")

        if not result.get('success'):
            print(f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    except Exception as e:
        print(f"V3MainProcessoræµ‹è¯•å¤±è´¥: {e}")
