# **å›¾ç‰‡å¢å¼ºå¤„ç†å®Œæ•´ä¼˜åŒ–æ–¹æ¡ˆ**

## **ï¿½ï¿½ æ–¹æ¡ˆæ¦‚è¿°**

é‡æ–°è®¾è®¡å›¾ç‰‡å¢å¼ºå¤„ç†æµç¨‹ï¼Œå®ç°ï¼š
1. **ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å¢å¼ºä¿¡æ¯**ï¼Œé¿å…å¤šå±‚å¤„ç†å¯¼è‡´çš„é‡å¤
2. **æ™ºèƒ½å»é‡æœºåˆ¶**ï¼Œç¡®ä¿ä¿¡æ¯è´¨é‡å’Œä¸€è‡´æ€§
3. **å®Œå…¨æ¶æ„åŒ–**ï¼Œä½¿ç”¨é…ç½®ç®¡ç†å’Œå¤±è´¥å¤„ç†
4. **ä¿æŒåŠŸèƒ½å®Œæ•´æ€§**ï¼Œå‚è€ƒåŸæœ‰å®ç°çš„ä¼˜ç§€è®¾è®¡
5. **å…ƒæ•°æ®å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ**

---

## **ğŸ”§ ç¬¬ä¸€éƒ¨åˆ†ï¼šå›¾ç‰‡å¢å¼ºå¤„ç†å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰**

### **1.1 æ ¸å¿ƒç±»è®¾è®¡**

```python
class ImageEnhancer:
    """
    å›¾ç‰‡å¢å¼ºå¤„ç†å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å¢å¼ºä¿¡æ¯ï¼Œé¿å…é‡å¤å†…å®¹
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # ä½¿ç”¨é…ç½®
        self.enhancement_model = self.config.get('image_processing.enhancement_model', 'qwen-vl-plus')
        self.enhancement_model_api = self.config.get('image_processing.enhancement_model_api', 'dashscope')
        self.max_tokens = self.config.get('image_processing.enhancement_max_tokens', 1000)
        self.temperature = self.config.get('image_processing.enhancement_temperature', 0.1)
        self.batch_size = self.config.get('api_rate_limiting.enhancement_batch_size', 5)
        self.delay_seconds = self.config.get('api_rate_limiting.enhancement_delay_seconds', 2)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†
        self.failure_handler = config_manager.get_failure_handler()
        
        # åˆå§‹åŒ–APIå¯†é’¥
        self.dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')
        if not self.dashscope_api_key:
            raise ValueError("æœªè®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
        
        # åˆå§‹åŒ–DashScope
        import dashscope
        dashscope.api_key = self.dashscope_api_key
        
        # åŠ è½½å¤„ç†æ ‡è®°é…ç½®
        self._load_processing_markers()
        
        logging.info("å›¾ç‰‡å¢å¼ºå¤„ç†å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰åˆå§‹åŒ–å®Œæˆ")
    
    def _load_processing_markers(self):
        """
        ä»é…ç½®åŠ è½½å¤„ç†æ ‡è®°
        """
        # é»˜è®¤æ ‡è®°é…ç½®
        default_markers = {
            'layer_markers': [
                'åŸºç¡€è§†è§‰æè¿°', 'å†…å®¹ç†è§£æè¿°', 'æ•°æ®è¶‹åŠ¿æè¿°', 'è¯­ä¹‰ç‰¹å¾æè¿°'
            ],
            'structure_markers': [
                'å›¾è¡¨ç±»å‹', 'æ•°æ®ç‚¹', 'è¶‹åŠ¿åˆ†æ', 'å…³é”®æ´å¯Ÿ'
            ],
            'format_variants': [
                '**', '-', 'ï¼š', ':'
            ]
        }
        
        # ä»é…ç½®åŠ è½½ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        config_markers = self.config.get('image_processing.processing_markers', default_markers)
        
        # åŠ¨æ€ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æ ‡è®°ç»„åˆ
        self.all_markers = self._generate_marker_combinations(config_markers)
    
    def _generate_marker_combinations(self, marker_config: Dict) -> List[str]:
        """
        åŠ¨æ€ç”Ÿæˆæ ‡è®°ç»„åˆ
        """
        all_markers = []
        
        # ä¸ºæ¯ä¸ªæ ‡è®°ç”Ÿæˆæ‰€æœ‰æ ¼å¼å˜ä½“
        for marker in marker_config.get('layer_markers', []):
            for variant in marker_config.get('format_variants', []):
                all_markers.append(f"{variant}{marker}{variant}")
                all_markers.append(f"{variant}{marker}")
                all_markers.append(f"{marker}{variant}")
                all_markers.append(marker)
        
        # æ·»åŠ ç»“æ„æ ‡è®°
        for marker in marker_config.get('structure_markers', []):
            for variant in marker_config.get('format_variants', []):
                all_markers.append(f"{variant}{marker}{variant}")
                all_markers.append(f"{variant}{marker}")
                all_markers.append(f"{marker}{variant}")
                all_markers.append(marker)
        
        # å»é‡å¹¶æ’åºï¼ˆæŒ‰é•¿åº¦é™åºï¼Œç¡®ä¿é•¿æ ‡è®°ä¼˜å…ˆåŒ¹é…ï¼‰
        all_markers = sorted(list(set(all_markers)), key=len, reverse=True)
        
        return all_markers
```

### **1.2 ä¸€æ¬¡æ€§å¢å¼ºæ–¹æ³•**

```python
def enhance_image_complete(self, image_path: str, mineru_info: Dict) -> Dict[str, Any]:
    """
    ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´çš„å›¾ç‰‡å¢å¼ºä¿¡æ¯ï¼Œé¿å…é‡å¤
    """
    try:
        # 1. è·å–MinerUåŸå§‹ä¿¡æ¯
        img_caption = mineru_info.get('img_caption', [])
        img_footnote = mineru_info.get('img_footnote', [])
        
        # 2. è°ƒç”¨è§†è§‰æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ
        vision_response = self._call_vision_model(image_path)
        if not vision_response:
            # å¦‚æœè§†è§‰æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯
            return self._create_fallback_description(img_caption, img_footnote)
        
        # 3. æ™ºèƒ½ç”Ÿæˆå®Œæ•´æè¿°ï¼ˆé¿å…é‡å¤ï¼‰
        complete_description = self._generate_complete_description(
            img_caption, img_footnote, vision_response
        )
        
        # 4. æå–åˆ†å±‚æè¿°å’Œç»“æ„åŒ–ä¿¡æ¯
        layered_descriptions = self._extract_layered_descriptions(vision_response)
        structured_info = self._extract_structured_info(vision_response)
        
        # 5. è¿”å›å®Œæ•´ç»“æœ
        return {
            'enhanced_description': complete_description,
            'layered_descriptions': layered_descriptions,
            'structured_info': structured_info,
            'enhancement_timestamp': int(time.time()),
            'enhancement_status': 'success',
            'enhancement_model': self.enhancement_model,
            'enhancement_api': self.enhancement_model_api,
            'mineru_original': {
                'img_caption': img_caption,
                'img_footnote': img_footnote,
                'img_path': mineru_info.get('img_path', '')
            },
            'vision_analysis': {
                'raw_response': vision_response,
                'analysis_timestamp': int(time.time())
            }
        }
        
    except Exception as e:
        logging.error(f"å›¾ç‰‡å¢å¼ºå¤±è´¥: {e}")
        return self._create_fallback_description(img_caption, img_footnote, str(e))

def _create_fallback_description(self, img_caption: List[str], img_footnote: List[str], error: str = None) -> Dict[str, Any]:
    """
    åˆ›å»ºå›é€€æè¿°ï¼ˆå½“å¢å¼ºå¤±è´¥æ—¶ï¼‰
    """
    description_parts = []
    
    if img_caption:
        description_parts.append(' '.join(img_caption))
    if img_footnote:
        description_parts.append(' '.join(img_footnote))
    
    fallback_description = ' | '.join(description_parts) if description_parts else 'å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥'
    
    return {
        'enhanced_description': fallback_description,
        'layered_descriptions': {},
        'structured_info': {},
        'enhancement_timestamp': int(time.time()),
        'enhancement_status': 'failed',
        'enhancement_error': error or 'è§†è§‰æ¨¡å‹è°ƒç”¨å¤±è´¥',
        'enhancement_model': self.enhancement_model,
        'enhancement_api': self.enhancement_model_api,
        'mineru_original': {
            'img_caption': img_caption,
            'img_footnote': img_footnote,
            'img_path': ''
        }
    }
```

---

## **ğŸ”§ ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºäºJSONæ–‡ä»¶çš„æ–‡æœ¬å’Œè¡¨æ ¼å¤„ç†å™¨**

### **2.1 å†…å®¹å…ƒæ•°æ®æå–å™¨**

```python
class ContentMetadataExtractor:
    """
    å†…å®¹å…ƒæ•°æ®æå–å™¨
    åŸºäºMinerUè§£æçš„JSONæ–‡ä»¶æå–textã€tableã€imageçš„å…ƒæ•°æ®
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£çš„å…ƒæ•°æ®è§„èŒƒ
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # ä½¿ç”¨é…ç½®
        self.chunk_size = self.config.get('document_processing.chunk_size', 1000)
        self.chunk_overlap = self.config.get('document_processing.chunk_overlap', 200)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("å†…å®¹å…ƒæ•°æ®æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_metadata_from_json(self, json_path: str, doc_name: str) -> Dict[str, Any]:
        """
        ä»JSONæ–‡ä»¶æå–å…ƒæ•°æ®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        """
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æå–æ–‡æœ¬å—
            text_chunks = self._extract_text_chunks(data, doc_name)
            
            # æå–è¡¨æ ¼ä¿¡æ¯
            tables = self._extract_table_info(data, doc_name)
            
            # æå–å›¾ç‰‡ä¿¡æ¯
            images = self._extract_image_info(data, doc_name)
            
            return {
                'text_chunks': text_chunks,
                'tables': tables,
                'images': images,
                'document_name': doc_name,
                'total_items': len(data)
            }
            
        except Exception as e:
            self.failure_handler.record_processing_failure(json_path, 'metadata_extraction', str(e))
            logging.error(f"å…ƒæ•°æ®æå–å¤±è´¥: {json_path}, é”™è¯¯: {e}")
            return {'text_chunks': [], 'tables': [], 'images': []}
    
    def _extract_text_chunks(self, data: List[Dict], doc_name: str) -> List[Dict]:
        """
        æå–æ–‡æœ¬å—ï¼Œå®Œå…¨ç¬¦åˆTEXT_METADATA_SCHEMAè§„èŒƒ
        """
        text_chunks = []
        chunk_index = 0
        
        for item in data:
            if item.get('type') == 'text':
                # è·å–æ–‡æœ¬å†…å®¹
                text_content = item.get('content', '')
                if not text_content.strip():
                    continue
                
                # æ™ºèƒ½åˆ†å—å¤„ç†
                chunks = self._smart_text_chunking(text_content, chunk_index)
                
                for i, chunk_content in enumerate(chunks):
                    chunk = {
                        # åŸºç¡€æ ‡è¯†å­—æ®µï¼ˆç¬¦åˆCOMMON_METADATA_FIELDSï¼‰
                        'chunk_id': f"{doc_name}_text_{chunk_index}_{i}",
                        'chunk_type': 'text',
                        'source_type': 'pdf',
                        'document_name': doc_name,
                        'document_path': f"{doc_name}.pdf",
                        'page_number': item.get('page_idx', 1),
                        'page_idx': item.get('page_idx', 1),
                        'created_timestamp': int(time.time()),
                        'updated_timestamp': int(time.time()),
                        'processing_version': '3.0.0',
                        
                        # å‘é‡åŒ–ä¿¡æ¯å­—æ®µ
                        'vectorized': False,
                        'vectorization_timestamp': None,
                        'embedding_model': None,
                        
                        # æ–‡æœ¬ç‰¹æœ‰å­—æ®µï¼ˆç¬¦åˆTEXT_METADATA_SCHEMAï¼‰
                        'text_content': chunk_content,
                        'text_length': len(chunk_content),
                        'chunk_size': len(chunk_content),
                        'chunk_overlap': 0,
                        'chunk_position': {
                            'start_char': i * self.chunk_size,
                            'end_char': min((i + 1) * self.chunk_size, len(text_content)),
                            'chunk_index': i,
                            'total_chunks': len(chunks)
                        },
                        
                        # å…³è”ä¿¡æ¯å­—æ®µ
                        'related_images': [],
                        'related_tables': [],
                        'parent_chunk_id': None
                    }
                    
                    text_chunks.append(chunk)
                    chunk_index += 1
        
        return text_chunks
    
    def _smart_text_chunking(self, text: str, base_index: int) -> List[str]:
        """
        æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼Œé¿å…åœ¨å¥å­ä¸­é—´åˆ‡æ–­
        """
        if len(text) <= self.chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å­è¾¹ç•Œåˆ‡æ–­
            if end < len(text):
                # å‘åæŸ¥æ‰¾å¥å­ç»“æŸæ ‡è®°
                sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n\n']
                for ending in sentence_endings:
                    pos = text.rfind(ending, start, end)
                    if pos > start:
                        end = pos + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def _extract_table_info(self, data: List[Dict], doc_name: str) -> List[Dict]:
        """
        æå–è¡¨æ ¼ä¿¡æ¯ï¼Œå®Œå…¨ç¬¦åˆTABLE_METADATA_SCHEMAè§„èŒƒ
        """
        tables = []
        table_index = 0
        
        for item in data:
            if item.get('type') == 'table':
                # è·å–è¡¨æ ¼å†…å®¹
                table_content = item.get('table_content', '')
                if not table_content.strip():
                    continue
                
                # åˆ†æè¡¨æ ¼ç»“æ„
                table_structure = self._analyze_table_structure(table_content)
                
                # æ™ºèƒ½åˆ†å—å¤„ç†ï¼ˆå¤§è¡¨æ ¼åˆ†å—ï¼‰
                table_chunks = self._smart_table_chunking(table_content, table_structure)
                
                for i, chunk_content in enumerate(table_chunks):
                    table = {
                        # åŸºç¡€æ ‡è¯†å­—æ®µï¼ˆç¬¦åˆCOMMON_METADATA_FIELDSï¼‰
                        'chunk_id': f"{doc_name}_table_{table_index}_{i}",
                        'chunk_type': 'table',
                        'source_type': 'pdf',
                        'document_name': doc_name,
                        'document_path': f"{doc_name}.pdf",
                        'page_number': item.get('page_idx', 1),
                        'page_idx': item.get('page_idx', 1),
                        'created_timestamp': int(time.time()),
                        'updated_timestamp': int(time.time()),
                        'processing_version': '3.0.0',
                        
                        # å‘é‡åŒ–ä¿¡æ¯å­—æ®µ
                        'vectorized': False,
                        'vectorization_timestamp': None,
                        'embedding_model': None,
                        
                        # è¡¨æ ¼ç‰¹æœ‰å­—æ®µï¼ˆç¬¦åˆTABLE_METADATA_SCHEMAï¼‰
                        'table_id': f"{doc_name}_table_{table_index}_{i}",
                        'table_type': 'data_table',
                        'table_rows': table_structure.get('rows', 0),
                        'table_columns': table_structure.get('columns', 0),
                        'table_headers': table_structure.get('headers', []),
                        'table_title': item.get('table_title', ''),
                        'table_summary': self._generate_table_summary(chunk_content),
                        
                        # å†…å®¹å­—æ®µï¼ˆç®€åŒ–è®¾è®¡ï¼Œå»é™¤å†—ä½™ï¼‰
                        'table_content': chunk_content,
                        'table_html': self._generate_table_html(chunk_content, table_structure),
                        
                        # åˆ†å—ä¿¡æ¯å­—æ®µï¼ˆæ”¯æŒå¤§è¡¨æ ¼åˆ†å—ï¼‰
                        'is_subtable': len(table_chunks) > 1,
                        'parent_table_id': f"{doc_name}_table_{table_index}" if len(table_chunks) > 1 else None,
                        'subtable_index': i if len(table_chunks) > 1 else None,
                        'chunk_start_row': i * self.chunk_size if len(table_chunks) > 1 else 0,
                        'chunk_end_row': min((i + 1) * self.chunk_size, table_structure.get('rows', 0)) if len(table_chunks) > 1 else table_structure.get('rows', 0),
                        
                        # å…³è”ä¿¡æ¯å­—æ®µ
                        'related_text': item.get('related_text', ''),
                        'related_images': [],
                        'related_text_chunks': [],
                        'table_context': item.get('table_context', '')
                    }
                    
                    tables.append(table)
                    table_index += 1
        
        return tables
    
    def _analyze_table_structure(self, table_content: str) -> Dict[str, Any]:
        """
        åˆ†æè¡¨æ ¼ç»“æ„
        """
        lines = table_content.strip().split('\n')
        if not lines:
            return {'rows': 0, 'columns': 0, 'headers': []}
        
        # åˆ†æç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜è¡Œ
        headers = []
        if lines:
            first_line = lines[0]
            # ç®€å•çš„åˆ†éš”ç¬¦æ£€æµ‹
            if '|' in first_line:
                headers = [h.strip() for h in first_line.split('|')]
            elif '\t' in first_line:
                headers = [h.strip() for h in first_line.split('\t')]
            else:
                headers = [first_line.strip()]
        
        # è®¡ç®—è¡Œæ•°å’Œåˆ—æ•°
        rows = len(lines)
        columns = len(headers) if headers else 1
        
        return {
            'rows': rows,
            'columns': columns,
            'headers': headers
        }
    
    def _smart_table_chunking(self, table_content: str, table_structure: Dict) -> List[str]:
        """
        æ™ºèƒ½è¡¨æ ¼åˆ†å—ï¼Œé¿å…åœ¨è¡Œä¸­é—´åˆ‡æ–­
        """
        if table_structure.get('rows', 0) <= self.chunk_size:
            return [table_content]
        
        lines = table_content.strip().split('\n')
        chunks = []
        start_row = 0
        
        while start_row < len(lines):
            end_row = start_row + self.chunk_size
            
            # ç¡®ä¿åœ¨è¡Œè¾¹ç•Œåˆ‡æ–­
            chunk_lines = lines[start_row:end_row]
            chunk_content = '\n'.join(chunk_lines)
            
            if chunk_content.strip():
                chunks.append(chunk_content)
            
            start_row = end_row
        
        return chunks
    
    def _generate_table_summary(self, table_content: str) -> str:
        """
        ç”Ÿæˆè¡¨æ ¼æ‘˜è¦
        """
        lines = table_content.strip().split('\n')
        if not lines:
            return "ç©ºè¡¨æ ¼"
        
        # ç®€å•çš„æ‘˜è¦ç”Ÿæˆ
        row_count = len(lines)
        if row_count == 1:
            return f"å•è¡Œè¡¨æ ¼ï¼ŒåŒ…å« {len(lines[0].split('|'))} åˆ—"
        else:
            return f"è¡¨æ ¼åŒ…å« {row_count} è¡Œï¼Œ{len(lines[0].split('|'))} åˆ—"
    
    def _generate_table_html(self, table_content: str, table_structure: Dict) -> str:
        """
        ç”ŸæˆHTMLæ ¼å¼è¡¨æ ¼å†…å®¹ï¼ˆç”¨äºwebå±•ç°ï¼‰
        """
        lines = table_content.strip().split('\n')
        if not lines:
            return "<table><tr><td>ç©ºè¡¨æ ¼</td></tr></table>"
        
        html_parts = ['<table border="1">']
        
        # æ·»åŠ æ ‡é¢˜è¡Œ
        if table_structure.get('headers'):
            html_parts.append('<thead><tr>')
            for header in table_structure['headers']:
                html_parts.append(f'<th>{header}</th>')
            html_parts.append('</tr></thead>')
        
        # æ·»åŠ æ•°æ®è¡Œ
        html_parts.append('<tbody>')
        for line in lines:
            if '|' in line:
                cells = [cell.strip() for cell in line.split('|')]
            elif '\t' in line:
                cells = [cell.strip() for cell in line.split('\t')]
            else:
                cells = [line.strip()]
            
            html_parts.append('<tr>')
            for cell in cells:
                html_parts.append(f'<td>{cell}</td>')
            html_parts.append('</tr>')
        
        html_parts.append('</tbody></table>')
        
        return ''.join(html_parts)
    
    def _extract_image_info(self, data: List[Dict], doc_name: str) -> List[Dict]:
        """
        æå–å›¾ç‰‡ä¿¡æ¯ï¼Œå®Œå…¨ç¬¦åˆIMAGE_METADATA_SCHEMAè§„èŒƒ
        """
        images = []
        image_index = 0
        
        for item in data:
            if item.get('type') == 'image':
                # è·å–å›¾ç‰‡è·¯å¾„
                img_path = item.get('img_path', '')
                
                # æ„å»ºå®Œæ•´è·¯å¾„
                mineru_output_dir = self.config_manager.get_path('mineru_output_dir')
                source_image_path = os.path.join(mineru_output_dir, 'images', os.path.basename(img_path))
                
                # æ„å»ºæœ€ç»ˆå›¾ç‰‡è·¯å¾„
                final_image_dir = self.config_manager.get_path('final_image_dir')
                final_image_path = os.path.join(final_image_dir, os.path.basename(img_path))
                
                image = {
                    # åŸºç¡€æ ‡è¯†å­—æ®µï¼ˆç¬¦åˆCOMMON_METADATA_FIELDSï¼‰
                    'chunk_id': f"{doc_name}_image_{image_index}",
                    'chunk_type': 'image',
                    'source_type': 'pdf',
                    'document_name': doc_name,
                    'document_path': f"{doc_name}.pdf",
                    'page_number': item.get('page_idx', 1),
                    'page_idx': item.get('page_idx', 1),
                    'created_timestamp': int(time.time()),
                    'updated_timestamp': int(time.time()),
                    'processing_version': '3.0.0',
                    
                    # å‘é‡åŒ–ä¿¡æ¯å­—æ®µ
                    'vectorized': False,
                    'vectorization_timestamp': None,
                    'embedding_model': None,
                    
                    # å›¾ç‰‡ç‰¹æœ‰å­—æ®µï¼ˆç¬¦åˆIMAGE_METADATA_SCHEMAï¼‰
                    'image_id': f"{doc_name}_image_{image_index}",
                    'image_path': final_image_path,
                    'image_filename': os.path.basename(img_path),
                    'image_type': 'general',
                    'image_format': self._get_image_format(img_path),
                    'image_dimensions': {'width': 0, 'height': 0},  # ç¨åå¡«å……
                    
                    # å†…å®¹æè¿°å­—æ®µï¼ˆä¿ç•™ç°æœ‰ç³»ç»Ÿçš„ä¼˜ç§€éƒ¨åˆ†ï¼‰
                    'basic_description': ' | '.join(item.get('img_caption', [])),
                    'enhanced_description': '',  # ç¨åå¡«å……
                    'layered_descriptions': {},  # ç¨åå¡«å……
                    'structured_info': {},  # ç¨åå¡«å……
                    
                    # å›¾ç‰‡æ ‡é¢˜å’Œè„šæ³¨ï¼ˆä¿ç•™ç°æœ‰ç³»ç»Ÿçš„ä¼˜ç§€éƒ¨åˆ†ï¼‰
                    'img_caption': item.get('img_caption', []),
                    'img_footnote': item.get('img_footnote', []),
                    
                    # å¢å¼ºå¤„ç†å­—æ®µï¼ˆæ”¯æŒå¤±è´¥å¤„ç†å’Œè¡¥åšï¼‰
                    'enhancement_enabled': True,
                    'enhancement_model': None,  # ç¨åå¡«å……
                    'enhancement_status': 'pending',
                    'enhancement_timestamp': None,
                    'enhancement_error': None,
                    
                    # åŒé‡embeddingå­—æ®µï¼ˆæ–°éœ€æ±‚ï¼‰
                    'image_embedding': None,  # ç¨åå¡«å……
                    'description_embedding': None,  # ç¨åå¡«å……
                    'image_embedding_model': None,  # ç¨åå¡«å……
                    'description_embedding_model': None,  # ç¨åå¡«å……
                    
                    # å…³è”ä¿¡æ¯å­—æ®µ
                    'related_text_chunks': [],
                    'related_table_chunks': [],
                    'parent_document_id': doc_name,
                    
                    # åŸå§‹è·¯å¾„ä¿¡æ¯
                    'source_image_path': source_image_path,
                    'img_path': img_path
                }
                
                images.append(image)
                image_index += 1
        
        return images
    
    def _get_image_format(self, img_path: str) -> str:
        """è·å–å›¾ç‰‡æ ¼å¼"""
        try:
            if img_path.lower().endswith('.jpg') or img_path.lower().endswith('.jpeg'):
                return 'JPEG'
            elif img_path.lower().endswith('.png'):
                return 'PNG'
            elif img_path.lower().endswith('.gif'):
                return 'GIF'
            else:
                return 'UNKNOWN'
        except:
            return 'UNKNOWN'
```

---

## **ï¿½ï¿½ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå›¾ç‰‡å¤„ç†æµç¨‹ç®¡ç†å™¨**

### **3.1 å›¾ç‰‡å¤„ç†æµç¨‹ç®¡ç†å™¨**

```python
class ImageProcessingPipeline:
    """
    å®Œæ•´çš„å›¾ç‰‡å¤„ç†æµç¨‹ç®¡ç†å™¨
    æ•´åˆï¼šå¤åˆ¶ â†’ å¢å¼º â†’ å‘é‡åŒ– â†’ å­˜å‚¨
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.image_copy_manager = ImageCopyManager(config_manager)
        self.image_enhancer = ImageEnhancer(config_manager)
        self.image_vectorizer = ImageVectorizationManager(config_manager)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("å›¾ç‰‡å¤„ç†æµç¨‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_images(self, images: List[Dict]) -> List[Dict]:
        """
        å®Œæ•´çš„å›¾ç‰‡å¤„ç†æµç¨‹
        """
        try:
            print(f" å¼€å§‹å¤„ç† {len(images)} å¼ å›¾ç‰‡...")
            
            # æ­¥éª¤1: å›¾ç‰‡å¤åˆ¶åˆ°æœ€ç»ˆç›®å½•
            print("æ­¥éª¤1: å›¾ç‰‡å¤åˆ¶...")
            copied_images = self.image_copy_manager.copy_images_to_final_dir(images)
            success_count = sum(1 for img in copied_images if img.get('copy_status') == 'success')
            print(f"âœ… å›¾ç‰‡å¤åˆ¶å®Œæˆ: {success_count}/{len(images)} æˆåŠŸ")
            
            # æ­¥éª¤2: ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å¢å¼ºä¿¡æ¯ï¼ˆé¿å…é‡å¤ï¼‰
            print("æ­¥éª¤2: å›¾ç‰‡å¢å¼ºæè¿°...")
            enhanced_images = []
            for i, image in enumerate(copied_images):
                if image.get('copy_status') == 'success':
                    print(f" ï¸ å¢å¼ºå›¾ç‰‡ {i+1}/{len(copied_images)}: {os.path.basename(image.get('final_image_path', ''))}")
                    
                    # ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å¢å¼ºä¿¡æ¯
                    enhancement_result = self.image_enhancer.enhance_image_complete(
                        image.get('final_image_path', ''),
                        {
                            'img_caption': image.get('img_caption', []),
                            'img_footnote': image.get('img_footnote', []),
                            'img_path': image.get('img_path', '')
                        }
                    )
                    
                    # æ›´æ–°å›¾ç‰‡ä¿¡æ¯
                    image.update(enhancement_result)
                    enhanced_images.append(image)
                    
                    print(f"  âœ… å›¾ç‰‡å¢å¼ºå®Œæˆ: {os.path.basename(image.get('final_image_path', ''))}")
                else:
                    enhanced_images.append(image)
            
            success_count = sum(1 for img in enhanced_images if img.get('enhancement_status') == 'success')
            print(f"âœ… å›¾ç‰‡å¢å¼ºå®Œæˆ: {success_count}/{len(images)} æˆåŠŸ")
            
            # æ­¥éª¤3: å›¾ç‰‡åŒé‡å‘é‡åŒ–
            print("æ­¥éª¤3: å›¾ç‰‡åŒé‡å‘é‡åŒ–...")
            vectorized_images = self.image_vectorizer.vectorize_images(enhanced_images)
            success_count = sum(1 for img in vectorized_images if img.get('vectorization_status') == 'success')
            print(f"âœ… å›¾ç‰‡å‘é‡åŒ–å®Œæˆ: {success_count}/{len(images)} æˆåŠŸ")
            
            # æ­¥éª¤4: ç”Ÿæˆå®Œæ•´å…ƒæ•°æ®
            print("æ­¥éª¤4: ç”Ÿæˆå®Œæ•´å…ƒæ•°æ®...")
            final_images = []
            for image in vectorized_images:
                complete_metadata = self._create_complete_image_metadata(image)
                final_images.append(complete_metadata)
            
            print(f"âœ… å›¾ç‰‡å¤„ç†æµç¨‹å®Œæˆ: {len(final_images)} å¼ å›¾ç‰‡")
            return final_images
            
        except Exception as e:
            error_msg = f"å›¾ç‰‡å¤„ç†æµç¨‹å¤±è´¥: {e}"
            logging.error(error_msg)
            self.failure_handler.record_processing_failure('image_pipeline', 'image_processing_pipeline', str(e))
            raise RuntimeError(error_msg)
    
    def _create_complete_image_metadata(self, image: Dict) -> Dict[str, Any]:
        """
        åˆ›å»ºå®Œæ•´çš„å›¾ç‰‡å…ƒæ•°æ®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ
        """
        return {
            # åŸºç¡€æ ‡è¯†å­—æ®µï¼ˆç¬¦åˆCOMMON_METADATA_FIELDSï¼‰
            'chunk_id': image.get('chunk_id', ''),
            'chunk_type': 'image',
            'source_type': 'pdf',
            'document_name': image.get('document_name', ''),
            'document_path': image.get('document_path', ''),
            'page_number': image.get('page_number', 1),
            'page_idx': image.get('page_idx', 1),
            'created_timestamp': image.get('created_timestamp', int(time.time())),
            'updated_timestamp': int(time.time()),
            'processing_version': '3.0.0',
            
            # å‘é‡åŒ–ä¿¡æ¯å­—æ®µ
            'vectorized': image.get('vectorization_status') == 'success',
            'vectorization_timestamp': image.get('vectorization_timestamp'),
            'embedding_model': f"{image.get('visual_model', '')}+{image.get('semantic_model', '')}" if image.get('visual_model') and image.get('semantic_model') else None,
            
            # å›¾ç‰‡ç‰¹æœ‰å­—æ®µï¼ˆç¬¦åˆIMAGE_METADATA_SCHEMAï¼‰
            'image_id': image.get('image_id', ''),
            'image_path': image.get('final_image_path', ''),
            'image_filename': image.get('image_filename', ''),
            'image_type': image.get('image_type', 'general'),
            'image_format': image.get('image_format', 'UNKNOWN'),
            'image_dimensions': image.get('image_dimensions', {'width': 0, 'height': 0}),
            
            # å†…å®¹æè¿°å­—æ®µï¼ˆä¿ç•™ç°æœ‰ç³»ç»Ÿçš„ä¼˜ç§€éƒ¨åˆ†ï¼‰
            'basic_description': image.get('basic_description', ''),
            'enhanced_description': image.get('enhanced_description', ''),
            'layered_descriptions': image.get('layered_descriptions', {}),
            'structured_info': image.get('structured_info', {}),
            
            # å›¾ç‰‡æ ‡é¢˜å’Œè„šæ³¨ï¼ˆä¿ç•™ç°æœ‰ç³»ç»Ÿçš„ä¼˜ç§€éƒ¨åˆ†ï¼‰
            'img_caption': image.get('img_caption', []),
            'img_footnote': image.get('img_footnote', []),
            
            # å¢å¼ºå¤„ç†å­—æ®µï¼ˆæ”¯æŒå¤±è´¥å¤„ç†å’Œè¡¥åšï¼‰
            'enhancement_enabled': image.get('enhancement_enabled', True),
            'enhancement_model': image.get('enhancement_model', ''),
            'enhancement_status': image.get('enhancement_status', 'unknown'),
            'enhancement_timestamp': image.get('enhancement_timestamp'),
            'enhancement_error': image.get('enhancement_error', ''),
            
            # åŒé‡embeddingå­—æ®µï¼ˆæ–°éœ€æ±‚ï¼‰
            'image_embedding': image.get('visual_vector', []),
            'description_embedding': image.get('semantic_vector', []),
            'image_embedding_model': image.get('visual_model', ''),
            'description_embedding_model': image.get('semantic_model', ''),
            
            # å…³è”ä¿¡æ¯å­—æ®µ
            'related_text_chunks': image.get('related_text_chunks', []),
            'related_table_chunks': image.get('related_table_chunks', []),
            'parent_document_id': image.get('parent_document_id', ''),
            
            # å¤„ç†çŠ¶æ€ä¿¡æ¯
            'copy_status': image.get('copy_status', 'unknown'),
            'enhancement_status': image.get('enhancement_status', 'unknown'),
            'vectorization_status': image.get('vectorization_status', 'unknown'),
            
            # åŸå§‹ä¿¡æ¯
            'mineru_original': image.get('mineru_original', {}),
            'vision_analysis': image.get('vision_analysis', {}),
            
            # æ¶æ„æ ‡è¯†
            'metadata_schema': 'IMAGE_METADATA_SCHEMA',
            'metadata_version': '3.0.0',
            'processing_pipeline': 'MinerU_Enhancement_Pipeline',
            'optimization_features': [
                'one_time_enhancement',
                'smart_deduplication',
                'complete_metadata',
                'dual_vectorization'
            ]
        }
```

---

## **ğŸ”§ ç¬¬å››éƒ¨åˆ†ï¼šå›¾ç‰‡å‘é‡åŒ–ç®¡ç†å™¨**

### **4.1 åŒé‡å‘é‡åŒ–ç®¡ç†å™¨**

```python
class ImageVectorizationManager:
    """
    å›¾ç‰‡å‘é‡åŒ–ç®¡ç†å™¨
    å®ç°åŒé‡embeddingç­–ç•¥ï¼šè§†è§‰embedding + è¯­ä¹‰embedding
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # ä½¿ç”¨é…ç½®
        self.image_embedding_model = self.config.get('vectorization.image_embedding_model', 'multimodal-embedding-one-peace-v1')
        self.text_embedding_model = self.config.get('vectorization.text_embedding_model', 'text-embedding-v1')
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†
        self.failure_handler = config_manager.get_failure_handler()
        
        # åˆå§‹åŒ–ModelCaller
        self.model_caller = ModelCaller(config_manager)
        
        logging.info("å›¾ç‰‡å‘é‡åŒ–ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def vectorize_images(self, images: List[Dict]) -> List[Dict]:
        """
        å¯¹å›¾ç‰‡è¿›è¡ŒåŒé‡å‘é‡åŒ–
        """
        vectorized_images = []
        
        for i, image in enumerate(images):
            try:
                print(f"  æ­£åœ¨å‘é‡åŒ–å›¾ç‰‡ {i+1}/{len(images)}: {os.path.basename(image.get('final_image_path', ''))}")
                
                # åŒé‡å‘é‡åŒ–
                vectorization_result = self._dual_vectorize_image(image)
                
                # æ›´æ–°å›¾ç‰‡ä¿¡æ¯
                image.update(vectorization_result)
                vectorized_images.append(image)
                
                print(f"  âœ… å›¾ç‰‡å‘é‡åŒ–å®Œæˆ: {os.path.basename(image.get('final_image_path', ''))}")
                
            except Exception as e:
                error_msg = f"å›¾ç‰‡å‘é‡åŒ–å¤±è´¥: {os.path.basename(image.get('final_image_path', ''))}, é”™è¯¯: {e}"
                print(f"  âš ï¸ {error_msg}")
                
                # è®°å½•å¤±è´¥
                self.failure_handler.record_processing_failure(
                    image.get('final_image_path', ''), 
                    'image_vectorization', 
                    str(e)
                )
                
                # æ ‡è®°å¤±è´¥çŠ¶æ€
                image['vectorization_status'] = 'failed'
                image['vectorization_error'] = str(e)
                vectorized_images.append(image)
        
        return vectorized_images
    
    def _dual_vectorize_image(self, image: Dict) -> Dict[str, Any]:
        """
        å¯¹å•å¼ å›¾ç‰‡è¿›è¡ŒåŒé‡å‘é‡åŒ–
        """
        try:
            image_path = image.get('final_image_path', '')
            enhanced_description = image.get('enhanced_description', '')
            
            # 1. è§†è§‰å‘é‡åŒ–ï¼ˆä½¿ç”¨One_Peaceæ¨¡å‹ï¼‰
            visual_vector = self.model_caller.call_visual_embedding(image_path)
            
            # 2. è¯­ä¹‰å‘é‡åŒ–ï¼ˆä½¿ç”¨text-embeddingæ¨¡å‹ï¼‰
            semantic_vector = self.model_caller.call_text_embedding(enhanced_description)
            
            return {
                'visual_vector': visual_vector,
                'semantic_vector': semantic_vector,
                'vectorization_status': 'success',
                'vectorization_timestamp': int(time.time()),
                'visual_model': self.image_embedding_model,
                'semantic_model': self.text_embedding_model,
                'vector_dimensions': {
                    'visual': len(visual_vector) if visual_vector else 0,
                    'semantic': len(semantic_vector) if semantic_vector else 0
                }
            }
            
        except Exception as e:
            logging.error(f"å›¾ç‰‡åŒé‡å‘é‡åŒ–å¤±è´¥: {e}")
            return {
                'vectorization_status': 'failed',
                'vectorization_error': str(e)
            }
```

---

## **ğŸ”§ ç¬¬äº”éƒ¨åˆ†ï¼šå›¾ç‰‡å¤åˆ¶ç®¡ç†å™¨**

### **5.1 å›¾ç‰‡å¤åˆ¶ç®¡ç†å™¨**

```python
class ImageCopyManager:
    """
    å›¾ç‰‡å¤åˆ¶ç®¡ç†å™¨
    è´Ÿè´£å°†å›¾ç‰‡ä»MinerUè¾“å‡ºç›®å½•å¤åˆ¶åˆ°æœ€ç»ˆç›®å½•
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("å›¾ç‰‡å¤åˆ¶ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def copy_images_to_final_dir(self, images: List[Dict]) -> List[Dict]:
        """
        å°†å›¾ç‰‡å¤åˆ¶åˆ°æœ€ç»ˆç›®å½•
        """
        copied_images = []
        
        for image in images:
            try:
                source_path = image.get('source_image_path', '')
                target_path = image.get('final_image_path', '')
                
                if os.path.exists(source_path):
                    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(target_path), exist_ok=True)
                    
                    # å¤åˆ¶å›¾ç‰‡
                    shutil.copy2(source_path, target_path)
                    
                    # æ›´æ–°å›¾ç‰‡ä¿¡æ¯
                    image['copy_status'] = 'success'
                    image['final_image_path'] = target_path
                    image['image_size'] = os.path.getsize(target_path)
                    
                    # è·å–å›¾ç‰‡å°ºå¯¸
                    image['image_dimensions'] = self._get_image_dimensions(target_path)
                    
                    copied_images.append(image)
                    logging.info(f"å›¾ç‰‡å¤åˆ¶æˆåŠŸ: {os.path.basename(source_path)}")
                else:
                    image['copy_status'] = 'failed'
                    image['error'] = 'æºæ–‡ä»¶ä¸å­˜åœ¨'
                    self.failure_handler.record_processing_failure(source_path, 'image_copy', 'æºæ–‡ä»¶ä¸å­˜åœ¨')
                    
            except Exception as e:
                image['copy_status'] = 'failed'
                image['error'] = str(e)
                self.failure_handler.record_processing_failure(source_path, 'image_copy', str(e))
                logging.error(f"å›¾ç‰‡å¤åˆ¶å¤±è´¥: {source_path}, é”™è¯¯: {e}")
        
        return copied_images
    
    def _get_image_dimensions(self, image_path: str) -> Dict[str, int]:
        """è·å–å›¾ç‰‡å°ºå¯¸"""
        try:
            from PIL import Image
            with Image.open(image_path) as img:
                return {
                    'width': img.width,
                    'height': img.height
                }
        except Exception as e:
            logging.warning(f"è·å–å›¾ç‰‡å°ºå¯¸å¤±è´¥: {e}")
            return {'width': 0, 'height': 0}
```

---

## **ï¿½ï¿½ ç¬¬å…­éƒ¨åˆ†ï¼šä¸»å¤„ç†å™¨é›†æˆ**

### **6.1 ä¸»å¤„ç†å™¨é›†æˆ**

```python
class MainProcessor:
    """
    ä¸»å¤„ç†å™¨
    é›†æˆæ‰€æœ‰å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬JSONæ–‡ä»¶è§£æã€æ–‡æœ¬å¤„ç†ã€è¡¨æ ¼å¤„ç†ã€å›¾ç‰‡å¤„ç†
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.content_metadata_extractor = ContentMetadataExtractor(config_manager)
        self.image_processing_pipeline = ImageProcessingPipeline(config_manager)
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†
        self.failure_handler = config_manager.get_failure_handler()
        
        logging.info("ä¸»å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_mineru_output(self, mineru_output_dir: str) -> Dict[str, Any]:
        """
        å¤„ç†MinerUè¾“å‡ºï¼ŒåŒ…æ‹¬JSONæ–‡ä»¶è§£æã€æ–‡æœ¬å¤„ç†ã€è¡¨æ ¼å¤„ç†ã€å›¾ç‰‡å¤„ç†
        """
        try:
            print(f" å¼€å§‹å¤„ç†MinerUè¾“å‡ºç›®å½•: {mineru_output_dir}")
            
            # æ­¥éª¤1: è§£æJSONæ–‡ä»¶ï¼Œæå–å…ƒæ•°æ®
            print("æ­¥éª¤1: è§£æJSONæ–‡ä»¶ï¼Œæå–å…ƒæ•°æ®...")
            metadata_results = self._extract_all_metadata(mineru_output_dir)
            
            # æ­¥éª¤2: å¤„ç†å›¾ç‰‡ï¼ˆå¤åˆ¶ã€å¢å¼ºã€å‘é‡åŒ–ï¼‰
            print("æ­¥éª¤2: å¤„ç†å›¾ç‰‡...")
            if metadata_results.get('images'):
                processed_images = self.image_processing_pipeline.process_images(metadata_results['images'])
                metadata_results['images'] = processed_images
            
            # æ­¥éª¤3: ç”Ÿæˆæœ€ç»ˆç»“æœ
            print("æ­¥éª¤3: ç”Ÿæˆæœ€ç»ˆç»“æœ...")
            final_result = self._generate_final_result(metadata_results)
            
            print(f"âœ… MinerUè¾“å‡ºå¤„ç†å®Œæˆ")
            return final_result
            
        except Exception as e:
            error_msg = f"MinerUè¾“å‡ºå¤„ç†å¤±è´¥: {e}"
            logging.error(error_msg)
            self.failure_handler.record_processing_failure(mineru_output_dir, 'mineru_output_processing', str(e))
            raise RuntimeError(error_msg)
    
    def _extract_all_metadata(self, mineru_output_dir: str) -> Dict[str, Any]:
        """
        æå–æ‰€æœ‰å…ƒæ•°æ®
        """
        metadata_results = {
            'text_chunks': [],
            'tables': [],
            'images': []
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        json_files = list(Path(mineru_output_dir).glob("*_1.json"))
        
        for json_file in json_files:
            try:
                doc_name = json_file.stem.replace('_1', '')
                print(f"  å¤„ç†æ–‡æ¡£: {doc_name}")
                
                # æå–å…ƒæ•°æ®
                metadata = self.content_metadata_extractor.extract_metadata_from_json(
                    str(json_file), doc_name
                )
                
                # åˆå¹¶ç»“æœ
                metadata_results['text_chunks'].extend(metadata.get('text_chunks', []))
                metadata_results['tables'].extend(metadata.get('tables', []))
                metadata_results['images'].extend(metadata.get('images', []))
                
            except Exception as e:
                logging.error(f"å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {json_file}, é”™è¯¯: {e}")
                continue
        
        print(f"  æå–å®Œæˆ: {len(metadata_results['text_chunks'])} ä¸ªæ–‡æœ¬å—, {len(metadata_results['tables'])} ä¸ªè¡¨æ ¼, {len(metadata_results['images'])} å¼ å›¾ç‰‡")
        
        return metadata_results
    
    def _generate_final_result(self, metadata_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæœ€ç»ˆç»“æœ
        """
        return {
            'success': True,
            'timestamp': int(time.time()),
            'processing_version': '3.0.0',
            'statistics': {
                'text_chunks': len(metadata_results.get('text_chunks', [])),
                'tables': len(metadata_results.get('tables', [])),
                'images': len(metadata_results.get('images', [])),
                'total_items': sum([
                    len(metadata_results.get('text_chunks', [])),
                    len(metadata_results.get('tables', [])),
                    len(metadata_results.get('images', []))
                ])
            },
            'results': metadata_results
        }
```

---

## ** ä¼˜åŒ–æ•ˆæœæ€»ç»“**

### **1. æ¶ˆé™¤é‡å¤ä¿¡æ¯**
- **åŸæ¥**ï¼šåŸºç¡€å¢å¼º â†’ æ·±åº¦å¢å¼º â†’ åˆå¹¶ï¼ˆå¯èƒ½é‡å¤ï¼‰
- **ç°åœ¨**ï¼šä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´ä¿¡æ¯ï¼Œæ™ºèƒ½å»é‡

### **2. ç®€åŒ–å¤„ç†æµç¨‹**
- **åŸæ¥**ï¼šå¤šå±‚å¤„ç†ï¼Œä¿¡æ¯ä¼ é€’å¤æ‚
- **ç°åœ¨**ï¼šå•å±‚å¤„ç†ï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆç»“æœ

### **3. æé«˜ä¿¡æ¯è´¨é‡**
- æ™ºèƒ½æ£€æµ‹å†…å®¹é‡å¤
- ä¿ç•™æœ‰ä»·å€¼çš„åŸå§‹ä¿¡æ¯
- é¿å…å†—ä½™å’ŒçŸ›ç›¾

### **4. æ€§èƒ½æå‡**
- å‡å°‘APIè°ƒç”¨æ¬¡æ•°
- ç®€åŒ–æ–‡æœ¬å¤„ç†é€»è¾‘
- æé«˜æ•´ä½“å¤„ç†æ•ˆç‡

### **5. å®Œå…¨æ¶æ„åŒ–**
- ä½¿ç”¨é…ç½®ç®¡ç†
- é›†æˆå¤±è´¥å¤„ç†
- æ”¯æŒé€Ÿç‡é™åˆ¶å’Œæ‰¹å¤„ç†

### **6. å…ƒæ•°æ®å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ**
- éµå¾ªCOMMON_METADATA_FIELDS
- éµå¾ªTEXT_METADATA_SCHEMA
- éµå¾ªTABLE_METADATA_SCHEMA
- éµå¾ªIMAGE_METADATA_SCHEMA

### **7. åŸºäºJSONæ–‡ä»¶çš„å®Œæ•´å¤„ç†**
- æ–‡æœ¬å—æ™ºèƒ½åˆ†å—
- è¡¨æ ¼ç»“æ„åˆ†æå’Œåˆ†å—
- å›¾ç‰‡ä¿¡æ¯æå–å’Œå¢å¼º
- æ”¯æŒå¤§å†…å®¹çš„åˆ†å—å¤„ç†

---

## ** å®æ–½æ­¥éª¤**

### **æ­¥éª¤1ï¼šåˆ›å»ºä¼˜åŒ–ç‰ˆå›¾ç‰‡å¢å¼ºå¤„ç†å™¨**
- å®ç°`ImageEnhancer`ç±»
- é›†æˆæ™ºèƒ½å»é‡æœºåˆ¶
- å®ç°ä¸€æ¬¡æ€§å¢å¼ºæ–¹æ³•

### **æ­¥éª¤2ï¼šåˆ›å»ºå†…å®¹å…ƒæ•°æ®æå–å™¨**
- å®ç°`ContentMetadataExtractor`ç±»
- åŸºäºJSONæ–‡ä»¶æå–textã€tableã€imageå…ƒæ•°æ®
- å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒ

### **æ­¥éª¤3ï¼šåˆ›å»ºå›¾ç‰‡å¤„ç†æµç¨‹ç®¡ç†å™¨**
- å®ç°`ImageProcessingPipeline`ç±»
- æ•´åˆæ‰€æœ‰å¤„ç†æ­¥éª¤
- ç¡®ä¿æµç¨‹çš„å®Œæ•´æ€§

### **æ­¥éª¤4ï¼šåˆ›å»ºå›¾ç‰‡å‘é‡åŒ–ç®¡ç†å™¨**
- å®ç°`ImageVectorizationManager`ç±»
- æ”¯æŒåŒé‡å‘é‡åŒ–ç­–ç•¥
- é›†æˆå¤±è´¥å¤„ç†

### **æ­¥éª¤5ï¼šåˆ›å»ºå›¾ç‰‡å¤åˆ¶ç®¡ç†å™¨**
- å®ç°`ImageCopyManager`ç±»
- ç®¡ç†å›¾ç‰‡æ–‡ä»¶å¤åˆ¶
- å¤„ç†è·¯å¾„å’Œæƒé™é—®é¢˜

### **æ­¥éª¤6ï¼šé›†æˆåˆ°ä¸»å¤„ç†å™¨**
- åœ¨`MainProcessor`ä¸­é›†æˆ
- ç¡®ä¿ä¸ç°æœ‰æµç¨‹å…¼å®¹
- æµ‹è¯•å®Œæ•´æµç¨‹

**è¿™ä¸ªå®Œæ•´çš„ä¼˜åŒ–æ–¹æ¡ˆå®Œå…¨è§£å†³äº†é‡å¤ä¿¡æ¯é—®é¢˜ï¼Œå®ç°äº†åŸºäºJSONæ–‡ä»¶çš„å®Œæ•´å¤„ç†ï¼Œå…ƒæ•°æ®å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒã€‚è¯·ç¡®è®¤æ˜¯å¦å¯ä»¥å¼€å§‹å®æ–½ï¼Ÿ**