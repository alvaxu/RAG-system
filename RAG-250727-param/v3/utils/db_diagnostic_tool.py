#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
V3ç‰ˆæœ¬æ•°æ®åº“è¯Šæ–­å·¥å…·

åŠŸèƒ½ï¼š
1. åˆ†æå‘é‡æ•°æ®åº“ç»“æ„å’Œå†…å®¹
2. æ£€æŸ¥æ–‡æ¡£ç±»å‹åˆ†å¸ƒ
3. åˆ†æå…ƒæ•°æ®å­—æ®µ
4. è¾“å‡ºè¯Šæ–­æŠ¥å‘Š
5. æ”¯æŒä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
6. ğŸ”¢ å‘é‡æ•°æ®æ·±åº¦åˆ†æï¼ˆèŒƒæ•°åˆ†å¸ƒã€ç»´åº¦éªŒè¯ï¼‰
7. ğŸ”— å‘é‡ç›¸ä¼¼åº¦åˆ†æï¼ˆè´¨é‡è¯„ä¼°ã€å¼‚å¸¸æ£€æµ‹ï¼‰
8. ğŸ“‹ æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Šï¼ˆ100åˆ†åˆ¶è¯„åˆ†ï¼‰
9. ğŸ“‹ è¯¦ç»†æ–‡æ¡£åˆ—è¡¨æ˜¾ç¤ºï¼ˆå¯é€‰äº¤äº’åŠŸèƒ½ï¼‰
'''

import os
import json
import logging
import pickle
import numpy as np
from typing import Dict, List, Any, Optional
from pathlib import Path
from collections import defaultdict

from core.vector_store_manager import LangChainVectorStoreManager
from config.config_manager import ConfigManager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class DatabaseDiagnosticTool:
    """æ•°æ®åº“è¯Šæ–­å·¥å…·"""
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–è¯Šæ–­å·¥å…·"""
        try:
            if config_path:
                self.config_manager = ConfigManager(config_path)
            else:
                self.config_manager = ConfigManager()
            
            self.vector_store_manager = LangChainVectorStoreManager(self.config_manager)
            logging.info("æ•°æ®åº“è¯Šæ–­å·¥å…·åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def run_diagnostic(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®åº“è¯Šæ–­"""
        print("ğŸ” V3ç‰ˆæœ¬æ•°æ®åº“è¯Šæ–­å·¥å…·å¯åŠ¨")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½å‘é‡æ•°æ®åº“
            print("ğŸ“š åŠ è½½å‘é‡æ•°æ®åº“...")
            if not self.vector_store_manager.load():
                print("âŒ æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“")
                return {'success': False, 'error': 'æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“'}
            
            # 2. è·å–æ•°æ®åº“åŸºæœ¬ä¿¡æ¯
            print("ğŸ“Š è·å–æ•°æ®åº“åŸºæœ¬ä¿¡æ¯...")
            basic_info = self._get_basic_info()
            
            # 3. åˆ†ææ–‡æ¡£ç»“æ„
            print("ğŸ” åˆ†ææ–‡æ¡£ç»“æ„...")
            structure_info = self._analyze_document_structure()
            
            # 4. åˆ†æå…ƒæ•°æ®
            print("ğŸ“‹ åˆ†æå…ƒæ•°æ®...")
            metadata_info = self._analyze_metadata()
            
            # 5. æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£
            print("ğŸ“· æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£...")
            image_info = self._check_image_docs()
            
            # 6. æ£€æŸ¥è¡¨æ ¼æ–‡æ¡£
            print("ğŸ“Š æ£€æŸ¥è¡¨æ ¼æ–‡æ¡£...")
            table_info = self._check_table_docs()
            
            # 7. æ£€æŸ¥æ–‡æœ¬æ–‡æ¡£
            print("ğŸ“ æ£€æŸ¥æ–‡æœ¬æ–‡æ¡£...")
            text_info = self._check_text_docs()

            # 8. åˆ†æå‘é‡æ•°æ®
            print("ğŸ”¢ åˆ†æå‘é‡æ•°æ®...")
            vector_info = self._analyze_vector_data(self.vector_store_manager.vector_store)

            # 9. ç”Ÿæˆæ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š
            print("ğŸ“‹ ç”Ÿæˆæ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š...")
            quality_report = self._generate_quality_report(structure_info, image_info, table_info, vector_info)

            # 10. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
            print("ğŸ“‹ ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š...")
            diagnostic_report = {
                'success': True,
                'basic_info': basic_info,
                'structure_info': structure_info,
                'metadata_info': metadata_info,
                'image_info': image_info,
                'table_info': table_info,
                'text_info': text_info,
                'vector_info': vector_info,
                'quality_report': quality_report
            }

            # 11. æ˜¾ç¤ºè¯¦ç»†æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            show_detailed = input("\næ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„æ–‡æ¡£åˆ—è¡¨? (y/n): ").strip().lower()
            if show_detailed == 'y':
                self._show_detailed_document_list(structure_info)

            # 12. è¯¢é—®æ˜¯å¦ä¿å­˜ç»“æœ
            save_choice = input("\næ˜¯å¦ä¿å­˜è¯Šæ–­ç»“æœåˆ°æ–‡ä»¶? (y/n): ").strip().lower()
            if save_choice == 'y':
                output_file = "v3_db_diagnostic_results.json"
                self._save_results(diagnostic_report, output_file)
                print(f"ğŸ’¾ è¯Šæ–­ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            print("\nâœ… æ•°æ®åº“è¯Šæ–­å®Œæˆï¼")
            return diagnostic_report
            
        except Exception as e:
            logging.error(f"æ•°æ®åº“è¯Šæ–­å¤±è´¥: {e}")
            print(f"âŒ æ•°æ®åº“è¯Šæ–­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}
    
    def _get_basic_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“åŸºæœ¬ä¿¡æ¯"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            total_docs = len(docstore)
            
            # è·å–å‘é‡æ•°æ®åº“è·¯å¾„
            vector_db_path = self.config_manager.get_path('vector_db_dir')
            
            basic_info = {
                'total_documents': total_docs,
                'vector_db_path': vector_db_path,
                'vector_db_exists': os.path.exists(vector_db_path) if vector_db_path else False
            }
            
            # æ£€æŸ¥å‘é‡æ•°æ®åº“æ–‡ä»¶
            if vector_db_path and os.path.exists(vector_db_path):
                files = list(Path(vector_db_path).glob('*'))
                basic_info['vector_db_files'] = [
                    {
                        'name': f.name,
                        'size': f.stat().st_size if f.is_file() else 0,
                        'type': 'file' if f.is_file() else 'directory'
                    }
                    for f in files
                ]
            
            print(f"ğŸ“š æ€»æ–‡æ¡£æ•°: {total_docs}")
            print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
            print(f"âœ… å‘é‡æ•°æ®åº“å­˜åœ¨: {basic_info['vector_db_exists']}")
            
            return basic_info
            
        except Exception as e:
            logging.error(f"è·å–åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_document_structure(self) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£ç»“æ„"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            
            # ç»Ÿè®¡æ–‡æ¡£ç±»å‹åˆ†å¸ƒ
            chunk_types = defaultdict(int)
            document_names = set()
            all_fields = set()
            
            for doc_id, doc in docstore.items():
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                
                # ç»Ÿè®¡åˆ†å—ç±»å‹
                chunk_type = metadata.get('chunk_type', 'unknown')
                chunk_types[chunk_type] += 1
                
                # ç»Ÿè®¡æ–‡æ¡£åç§°
                doc_name = metadata.get('document_name', 'unknown')
                document_names.add(doc_name)
                
                # æ”¶é›†æ‰€æœ‰å­—æ®µ
                all_fields.update(metadata.keys())
            
            structure_info = {
                'chunk_type_distribution': dict(chunk_types),
                'unique_document_names': list(document_names),
                'total_unique_documents': len(document_names),
                'all_metadata_fields': list(all_fields),
                'total_metadata_fields': len(all_fields)
            }
            
            print(f"\nğŸ“Š åˆ†å—ç±»å‹åˆ†å¸ƒ:")
            for chunk_type, count in sorted(chunk_types.items()):
                print(f"   {chunk_type}: {count}")
            
            print(f"\nğŸ“š æ–‡æ¡£ç»Ÿè®¡:")
            print(f"  æ€»æ–‡æ¡£æ•°: {len(docstore)}")
            print(f"  å”¯ä¸€æ–‡æ¡£å: {len(document_names)}")
            print(f"  å…·ä½“æ–‡æ¡£å: {list(document_names)}")
            print(f"  å…ƒæ•°æ®å­—æ®µæ•°: {len(all_fields)}")
            
            return structure_info
            
        except Exception as e:
            logging.error(f"åˆ†ææ–‡æ¡£ç»“æ„å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_metadata(self) -> Dict[str, Any]:
        """åˆ†æå…ƒæ•°æ®ç»“æ„"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            
            # åˆ†æå­—æ®µç±»å‹å’Œç¤ºä¾‹
            field_types = defaultdict(set)
            field_examples = defaultdict(list)
            chunk_type_fields = defaultdict(set)
            
            for doc_id, doc in docstore.items():
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                chunk_type = metadata.get('chunk_type', 'unknown')
                
                for field, value in metadata.items():
                    field_types[field].add(type(value).__name__)
                    chunk_type_fields[chunk_type].add(field)
                    
                    # ä¿å­˜å‰3ä¸ªç¤ºä¾‹
                    if len(field_examples[field]) < 3:
                        field_examples[field].append({
                            'chunk_type': chunk_type,
                            'value': str(value)[:100] + '...' if len(str(value)) > 100 else str(value)
                        })
            
            metadata_info = {
                'field_types': {k: list(v) for k, v in field_types.items()},
                'chunk_type_fields': {k: list(v) for k, v in chunk_type_fields.items()},
                'field_examples': {k: v for k, v in field_examples.items()}
            }
            
            print(f"\nğŸ” å­—æ®µç±»å‹åˆ†æ:")
            for field, types in sorted(field_types.items()):
                print(f"  {field}: {', '.join(sorted(types))}")
            
            return metadata_info
            
        except Exception as e:
            logging.error(f"åˆ†æå…ƒæ•°æ®å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _check_image_docs(self) -> Dict[str, Any]:
        """æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            image_docs = []
            
            for doc_id, doc in docstore.items():
                if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'image':
                    image_docs.append((doc_id, doc))
            
            image_info = {
                'total_image_docs': len(image_docs),
                'enhanced_description_stats': {
                    'with_enhanced': 0,
                    'without_enhanced': 0
                },
                'samples': []
            }
            
            print(f"\nğŸ“· æ‰¾åˆ° {len(image_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
            
            # åˆ†æenhanced_descriptionå­—æ®µ
            enhanced_count = 0
            empty_count = 0
            
            for doc_id, doc in image_docs:
                enhanced_desc = doc.metadata.get('enhanced_description', '')
                if enhanced_desc:
                    enhanced_count += 1
                else:
                    empty_count += 1
            
            image_info['enhanced_description_stats']['with_enhanced'] = enhanced_count
            image_info['enhanced_description_stats']['without_enhanced'] = empty_count
            
            print(f"âœ… æœ‰enhanced_descriptionçš„å›¾ç‰‡: {enhanced_count}")
            print(f"âŒ æ— enhanced_descriptionçš„å›¾ç‰‡: {empty_count}")
            if (enhanced_count + empty_count) > 0:
                print(f"ğŸ“ˆ è¦†ç›–ç‡: {enhanced_count/(enhanced_count+empty_count)*100:.1f}%")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªå›¾ç‰‡æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
            for i, (doc_id, doc) in enumerate(image_docs[:3]):
                sample_info = {
                    'index': i+1,
                    'doc_id': doc_id,
                    'document_name': doc.metadata.get('document_name', 'N/A'),
                    'page_number': doc.metadata.get('page_number', 'N/A'),
                    'image_id': doc.metadata.get('image_id', 'N/A'),
                    'enhanced_description': doc.metadata.get('enhanced_description', '')[:100] + '...' if len(doc.metadata.get('enhanced_description', '')) > 100 else doc.metadata.get('enhanced_description', '')
                }
                image_info['samples'].append(sample_info)
                
                print(f"\nğŸ“· å›¾ç‰‡æ–‡æ¡£ {i+1}:")
                print(f"  ID: {doc_id}")
                print(f"  æ–‡æ¡£å: {doc.metadata.get('document_name', 'N/A')}")
                print(f"  é¡µç : {doc.metadata.get('page_number', 'N/A')}")
                print(f"  å›¾ç‰‡ID: {doc.metadata.get('image_id', 'N/A')}")
                print(f"  å¢å¼ºæè¿°: {doc.metadata.get('enhanced_description', '')[:100] + '...' if len(doc.metadata.get('enhanced_description', '')) > 100 else doc.metadata.get('enhanced_description', '')}")
            
            return image_info
            
        except Exception as e:
            logging.error(f"æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _check_table_docs(self) -> Dict[str, Any]:
        """æ£€æŸ¥è¡¨æ ¼æ–‡æ¡£"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            table_docs = []
            
            for doc_id, doc in docstore.items():
                if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'table':
                    table_docs.append((doc_id, doc))
            
            table_info = {
                'total_table_docs': len(table_docs),
                'metadata_fields': set(),
                'table_types': defaultdict(int),
                'document_names': set(),
                'samples': []
            }
            
            print(f"\nğŸ“Š æ‰¾åˆ° {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
            
            # åˆ†æå‰å‡ ä¸ªè¡¨æ ¼æ–‡æ¡£
            for i, (doc_id, doc) in enumerate(table_docs[:3]):
                sample_info = {
                    'index': i+1,
                    'doc_id': doc_id,
                    'document_name': doc.metadata.get('document_name', 'N/A'),
                    'page_number': doc.metadata.get('page_number', 'N/A'),
                    'table_id': doc.metadata.get('table_id', 'N/A'),
                    'table_type': doc.metadata.get('table_type', 'N/A')
                }
                table_info['samples'].append(sample_info)
                
                print(f"\nğŸ“„ è¡¨æ ¼æ–‡æ¡£ {i+1}:")
                print(f"  æ–‡æ¡£ID: {doc_id}")
                print(f"  æ–‡æ¡£å: {doc.metadata.get('document_name', 'N/A')}")
                print(f"  é¡µç : {doc.metadata.get('page_number', 'N/A')}")
                print(f"  è¡¨æ ¼ID: {doc.metadata.get('table_id', 'N/A')}")
                print(f"  è¡¨æ ¼ç±»å‹: {doc.metadata.get('table_type', 'N/A')}")
                
                # æ”¶é›†å…ƒæ•°æ®å­—æ®µ
                if hasattr(doc, 'metadata') and doc.metadata:
                    table_info['metadata_fields'].update(doc.metadata.keys())
                    
                    # ç»Ÿè®¡ç‰¹å®šå­—æ®µ
                    if doc.metadata.get('table_type'):
                        table_info['table_types'][doc.metadata['table_type']] += 1
                    if doc.metadata.get('document_name'):
                        table_info['document_names'].add(doc.metadata['document_name'])
            
            # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
            table_info['metadata_fields'] = list(table_info['metadata_fields'])
            table_info['document_names'] = list(table_info['document_names'])
            table_info['table_types'] = dict(table_info['table_types'])
            
            print(f"\nğŸ“Š è¡¨æ ¼æ–‡æ¡£ç»Ÿè®¡:")
            print(f"  æ€»æ–‡æ¡£æ•°: {table_info['total_table_docs']}")
            print(f"  å…ƒæ•°æ®å­—æ®µæ•°: {len(table_info['metadata_fields'])}")
            
            return table_info
            
        except Exception as e:
            logging.error(f"æ£€æŸ¥è¡¨æ ¼æ–‡æ¡£å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _check_text_docs(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡æœ¬æ–‡æ¡£"""
        try:
            vector_store = self.vector_store_manager.vector_store
            if not vector_store or not hasattr(vector_store, 'docstore'):
                return {'error': 'å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸'}
            
            docstore = vector_store.docstore._dict
            text_docs = []
            
            for doc_id, doc in docstore.items():
                if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'text':
                    text_docs.append((doc_id, doc))
            
            text_info = {
                'total_text_docs': len(text_docs),
                'document_names': set(),
                'page_numbers': set(),
                'samples': []
            }
            
            print(f"\nğŸ“ æ‰¾åˆ° {len(text_docs)} ä¸ªæ–‡æœ¬æ–‡æ¡£")
            
            # åˆ†æå‰å‡ ä¸ªæ–‡æœ¬æ–‡æ¡£
            for i, (doc_id, doc) in enumerate(text_docs[:3]):
                sample_info = {
                    'index': i+1,
                    'doc_id': doc_id,
                    'document_name': doc.metadata.get('document_name', 'N/A'),
                    'page_number': doc.metadata.get('page_number', 'N/A'),
                    'chunk_index': doc.metadata.get('chunk_index', 'N/A'),
                    'content_preview': doc.page_content[:100] + '...' if hasattr(doc, 'page_content') and len(doc.page_content) > 100 else (doc.page_content if hasattr(doc, 'page_content') else 'N/A')
                }
                text_info['samples'].append(sample_info)
                
                print(f"\nğŸ“ æ–‡æœ¬æ–‡æ¡£ {i+1}:")
                print(f"  æ–‡æ¡£ID: {doc_id}")
                print(f"  æ–‡æ¡£å: {doc.metadata.get('document_name', 'N/A')}")
                print(f"  é¡µç : {doc.metadata.get('page_number', 'N/A')}")
                print(f"  å—ç´¢å¼•: {doc.metadata.get('chunk_index', 'N/A')}")
                print(f"  å†…å®¹é¢„è§ˆ: {doc.page_content[:100] + '...' if hasattr(doc, 'page_content') and len(doc.page_content) > 100 else (doc.page_content if hasattr(doc, 'page_content') else 'N/A')}")
                
                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                if doc.metadata.get('document_name'):
                    text_info['document_names'].add(doc.metadata['document_name'])
                if doc.metadata.get('page_number'):
                    text_info['page_numbers'].add(doc.metadata['page_number'])
            
            # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
            text_info['document_names'] = list(text_info['document_names'])
            text_info['page_numbers'] = list(text_info['page_numbers'])
            
            print(f"\nğŸ“ æ–‡æœ¬æ–‡æ¡£ç»Ÿè®¡:")
            print(f"  æ€»æ–‡æ¡£æ•°: {text_info['total_text_docs']}")
            print(f"  å”¯ä¸€æ–‡æ¡£å: {len(text_info['document_names'])}")
            print(f"  é¡µç èŒƒå›´: {min(text_info['page_numbers']) if text_info['page_numbers'] else 'N/A'} - {max(text_info['page_numbers']) if text_info['page_numbers'] else 'N/A'}")
            
            return text_info

        except Exception as e:
            logging.error(f"æ£€æŸ¥æ–‡æœ¬æ–‡æ¡£å¤±è´¥: {e}")
            return {'error': str(e)}

    def _analyze_vector_data(self, vector_store) -> Dict[str, Any]:
        """åˆ†æå‘é‡æ•°æ®çš„åˆ†å¸ƒå’Œæ ·æœ¬"""
        print("\nğŸ”¢ å‘é‡æ•°æ®åˆ†æ")
        print("=" * 60)

        if not vector_store:
            print("âŒ å‘é‡å­˜å‚¨å¯¹è±¡æ— æ•ˆ")
            return None

        try:
            # è·å–å‘é‡æ•°æ®
            vectors = vector_store.index.reconstruct_n(0, vector_store.index.ntotal)
            vector_info = {
                'vector_count': vector_store.index.ntotal,
                'vector_dimension': vector_store.index.d,
                'vectors': vectors
            }

            print(f"ğŸ“Š å‘é‡ç»Ÿè®¡:")
            print(f"  å‘é‡æ•°é‡: {vector_info['vector_count']}")
            print(f"  å‘é‡ç»´åº¦: {vector_info['vector_dimension']}")

            if vector_info['vector_count'] > 0:
                # å‘é‡èŒƒæ•°åˆ†æ
                norms = np.linalg.norm(vectors, axis=1)
                vector_info['norm_stats'] = {
                    'min': float(norms.min()),
                    'max': float(norms.max()),
                    'mean': float(norms.mean()),
                    'std': float(norms.std())
                }

                print(f"\nğŸ“ å‘é‡èŒƒæ•°ç»Ÿè®¡:")
                print(f"  æœ€å°å€¼: {norms.min():.4f}")
                print(f"  æœ€å¤§å€¼: {norms.max():.4f}")
                print(f"  å¹³å‡å€¼: {norms.mean():.4f}")
                print(f"  æ ‡å‡†å·®: {norms.std():.4f}")

                # æ˜¾ç¤ºå‘é‡æ ·æœ¬
                show_samples = input("\næ˜¯å¦æ˜¾ç¤ºå‘é‡æ•°æ®æ ·æœ¬? (y/n): ").strip().lower()
                if show_samples == 'y':
                    print(f"\nğŸ” å‘é‡æ ·æœ¬:")
                    sample_count = min(5, vector_info['vector_count'])
                    for i in range(sample_count):
                        print(f"  å‘é‡ {i+1}: [{vectors[i][0]:.4f}, {vectors[i][1]:.4f}, ..., {vectors[i][-1]:.4f}]")
                        print(f"    èŒƒæ•°: {norms[i]:.4f}")

                    # åˆ†æå‘é‡ç›¸ä¼¼åº¦
                    if vector_info['vector_count'] > 1:
                        self._analyze_vector_similarity(vectors, vector_info)

            return vector_info

        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åˆ†æå¤±è´¥: {e}")
            return None

    def _analyze_vector_similarity(self, vectors, vector_info):
        """åˆ†æå‘é‡ç›¸ä¼¼åº¦ - è¯„ä¼°å‘é‡åŒ–è´¨é‡å’Œå‘ç°æ½œåœ¨é—®é¢˜"""
        print("\nğŸ”— å‘é‡ç›¸ä¼¼åº¦åˆ†æ")
        print("-" * 40)
        print("ğŸ¯ ä½œç”¨ï¼šè¯„ä¼°å‘é‡åŒ–è´¨é‡ï¼Œå‘ç°é‡å¤å†…å®¹ï¼Œä¼˜åŒ–æ£€ç´¢å‚æ•°")

        try:
            # è®¡ç®—å‰10ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦
            sample_size = min(10, vector_info['vector_count'])
            sample_vectors = vectors[:sample_size]

            # å½’ä¸€åŒ–å‘é‡ç”¨äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            norms = np.linalg.norm(sample_vectors, axis=1, keepdims=True)
            normalized_vectors = sample_vectors / norms

            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = np.dot(normalized_vectors, normalized_vectors.T)

            print(f"ğŸ“ˆ å‰{sample_size}ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦çŸ©é˜µ:")
            print("     ", end="")
            for i in range(sample_size):
                print("8d")
            print()

            for i in range(sample_size):
                print("3d", end="")
                for j in range(sample_size):
                    if i == j:
                        print("1.0000", end="")
                    else:
                        sim = similarity_matrix[i, j]
                        # ç”¨ä¸åŒæ ‡è¯†ç¬¦è¡¨ç¤ºç›¸ä¼¼åº¦çº§åˆ«
                        if sim > 0.8:
                            print(".4f", end="")
                        elif sim > 0.6:
                            print(".4f", end="")
                        elif sim > 0.4:
                            print(".4f", end="")
                        else:
                            print(".4f", end="")
                print()

            # åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ
            similarities = []
            for i in range(sample_size):
                for j in range(i+1, sample_size):
                    similarities.append(similarity_matrix[i, j])

            if similarities:
                similarities = np.array(similarities)
                print("\nğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡:")
                print(f"  å¹³å‡ç›¸ä¼¼åº¦: {similarities.mean():.4f}")
                print(f"  æœ€å¤§ç›¸ä¼¼åº¦: {similarities.max():.4f}")
                print(f"  æœ€å°ç›¸ä¼¼åº¦: {similarities.min():.4f}")
                print(f"  ç›¸ä¼¼åº¦æ ‡å‡†å·®: {similarities.std():.4f}")
                # è´¨é‡è¯„ä¼°
                print("\nğŸ” è´¨é‡è¯„ä¼°:")
                if similarities.mean() > 0.7:
                    print("  âš ï¸  ç›¸ä¼¼åº¦è¿‡é«˜ï¼šå¯èƒ½å­˜åœ¨å¤§é‡é‡å¤å†…å®¹")
                elif similarities.mean() > 0.5:
                    print("  âœ… ç›¸ä¼¼åº¦é€‚ä¸­ï¼šå†…å®¹å¤šæ ·æ€§è‰¯å¥½")
                elif similarities.mean() > 0.3:
                    print("  âœ… ç›¸ä¼¼åº¦æ­£å¸¸ï¼šå†…å®¹åŒºåˆ†åº¦è‰¯å¥½")
                else:
                    print("  âœ… ç›¸ä¼¼åº¦è¾ƒä½ï¼šå†…å®¹é«˜åº¦å¤šæ ·åŒ–")

                # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é«˜çš„ç›¸ä¼¼åº¦
                high_sim_pairs = [(i, j) for i in range(sample_size) for j in range(i+1, sample_size) if similarity_matrix[i, j] > 0.9]
                if high_sim_pairs:
                    print(f"  ğŸš¨ å‘ç° {len(high_sim_pairs)} å¯¹é«˜åº¦ç›¸ä¼¼çš„å‘é‡ (>0.9)")
                    print("     å¯èƒ½è¡¨ç¤ºé‡å¤å†…å®¹æˆ–å‘é‡åŒ–å¼‚å¸¸")

        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {e}")

    def _generate_quality_report(self, structure_info, image_info, table_info, vector_info) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š"""
        print("\nğŸ“‹ æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š")
        print("=" * 60)

        report = {
            'overall_score': 0,
            'issues': [],
            'recommendations': []
        }

        # æ£€æŸ¥1: æ–‡æ¡£æ•°é‡åˆç†æ€§
        if structure_info and structure_info.get('total_docs', 0) > 0:
            print("âœ… æ–‡æ¡£æ•°é‡æ£€æŸ¥é€šè¿‡")
            report['overall_score'] += 20
        else:
            print("âŒ æ–‡æ¡£æ•°é‡å¼‚å¸¸")
            report['issues'].append("æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
            report['recommendations'].append("æ£€æŸ¥æ–‡æ¡£å¤„ç†æµç¨‹æ˜¯å¦æ­£å¸¸")

        # æ£€æŸ¥2: å…ƒæ•°æ®å®Œæ•´æ€§
        if structure_info and len(structure_info.get('all_metadata_fields', [])) > 10:
            print("âœ… å…ƒæ•°æ®å­—æ®µä¸°å¯Œ")
            report['overall_score'] += 25
        else:
            print("âŒ å…ƒæ•°æ®å­—æ®µä¸è¶³")
            report['issues'].append("å…ƒæ•°æ®å­—æ®µæ•°é‡ä¸è¶³")
            report['recommendations'].append("æ£€æŸ¥metadataæå–é€»è¾‘")

        # æ£€æŸ¥3: å›¾ç‰‡æ–‡æ¡£è´¨é‡
        if image_info and image_info['total_image_docs'] > 0:
            enhanced_ratio = image_info['enhanced_description_stats']['with_enhanced'] / max(1, image_info['total_image_docs'])
            if enhanced_ratio > 0.8:
                print("âœ… å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡è‰¯å¥½")
                report['overall_score'] += 20
            elif enhanced_ratio > 0.5:
                print("âš ï¸  å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡ä¸€èˆ¬")
                report['overall_score'] += 15
                report['recommendations'].append("æå‡å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡")
            else:
                print("âŒ å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡ä¸è¶³")
                report['issues'].append("å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡ä½")
                report['recommendations'].append("æ£€æŸ¥å›¾ç‰‡å¢å¼ºå¤„ç†æµç¨‹")
        else:
            print("âš ï¸  æ²¡æœ‰å›¾ç‰‡æ–‡æ¡£")
            report['recommendations'].append("æ£€æŸ¥å›¾ç‰‡å¤„ç†æµç¨‹")

        # æ£€æŸ¥4: è¡¨æ ¼æ–‡æ¡£è´¨é‡
        if table_info and table_info['total_table_docs'] > 0:
            processed_ratio = table_info.get('has_processed_content', 0) / max(1, table_info['total_table_docs'])
            if processed_ratio > 0.8:
                print("âœ… è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†è¦†ç›–ç‡è‰¯å¥½")
                report['overall_score'] += 15
            else:
                print("âŒ è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†è¦†ç›–ç‡ä¸è¶³")
                report['issues'].append("è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†è¦†ç›–ç‡ä½")
                report['recommendations'].append("æ£€æŸ¥è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†æµç¨‹")
        else:
            print("âš ï¸  æ²¡æœ‰è¡¨æ ¼æ–‡æ¡£")

        # æ£€æŸ¥5: å‘é‡æ•°æ®è´¨é‡
        if vector_info and vector_info['vector_count'] > 0:
            if 'norm_stats' in vector_info:
                norm_std = vector_info['norm_stats']['std']
                if norm_std < 1.0:
                    print("âœ… å‘é‡èŒƒæ•°åˆ†å¸ƒå‡åŒ€")
                    report['overall_score'] += 20
                else:
                    print("âš ï¸  å‘é‡èŒƒæ•°åˆ†å¸ƒä¸å‡åŒ€")
                    report['overall_score'] += 15
                    report['recommendations'].append("æ£€æŸ¥å‘é‡æ ‡å‡†åŒ–å¤„ç†")

            # æ£€æŸ¥å‘é‡ç»´åº¦ä¸€è‡´æ€§
            if vector_info['vector_dimension'] == 1536:  # DashScopeé»˜è®¤ç»´åº¦
                print("âœ… å‘é‡ç»´åº¦æ­£ç¡®")
            else:
                print(f"âš ï¸  å‘é‡ç»´åº¦å¼‚å¸¸: {vector_info['vector_dimension']}")
                report['issues'].append(f"å‘é‡ç»´åº¦å¼‚å¸¸: {vector_info['vector_dimension']}")
                report['recommendations'].append("æ£€æŸ¥embeddingæ¨¡å‹é…ç½®")
        else:
            print("âŒ æ²¡æœ‰å‘é‡æ•°æ®")
            report['issues'].append("æ²¡æœ‰å‘é‡æ•°æ®")
            report['recommendations'].append("æ£€æŸ¥å‘é‡åŒ–æµç¨‹")

        # ç”Ÿæˆæ€»ä½“è¯„ä»·
        print("\nğŸ† æ€»ä½“è¯„ä»·:")
        if report['overall_score'] >= 90:
            print("  ğŸ‰ ä¼˜ç§€ - æ•°æ®è´¨é‡éå¸¸è‰¯å¥½")
        elif report['overall_score'] >= 70:
            print("  âœ… è‰¯å¥½ - æ•°æ®è´¨é‡åŸºæœ¬æ»¡è¶³è¦æ±‚")
        elif report['overall_score'] >= 50:
            print("  âš ï¸  ä¸€èˆ¬ - æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›")
        else:
            print("  âŒ è¾ƒå·® - æ•°æ®è´¨é‡å­˜åœ¨ä¸¥é‡é—®é¢˜")

        print(f"  ğŸ“Š ç»¼åˆå¾—åˆ†: {report['overall_score']}/100")

        if report['issues']:
            print("\nğŸ”§ å‘ç°çš„é—®é¢˜:")
            for issue in report['issues']:
                print(f"  â€¢ {issue}")

        if report['recommendations']:
            print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for rec in report['recommendations']:
                print(f"  â€¢ {rec}")

        report['overall_score'] = report['overall_score']
        return report

    def _show_detailed_document_list(self, structure_info):
        """æ˜¾ç¤ºè¯¦ç»†çš„æ–‡æ¡£åˆ—è¡¨"""
        print("\nğŸ“‹ è¯¦ç»†æ–‡æ¡£åˆ—è¡¨")
        print("=" * 80)

        # ä»docstoreä¸­è·å–è¯¦ç»†ä¿¡æ¯
        vector_store = self.vector_store_manager.vector_store
        if not vector_store or not hasattr(vector_store, 'docstore'):
            print("âŒ æ— æ³•è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯")
            return

        docstore = vector_store.docstore._dict

        # æŒ‰chunk_typeåˆ†ç»„æ˜¾ç¤º
        from collections import defaultdict
        docs_by_type = defaultdict(list)

        for doc_id, doc in docstore.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            chunk_type = metadata.get('chunk_type', 'unknown')

            doc_info = {
                'doc_id': doc_id,
                'chunk_type': chunk_type,
                'document_name': metadata.get('document_name', 'N/A'),
                'page_number': metadata.get('page_number', 'N/A'),
                'content_length': len(doc.page_content) if hasattr(doc, 'page_content') else 0,
                'metadata_fields_count': len(metadata),
                'metadata_keys': list(metadata.keys())
            }
            docs_by_type[chunk_type].append(doc_info)

        for chunk_type, docs in docs_by_type.items():
            print(f"\nğŸ”¹ {chunk_type.upper()} æ–‡æ¡£ ({len(docs)} ä¸ª):")
            print("-" * 60)

            # æ˜¾ç¤ºå‰10ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
            for i, doc in enumerate(docs[:10]):
                print("2d")
                print(f"      æ–‡æ¡£å: {doc['document_name']}")
                print(f"      é¡µç : {doc['page_number']}")
                print(f"      å†…å®¹é•¿åº¦: {doc['content_length']}")
                print(f"      å…ƒæ•°æ®å­—æ®µæ•°: {doc['metadata_fields_count']}")
                print(f"      å…ƒæ•°æ®å­—æ®µ: {', '.join(doc['metadata_keys'][:8])}...")
                if len(doc['metadata_keys']) > 8:
                    print(f"                   ... è¿˜æœ‰ {len(doc['metadata_keys']) - 8} ä¸ªå­—æ®µ")

            if len(docs) > 10:
                print(f"      ... è¿˜æœ‰ {len(docs) - 10} ä¸ªæ–‡æ¡£")

            # æ˜¾ç¤ºè¯¥ç±»å‹çš„ç»Ÿè®¡ä¿¡æ¯
            total_content_length = sum(doc['content_length'] for doc in docs)
            avg_content_length = total_content_length / len(docs) if docs else 0
            print(f"\n  ğŸ“Š {chunk_type} ç»Ÿè®¡:")
            print(f"    æ€»å†…å®¹é•¿åº¦: {total_content_length}")
            print(f"    å¹³å‡å†…å®¹é•¿åº¦: {avg_content_length:.1f}")
            print(f"    å¹³å‡å…ƒæ•°æ®å­—æ®µæ•°: {sum(doc['metadata_fields_count'] for doc in docs) / len(docs):.1f}")
    
    def _save_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜è¯Šæ–­ç»“æœåˆ°æ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ è¯Šæ–­ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            logging.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        tool = DatabaseDiagnosticTool()
        tool.run_diagnostic()
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
