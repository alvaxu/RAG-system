"""
æ–‡æœ¬å¤„ç†å™¨

è´Ÿè´£å¤„ç†å„ç§ç±»å‹çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…æ‹¬ä»PDFè§£æå‡ºçš„æ–‡æœ¬ã€Markdownæ–‡ä»¶å†…å®¹ã€
ä»¥åŠJSONæ ¼å¼çš„ç»“æ„åŒ–æ–‡æœ¬æ•°æ®ã€‚å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒã€‚
"""

import os
import time
import logging
import re
from typing import Dict, List, Any

class TextProcessor:
    """
    æ–‡æœ¬å¤„ç†å™¨
    æ•´åˆï¼šåˆ†æ â†’ åˆ†å— â†’ å‘é‡åŒ–
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼Œä½äºprocessorsæ¨¡å—ä¸‹
    """
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.config = config_manager.get_all_config()
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        from .text_analyzer import TextAnalyzer
        self.text_analyzer = TextAnalyzer()
        
        # ä½¿ç”¨å¤±è´¥å¤„ç†ï¼ˆç¬¦åˆè®¾è®¡æ–‡æ¡£è§„èŒƒï¼‰
        self.failure_handler = config_manager.get_failure_handler()
        
        # åŠ è½½é…ç½®
        self._load_configuration()
        
        logging.info("æ–‡æœ¬å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_configuration(self):
        """
        åŠ è½½é…ç½®
        """
        # åˆ†å—é…ç½®
        self.chunk_size = self.config.get('document_processing.chunk_size', 1000)
        self.chunk_overlap = self.config.get('document_processing.chunk_overlap', 200)
        
        # å‘é‡åŒ–é…ç½®
        self.embedding_model = self.config.get('vectorization.text_embedding_model', 'text-embedding-v1')
    
    def process_text(self, text_data: Dict) -> List[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ªæ–‡æœ¬
        """
        try:
            print(f"  ğŸ“ å¤„ç†æ–‡æœ¬: {text_data.get('text_title', 'æœªå‘½åæ–‡æœ¬')}")
            
            # æ­¥éª¤1: æ–‡æœ¬ç»“æ„åˆ†æ
            print("  æ­¥éª¤1: æ–‡æœ¬ç»“æ„åˆ†æ...")
            analysis_result = self.text_analyzer.analyze(text_data)
            if analysis_result.get('analysis_status') != 'success':
                print(f"  âš ï¸ æ–‡æœ¬åˆ†æå¤±è´¥: {analysis_result.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
            
            # æ­¥éª¤2: æ™ºèƒ½æ–‡æœ¬åˆ†å—
            print("  æ­¥éª¤2: æ™ºèƒ½æ–‡æœ¬åˆ†å—...")
            chunks = self._create_text_chunks(text_data, analysis_result)
            print(f"  âœ… æ–‡æœ¬åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªåˆ†å—")
            
            # æ­¥éª¤3: ç”Ÿæˆå®Œæ•´å…ƒæ•°æ®
            print("  æ­¥éª¤3: ç”Ÿæˆå®Œæ•´å…ƒæ•°æ®...")
            complete_metadata = []
            for i, chunk in enumerate(chunks):
                chunk_metadata = self._create_complete_text_metadata(
                    text_data, chunk, i, analysis_result
                )
                complete_metadata.append(chunk_metadata)
            
            print(f"  âœ… æ–‡æœ¬å¤„ç†å®Œæˆ: {text_data.get('text_title', 'æœªå‘½åæ–‡æœ¬')}")
            return complete_metadata
            
        except Exception as e:
            error_msg = f"æ–‡æœ¬å¤„ç†å¤±è´¥: {e}"
            logging.error(error_msg)
            self.failure_handler.record_failure(text_data, 'text_processing', str(e))
            
            # è¿”å›é”™è¯¯ç»“æœ
            return [self._create_error_text_metadata(text_data, str(e))]
    
    def process_batch(self, texts: List[Dict]) -> List[List[Dict[str, Any]]]:
        """
        æ‰¹é‡å¤„ç†æ–‡æœ¬
        """
        processed_texts = []
        
        for i, text in enumerate(texts):
            try:
                print(f"æ­£åœ¨å¤„ç†æ–‡æœ¬ {i+1}/{len(texts)}")
                result = self.process_text(text)
                processed_texts.append(result)
            except Exception as e:
                error_msg = f"æ–‡æœ¬æ‰¹é‡å¤„ç†å¤±è´¥: {e}"
                logging.error(error_msg)
                self.failure_handler.record_failure(text, 'text_batch_processing', str(e))
                
                # åˆ›å»ºé”™è¯¯ç»“æœ
                error_result = [self._create_error_text_metadata(text, str(e))]
                processed_texts.append(error_result)
        
        return processed_texts
    
    def _create_text_chunks(self, text_data: Dict, analysis_result: Dict) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºæ–‡æœ¬åˆ†å—
        """
        text_content = text_data.get('text_content', '')
        if not text_content:
            return []
        
        # è·å–æ–‡æœ¬ç»“æ„ä¿¡æ¯
        structure_info = analysis_result.get('structure', {})
        paragraphs = structure_info.get('paragraphs', 0)
        
        # å¦‚æœæ®µè½æ•°å¾ˆå°‘ï¼Œä¸éœ€è¦åˆ†å—
        if paragraphs <= 1:
            return [{
                'content': text_content,
                'start_pos': 0,
                'end_pos': len(text_content),
                'chunk_index': 0,
                'is_subchunk': False
            }]
        
        # æ™ºèƒ½åˆ†å—ï¼šåœ¨æ®µè½è¾¹ç•Œåˆ†å‰²
        chunks = []
        current_chunk = []
        current_length = 0
        chunk_index = 0
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraph_list = text_content.strip().split('\n\n')
        
        for i, paragraph in enumerate(paragraph_list):
            if paragraph.strip():
                paragraph_length = len(paragraph)
                
                # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¼šè¶…è¿‡é™åˆ¶ï¼Œä¸”å½“å‰å—ä¸ä¸ºç©º
                if current_length + paragraph_length > self.chunk_size and current_chunk:
                    # ä¿å­˜å½“å‰å—
                    chunk_content = '\n\n'.join(current_chunk)
                    chunks.append({
                        'content': chunk_content,
                        'start_pos': 0,  # ç®€åŒ–å¤„ç†
                        'end_pos': len(chunk_content),
                        'chunk_index': chunk_index,
                        'is_subchunk': len(chunks) > 0
                    })
                    
                    # å¼€å§‹æ–°å—
                    current_chunk = [paragraph]
                    current_length = paragraph_length
                    chunk_index += 1
                else:
                    current_chunk.append(paragraph)
                    current_length += paragraph_length
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunk_content = '\n\n'.join(current_chunk)
            chunks.append({
                'content': chunk_content,
                'start_pos': 0,  # ç®€åŒ–å¤„ç†
                'end_pos': len(chunk_content),
                'chunk_index': chunk_index,
                'is_subchunk': len(chunks) > 0
            })
        
        return chunks if chunks else [{
            'content': text_content,
            'start_pos': 0,
            'end_pos': len(text_content),
            'chunk_index': 0,
            'is_subchunk': False
        }]
    
    def _create_complete_text_metadata(self, text_data: Dict, chunk: Dict, 
                                     chunk_index: int, analysis_result: Dict) -> Dict[str, Any]:
        """
        åˆ›å»ºå®Œæ•´çš„æ–‡æœ¬å…ƒæ•°æ®ï¼Œå®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£çš„TEXT_METADATA_SCHEMAè§„èŒƒ
        """
        return {
            # åŸºç¡€æ ‡è¯†å­—æ®µï¼ˆç¬¦åˆCOMMON_METADATA_FIELDSï¼‰
            'chunk_id': f"{text_data.get('chunk_id', '')}_chunk_{chunk_index}",
            'chunk_type': 'text',
            'source_type': 'pdf',
            'document_name': text_data.get('document_name', ''),
            'document_path': text_data.get('document_path', ''),
            'page_number': text_data.get('page_number', 1),
            'page_idx': text_data.get('page_idx', 1),
            'created_timestamp': text_data.get('created_timestamp', int(time.time())),
            'updated_timestamp': int(time.time()),
            'processing_version': '3.0.0',
            
            # æ–‡æœ¬ç‰¹æœ‰å­—æ®µï¼ˆç¬¦åˆTEXT_METADATA_SCHEMAï¼‰
            'text_id': text_data.get('text_id', ''),
            'text_title': text_data.get('text_title', ''),
            'text_content': chunk.get('content', ''),
            'text_summary': self._generate_text_summary(chunk.get('content', '')),
            
            # åˆ†å—ä¿¡æ¯
            'chunk_info': {
                'chunk_index': chunk_index,
                'start_position': chunk.get('start_pos', 0),
                'end_position': chunk.get('end_pos', 0),
                'chunk_length': len(chunk.get('content', '')),
                'is_subchunk': chunk.get('is_subchunk', False)
            },
            
            # æ–‡æœ¬ç»“æ„ä¿¡æ¯
            'text_structure': analysis_result.get('structure', {}),
            
            # æ–‡æœ¬ç‰¹å¾ä¿¡æ¯
            'text_features': analysis_result.get('features', {}),
            
            # è¯­ä¹‰ä¿¡æ¯
            'semantic_info': analysis_result.get('semantic', {}),
            
            # ç›¸å…³æ–‡æœ¬ä¿¡æ¯
            'related_text': text_data.get('related_text', ''),
            'context_info': text_data.get('context_info', {}),
            
            # å¤„ç†çŠ¶æ€ä¿¡æ¯
            'analysis_status': analysis_result.get('analysis_status', 'unknown'),
            'chunking_status': 'success',
            
            # å‘é‡åŒ–ä¿¡æ¯å­—æ®µï¼ˆé¢„ç•™ï¼‰
            'vectorized': False,
            'vectorization_timestamp': None,
            'embedding_model': self.embedding_model,
            'text_embedding': [],
            'vectorization_status': 'pending',
            
            # å…³è”ä¿¡æ¯å­—æ®µ
            'related_image_chunks': text_data.get('related_image_chunks', []),
            'related_table_chunks': text_data.get('related_table_chunks', []),
            'parent_document_id': text_data.get('parent_document_id', ''),
            
            # æ¶æ„æ ‡è¯†
            'metadata_schema': 'TEXT_METADATA_SCHEMA',
            'metadata_version': '3.0.0',
            'processing_pipeline': 'Text_Analysis_Chunking_Pipeline',
            'optimization_features': [
                'modular_design',
                'smart_chunking',
                'complete_metadata',
                'semantic_analysis'
            ]
        }
    
    def _generate_text_summary(self, text_content: str) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
        """
        if not text_content:
            return "ç©ºæ–‡æœ¬"
        
        # ç®€å•çš„æ‘˜è¦ç”Ÿæˆï¼šå–å‰100ä¸ªå­—ç¬¦
        if len(text_content) <= 100:
            return text_content
        else:
            return text_content[:100] + "..."
    
    def _create_error_text_metadata(self, text_data: Dict, error_message: str) -> Dict[str, Any]:
        """
        åˆ›å»ºé”™è¯¯æ–‡æœ¬å…ƒæ•°æ®
        """
        return {
            # åŸºç¡€æ ‡è¯†å­—æ®µ
            'chunk_id': text_data.get('chunk_id', ''),
            'chunk_type': 'text',
            'source_type': 'pdf',
            'document_name': text_data.get('document_name', ''),
            'document_path': text_data.get('document_path', ''),
            'page_number': text_data.get('page_number', 1),
            'page_idx': text_data.get('page_idx', 1),
            'created_timestamp': text_data.get('created_timestamp', int(time.time())),
            'updated_timestamp': int(time.time()),
            'processing_version': '3.0.0',
            
            # é”™è¯¯ä¿¡æ¯
            'error': error_message,
            'processing_status': 'failed',
            
            # ç©ºçš„æ–‡æœ¬å­—æ®µ
            'text_id': text_data.get('text_id', ''),
            'text_title': text_data.get('text_title', ''),
            'text_content': text_data.get('text_content', ''),
            'text_summary': 'å¤„ç†å¤±è´¥',
            
            # ç©ºçš„å¤„ç†ç»“æœ
            'chunk_info': {'chunk_index': 0, 'start_position': 0, 'end_position': 0, 'chunk_length': 0, 'is_subchunk': False},
            'text_structure': {'paragraphs': 0, 'sentences': 0, 'words': 0, 'characters': 0},
            'text_features': {'text_type': 'unknown', 'is_structured': True, 'has_numbers': False, 'has_special_chars': False, 'language': 'unknown', 'complexity_score': 0},
            'semantic_info': {'key_topics': [], 'key_phrases': [], 'sentiment': 'neutral', 'formality': 'medium'},
            
            # æ¶æ„æ ‡è¯†
            'metadata_schema': 'TEXT_METADATA_SCHEMA',
            'metadata_version': '3.0.0',
            'processing_pipeline': 'Text_Error_Handling',
            'optimization_features': ['error_handling', 'graceful_degradation']
        }
    
    def get_processing_status(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†çŠ¶æ€
        """
        return {
            'analyzer_status': 'ready',
            'chunking_status': 'ready',
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'embedding_model': self.embedding_model,
            'total_texts_processed': 0  # å¯ä»¥æ·»åŠ è®¡æ•°å™¨
        }
