

## �� **文本召回技术实现详细说明**

### **1. 🏗️ 整体架构设计**

文本召回系统采用**5层策略降级**的架构设计，确保即使高优先级策略失败，仍有结果返回。整个系统基于 `TextEngine` 类实现，继承自 `BaseEngine`。

### **2. �� 文档加载管理**

#### **2.1 统一文档加载器集成**

```python
def _load_from_document_loader(self):
    """从统一文档加载器获取文本文档"""
    if self.document_loader:
        try:
            self.text_docs = self.document_loader.get_documents_by_type('text')
            self._docs_loaded = True
            self.logger.info(f"从统一加载器获取文本文档: {len(self.text_docs)} 个")
        except Exception as e:
            self.logger.error(f"从统一加载器获取文本文档失败: {e}")
            # 降级到传统加载方式
            self._load_text_documents()
    else:
        self.logger.warning("文档加载器未提供，使用传统加载方式")
        self._load_text_documents()
```

**功能说明**：
- 优先使用统一文档加载器，避免重复加载
- 支持降级到传统加载方式，保证向后兼容性
- 提供延迟加载机制，按需加载文档

#### **2.2 传统文档加载（降级策略）**

```python
def _load_text_documents(self):
    """加载文本文档到缓存"""
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # 清空之前的缓存
            self.text_docs = {}
            
            # 从向量数据库加载所有文本文档
            for doc_id, doc in self.vector_store.docstore._dict.items():
                chunk_type = doc.metadata.get('chunk_type', '')
                
                # 判断是否为文本文档 - 简化判断逻辑
                is_text = chunk_type == 'text'
                
                if is_text:
                    self.text_docs[doc_id] = doc
                    self.logger.debug(f"加载文本文档: {doc_id}, chunk_type: {chunk_type}")
            
            self.logger.info(f"成功加载 {len(self.text_docs)} 个文本文档")
            
            # 如果没有找到文本文档，尝试其他方法
            if not self.text_docs:
                self.logger.warning("未找到文本文档，尝试搜索所有文档...")
                self._search_all_documents_for_texts()
            
            # 如果成功加载了文档，退出重试循环
            if len(self.text_docs) > 0:
                self.logger.info(f"文本文档加载成功，共 {len(self.text_docs)} 个文档")
                return
            else:
                raise ValueError("未找到任何文本文档")
                
        except Exception as e:
            retry_count += 1
            self.logger.warning(f"文本文档加载失败，第{retry_count}次尝试: {e}")
            
            if retry_count >= max_retries:
                # 最终失败，记录错误并清空缓存
                self.logger.error(f"文本文档加载最终失败，已重试{max_retries}次: {e}")
                self.text_docs = {}
                return
            else:
                # 等待后重试
                import time
                time.sleep(1)
                self.logger.info(f"等待1秒后进行第{retry_count + 1}次重试...")
```

**功能说明**：
- 支持重试机制，最多重试3次
- 严格的类型过滤，只加载 `chunk_type == 'text'` 的文档
- 提供备选搜索策略，确保能找到文本文档
- 详细的日志记录，便于调试和监控

### **3. 🔍 核心搜索策略实现**

#### **3.1 主搜索函数**

```python
def _search_texts(self, query: str, **kwargs) -> List[Any]:
    """智能文本搜索 - V2.0增强版（严格类型过滤）"""
    results = []
    
    # 策略1: 优先使用已加载的文本文档进行搜索
    if self.text_docs:
        try:
            # 对文本文档进行向量相似度搜索
            for doc_id, doc in self.text_docs.items():
                score = self._calculate_text_score(doc, query)
                if score >= self.config.text_similarity_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'text_doc_search'
                    })
            
            self.logger.debug(f"文本文档搜索找到 {len(results)} 个结果")
        except Exception as e:
            self.logger.warning(f"文本文档搜索失败: {e}")
    
    # 策略2: 如果文本文档搜索没有结果，在内存中使用宽松搜索策略
    if not results and self.text_docs:
        try:
            self.logger.debug("策略1无结果，启用策略2：内存宽松搜索")
            
            # 在内存中文档中使用宽松搜索策略
            for doc_id, doc in self.text_docs.items():
                # 使用宽松的评分算法
                score = self._calculate_text_score_relaxed(doc, query)
                
                # 降低阈值，使用更宽松的匹配条件
                relaxed_threshold = self.config.text_similarity_threshold * 0.5
                if score >= relaxed_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'memory_relaxed_search'
                    })
            
            self.logger.debug(f"内存宽松搜索找到 {len(results)} 个结果")
        except Exception as e:
            self.logger.warning(f"内存宽松搜索失败: {e}")
    
    # 策略3: 如果仍然没有结果，尝试关键词搜索
    if not results:
        keyword_results = self._keyword_search(query)
        results.extend(keyword_results)
        self.logger.debug(f"关键词搜索找到 {len(keyword_results)} 个结果")
    
    # 策略4: 如果还是没有结果，尝试模糊匹配（但只针对文本文档）
    if not results:
        fuzzy_results = self._fuzzy_search(query)
        results.extend(fuzzy_results)
        self.logger.debug(f"模糊搜索找到 {len(fuzzy_results)} 个结果")
    
    # 策略5: 如果还是没有结果，在内存中降低阈值重新搜索
    if not results and self.config.text_similarity_threshold > 0.05 and self.text_docs:
        self.logger.debug("策略5：在内存中降低阈值重新搜索...")
        original_threshold = self.config.text_similarity_threshold
        self.config.text_similarity_threshold = 0.05
        
        # 在内存中文档中重新搜索
        try:
            for doc_id, doc in self.text_docs.items():
                score = self._calculate_text_score(doc, query)
                if score >= self.config.text_similarity_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'memory_low_threshold_search'
                    })
            
            self.logger.debug(f"内存低阈值搜索找到 {len(results)} 个结果")
        except Exception as e:
            self.logger.warning(f"内存低阈值搜索失败: {e}")
        
        # 恢复原始阈值
        self.config.text_similarity_threshold = original_threshold
    
    # 去重和排序
    seen_ids = set()
    unique_results = []
    for result in results:
        doc_id = result.get('doc_id', 'unknown')
        if doc_id not in seen_ids:
            seen_ids.add(doc_id)
            unique_results.append(result)
    
    # 按分数排序
    unique_results.sort(key=lambda x: x['score'], reverse=True)
    
    self.logger.debug(f"最终去重后得到 {len(unique_results)} 个结果")
    return unique_results
```

**功能说明**：
- **5层策略降级**：确保即使高优先级策略失败，仍有结果返回
- **智能策略切换**：根据结果数量自动启用下一层策略
- **内存优先优化**：策略2和策略5都在内存中执行，避免重复I/O操作
- **阈值动态调整**：策略5临时降低阈值到0.05，确保有结果返回
- **结果去重排序**：避免重复内容，按相关性分数排序

### **4. �� 评分算法详解**

#### **4.1 严格评分算法（策略1）**

```python
def _calculate_text_score(self, doc: Any, query: str) -> float:
    """计算文本匹配分数 - 智能综合评分（严格相关性判断）"""
    score = 0.0
    
    # 获取文本内容
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    # 1. 语义相似度分数（核心指标）
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight
    
    # 2. 关键词匹配分数（严格匹配）
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        # 关键词匹配必须达到一定阈值才给分
        if keyword_score > 0.3:  # 至少30%的关键词匹配
            score += keyword_score * self.config.keyword_weight
        else:
            # 关键词匹配不足，大幅降低分数
            score *= 0.3
    
    # 3. 向量相似度分数（如果有向量嵌入）
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.3  # 降低默认向量分数
        score += vector_score * self.config.vector_weight
    
    # 4. 文档类型匹配奖励（降低奖励）
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.05  # 降低文本文档类型匹配奖励
    
    # 5. 内容长度奖励（降低奖励）
    if len(content) > 100:
        score += 0.02  # 降低内容长度奖励
    
    # 6. 相关性惩罚机制
    if semantic_score < 0.1:  # 语义相似度过低
        score *= 0.5  # 大幅降低分数
    
    return min(score, 1.0)
```

**评分权重配置**：
- `semantic_weight`: 0.3 (语义相似度权重)
- `keyword_weight`: 0.5 (关键词匹配权重)  
- `vector_weight`: 0.2 (向量相似度权重)

**严格性特点**：
- 关键词匹配必须达到30%才给分，否则大幅降分
- 语义相似度过低时，总分乘以0.5惩罚
- 降低文档类型和内容长度的奖励分数

#### **4.2 宽松评分算法（策略2）**

```python
def _calculate_text_score_relaxed(self, doc: Any, query: str) -> float:
    """计算文本匹配分数 - 宽松评分算法（用于策略2）"""
    score = 0.0
    
    # 获取文本内容
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    # 1. 语义相似度分数（宽松处理）
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight
    
    # 2. 关键词匹配分数（宽松匹配）
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        # 宽松处理：即使关键词匹配不足，也给予一定分数
        if keyword_score > 0.1:  # 降低到10%的关键词匹配
            score += keyword_score * self.config.keyword_weight
        else:
            # 关键词匹配不足，轻微降低分数
            score *= 0.7  # 从0.3提升到0.7
    
    # 3. 向量相似度分数（如果有向量嵌入）
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.4  # 提高默认向量分数
        score += vector_score * self.config.vector_weight
    
    # 4. 文档类型匹配奖励（提高奖励）
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.1  # 提高文本文档类型匹配奖励
    
    # 5. 内容长度奖励（提高奖励）
    if len(content) > 100:
        score += 0.05  # 提高内容长度奖励
    
    # 6. 相关性惩罚机制（宽松处理）
    if semantic_score < 0.05:  # 从0.1降低到0.05
        score *= 0.7  # 从0.5提升到0.7
    
    return min(score, 1.0)
```

**宽松性特点**：
- 关键词匹配阈值降低到10%
- 惩罚系数从0.3提升到0.7，更宽松
- 语义相似度惩罚阈值从0.1降低到0.05
- 提高文档类型和内容长度的奖励分数

### **5. �� 关键词处理机制**

#### **5.1 关键词提取**

```python
def _extract_keywords(self, query: str) -> List[str]:
    """提取查询关键词"""
    stop_words = {'的', '是', '在', '有', '和', '与', '或', '但', '而', '了', '吗', '呢', '啊'}
    
    import re
    clean_query = re.sub(r'[^\w\s]', '', query)
    
    words = clean_query.split()
    keywords = [word for word in words if word not in stop_words and len(word) > 1]
    
    return keywords
```

**功能说明**：
- 去除停用词（中文常见虚词）
- 清理特殊字符，只保留字母数字和空格
- 过滤长度小于2的单词
- 返回清洗后的关键词列表

#### **5.2 关键词评分**

```python
def _calculate_keyword_score(self, doc: Any, keywords: List[str]) -> float:
    """计算关键词匹配分数"""
    if not keywords:
        return 0.0
    
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    total_score = 0.0
    for keyword in keywords:
        if keyword in content:
            total_score += 1.0
    
    return min(total_score / len(keywords), 1.0)
```

**评分逻辑**：
- 统计文档内容中包含的关键词数量
- 分数 = 匹配关键词数 / 总关键词数
- 最高分数为1.0

### **6. 🌐 语义相似度计算**

```python
def _calculate_text_similarity(self, query: str, text: str) -> float:
    """计算文本相似度"""
    if not text or not query:
        return 0.0
    
    # 简单的词汇重叠计算
    query_words = set(query.lower().split())
    text_words = set(text.lower().split())
    
    if not query_words or not text_words:
        return 0.0
    
    intersection = query_words.intersection(text_words)
    union = query_words.union(text_words)
    
    if union:
        return len(intersection) / len(union)
    return 0.0
```

**计算原理**：
- 使用Jaccard相似度：交集大小 / 并集大小
- 将查询和文档转换为词汇集合
- 计算词汇重叠程度
- 返回0-1之间的相似度分数

### **7. 🔍 智能模糊搜索**

```python
def _fuzzy_search(self, query: str) -> List[Any]:
    """智能模糊搜索 - 只在真正相关时才启用"""
    results = []
    query_lower = query.lower()
    
    # 分析查询意图，判断是否与中芯国际相关
    smic_keywords = ['中芯国际', '中芯', '晶圆', '芯片', '半导体', '集成电路', 'IC', '代工']
    query_has_smic_context = any(keyword in query_lower for keyword in smic_keywords)
    
    # 如果查询与中芯国际无关，不启用模糊搜索
    if not query_has_smic_context:
        self.logger.debug(f"查询 '{query}' 与中芯国际无关，跳过模糊搜索")
        return results
    
    # 提取查询中的关键概念（只针对中芯国际相关内容）
    key_concepts = ['中芯国际', '晶圆', '芯片', '半导体', '技术', '业务', '市场', '发展', '营收', '利润']
    
    for doc_id, doc in self.text_docs.items():
        content = doc.page_content if hasattr(doc, 'page_content') else ''
        content_lower = content.lower()
        
        # 检查是否包含关键概念
        concept_matches = sum(1 for concept in key_concepts if concept in content_lower)
        if concept_matches > 0:
            # 计算相关性分数，要求至少2个概念匹配
            if concept_matches >= 2:
                score = min(concept_matches * 0.15, 0.8)  # 降低分数，最高0.8
                results.append({
                    'doc_id': doc_id,
                    'doc': doc,
                    'score': score,
                    'match_type': 'smart_fuzzy_search'
                })
    
    return results
```

**智能特点**：
- **领域相关性判断**：只在查询与中芯国际相关时启用
- **概念匹配计数**：统计文档中包含的关键概念数量
- **最低匹配要求**：要求至少2个概念匹配才返回结果
- **分数上限控制**：最高分数限制在0.8，避免过度匹配

### **8. �� 结果排序和优化**

#### **8.1 智能排序**

```python
def _rank_text_results(self, results: List[Any], query: str) -> List[Any]:
    """对文本结果进行智能排序"""
    if not results:
        return []
    
    # 按分数排序
    sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)
    
    # 限制结果数量
    return sorted_results[:self.config.max_results]
```

**排序逻辑**：
- 按相关性分数降序排列
- 限制返回结果数量（默认10个）
- 确保最相关的结果排在前面

#### **8.2 结果去重**

```python
# 去重和排序
seen_ids = set()
unique_results = []
for result in results:
    doc_id = result.get('doc_id', 'unknown')
    if doc_id not in seen_ids:
        seen_ids.add(doc_id)
        unique_results.append(result)
```

**去重机制**：
- 基于文档ID去重
- 使用集合（set）提高查找效率
- 保留第一个出现的重复结果

### **9. ⚙️ 配置管理**

#### **9.1 配置验证**

```python
def _validate_config(self):
    """验证文本引擎配置 - 支持两种配置类型"""
            # 配置类型检查
        from ..config.v2_config import TextEngineConfigV2
        
        if not isinstance(self.config, TextEngineConfigV2):
            raise ValueError("配置必须是TextEngineConfigV2类型")
    
    # 获取相似度阈值，支持两种配置类型
    threshold = getattr(self.config, 'text_similarity_threshold', 0.7)
    if threshold < 0 or threshold > 1:
        raise ValueError("文本相似度阈值必须在0-1之间")
```

**配置特点**：
- 支持新旧两种配置类型
- 验证相似度阈值的有效性
- 提供默认值支持

### **10. �� 性能监控和统计**

#### **10.1 文档统计**

```python
def get_text_statistics(self) -> Dict[str, Any]:
    """获取文本统计信息"""
    return {
        'total_texts': len(self.text_docs),
        'total_chars': sum(len(doc.page_content) if hasattr(doc, 'page_content') else 0 
                         for doc in self.text_docs.values())
    }
```

**统计信息**：
- 文本文档总数
- 总字符数
- 便于监控系统状态和性能

#### **10.2 缓存管理**

```python
def refresh_text_cache(self):
    """刷新文本缓存"""
    self._load_text_documents()
    self.logger.info("文本缓存已刷新")

def get_text_by_id(self, text_id: str) -> Optional[Any]:
    """根据ID获取文本"""
    return self.text_docs.get(text_id)

def get_all_texts(self) -> List[Any]:
    """获取所有文本"""
    return list(self.text_docs.values())
```

**缓存功能**：
- 支持手动刷新缓存
- 提供按ID查询文档的接口
- 支持获取所有文档列表

## �� **技术实现总结**

文本召回系统通过以下技术特点实现了高性能、高准确性的搜索：

1. **5层策略降级**：确保系统稳定性和结果可用性
2. **内存优先优化**：减少I/O操作，提升响应速度
3. **智能评分算法**：严格和宽松两种模式，适应不同场景
4. **领域相关搜索**：针对中芯国际业务场景的智能模糊搜索
5. **向后兼容性**：支持新旧配置和传统加载方式
6. **性能监控**：详细的日志记录和统计信息
7. **错误处理**：重试机制和降级策略，提高系统稳定性

这个设计充分体现了**智能化**、**性能优化**和**工程化**的特点，为RAG系统提供了可靠的文本召回能力。



我来详细解释一下 `text_engine.py` 中 `_search_texts` 方法定义的5种匹配策略。让我先仔细阅读这个方法，然后用自然语言详细描述每种策略。

## 🔍 文本搜索5种匹配策略详解

### 📋 策略概览

`_search_texts` 方法采用了**分层递进**的搜索策略，从严格匹配逐步放宽到模糊匹配，确保既能找到高相关性结果，又不会遗漏潜在相关内容。

---

### 🎯 **策略1：文本文档严格搜索**
**目标**：在已加载的文本文档中寻找高相关性内容

**执行过程**：
1. 遍历所有已缓存的文本文档（`self.text_docs`）
2. 对每个文档计算**严格评分**（使用 `_calculate_text_score` 方法）
3. 只保留分数达到配置阈值的结果
4. 记录匹配类型为 `'text_doc_search'`

**特点**：
- 使用**严格的相关性判断**
- 要求关键词匹配至少30%才给分
- 语义相似度过低会大幅降低分数
- 这是**最精确**的搜索策略

---

### 🔄 **策略2：内存宽松搜索**
**触发条件**：策略1没有找到结果时启用

**执行过程**：
1. 在同样的文本文档中重新搜索
2. 使用**宽松评分算法**（`_calculate_text_score_relaxed`）
3. 降低阈值到原始阈值的50%
4. 记录匹配类型为 `'memory_relaxed_search'`

**特点**：
- 关键词匹配要求降低到10%
- 相关性惩罚机制更宽松
- 文档类型和长度奖励更高
- 这是**适度放宽**的搜索策略

---

### �� **策略3：关键词搜索**
**触发条件**：策略1和2都没有结果时启用

**执行过程**：
1. 提取查询中的关键词（过滤停用词）
2. 在文本文档中寻找包含这些关键词的内容
3. 计算关键词匹配分数
4. 记录匹配类型为 `'keyword_search'`

**特点**：
- 基于**词汇匹配**而非语义理解
- 适合查找包含特定术语的文档
- 不考虑上下文和语义相关性
- 这是**词汇层面**的搜索策略

---

### ��️ **策略4：智能模糊搜索**
**触发条件**：前三种策略都没有结果时启用

**执行过程**：
1. 分析查询是否与**中芯国际**相关
2. 如果相关，提取关键概念（如：晶圆、芯片、半导体等）
3. 在文档中寻找包含这些概念的文本
4. 要求至少2个概念匹配才给分
5. 记录匹配类型为 `'smart_fuzzy_search'`

**特点**：
- 只在**特定业务领域**启用
- 基于**概念匹配**而非精确词汇
- 分数上限为0.8（避免过度匹配）
- 这是**业务导向**的模糊搜索

---

### �� **策略5：内存低阈值搜索**
**触发条件**：所有策略都没有结果，且配置阈值大于0.05时启用

**执行过程**：
1. 临时将相似度阈值降低到0.05
2. 在内存中文档中重新搜索
3. 使用标准评分算法
4. 搜索完成后恢复原始阈值
5. 记录匹配类型为 `'memory_low_threshold_search'`

**特点**：
- 这是**最后的兜底策略**
- 阈值降低到极低水平（0.05）
- 确保即使相关性很低也能找到结果
- 搜索完成后会恢复原始配置

---

## �� **策略执行流程**

```
开始搜索
    ↓
策略1：严格搜索 → 有结果？ → 是 → 返回结果
    ↓ 否
策略2：宽松搜索 → 有结果？ → 是 → 返回结果
    ↓ 否
策略3：关键词搜索 → 有结果？ → 是 → 返回结果
    ↓ 否
策略4：智能模糊搜索 → 有结果？ → 是 → 返回结果
    ↓ 否
策略5：低阈值搜索 → 有结果？ → 是 → 返回结果
    ↓ 否
返回空结果
```

## 💡 **设计理念**

这种分层策略体现了**"精确优先，逐步放宽"**的设计思想：

1. **优先保证质量**：前两种策略确保结果的高相关性
2. **避免遗漏内容**：后续策略确保能找到潜在相关内容
3. **智能业务理解**：策略4针对特定业务场景优化
4. **兜底保障**：策略5确保即使低相关性也能返回结果

## �� **实际应用场景**

- **策略1-2**：适合精确查询，如"中芯国际2023年营收"
- **策略3**：适合术语查询，如"晶圆代工"
- **策略4**：适合业务相关查询，如"半导体技术发展"
- **策略5**：确保任何查询都能返回结果，避免空结果

这样的设计既保证了搜索质量，又确保了搜索的覆盖面和用户体验！

好的！我来详细解释每一步的算分机制。让我仔细分析代码中的评分算法，用具体的例子来说明。

## �� 文本搜索评分算法详解

### �� **策略1：严格评分算法** (`_calculate_text_score`)

```python
def _calculate_text_score(self, doc: Any, query: str) -> float:
    score = 0.0
    
    # 1. 语义相似度分数（权重最高）
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight  # 默认0.3
    
    # 2. 关键词匹配分数（严格匹配）
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        if keyword_score > 0.3:  # 至少30%的关键词匹配
            score += keyword_score * self.config.keyword_weight  # 默认0.5
        else:
            score *= 0.3  # 关键词匹配不足，大幅降低分数
    
    # 3. 向量相似度分数
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.3
        score += vector_score * self.config.vector_weight  # 默认0.2
    
    # 4. 文档类型匹配奖励
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.05
    
    # 5. 内容长度奖励
    if len(content) > 100:
        score += 0.02
    
    # 6. 相关性惩罚机制
    if semantic_score < 0.1:  # 语义相似度过低
        score *= 0.5  # 大幅降低分数
    
    return min(score, 1.0)
```

**具体算分示例**：
假设查询："中芯国际2023年营收情况"

- **语义相似度**：0.6 × 0.3 = 0.18
- **关键词匹配**：0.8 × 0.5 = 0.4（假设80%关键词匹配）
- **向量相似度**：0.3 × 0.2 = 0.06
- **文档类型奖励**：+0.05
- **内容长度奖励**：+0.02
- **最终分数**：0.18 + 0.4 + 0.06 + 0.05 + 0.02 = 0.71

---

### �� **策略2：宽松评分算法** (`_calculate_text_score_relaxed`)

```python
def _calculate_text_score_relaxed(self, doc: Any, query: str) -> float:
    score = 0.0
    
    # 1. 语义相似度分数（宽松处理）
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight  # 0.3
    
    # 2. 关键词匹配分数（宽松匹配）
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        if keyword_score > 0.1:  # 降低到10%的关键词匹配
            score += keyword_score * self.config.keyword_weight  # 0.5
        else:
            score *= 0.7  # 轻微降低分数（从0.3提升到0.7）
    
    # 3. 向量相似度分数（提高默认分数）
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.4  # 从0.3提升到0.4
        score += vector_score * self.config.vector_weight  # 0.2
    
    # 4. 文档类型匹配奖励（提高奖励）
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.1  # 从0.05提升到0.1
    
    # 5. 内容长度奖励（提高奖励）
    if len(content) > 100:
        score += 0.05  # 从0.02提升到0.05
    
    # 6. 相关性惩罚机制（宽松处理）
    if semantic_score < 0.05:  # 从0.1降低到0.05
        score *= 0.7  # 从0.5提升到0.7
    
    return min(score, 1.0)
```

**具体算分示例**：
同样的查询："中芯国际2023年营收情况"

- **语义相似度**：0.6 × 0.3 = 0.18
- **关键词匹配**：0.8 × 0.5 = 0.4
- **向量相似度**：0.4 × 0.2 = 0.08
- **文档类型奖励**：+0.1
- **内容长度奖励**：+0.05
- **最终分数**：0.18 + 0.4 + 0.08 + 0.1 + 0.05 = 0.81

**宽松策略的优势**：分数更高，更容易达到阈值！

---

### �� **策略3：关键词匹配评分** (`_calculate_keyword_score`)

```python
def _calculate_keyword_score(self, doc: Any, keywords: List[str]) -> float:
    if not keywords:
        return 0.0
    
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    total_score = 0.0
    for keyword in keywords:
        if keyword in content:  # 简单包含判断
            total_score += 1.0
    
    return min(total_score / len(keywords), 1.0)
```

**具体算分示例**：
查询："中芯国际2023年营收情况"
提取关键词：["中芯国际", "2023", "营收", "情况"]

假设文档包含：["中芯国际", "营收", "情况"]（3个关键词匹配）

- **关键词分数**：3 ÷ 4 = 0.75

---

### �� **策略4：智能模糊搜索评分**

```python
def _smart_fuzzy_search(self, query: str) -> List[Any]:
    # 分析查询意图，判断是否与中芯国际相关
    smic_keywords = ['中芯国际', '中芯', '晶圆', '芯片', '半导体', '集成电路', 'IC', '代工']
    query_has_smic_context = any(keyword in query_lower for keyword in smic_keywords)
    
    if not query_has_smic_context:
        return []  # 不相关就跳过
    
    # 提取关键概念
    key_concepts = ['中芯国际', '晶圆', '芯片', '半导体', '技术', '业务', '市场', '发展', '营收', '利润']
    
    for doc_id, doc in self.text_docs.items():
        content_lower = content.lower()
        
        # 检查是否包含关键概念
        concept_matches = sum(1 for concept in key_concepts if concept in content_lower)
        if concept_matches > 0:
            if concept_matches >= 2:  # 至少2个概念匹配
                score = min(concept_matches * 0.15, 0.8)  # 最高0.8分
                results.append({
                    'doc_id': doc_id,
                    'doc': doc,
                    'score': score,
                    'match_type': 'smart_fuzzy_search'
                })
```

**具体算分示例**：
查询："半导体技术发展"

- **概念匹配检查**：
  - 文档包含：["半导体", "技术", "发展"]（3个概念）
  - **分数计算**：3 × 0.15 = 0.45（但最高0.8分）

---

### 📉 **策略5：低阈值搜索评分**

```python
# 临时降低阈值
original_threshold = self.config.text_similarity_threshold
self.config.text_similarity_threshold = 0.05  # 降低到0.05

try:
    # 使用标准评分算法，但阈值极低
    for doc_id, doc in self.text_docs.items():
        score = self._calculate_text_score(doc, query)
        if score >= 0.05:  # 极低阈值
            results.append({
                'doc_id': doc_id,
                'doc': doc,
                'score': score,
                'match_type': 'memory_low_threshold_search'
            })
finally:
    # 恢复原始阈值
    self.config.text_similarity_threshold = original_threshold
```

**具体算分示例**：
假设原始阈值是0.7，现在临时降低到0.05

- 使用**策略1的严格评分算法**
- 但接受分数≥0.05的结果
- 这样即使相关性很低也能找到结果

---

## 🎯 **评分权重配置**

```python
# 默认权重配置
self.config.semantic_weight = 0.3      # 语义相似度权重30%
self.config.keyword_weight = 0.5       # 关键词匹配权重50%
self.config.vector_weight = 0.2        # 向量相似度权重20%
```

## 🔍 **语义相似度计算** (`_calculate_text_similarity`)

```python
def _calculate_text_similarity(self, query: str, text: str) -> float:
    query_words = set(query.lower().split())
    text_words = set(text.lower().split())
    
    intersection = query_words.intersection(text_words)  # 交集
    union = query_words.union(text_words)               # 并集
    
    if union:
        return len(intersection) / len(union)  # Jaccard相似度
    return 0.0
```

**具体算分示例**：
查询："中芯国际营收"
文档："中芯国际2023年营收情况"

- **查询词汇**：{"中芯国际", "营收"}
- **文档词汇**：{"中芯国际", "2023", "年", "营收", "情况"}
- **交集**：{"中芯国际", "营收"}（2个）
- **并集**：{"中芯国际", "2023", "年", "营收", "情况"}（5个）
- **相似度**：2 ÷ 5 = 0.4

## �� **关键理解点**

1. **权重分配**：关键词匹配占50%，语义相似度占30%，向量相似度占20%
2. **惩罚机制**：关键词匹配不足会大幅降低分数
3. **奖励机制**：文档类型、内容长度等给予小额奖励
4. **阈值控制**：不同策略使用不同阈值，确保结果质量
5. **兜底保障**：策略5确保即使低相关性也能返回结果

这样的评分机制既保证了搜索的精确性，又确保了搜索的覆盖面和用户体验！