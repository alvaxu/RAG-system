

## ï¿½ï¿½ **æ–‡æœ¬å¬å›æŠ€æœ¯å®ç°è¯¦ç»†è¯´æ˜**

### **1. ğŸ—ï¸ æ•´ä½“æ¶æ„è®¾è®¡**

æ–‡æœ¬å¬å›ç³»ç»Ÿé‡‡ç”¨**5å±‚ç­–ç•¥é™çº§**çš„æ¶æ„è®¾è®¡ï¼Œç¡®ä¿å³ä½¿é«˜ä¼˜å…ˆçº§ç­–ç•¥å¤±è´¥ï¼Œä»æœ‰ç»“æœè¿”å›ã€‚æ•´ä¸ªç³»ç»ŸåŸºäº `TextEngine` ç±»å®ç°ï¼Œç»§æ‰¿è‡ª `BaseEngine`ã€‚

### **2. ï¿½ï¿½ æ–‡æ¡£åŠ è½½ç®¡ç†**

#### **2.1 ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨é›†æˆ**

```python
def _load_from_document_loader(self):
    """ä»ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨è·å–æ–‡æœ¬æ–‡æ¡£"""
    if self.document_loader:
        try:
            self.text_docs = self.document_loader.get_documents_by_type('text')
            self._docs_loaded = True
            self.logger.info(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–æ–‡æœ¬æ–‡æ¡£: {len(self.text_docs)} ä¸ª")
        except Exception as e:
            self.logger.error(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–æ–‡æœ¬æ–‡æ¡£å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»ŸåŠ è½½æ–¹å¼
            self._load_text_documents()
    else:
        self.logger.warning("æ–‡æ¡£åŠ è½½å™¨æœªæä¾›ï¼Œä½¿ç”¨ä¼ ç»ŸåŠ è½½æ–¹å¼")
        self._load_text_documents()
```

**åŠŸèƒ½è¯´æ˜**ï¼š
- ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨ï¼Œé¿å…é‡å¤åŠ è½½
- æ”¯æŒé™çº§åˆ°ä¼ ç»ŸåŠ è½½æ–¹å¼ï¼Œä¿è¯å‘åå…¼å®¹æ€§
- æä¾›å»¶è¿ŸåŠ è½½æœºåˆ¶ï¼ŒæŒ‰éœ€åŠ è½½æ–‡æ¡£

#### **2.2 ä¼ ç»Ÿæ–‡æ¡£åŠ è½½ï¼ˆé™çº§ç­–ç•¥ï¼‰**

```python
def _load_text_documents(self):
    """åŠ è½½æ–‡æœ¬æ–‡æ¡£åˆ°ç¼“å­˜"""
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # æ¸…ç©ºä¹‹å‰çš„ç¼“å­˜
            self.text_docs = {}
            
            # ä»å‘é‡æ•°æ®åº“åŠ è½½æ‰€æœ‰æ–‡æœ¬æ–‡æ¡£
            for doc_id, doc in self.vector_store.docstore._dict.items():
                chunk_type = doc.metadata.get('chunk_type', '')
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬æ–‡æ¡£ - ç®€åŒ–åˆ¤æ–­é€»è¾‘
                is_text = chunk_type == 'text'
                
                if is_text:
                    self.text_docs[doc_id] = doc
                    self.logger.debug(f"åŠ è½½æ–‡æœ¬æ–‡æ¡£: {doc_id}, chunk_type: {chunk_type}")
            
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(self.text_docs)} ä¸ªæ–‡æœ¬æ–‡æ¡£")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬æ–‡æ¡£ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if not self.text_docs:
                self.logger.warning("æœªæ‰¾åˆ°æ–‡æœ¬æ–‡æ¡£ï¼Œå°è¯•æœç´¢æ‰€æœ‰æ–‡æ¡£...")
                self._search_all_documents_for_texts()
            
            # å¦‚æœæˆåŠŸåŠ è½½äº†æ–‡æ¡£ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
            if len(self.text_docs) > 0:
                self.logger.info(f"æ–‡æœ¬æ–‡æ¡£åŠ è½½æˆåŠŸï¼Œå…± {len(self.text_docs)} ä¸ªæ–‡æ¡£")
                return
            else:
                raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡æ¡£")
                
        except Exception as e:
            retry_count += 1
            self.logger.warning(f"æ–‡æœ¬æ–‡æ¡£åŠ è½½å¤±è´¥ï¼Œç¬¬{retry_count}æ¬¡å°è¯•: {e}")
            
            if retry_count >= max_retries:
                # æœ€ç»ˆå¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶æ¸…ç©ºç¼“å­˜
                self.logger.error(f"æ–‡æœ¬æ–‡æ¡£åŠ è½½æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
                self.text_docs = {}
                return
            else:
                # ç­‰å¾…åé‡è¯•
                import time
                time.sleep(1)
                self.logger.info(f"ç­‰å¾…1ç§’åè¿›è¡Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
```

**åŠŸèƒ½è¯´æ˜**ï¼š
- æ”¯æŒé‡è¯•æœºåˆ¶ï¼Œæœ€å¤šé‡è¯•3æ¬¡
- ä¸¥æ ¼çš„ç±»å‹è¿‡æ»¤ï¼ŒåªåŠ è½½ `chunk_type == 'text'` çš„æ–‡æ¡£
- æä¾›å¤‡é€‰æœç´¢ç­–ç•¥ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°æ–‡æœ¬æ–‡æ¡£
- è¯¦ç»†çš„æ—¥å¿—è®°å½•ï¼Œä¾¿äºè°ƒè¯•å’Œç›‘æ§

### **3. ğŸ” æ ¸å¿ƒæœç´¢ç­–ç•¥å®ç°**

#### **3.1 ä¸»æœç´¢å‡½æ•°**

```python
def _search_texts(self, query: str, **kwargs) -> List[Any]:
    """æ™ºèƒ½æ–‡æœ¬æœç´¢ - V2.0å¢å¼ºç‰ˆï¼ˆä¸¥æ ¼ç±»å‹è¿‡æ»¤ï¼‰"""
    results = []
    
    # ç­–ç•¥1: ä¼˜å…ˆä½¿ç”¨å·²åŠ è½½çš„æ–‡æœ¬æ–‡æ¡£è¿›è¡Œæœç´¢
    if self.text_docs:
        try:
            # å¯¹æ–‡æœ¬æ–‡æ¡£è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
            for doc_id, doc in self.text_docs.items():
                score = self._calculate_text_score(doc, query)
                if score >= self.config.text_similarity_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'text_doc_search'
                    })
            
            self.logger.debug(f"æ–‡æœ¬æ–‡æ¡£æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        except Exception as e:
            self.logger.warning(f"æ–‡æœ¬æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
    
    # ç­–ç•¥2: å¦‚æœæ–‡æœ¬æ–‡æ¡£æœç´¢æ²¡æœ‰ç»“æœï¼Œåœ¨å†…å­˜ä¸­ä½¿ç”¨å®½æ¾æœç´¢ç­–ç•¥
    if not results and self.text_docs:
        try:
            self.logger.debug("ç­–ç•¥1æ— ç»“æœï¼Œå¯ç”¨ç­–ç•¥2ï¼šå†…å­˜å®½æ¾æœç´¢")
            
            # åœ¨å†…å­˜ä¸­æ–‡æ¡£ä¸­ä½¿ç”¨å®½æ¾æœç´¢ç­–ç•¥
            for doc_id, doc in self.text_docs.items():
                # ä½¿ç”¨å®½æ¾çš„è¯„åˆ†ç®—æ³•
                score = self._calculate_text_score_relaxed(doc, query)
                
                # é™ä½é˜ˆå€¼ï¼Œä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…æ¡ä»¶
                relaxed_threshold = self.config.text_similarity_threshold * 0.5
                if score >= relaxed_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'memory_relaxed_search'
                    })
            
            self.logger.debug(f"å†…å­˜å®½æ¾æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        except Exception as e:
            self.logger.warning(f"å†…å­˜å®½æ¾æœç´¢å¤±è´¥: {e}")
    
    # ç­–ç•¥3: å¦‚æœä»ç„¶æ²¡æœ‰ç»“æœï¼Œå°è¯•å…³é”®è¯æœç´¢
    if not results:
        keyword_results = self._keyword_search(query)
        results.extend(keyword_results)
        self.logger.debug(f"å…³é”®è¯æœç´¢æ‰¾åˆ° {len(keyword_results)} ä¸ªç»“æœ")
    
    # ç­–ç•¥4: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆä½†åªé’ˆå¯¹æ–‡æœ¬æ–‡æ¡£ï¼‰
    if not results:
        fuzzy_results = self._fuzzy_search(query)
        results.extend(fuzzy_results)
        self.logger.debug(f"æ¨¡ç³Šæœç´¢æ‰¾åˆ° {len(fuzzy_results)} ä¸ªç»“æœ")
    
    # ç­–ç•¥5: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œåœ¨å†…å­˜ä¸­é™ä½é˜ˆå€¼é‡æ–°æœç´¢
    if not results and self.config.text_similarity_threshold > 0.05 and self.text_docs:
        self.logger.debug("ç­–ç•¥5ï¼šåœ¨å†…å­˜ä¸­é™ä½é˜ˆå€¼é‡æ–°æœç´¢...")
        original_threshold = self.config.text_similarity_threshold
        self.config.text_similarity_threshold = 0.05
        
        # åœ¨å†…å­˜ä¸­æ–‡æ¡£ä¸­é‡æ–°æœç´¢
        try:
            for doc_id, doc in self.text_docs.items():
                score = self._calculate_text_score(doc, query)
                if score >= self.config.text_similarity_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'memory_low_threshold_search'
                    })
            
            self.logger.debug(f"å†…å­˜ä½é˜ˆå€¼æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        except Exception as e:
            self.logger.warning(f"å†…å­˜ä½é˜ˆå€¼æœç´¢å¤±è´¥: {e}")
        
        # æ¢å¤åŸå§‹é˜ˆå€¼
        self.config.text_similarity_threshold = original_threshold
    
    # å»é‡å’Œæ’åº
    seen_ids = set()
    unique_results = []
    for result in results:
        doc_id = result.get('doc_id', 'unknown')
        if doc_id not in seen_ids:
            seen_ids.add(doc_id)
            unique_results.append(result)
    
    # æŒ‰åˆ†æ•°æ’åº
    unique_results.sort(key=lambda x: x['score'], reverse=True)
    
    self.logger.debug(f"æœ€ç»ˆå»é‡åå¾—åˆ° {len(unique_results)} ä¸ªç»“æœ")
    return unique_results
```

**åŠŸèƒ½è¯´æ˜**ï¼š
- **5å±‚ç­–ç•¥é™çº§**ï¼šç¡®ä¿å³ä½¿é«˜ä¼˜å…ˆçº§ç­–ç•¥å¤±è´¥ï¼Œä»æœ‰ç»“æœè¿”å›
- **æ™ºèƒ½ç­–ç•¥åˆ‡æ¢**ï¼šæ ¹æ®ç»“æœæ•°é‡è‡ªåŠ¨å¯ç”¨ä¸‹ä¸€å±‚ç­–ç•¥
- **å†…å­˜ä¼˜å…ˆä¼˜åŒ–**ï¼šç­–ç•¥2å’Œç­–ç•¥5éƒ½åœ¨å†…å­˜ä¸­æ‰§è¡Œï¼Œé¿å…é‡å¤I/Oæ“ä½œ
- **é˜ˆå€¼åŠ¨æ€è°ƒæ•´**ï¼šç­–ç•¥5ä¸´æ—¶é™ä½é˜ˆå€¼åˆ°0.05ï¼Œç¡®ä¿æœ‰ç»“æœè¿”å›
- **ç»“æœå»é‡æ’åº**ï¼šé¿å…é‡å¤å†…å®¹ï¼ŒæŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº

### **4. ï¿½ï¿½ è¯„åˆ†ç®—æ³•è¯¦è§£**

#### **4.1 ä¸¥æ ¼è¯„åˆ†ç®—æ³•ï¼ˆç­–ç•¥1ï¼‰**

```python
def _calculate_text_score(self, doc: Any, query: str) -> float:
    """è®¡ç®—æ–‡æœ¬åŒ¹é…åˆ†æ•° - æ™ºèƒ½ç»¼åˆè¯„åˆ†ï¼ˆä¸¥æ ¼ç›¸å…³æ€§åˆ¤æ–­ï¼‰"""
    score = 0.0
    
    # è·å–æ–‡æœ¬å†…å®¹
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight
    
    # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        # å…³é”®è¯åŒ¹é…å¿…é¡»è¾¾åˆ°ä¸€å®šé˜ˆå€¼æ‰ç»™åˆ†
        if keyword_score > 0.3:  # è‡³å°‘30%çš„å…³é”®è¯åŒ¹é…
            score += keyword_score * self.config.keyword_weight
        else:
            # å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œå¤§å¹…é™ä½åˆ†æ•°
            score *= 0.3
    
    # 3. å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¦‚æœæœ‰å‘é‡åµŒå…¥ï¼‰
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.3  # é™ä½é»˜è®¤å‘é‡åˆ†æ•°
        score += vector_score * self.config.vector_weight
    
    # 4. æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±ï¼ˆé™ä½å¥–åŠ±ï¼‰
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.05  # é™ä½æ–‡æœ¬æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±
    
    # 5. å†…å®¹é•¿åº¦å¥–åŠ±ï¼ˆé™ä½å¥–åŠ±ï¼‰
    if len(content) > 100:
        score += 0.02  # é™ä½å†…å®¹é•¿åº¦å¥–åŠ±
    
    # 6. ç›¸å…³æ€§æƒ©ç½šæœºåˆ¶
    if semantic_score < 0.1:  # è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡ä½
        score *= 0.5  # å¤§å¹…é™ä½åˆ†æ•°
    
    return min(score, 1.0)
```

**è¯„åˆ†æƒé‡é…ç½®**ï¼š
- `semantic_weight`: 0.3 (è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡)
- `keyword_weight`: 0.5 (å…³é”®è¯åŒ¹é…æƒé‡)  
- `vector_weight`: 0.2 (å‘é‡ç›¸ä¼¼åº¦æƒé‡)

**ä¸¥æ ¼æ€§ç‰¹ç‚¹**ï¼š
- å…³é”®è¯åŒ¹é…å¿…é¡»è¾¾åˆ°30%æ‰ç»™åˆ†ï¼Œå¦åˆ™å¤§å¹…é™åˆ†
- è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡ä½æ—¶ï¼Œæ€»åˆ†ä¹˜ä»¥0.5æƒ©ç½š
- é™ä½æ–‡æ¡£ç±»å‹å’Œå†…å®¹é•¿åº¦çš„å¥–åŠ±åˆ†æ•°

#### **4.2 å®½æ¾è¯„åˆ†ç®—æ³•ï¼ˆç­–ç•¥2ï¼‰**

```python
def _calculate_text_score_relaxed(self, doc: Any, query: str) -> float:
    """è®¡ç®—æ–‡æœ¬åŒ¹é…åˆ†æ•° - å®½æ¾è¯„åˆ†ç®—æ³•ï¼ˆç”¨äºç­–ç•¥2ï¼‰"""
    score = 0.0
    
    # è·å–æ–‡æœ¬å†…å®¹
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå®½æ¾å¤„ç†ï¼‰
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight
    
    # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆå®½æ¾åŒ¹é…ï¼‰
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        # å®½æ¾å¤„ç†ï¼šå³ä½¿å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œä¹Ÿç»™äºˆä¸€å®šåˆ†æ•°
        if keyword_score > 0.1:  # é™ä½åˆ°10%çš„å…³é”®è¯åŒ¹é…
            score += keyword_score * self.config.keyword_weight
        else:
            # å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œè½»å¾®é™ä½åˆ†æ•°
            score *= 0.7  # ä»0.3æå‡åˆ°0.7
    
    # 3. å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¦‚æœæœ‰å‘é‡åµŒå…¥ï¼‰
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.4  # æé«˜é»˜è®¤å‘é‡åˆ†æ•°
        score += vector_score * self.config.vector_weight
    
    # 4. æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±ï¼ˆæé«˜å¥–åŠ±ï¼‰
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.1  # æé«˜æ–‡æœ¬æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±
    
    # 5. å†…å®¹é•¿åº¦å¥–åŠ±ï¼ˆæé«˜å¥–åŠ±ï¼‰
    if len(content) > 100:
        score += 0.05  # æé«˜å†…å®¹é•¿åº¦å¥–åŠ±
    
    # 6. ç›¸å…³æ€§æƒ©ç½šæœºåˆ¶ï¼ˆå®½æ¾å¤„ç†ï¼‰
    if semantic_score < 0.05:  # ä»0.1é™ä½åˆ°0.05
        score *= 0.7  # ä»0.5æå‡åˆ°0.7
    
    return min(score, 1.0)
```

**å®½æ¾æ€§ç‰¹ç‚¹**ï¼š
- å…³é”®è¯åŒ¹é…é˜ˆå€¼é™ä½åˆ°10%
- æƒ©ç½šç³»æ•°ä»0.3æå‡åˆ°0.7ï¼Œæ›´å®½æ¾
- è¯­ä¹‰ç›¸ä¼¼åº¦æƒ©ç½šé˜ˆå€¼ä»0.1é™ä½åˆ°0.05
- æé«˜æ–‡æ¡£ç±»å‹å’Œå†…å®¹é•¿åº¦çš„å¥–åŠ±åˆ†æ•°

### **5. ï¿½ï¿½ å…³é”®è¯å¤„ç†æœºåˆ¶**

#### **5.1 å…³é”®è¯æå–**

```python
def _extract_keywords(self, query: str) -> List[str]:
    """æå–æŸ¥è¯¢å…³é”®è¯"""
    stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'äº†', 'å—', 'å‘¢', 'å•Š'}
    
    import re
    clean_query = re.sub(r'[^\w\s]', '', query)
    
    words = clean_query.split()
    keywords = [word for word in words if word not in stop_words and len(word) > 1]
    
    return keywords
```

**åŠŸèƒ½è¯´æ˜**ï¼š
- å»é™¤åœç”¨è¯ï¼ˆä¸­æ–‡å¸¸è§è™šè¯ï¼‰
- æ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
- è¿‡æ»¤é•¿åº¦å°äº2çš„å•è¯
- è¿”å›æ¸…æ´—åçš„å…³é”®è¯åˆ—è¡¨

#### **5.2 å…³é”®è¯è¯„åˆ†**

```python
def _calculate_keyword_score(self, doc: Any, keywords: List[str]) -> float:
    """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
    if not keywords:
        return 0.0
    
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    total_score = 0.0
    for keyword in keywords:
        if keyword in content:
            total_score += 1.0
    
    return min(total_score / len(keywords), 1.0)
```

**è¯„åˆ†é€»è¾‘**ï¼š
- ç»Ÿè®¡æ–‡æ¡£å†…å®¹ä¸­åŒ…å«çš„å…³é”®è¯æ•°é‡
- åˆ†æ•° = åŒ¹é…å…³é”®è¯æ•° / æ€»å…³é”®è¯æ•°
- æœ€é«˜åˆ†æ•°ä¸º1.0

### **6. ğŸŒ è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—**

```python
def _calculate_text_similarity(self, query: str, text: str) -> float:
    """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
    if not text or not query:
        return 0.0
    
    # ç®€å•çš„è¯æ±‡é‡å è®¡ç®—
    query_words = set(query.lower().split())
    text_words = set(text.lower().split())
    
    if not query_words or not text_words:
        return 0.0
    
    intersection = query_words.intersection(text_words)
    union = query_words.union(text_words)
    
    if union:
        return len(intersection) / len(union)
    return 0.0
```

**è®¡ç®—åŸç†**ï¼š
- ä½¿ç”¨Jaccardç›¸ä¼¼åº¦ï¼šäº¤é›†å¤§å° / å¹¶é›†å¤§å°
- å°†æŸ¥è¯¢å’Œæ–‡æ¡£è½¬æ¢ä¸ºè¯æ±‡é›†åˆ
- è®¡ç®—è¯æ±‡é‡å ç¨‹åº¦
- è¿”å›0-1ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°

### **7. ğŸ” æ™ºèƒ½æ¨¡ç³Šæœç´¢**

```python
def _fuzzy_search(self, query: str) -> List[Any]:
    """æ™ºèƒ½æ¨¡ç³Šæœç´¢ - åªåœ¨çœŸæ­£ç›¸å…³æ—¶æ‰å¯ç”¨"""
    results = []
    query_lower = query.lower()
    
    # åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ä¸­èŠ¯å›½é™…ç›¸å…³
    smic_keywords = ['ä¸­èŠ¯å›½é™…', 'ä¸­èŠ¯', 'æ™¶åœ†', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'IC', 'ä»£å·¥']
    query_has_smic_context = any(keyword in query_lower for keyword in smic_keywords)
    
    # å¦‚æœæŸ¥è¯¢ä¸ä¸­èŠ¯å›½é™…æ— å…³ï¼Œä¸å¯ç”¨æ¨¡ç³Šæœç´¢
    if not query_has_smic_context:
        self.logger.debug(f"æŸ¥è¯¢ '{query}' ä¸ä¸­èŠ¯å›½é™…æ— å…³ï¼Œè·³è¿‡æ¨¡ç³Šæœç´¢")
        return results
    
    # æå–æŸ¥è¯¢ä¸­çš„å…³é”®æ¦‚å¿µï¼ˆåªé’ˆå¯¹ä¸­èŠ¯å›½é™…ç›¸å…³å†…å®¹ï¼‰
    key_concepts = ['ä¸­èŠ¯å›½é™…', 'æ™¶åœ†', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'æŠ€æœ¯', 'ä¸šåŠ¡', 'å¸‚åœº', 'å‘å±•', 'è¥æ”¶', 'åˆ©æ¶¦']
    
    for doc_id, doc in self.text_docs.items():
        content = doc.page_content if hasattr(doc, 'page_content') else ''
        content_lower = content.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®æ¦‚å¿µ
        concept_matches = sum(1 for concept in key_concepts if concept in content_lower)
        if concept_matches > 0:
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼Œè¦æ±‚è‡³å°‘2ä¸ªæ¦‚å¿µåŒ¹é…
            if concept_matches >= 2:
                score = min(concept_matches * 0.15, 0.8)  # é™ä½åˆ†æ•°ï¼Œæœ€é«˜0.8
                results.append({
                    'doc_id': doc_id,
                    'doc': doc,
                    'score': score,
                    'match_type': 'smart_fuzzy_search'
                })
    
    return results
```

**æ™ºèƒ½ç‰¹ç‚¹**ï¼š
- **é¢†åŸŸç›¸å…³æ€§åˆ¤æ–­**ï¼šåªåœ¨æŸ¥è¯¢ä¸ä¸­èŠ¯å›½é™…ç›¸å…³æ—¶å¯ç”¨
- **æ¦‚å¿µåŒ¹é…è®¡æ•°**ï¼šç»Ÿè®¡æ–‡æ¡£ä¸­åŒ…å«çš„å…³é”®æ¦‚å¿µæ•°é‡
- **æœ€ä½åŒ¹é…è¦æ±‚**ï¼šè¦æ±‚è‡³å°‘2ä¸ªæ¦‚å¿µåŒ¹é…æ‰è¿”å›ç»“æœ
- **åˆ†æ•°ä¸Šé™æ§åˆ¶**ï¼šæœ€é«˜åˆ†æ•°é™åˆ¶åœ¨0.8ï¼Œé¿å…è¿‡åº¦åŒ¹é…

### **8. ï¿½ï¿½ ç»“æœæ’åºå’Œä¼˜åŒ–**

#### **8.1 æ™ºèƒ½æ’åº**

```python
def _rank_text_results(self, results: List[Any], query: str) -> List[Any]:
    """å¯¹æ–‡æœ¬ç»“æœè¿›è¡Œæ™ºèƒ½æ’åº"""
    if not results:
        return []
    
    # æŒ‰åˆ†æ•°æ’åº
    sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)
    
    # é™åˆ¶ç»“æœæ•°é‡
    return sorted_results[:self.config.max_results]
```

**æ’åºé€»è¾‘**ï¼š
- æŒ‰ç›¸å…³æ€§åˆ†æ•°é™åºæ’åˆ—
- é™åˆ¶è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤10ä¸ªï¼‰
- ç¡®ä¿æœ€ç›¸å…³çš„ç»“æœæ’åœ¨å‰é¢

#### **8.2 ç»“æœå»é‡**

```python
# å»é‡å’Œæ’åº
seen_ids = set()
unique_results = []
for result in results:
    doc_id = result.get('doc_id', 'unknown')
    if doc_id not in seen_ids:
        seen_ids.add(doc_id)
        unique_results.append(result)
```

**å»é‡æœºåˆ¶**ï¼š
- åŸºäºæ–‡æ¡£IDå»é‡
- ä½¿ç”¨é›†åˆï¼ˆsetï¼‰æé«˜æŸ¥æ‰¾æ•ˆç‡
- ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„é‡å¤ç»“æœ

### **9. âš™ï¸ é…ç½®ç®¡ç†**

#### **9.1 é…ç½®éªŒè¯**

```python
def _validate_config(self):
    """éªŒè¯æ–‡æœ¬å¼•æ“é…ç½® - æ”¯æŒä¸¤ç§é…ç½®ç±»å‹"""
    # æ”¯æŒä¸¤ç§é…ç½®ç±»å‹ï¼šTextEngineConfig å’Œ TextEngineConfigV2
    from ..config.v2_config import TextEngineConfigV2
    
    if not isinstance(self.config, (TextEngineConfig, TextEngineConfigV2)):
        raise ValueError("é…ç½®å¿…é¡»æ˜¯TextEngineConfigæˆ–TextEngineConfigV2ç±»å‹")
    
    # è·å–ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œæ”¯æŒä¸¤ç§é…ç½®ç±»å‹
    threshold = getattr(self.config, 'text_similarity_threshold', 0.7)
    if threshold < 0 or threshold > 1:
        raise ValueError("æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
```

**é…ç½®ç‰¹ç‚¹**ï¼š
- æ”¯æŒæ–°æ—§ä¸¤ç§é…ç½®ç±»å‹
- éªŒè¯ç›¸ä¼¼åº¦é˜ˆå€¼çš„æœ‰æ•ˆæ€§
- æä¾›é»˜è®¤å€¼æ”¯æŒ

### **10. ï¿½ï¿½ æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡**

#### **10.1 æ–‡æ¡£ç»Ÿè®¡**

```python
def get_text_statistics(self) -> Dict[str, Any]:
    """è·å–æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯"""
    return {
        'total_texts': len(self.text_docs),
        'total_chars': sum(len(doc.page_content) if hasattr(doc, 'page_content') else 0 
                         for doc in self.text_docs.values())
    }
```

**ç»Ÿè®¡ä¿¡æ¯**ï¼š
- æ–‡æœ¬æ–‡æ¡£æ€»æ•°
- æ€»å­—ç¬¦æ•°
- ä¾¿äºç›‘æ§ç³»ç»ŸçŠ¶æ€å’Œæ€§èƒ½

#### **10.2 ç¼“å­˜ç®¡ç†**

```python
def refresh_text_cache(self):
    """åˆ·æ–°æ–‡æœ¬ç¼“å­˜"""
    self._load_text_documents()
    self.logger.info("æ–‡æœ¬ç¼“å­˜å·²åˆ·æ–°")

def get_text_by_id(self, text_id: str) -> Optional[Any]:
    """æ ¹æ®IDè·å–æ–‡æœ¬"""
    return self.text_docs.get(text_id)

def get_all_texts(self) -> List[Any]:
    """è·å–æ‰€æœ‰æ–‡æœ¬"""
    return list(self.text_docs.values())
```

**ç¼“å­˜åŠŸèƒ½**ï¼š
- æ”¯æŒæ‰‹åŠ¨åˆ·æ–°ç¼“å­˜
- æä¾›æŒ‰IDæŸ¥è¯¢æ–‡æ¡£çš„æ¥å£
- æ”¯æŒè·å–æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨

## ï¿½ï¿½ **æŠ€æœ¯å®ç°æ€»ç»“**

æ–‡æœ¬å¬å›ç³»ç»Ÿé€šè¿‡ä»¥ä¸‹æŠ€æœ¯ç‰¹ç‚¹å®ç°äº†é«˜æ€§èƒ½ã€é«˜å‡†ç¡®æ€§çš„æœç´¢ï¼š

1. **5å±‚ç­–ç•¥é™çº§**ï¼šç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§å’Œç»“æœå¯ç”¨æ€§
2. **å†…å­˜ä¼˜å…ˆä¼˜åŒ–**ï¼šå‡å°‘I/Oæ“ä½œï¼Œæå‡å“åº”é€Ÿåº¦
3. **æ™ºèƒ½è¯„åˆ†ç®—æ³•**ï¼šä¸¥æ ¼å’Œå®½æ¾ä¸¤ç§æ¨¡å¼ï¼Œé€‚åº”ä¸åŒåœºæ™¯
4. **é¢†åŸŸç›¸å…³æœç´¢**ï¼šé’ˆå¯¹ä¸­èŠ¯å›½é™…ä¸šåŠ¡åœºæ™¯çš„æ™ºèƒ½æ¨¡ç³Šæœç´¢
5. **å‘åå…¼å®¹æ€§**ï¼šæ”¯æŒæ–°æ—§é…ç½®å’Œä¼ ç»ŸåŠ è½½æ–¹å¼
6. **æ€§èƒ½ç›‘æ§**ï¼šè¯¦ç»†çš„æ—¥å¿—è®°å½•å’Œç»Ÿè®¡ä¿¡æ¯
7. **é”™è¯¯å¤„ç†**ï¼šé‡è¯•æœºåˆ¶å’Œé™çº§ç­–ç•¥ï¼Œæé«˜ç³»ç»Ÿç¨³å®šæ€§

è¿™ä¸ªè®¾è®¡å……åˆ†ä½“ç°äº†**æ™ºèƒ½åŒ–**ã€**æ€§èƒ½ä¼˜åŒ–**å’Œ**å·¥ç¨‹åŒ–**çš„ç‰¹ç‚¹ï¼Œä¸ºRAGç³»ç»Ÿæä¾›äº†å¯é çš„æ–‡æœ¬å¬å›èƒ½åŠ›ã€‚