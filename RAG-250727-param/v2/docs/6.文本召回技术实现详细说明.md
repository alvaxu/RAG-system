

## �� **文本召回技术实现详细说明**

### **1. 🏗️ 整体架构设计**

文本召回系统采用**5层策略降级**的架构设计，确保即使高优先级策略失败，仍有结果返回。整个系统基于 `TextEngine` 类实现，继承自 `BaseEngine`。

### **2. �� 文档加载管理**

#### **2.1 统一文档加载器集成**

```python
def _load_from_document_loader(self):
    """从统一文档加载器获取文本文档"""
    if self.document_loader:
        try:
            self.text_docs = self.document_loader.get_documents_by_type('text')
            self._docs_loaded = True
            self.logger.info(f"从统一加载器获取文本文档: {len(self.text_docs)} 个")
        except Exception as e:
            self.logger.error(f"从统一加载器获取文本文档失败: {e}")
            # 降级到传统加载方式
            self._load_text_documents()
    else:
        self.logger.warning("文档加载器未提供，使用传统加载方式")
        self._load_text_documents()
```

**功能说明**：
- 优先使用统一文档加载器，避免重复加载
- 支持降级到传统加载方式，保证向后兼容性
- 提供延迟加载机制，按需加载文档

#### **2.2 传统文档加载（降级策略）**

```python
def _load_text_documents(self):
    """加载文本文档到缓存"""
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # 清空之前的缓存
            self.text_docs = {}
            
            # 从向量数据库加载所有文本文档
            for doc_id, doc in self.vector_store.docstore._dict.items():
                chunk_type = doc.metadata.get('chunk_type', '')
                
                # 判断是否为文本文档 - 简化判断逻辑
                is_text = chunk_type == 'text'
                
                if is_text:
                    self.text_docs[doc_id] = doc
                    self.logger.debug(f"加载文本文档: {doc_id}, chunk_type: {chunk_type}")
            
            self.logger.info(f"成功加载 {len(self.text_docs)} 个文本文档")
            
            # 如果没有找到文本文档，尝试其他方法
            if not self.text_docs:
                self.logger.warning("未找到文本文档，尝试搜索所有文档...")
                self._search_all_documents_for_texts()
            
            # 如果成功加载了文档，退出重试循环
            if len(self.text_docs) > 0:
                self.logger.info(f"文本文档加载成功，共 {len(self.text_docs)} 个文档")
                return
            else:
                raise ValueError("未找到任何文本文档")
                
        except Exception as e:
            retry_count += 1
            self.logger.warning(f"文本文档加载失败，第{retry_count}次尝试: {e}")
            
            if retry_count >= max_retries:
                # 最终失败，记录错误并清空缓存
                self.logger.error(f"文本文档加载最终失败，已重试{max_retries}次: {e}")
                self.text_docs = {}
                return
            else:
                # 等待后重试
                import time
                time.sleep(1)
                self.logger.info(f"等待1秒后进行第{retry_count + 1}次重试...")
```

**功能说明**：
- 支持重试机制，最多重试3次
- 严格的类型过滤，只加载 `chunk_type == 'text'` 的文档
- 提供备选搜索策略，确保能找到文本文档
- 详细的日志记录，便于调试和监控

### **3. 🔍 核心搜索策略实现**

#### **3.1 主搜索函数**

```python
def _search_texts(self, query: str, **kwargs) -> List[Any]:
    """智能文本搜索 - V2.0增强版（严格类型过滤）"""
    results = []
    
    # 策略1: 优先使用已加载的文本文档进行搜索
    if self.text_docs:
        try:
            # 对文本文档进行向量相似度搜索
            for doc_id, doc in self.text_docs.items():
                score = self._calculate_text_score(doc, query)
                if score >= self.config.text_similarity_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'text_doc_search'
                    })
            
            self.logger.debug(f"文本文档搜索找到 {len(results)} 个结果")
        except Exception as e:
            self.logger.warning(f"文本文档搜索失败: {e}")
    
    # 策略2: 如果文本文档搜索没有结果，在内存中使用宽松搜索策略
    if not results and self.text_docs:
        try:
            self.logger.debug("策略1无结果，启用策略2：内存宽松搜索")
            
            # 在内存中文档中使用宽松搜索策略
            for doc_id, doc in self.text_docs.items():
                # 使用宽松的评分算法
                score = self._calculate_text_score_relaxed(doc, query)
                
                # 降低阈值，使用更宽松的匹配条件
                relaxed_threshold = self.config.text_similarity_threshold * 0.5
                if score >= relaxed_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'memory_relaxed_search'
                    })
            
            self.logger.debug(f"内存宽松搜索找到 {len(results)} 个结果")
        except Exception as e:
            self.logger.warning(f"内存宽松搜索失败: {e}")
    
    # 策略3: 如果仍然没有结果，尝试关键词搜索
    if not results:
        keyword_results = self._keyword_search(query)
        results.extend(keyword_results)
        self.logger.debug(f"关键词搜索找到 {len(keyword_results)} 个结果")
    
    # 策略4: 如果还是没有结果，尝试模糊匹配（但只针对文本文档）
    if not results:
        fuzzy_results = self._fuzzy_search(query)
        results.extend(fuzzy_results)
        self.logger.debug(f"模糊搜索找到 {len(fuzzy_results)} 个结果")
    
    # 策略5: 如果还是没有结果，在内存中降低阈值重新搜索
    if not results and self.config.text_similarity_threshold > 0.05 and self.text_docs:
        self.logger.debug("策略5：在内存中降低阈值重新搜索...")
        original_threshold = self.config.text_similarity_threshold
        self.config.text_similarity_threshold = 0.05
        
        # 在内存中文档中重新搜索
        try:
            for doc_id, doc in self.text_docs.items():
                score = self._calculate_text_score(doc, query)
                if score >= self.config.text_similarity_threshold:
                    results.append({
                        'doc_id': doc_id,
                        'doc': doc,
                        'score': score,
                        'match_type': 'memory_low_threshold_search'
                    })
            
            self.logger.debug(f"内存低阈值搜索找到 {len(results)} 个结果")
        except Exception as e:
            self.logger.warning(f"内存低阈值搜索失败: {e}")
        
        # 恢复原始阈值
        self.config.text_similarity_threshold = original_threshold
    
    # 去重和排序
    seen_ids = set()
    unique_results = []
    for result in results:
        doc_id = result.get('doc_id', 'unknown')
        if doc_id not in seen_ids:
            seen_ids.add(doc_id)
            unique_results.append(result)
    
    # 按分数排序
    unique_results.sort(key=lambda x: x['score'], reverse=True)
    
    self.logger.debug(f"最终去重后得到 {len(unique_results)} 个结果")
    return unique_results
```

**功能说明**：
- **5层策略降级**：确保即使高优先级策略失败，仍有结果返回
- **智能策略切换**：根据结果数量自动启用下一层策略
- **内存优先优化**：策略2和策略5都在内存中执行，避免重复I/O操作
- **阈值动态调整**：策略5临时降低阈值到0.05，确保有结果返回
- **结果去重排序**：避免重复内容，按相关性分数排序

### **4. �� 评分算法详解**

#### **4.1 严格评分算法（策略1）**

```python
def _calculate_text_score(self, doc: Any, query: str) -> float:
    """计算文本匹配分数 - 智能综合评分（严格相关性判断）"""
    score = 0.0
    
    # 获取文本内容
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    # 1. 语义相似度分数（核心指标）
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight
    
    # 2. 关键词匹配分数（严格匹配）
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        # 关键词匹配必须达到一定阈值才给分
        if keyword_score > 0.3:  # 至少30%的关键词匹配
            score += keyword_score * self.config.keyword_weight
        else:
            # 关键词匹配不足，大幅降低分数
            score *= 0.3
    
    # 3. 向量相似度分数（如果有向量嵌入）
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.3  # 降低默认向量分数
        score += vector_score * self.config.vector_weight
    
    # 4. 文档类型匹配奖励（降低奖励）
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.05  # 降低文本文档类型匹配奖励
    
    # 5. 内容长度奖励（降低奖励）
    if len(content) > 100:
        score += 0.02  # 降低内容长度奖励
    
    # 6. 相关性惩罚机制
    if semantic_score < 0.1:  # 语义相似度过低
        score *= 0.5  # 大幅降低分数
    
    return min(score, 1.0)
```

**评分权重配置**：
- `semantic_weight`: 0.3 (语义相似度权重)
- `keyword_weight`: 0.5 (关键词匹配权重)  
- `vector_weight`: 0.2 (向量相似度权重)

**严格性特点**：
- 关键词匹配必须达到30%才给分，否则大幅降分
- 语义相似度过低时，总分乘以0.5惩罚
- 降低文档类型和内容长度的奖励分数

#### **4.2 宽松评分算法（策略2）**

```python
def _calculate_text_score_relaxed(self, doc: Any, query: str) -> float:
    """计算文本匹配分数 - 宽松评分算法（用于策略2）"""
    score = 0.0
    
    # 获取文本内容
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    # 1. 语义相似度分数（宽松处理）
    semantic_score = self._calculate_text_similarity(query, content)
    score += semantic_score * self.config.semantic_weight
    
    # 2. 关键词匹配分数（宽松匹配）
    keywords = self._extract_keywords(query)
    if keywords:
        keyword_score = self._calculate_keyword_score(doc, keywords)
        # 宽松处理：即使关键词匹配不足，也给予一定分数
        if keyword_score > 0.1:  # 降低到10%的关键词匹配
            score += keyword_score * self.config.keyword_weight
        else:
            # 关键词匹配不足，轻微降低分数
            score *= 0.7  # 从0.3提升到0.7
    
    # 3. 向量相似度分数（如果有向量嵌入）
    if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
        vector_score = 0.4  # 提高默认向量分数
        score += vector_score * self.config.vector_weight
    
    # 4. 文档类型匹配奖励（提高奖励）
    if doc.metadata.get('chunk_type') == 'text':
        score += 0.1  # 提高文本文档类型匹配奖励
    
    # 5. 内容长度奖励（提高奖励）
    if len(content) > 100:
        score += 0.05  # 提高内容长度奖励
    
    # 6. 相关性惩罚机制（宽松处理）
    if semantic_score < 0.05:  # 从0.1降低到0.05
        score *= 0.7  # 从0.5提升到0.7
    
    return min(score, 1.0)
```

**宽松性特点**：
- 关键词匹配阈值降低到10%
- 惩罚系数从0.3提升到0.7，更宽松
- 语义相似度惩罚阈值从0.1降低到0.05
- 提高文档类型和内容长度的奖励分数

### **5. �� 关键词处理机制**

#### **5.1 关键词提取**

```python
def _extract_keywords(self, query: str) -> List[str]:
    """提取查询关键词"""
    stop_words = {'的', '是', '在', '有', '和', '与', '或', '但', '而', '了', '吗', '呢', '啊'}
    
    import re
    clean_query = re.sub(r'[^\w\s]', '', query)
    
    words = clean_query.split()
    keywords = [word for word in words if word not in stop_words and len(word) > 1]
    
    return keywords
```

**功能说明**：
- 去除停用词（中文常见虚词）
- 清理特殊字符，只保留字母数字和空格
- 过滤长度小于2的单词
- 返回清洗后的关键词列表

#### **5.2 关键词评分**

```python
def _calculate_keyword_score(self, doc: Any, keywords: List[str]) -> float:
    """计算关键词匹配分数"""
    if not keywords:
        return 0.0
    
    content = doc.page_content if hasattr(doc, 'page_content') else ''
    
    total_score = 0.0
    for keyword in keywords:
        if keyword in content:
            total_score += 1.0
    
    return min(total_score / len(keywords), 1.0)
```

**评分逻辑**：
- 统计文档内容中包含的关键词数量
- 分数 = 匹配关键词数 / 总关键词数
- 最高分数为1.0

### **6. 🌐 语义相似度计算**

```python
def _calculate_text_similarity(self, query: str, text: str) -> float:
    """计算文本相似度"""
    if not text or not query:
        return 0.0
    
    # 简单的词汇重叠计算
    query_words = set(query.lower().split())
    text_words = set(text.lower().split())
    
    if not query_words or not text_words:
        return 0.0
    
    intersection = query_words.intersection(text_words)
    union = query_words.union(text_words)
    
    if union:
        return len(intersection) / len(union)
    return 0.0
```

**计算原理**：
- 使用Jaccard相似度：交集大小 / 并集大小
- 将查询和文档转换为词汇集合
- 计算词汇重叠程度
- 返回0-1之间的相似度分数

### **7. 🔍 智能模糊搜索**

```python
def _fuzzy_search(self, query: str) -> List[Any]:
    """智能模糊搜索 - 只在真正相关时才启用"""
    results = []
    query_lower = query.lower()
    
    # 分析查询意图，判断是否与中芯国际相关
    smic_keywords = ['中芯国际', '中芯', '晶圆', '芯片', '半导体', '集成电路', 'IC', '代工']
    query_has_smic_context = any(keyword in query_lower for keyword in smic_keywords)
    
    # 如果查询与中芯国际无关，不启用模糊搜索
    if not query_has_smic_context:
        self.logger.debug(f"查询 '{query}' 与中芯国际无关，跳过模糊搜索")
        return results
    
    # 提取查询中的关键概念（只针对中芯国际相关内容）
    key_concepts = ['中芯国际', '晶圆', '芯片', '半导体', '技术', '业务', '市场', '发展', '营收', '利润']
    
    for doc_id, doc in self.text_docs.items():
        content = doc.page_content if hasattr(doc, 'page_content') else ''
        content_lower = content.lower()
        
        # 检查是否包含关键概念
        concept_matches = sum(1 for concept in key_concepts if concept in content_lower)
        if concept_matches > 0:
            # 计算相关性分数，要求至少2个概念匹配
            if concept_matches >= 2:
                score = min(concept_matches * 0.15, 0.8)  # 降低分数，最高0.8
                results.append({
                    'doc_id': doc_id,
                    'doc': doc,
                    'score': score,
                    'match_type': 'smart_fuzzy_search'
                })
    
    return results
```

**智能特点**：
- **领域相关性判断**：只在查询与中芯国际相关时启用
- **概念匹配计数**：统计文档中包含的关键概念数量
- **最低匹配要求**：要求至少2个概念匹配才返回结果
- **分数上限控制**：最高分数限制在0.8，避免过度匹配

### **8. �� 结果排序和优化**

#### **8.1 智能排序**

```python
def _rank_text_results(self, results: List[Any], query: str) -> List[Any]:
    """对文本结果进行智能排序"""
    if not results:
        return []
    
    # 按分数排序
    sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)
    
    # 限制结果数量
    return sorted_results[:self.config.max_results]
```

**排序逻辑**：
- 按相关性分数降序排列
- 限制返回结果数量（默认10个）
- 确保最相关的结果排在前面

#### **8.2 结果去重**

```python
# 去重和排序
seen_ids = set()
unique_results = []
for result in results:
    doc_id = result.get('doc_id', 'unknown')
    if doc_id not in seen_ids:
        seen_ids.add(doc_id)
        unique_results.append(result)
```

**去重机制**：
- 基于文档ID去重
- 使用集合（set）提高查找效率
- 保留第一个出现的重复结果

### **9. ⚙️ 配置管理**

#### **9.1 配置验证**

```python
def _validate_config(self):
    """验证文本引擎配置 - 支持两种配置类型"""
    # 支持两种配置类型：TextEngineConfig 和 TextEngineConfigV2
    from ..config.v2_config import TextEngineConfigV2
    
    if not isinstance(self.config, (TextEngineConfig, TextEngineConfigV2)):
        raise ValueError("配置必须是TextEngineConfig或TextEngineConfigV2类型")
    
    # 获取相似度阈值，支持两种配置类型
    threshold = getattr(self.config, 'text_similarity_threshold', 0.7)
    if threshold < 0 or threshold > 1:
        raise ValueError("文本相似度阈值必须在0-1之间")
```

**配置特点**：
- 支持新旧两种配置类型
- 验证相似度阈值的有效性
- 提供默认值支持

### **10. �� 性能监控和统计**

#### **10.1 文档统计**

```python
def get_text_statistics(self) -> Dict[str, Any]:
    """获取文本统计信息"""
    return {
        'total_texts': len(self.text_docs),
        'total_chars': sum(len(doc.page_content) if hasattr(doc, 'page_content') else 0 
                         for doc in self.text_docs.values())
    }
```

**统计信息**：
- 文本文档总数
- 总字符数
- 便于监控系统状态和性能

#### **10.2 缓存管理**

```python
def refresh_text_cache(self):
    """刷新文本缓存"""
    self._load_text_documents()
    self.logger.info("文本缓存已刷新")

def get_text_by_id(self, text_id: str) -> Optional[Any]:
    """根据ID获取文本"""
    return self.text_docs.get(text_id)

def get_all_texts(self) -> List[Any]:
    """获取所有文本"""
    return list(self.text_docs.values())
```

**缓存功能**：
- 支持手动刷新缓存
- 提供按ID查询文档的接口
- 支持获取所有文档列表

## �� **技术实现总结**

文本召回系统通过以下技术特点实现了高性能、高准确性的搜索：

1. **5层策略降级**：确保系统稳定性和结果可用性
2. **内存优先优化**：减少I/O操作，提升响应速度
3. **智能评分算法**：严格和宽松两种模式，适应不同场景
4. **领域相关搜索**：针对中芯国际业务场景的智能模糊搜索
5. **向后兼容性**：支持新旧配置和传统加载方式
6. **性能监控**：详细的日志记录和统计信息
7. **错误处理**：重试机制和降级策略，提高系统稳定性

这个设计充分体现了**智能化**、**性能优化**和**工程化**的特点，为RAG系统提供了可靠的文本召回能力。