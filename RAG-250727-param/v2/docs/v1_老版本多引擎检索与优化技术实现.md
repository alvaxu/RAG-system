# V1老版本多引擎检索与优化技术实现

## 1. 系统架构总览

### 1.1 整体架构
老版本系统采用模块化设计，核心组件包括：

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   主程序入口    │    │   核心引擎      │    │   文档处理      │
│ V501_unified_  │───→│ enhanced_qa_    │───→│ pipeline       │
│ main.py        │    │ system.py       │    │                │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                              │
                              ▼
                       ┌─────────────────┐
                       │   记忆管理      │
                       │ memory_manager  │
                       └─────────────────┘
```

### 1.2 技术特点
- **单引擎架构**: 基于`enhanced_qa_system.py`的统一问答系统
- **模块化设计**: 文档处理、向量存储、记忆管理分离
- **配置驱动**: 通过`config.json`和`Settings`类管理配置

## 2. 核心引擎实现

### 2.1 增强问答系统 (enhanced_qa_system.py)

#### 2.1.1 核心架构
```python
class EnhancedQASystem:
    """增强版问答系统"""
    
    def __init__(self, vector_store_path: str, api_key: str, 
                 memory_manager: MemoryManager, config: dict):
        self.vector_store = self._load_vector_store(vector_store_path)
        self.memory_manager = memory_manager
        self.config = config
        self.llm_client = self._init_llm_client(api_key)
```

#### 2.1.2 关键技术实现
**向量检索优化**:
```python
def _retrieve_relevant_chunks(self, query: str, top_k: int = 5) -> List[Document]:
    """检索相关文档块"""
    try:
        # 使用向量相似度搜索
        results = self.vector_store.similarity_search(
            query, 
            k=top_k,
            filter=self._build_search_filter(query)
        )
        
        # 应用智能过滤
        filtered_results = self._apply_smart_filtering(results, query)
        
        return filtered_results
        
    except Exception as e:
        logger.error(f"检索失败: {str(e)}")
        return []
```

**智能过滤算法**:
```python
def _apply_smart_filtering(self, results: List[Document], query: str) -> List[Document]:
    """应用智能过滤"""
    filtered = []
    
    for doc in results:
        # 计算相关性分数
        relevance_score = self._calculate_relevance_score(doc, query)
        
        # 计算内容质量分数
        quality_score = self._calculate_content_quality(doc)
        
        # 综合评分
        final_score = relevance_score * 0.7 + quality_score * 0.3
        
        if final_score >= self.config.get('min_relevance_threshold', 0.6):
            filtered.append(doc)
    
    # 按分数排序
    filtered.sort(key=lambda x: x.metadata.get('score', 0), reverse=True)
    return filtered
```

### 2.2 记忆管理系统 (memory_manager.py)

#### 2.2.1 记忆架构
```python
class MemoryManager:
    """记忆管理器"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.memory_db = self._init_memory_db()
        self.short_term_memory = {}  # 短期记忆
        self.long_term_memory = {}   # 长期记忆
```

#### 2.2.2 记忆策略
**分层记忆管理**:
```python
def add_memory(self, user_id: str, question: str, answer: str, 
               sources: List[str], metadata: dict = None):
    """添加记忆"""
    # 短期记忆：最近对话
    if user_id not in self.short_term_memory:
        self.short_term_memory[user_id] = []
    
    memory_item = {
        'question': question,
        'answer': answer,
        'sources': sources,
        'timestamp': datetime.now(),
        'metadata': metadata or {}
    }
    
    self.short_term_memory[user_id].append(memory_item)
    
    # 限制短期记忆大小
    if len(self.short_term_memory[user_id]) > 10:
        self.short_term_memory[user_id] = self.short_term_memory[user_id][-10:]
    
    # 长期记忆：重要对话
    if self._is_important_memory(memory_item):
        self._save_to_long_term_memory(user_id, memory_item)
```

**重要性判断算法**:
```python
def _is_important_memory(self, memory_item: dict) -> bool:
    """判断是否为重要记忆"""
    # 基于多个因素评分
    score = 0
    
    # 问题长度
    question_length = len(memory_item['question'])
    if question_length > 50:
        score += 2
    elif question_length > 20:
        score += 1
    
    # 答案长度
    answer_length = len(memory_item['answer'])
    if answer_length > 200:
        score += 3
    elif answer_length > 100:
        score += 2
    
    # 来源数量
    sources_count = len(memory_item['sources'])
    if sources_count > 3:
        score += 2
    elif sources_count > 1:
        score += 1
    
    # 关键词匹配
    important_keywords = ['重要', '关键', '核心', '主要', '总结']
    for keyword in important_keywords:
        if keyword in memory_item['question'] or keyword in memory_item['answer']:
            score += 2
    
    return score >= 5  # 阈值可配置
```

### 2.3 向量存储系统 (vector_store.py)

#### 2.3.1 存储架构
```python
class VectorStore:
    """向量存储管理器"""
    
    def __init__(self, store_path: str):
        self.store_path = store_path
        self.index = None
        self.documents = []
        self.metadata_index = {}
```

#### 2.3.2 检索优化
**相似度搜索优化**:
```python
def similarity_search(self, query: str, k: int = 5, 
                     filter_dict: dict = None) -> List[Document]:
    """相似度搜索"""
    try:
        # 查询向量化
        query_vector = self._encode_query(query)
        
        # 应用过滤器
        if filter_dict:
            filtered_indices = self._apply_filters(filter_dict)
            if not filtered_indices:
                return []
        else:
            filtered_indices = list(range(len(self.documents)))
        
        # 计算相似度
        similarities = []
        for idx in filtered_indices:
            doc_vector = self.documents[idx].embedding
            similarity = self._calculate_similarity(query_vector, doc_vector)
            similarities.append((idx, similarity))
        
        # 排序并返回top-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_indices = [idx for idx, _ in similarities[:k]]
        
        return [self.documents[idx] for idx in top_indices]
        
    except Exception as e:
        logger.error(f"相似度搜索失败: {str(e)}")
        return []
```

## 3. 文档处理管道

### 3.1 主处理管道 (pipeline.py)

#### 3.1.1 管道架构
```python
class DocumentProcessingPipeline:
    """文档处理管道"""
    
    def __init__(self, config: dict):
        self.config = config
        self.pdf_processor = PDFProcessor(config)
        self.image_processor = ImageProcessor(config)
        self.table_processor = TableProcessor(config)
        self.chunker = EnhancedChunker(config)
        self.vector_generator = VectorGenerator(config)
```

#### 3.1.2 处理流程
**完整处理流程**:
```python
def process_pipeline(self, pdf_dir: str, output_dir: str, 
                    vector_db_path: str) -> bool:
    """执行完整处理流程"""
    try:
        # 1. PDF解析和文本提取
        logger.info("开始PDF解析...")
        text_docs = self.pdf_processor.process_directory(pdf_dir)
        
        # 2. 图像处理
        logger.info("开始图像处理...")
        image_docs = self.image_processor.process_images_from_pdfs(pdf_dir)
        
        # 3. 表格处理
        logger.info("开始表格处理...")
        table_docs = self.table_processor.process_tables_from_pdfs(pdf_dir)
        
        # 4. 文档分块
        logger.info("开始文档分块...")
        all_chunks = []
        all_chunks.extend(self.chunker.chunk_documents(text_docs))
        all_chunks.extend(self.chunker.chunk_documents(image_docs))
        all_chunks.extend(self.chunker.chunk_documents(table_docs))
        
        # 5. 向量化
        logger.info("开始向量化...")
        vector_store = self.vector_generator.create_vector_store(
            all_chunks, vector_db_path
        )
        
        # 6. 保存结果
        self._save_processing_results(output_dir, {
            'text_docs': len(text_docs),
            'image_docs': len(image_docs),
            'table_docs': len(table_docs),
            'total_chunks': len(all_chunks),
            'vector_store_path': vector_db_path
        })
        
        logger.info("处理流程完成")
        return True
        
    except Exception as e:
        logger.error(f"处理流程失败: {str(e)}")
        return False
```

### 3.2 增强分块器 (enhanced_chunker.py)

#### 3.2.1 智能分块策略
```python
class EnhancedChunker:
    """增强文档分块器"""
    
    def __init__(self, config: dict):
        self.chunk_size = config.get('chunk_size', 1000)
        self.chunk_overlap = config.get('chunk_overlap', 200)
        self.min_chunk_size = config.get('min_chunk_size', 100)
```

#### 3.2.2 分块算法
**语义感知分块**:
```python
def chunk_documents(self, documents: List[Document]) -> List[Document]:
    """智能分块文档"""
    chunks = []
    
    for doc in documents:
        if doc.doc_type == 'text':
            chunks.extend(self._chunk_text_document(doc))
        elif doc.doc_type == 'image':
            chunks.extend(self._chunk_image_document(doc))
        elif doc.doc_type == 'table':
            chunks.extend(self._chunk_table_document(doc))
    
    return chunks

def _chunk_text_document(self, doc: Document) -> List[Document]:
    """文本文档分块"""
    text = doc.content
    chunks = []
    
    # 按段落分割
    paragraphs = text.split('\n\n')
    
    current_chunk = ""
    for paragraph in paragraphs:
        if len(current_chunk) + len(paragraph) <= self.chunk_size:
            current_chunk += paragraph + "\n\n"
        else:
            if current_chunk.strip():
                chunks.append(self._create_chunk(doc, current_chunk.strip()))
            current_chunk = paragraph + "\n\n"
    
    # 处理最后一个块
    if current_chunk.strip():
        chunks.append(self._create_chunk(doc, current_chunk.strip()))
    
    return chunks
```

## 4. 性能优化策略

### 4.1 查询优化

#### 4.1.1 缓存机制
```python
class QueryCache:
    """查询缓存"""
    
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        self.access_count = {}
    
    def get(self, query: str) -> Optional[dict]:
        """获取缓存结果"""
        if query in self.cache:
            # 更新访问次数
            self.access_count[query] += 1
            return self.cache[query]
        return None
    
    def set(self, query: str, result: dict):
        """设置缓存结果"""
        if len(self.cache) >= self.max_size:
            # 淘汰最少访问的项
            self._evict_least_used()
        
        self.cache[query] = result
        self.access_count[query] = 1
```

#### 4.1.2 并行处理
```python
def _process_parallel_queries(self, queries: List[str]) -> List[dict]:
    """并行处理多个查询"""
    from concurrent.futures import ThreadPoolExecutor
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        future_to_query = {
            executor.submit(self._process_single_query, query): query 
            for query in queries
        }
        
        results = []
        for future in concurrent.futures.as_completed(future_to_query):
            query = future_to_query[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"查询处理失败: {query}, 错误: {str(e)}")
                results.append({'query': query, 'error': str(e)})
    
    return results
```

### 4.2 内存优化

#### 4.2.1 内存池管理
```python
class MemoryPool:
    """内存池管理器"""
    
    def __init__(self, max_memory_mb: int = 1024):
        self.max_memory = max_memory_mb * 1024 * 1024  # 转换为字节
        self.current_memory = 0
        self.memory_blocks = {}
    
    def allocate(self, size: int, block_id: str) -> bool:
        """分配内存块"""
        if self.current_memory + size > self.max_memory:
            # 尝试释放一些内存
            self._cleanup_old_blocks()
            
            if self.current_memory + size > self.max_memory:
                return False
        
        self.memory_blocks[block_id] = {
            'size': size,
            'timestamp': time.time()
        }
        self.current_memory += size
        return True
```

## 5. 关键技术点总结

### 5.1 检索优化
- **向量相似度搜索**: 基于FAISS的高效向量检索
- **智能过滤**: 多维度评分和阈值过滤
- **缓存机制**: 查询结果缓存，减少重复计算

### 5.2 记忆管理
- **分层记忆**: 短期记忆和长期记忆分离
- **重要性评估**: 多因素评分算法
- **自动清理**: 基于时间和重要性的记忆管理

### 5.3 文档处理
- **多模态处理**: 文本、图像、表格统一处理
- **智能分块**: 语义感知的文档分块策略
- **增量处理**: 支持增量文档更新

### 5.4 性能优化
- **并行处理**: 多线程查询处理
- **内存管理**: 内存池和自动清理
- **缓存策略**: 多层缓存优化

## 6. 与V2版本的对比

### 6.1 架构差异
- **V1**: 单引擎架构，模块化设计
- **V2**: 多引擎架构，优化管道设计

### 6.2 功能差异
- **V1**: 基础检索和记忆功能
- **V2**: 重排序、智能过滤、源过滤等高级功能

### 6.3 性能差异
- **V1**: 基础优化策略
- **V2**: 高级优化管道和并行处理

老版本系统为V2版本奠定了坚实的基础，其模块化设计和核心算法为后续的优化升级提供了重要的技术积累。
