#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š

## 1. å›¾ç‰‡å¼•æ“æ ¸å¿ƒå®ç° - V2.0ç‰ˆæœ¬
## 2. å®ç°äº”å±‚å¬å›ç­–ç•¥ï¼šå‘é‡æœç´¢ã€å…³é”®è¯åŒ¹é…ã€æ··åˆå¬å›ã€æ¨¡ç³ŠåŒ¹é…ã€æŸ¥è¯¢æ‰©å±•
## 3. é›†æˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢
## 4. æ”¯æŒå›¾ç‰‡æè¿°å’Œå…ƒæ•°æ®åŒ¹é…
## 5. ä¸ºåç»­ImageRerankingServiceå’Œç»Ÿä¸€Pipelineé›†æˆåšå‡†å¤‡
"""

import logging
import time
import re
from typing import List, Dict, Any, Optional, Set
from ..core.base_engine import BaseEngine
from ..core.base_engine import EngineConfig
from ..core.base_engine import QueryResult, QueryType

logger = logging.getLogger(__name__)


class ImageEngine(BaseEngine):
    """
    å›¾ç‰‡å¼•æ“ - V2.0ç‰ˆæœ¬
    
    ä¸“é—¨å¤„ç†å›¾ç‰‡æŸ¥è¯¢ï¼Œå®ç°äº”å±‚å¬å›ç­–ç•¥
    """
    
    def __init__(self, config, vector_store=None, document_loader=None, skip_initial_load=False):
        """
        åˆå§‹åŒ–å›¾ç‰‡å¼•æ“
        
        :param config: å›¾ç‰‡å¼•æ“é…ç½®
        :param vector_store: å‘é‡æ•°æ®åº“
        :param document_loader: æ–‡æ¡£åŠ è½½å™¨
        :param skip_initial_load: æ˜¯å¦è·³è¿‡åˆå§‹æ–‡æ¡£åŠ è½½
        """
        super().__init__(config)
        self.vector_store = vector_store
        self.document_loader = document_loader
        self.image_docs = []  # å›¾ç‰‡æ–‡æ¡£ç¼“å­˜
        self._docs_loaded = False
        
        if not skip_initial_load:
            self._load_documents()
    
    def _load_documents(self):
        """åŠ è½½å›¾ç‰‡æ–‡æ¡£"""
        if self._docs_loaded:
            return
            
        try:
            if self.document_loader:
                # ä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨
                # è·å–imageå’Œimage_textä¸¤ç§ç±»å‹çš„æ–‡æ¡£
                image_docs = self.document_loader.get_documents_by_type('image')
                image_text_docs = self.document_loader.get_documents_by_type('image_text')
                
                # åˆå¹¶ä¸¤ç§ç±»å‹çš„æ–‡æ¡£
                self.image_docs = []
                
                # æ·»åŠ imageç±»å‹çš„æ–‡æ¡£
                if image_docs:
                    if isinstance(image_docs, dict):
                        self.image_docs.extend(image_docs.values())
                    else:
                        self.image_docs.extend(image_docs)
                    logger.info(f"åŠ è½½imageæ–‡æ¡£: {len(image_docs)} ä¸ª")
                
                # æ·»åŠ image_textç±»å‹çš„æ–‡æ¡£
                if image_text_docs:
                    if isinstance(image_text_docs, dict):
                        self.image_docs.extend(image_text_docs.values())
                    else:
                        self.image_docs.extend(image_text_docs)
                    logger.info(f"åŠ è½½image_textæ–‡æ¡£: {len(image_text_docs)} ä¸ª")
                
                logger.info(f"å›¾ç‰‡å¼•æ“æ€»å…±åŠ è½½äº† {len(self.image_docs)} ä¸ªå›¾ç‰‡ç›¸å…³æ–‡æ¡£")
                
            elif self.vector_store:
                # ä»å‘é‡æ•°æ®åº“åŠ è½½
                self.image_docs = self.vector_store.get_image_documents()
                logger.info(f"ä»å‘é‡æ•°æ®åº“åŠ è½½äº† {len(self.image_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
            else:
                logger.warning("æœªæä¾›æ–‡æ¡£åŠ è½½å™¨æˆ–å‘é‡æ•°æ®åº“ï¼Œå›¾ç‰‡å¼•æ“å°†æ— æ³•å·¥ä½œ")
                return
                
            self._docs_loaded = True
            
        except Exception as e:
            logger.error(f"åŠ è½½å›¾ç‰‡æ–‡æ¡£å¤±è´¥: {e}")
            self._docs_loaded = False
    
    def _load_from_document_loader(self):
        """ä»ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨è·å–å›¾ç‰‡æ–‡æ¡£"""
        if self.document_loader:
            try:
                # è·å–imageå’Œimage_textä¸¤ç§ç±»å‹çš„æ–‡æ¡£
                image_docs = self.document_loader.get_documents_by_type('image')
                image_text_docs = self.document_loader.get_documents_by_type('image_text')
                
                # åˆå¹¶ä¸¤ç§ç±»å‹çš„æ–‡æ¡£
                self.image_docs = []
                
                # æ·»åŠ imageç±»å‹çš„æ–‡æ¡£
                if image_docs:
                    if isinstance(image_docs, dict):
                        self.image_docs.extend(image_docs.values())
                    else:
                        self.image_docs.extend(image_docs)
                    logger.info(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–imageæ–‡æ¡£: {len(image_docs)} ä¸ª")
                
                # æ·»åŠ image_textç±»å‹çš„æ–‡æ¡£
                if image_text_docs:
                    if isinstance(image_text_docs, dict):
                        self.image_docs.extend(image_text_docs.values())
                    else:
                        self.image_docs.extend(image_text_docs)
                    logger.info(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–image_textæ–‡æ¡£: {len(image_text_docs)} ä¸ª")
                
                self._docs_loaded = True
                logger.info(f"ä»ç»Ÿä¸€åŠ è½½å™¨æ€»å…±è·å–å›¾ç‰‡ç›¸å…³æ–‡æ¡£: {len(self.image_docs)} ä¸ª")
                
            except Exception as e:
                logger.error(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–å›¾ç‰‡æ–‡æ¡£å¤±è´¥: {e}")
                # é™çº§åˆ°ä¼ ç»ŸåŠ è½½æ–¹å¼
                self._load_from_vector_store()
        else:
            logger.warning("æ–‡æ¡£åŠ è½½å™¨æœªæä¾›ï¼Œä½¿ç”¨ä¼ ç»ŸåŠ è½½æ–¹å¼")
            self._load_from_vector_store()
    
    def _load_from_vector_store(self):
        """ä»å‘é‡æ•°æ®åº“åŠ è½½å›¾ç‰‡æ–‡æ¡£"""
        if not self.vector_store or not hasattr(self.vector_store, 'docstore'):
            logger.error("âŒ å‘é‡æ•°æ®åº“æœªæä¾›æˆ–æ²¡æœ‰docstoreå±æ€§")
            return
        
        try:
            logger.info("âœ… å‘é‡æ•°æ®åº“æ£€æŸ¥é€šè¿‡")
            logger.info(f"docstoreç±»å‹: {type(self.vector_store.docstore)}")
            
            # æ¸…ç©ºä¹‹å‰çš„ç¼“å­˜
            self.image_docs = []
            
            # æ£€æŸ¥docstore._dict
            if not hasattr(self.vector_store.docstore, '_dict'):
                logger.error("âŒ docstoreæ²¡æœ‰_dictå±æ€§")
                return
            
            docstore_dict = self.vector_store.docstore._dict
            logger.info(f"docstore._dicté•¿åº¦: {len(docstore_dict)}")
            
            # ä»å‘é‡æ•°æ®åº“åŠ è½½æ‰€æœ‰å›¾ç‰‡æ–‡æ¡£
            image_doc_count = 0
            for doc_id, doc in docstore_dict.items():
                chunk_type = doc.metadata.get('chunk_type', '') if hasattr(doc, 'metadata') else ''
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡æ¡£
                is_image = chunk_type in ['image', 'image_text']
                
                if is_image:
                    self.image_docs.append(doc)
                    image_doc_count += 1
                    if image_doc_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                        logger.debug(f"âœ… åŠ è½½å›¾ç‰‡æ–‡æ¡£: {doc_id}, chunk_type: {chunk_type}")
                        if hasattr(doc, 'metadata'):
                            logger.debug(f"  å…ƒæ•°æ®: {doc.metadata}")
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.image_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
            self._docs_loaded = True
            
        except Exception as e:
            logger.error(f"ä»å‘é‡æ•°æ®åº“åŠ è½½å›¾ç‰‡æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.image_docs = []
    
    def _ensure_docs_loaded(self):
        """ç¡®ä¿æ–‡æ¡£å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if not self._docs_loaded:
            if self.document_loader:
                self._load_from_document_loader()
            else:
                self._load_from_vector_store()
                self._docs_loaded = True
    
    def _validate_config(self):
        """éªŒè¯å›¾ç‰‡å¼•æ“é…ç½®"""
        # é…ç½®ç±»å‹æ£€æŸ¥
        from ..config.v2_config import ImageEngineConfigV2
        
        if not isinstance(self.config, ImageEngineConfigV2):
            raise ValueError("é…ç½®å¿…é¡»æ˜¯ImageEngineConfigV2ç±»å‹")
        
        # è·å–ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œæ”¯æŒä¸¤ç§é…ç½®ç±»å‹
        threshold = getattr(self.config, 'image_similarity_threshold', 0.7)
        if not isinstance(threshold, (int, float)) or threshold < 0 or threshold > 1:
            raise ValueError("å›¾ç‰‡ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
    
    def _setup_components(self):
        """è®¾ç½®å¼•æ“ç»„ä»¶"""
        try:
            # åŠ è½½å›¾ç‰‡æ–‡æ¡£
            self._load_image_docs()
            self.logger.info(f"å›¾ç‰‡å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œæ–‡æ¡£æ•°é‡: {len(self.image_docs)}")
            
        except Exception as e:
            self.logger.error(f"å›¾ç‰‡å¼•æ“ç»„ä»¶è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def process_query(self, query: str, **kwargs) -> QueryResult:
        """
        å¤„ç†å›¾ç‰‡æŸ¥è¯¢ - ä½¿ç”¨äº”å±‚å¬å›ç­–ç•¥ + ImageRerankingService + ç»Ÿä¸€Pipeline
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param kwargs: å…¶ä»–å‚æ•°
        :return: æœç´¢ç»“æœ
        """
        # ç¡®ä¿æ–‡æ¡£å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._ensure_docs_loaded()
        
        if not self.image_docs:
            logger.warning("å›¾ç‰‡å¼•æ“æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£")
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.IMAGE,
                results=[],
                total_count=0,
                processing_time=0.0,
                engine_name=self.name,
                metadata={},
                error_message="å›¾ç‰‡å¼•æ“æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£"
            )
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œäº”å±‚å¬å›ç­–ç•¥
            recall_results = self._search_images_with_five_layer_recall(query)
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¢å¼ºReranking
            if getattr(self.config, 'enable_enhanced_reranking', False):
                logger.info("å¯ç”¨å¢å¼ºRerankingæœåŠ¡")
                try:
                    # å¯¼å…¥RerankingæœåŠ¡
                    from .reranking_services import create_reranking_service
                    
                    # åˆ›å»ºImageRerankingService
                    reranking_config = getattr(self.config, 'reranking', {})
                    reranking_service = create_reranking_service('image', reranking_config)
                    
                    if reranking_service:
                        # æ‰§è¡ŒReranking
                        logger.info(f"å¼€å§‹æ‰§è¡ŒImageRerankingï¼Œå€™é€‰æ–‡æ¡£æ•°é‡: {len(recall_results)}")
                        reranked_results = reranking_service.rerank(query, recall_results)
                        logger.info(f"Rerankingå®Œæˆï¼Œè¿”å› {len(reranked_results)} ä¸ªç»“æœ")
                        
                        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°çš„ç»Ÿä¸€Pipeline
                        if getattr(self.config, 'use_new_pipeline', False):
                            logger.info("ä½¿ç”¨æ–°çš„ç»Ÿä¸€Pipelineå¤„ç†é‡æ’åºç»“æœ")
                            try:
                                # å¯¼å…¥ç»Ÿä¸€Pipeline
                                from .unified_pipeline import UnifiedPipeline
                                
                                # è·å–ç»Ÿä¸€Pipelineé…ç½®
                                from ..config.v2_config import V2ConfigManager
                                config_manager = V2ConfigManager()
                                pipeline_config = config_manager.get_engine_config('unified_pipeline')
                                
                                if pipeline_config and pipeline_config.enabled:
                                    # å°è¯•è·å–çœŸå®çš„LLMå¼•æ“å’Œæºè¿‡æ»¤å¼•æ“
                                    llm_engine = None
                                    source_filter_engine = None
                                    
                                    # ä»HybridEngineè·å–å¼•æ“ï¼ˆé€šè¿‡kwargsä¼ é€’ï¼‰
                                    if 'llm_engine' in kwargs:
                                        llm_engine = kwargs['llm_engine']
                                        logger.info("ä½¿ç”¨ä¼ å…¥çš„LLMå¼•æ“")
                                    if 'source_filter_engine' in kwargs:
                                        source_filter_engine = kwargs['source_filter_engine']
                                        logger.info("ä½¿ç”¨ä¼ å…¥çš„æºè¿‡æ»¤å¼•æ“")
                                    
                                    # å¦‚æœæ²¡æœ‰ä¼ å…¥çœŸå®å¼•æ“ï¼Œä½¿ç”¨Mockï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                                    if not llm_engine:
                                        from unittest.mock import Mock
                                        llm_engine = Mock()
                                        llm_engine.generate_answer.return_value = "åŸºäºæŸ¥è¯¢å’Œå›¾ç‰‡ä¿¡æ¯ç”Ÿæˆçš„ç­”æ¡ˆ"
                                        logger.warning("ä½¿ç”¨Mock LLMå¼•æ“ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
                                    
                                    if not source_filter_engine:
                                        from unittest.mock import Mock
                                        source_filter_engine = Mock()
                                        source_filter_engine.filter_sources.return_value = reranked_results[:3]
                                        logger.warning("ä½¿ç”¨Mockæºè¿‡æ»¤å¼•æ“ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
                                    
                                    # åˆ›å»ºç»Ÿä¸€Pipeline
                                    unified_pipeline = UnifiedPipeline(
                                        config=pipeline_config.__dict__,
                                        llm_engine=llm_engine,
                                        source_filter_engine=source_filter_engine
                                    )
                                    
                                    # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥ä¼ é€’ç»™unified_pipelineçš„æ•°æ®
                                    logger.info("=" * 50)
                                    logger.info("ğŸ” IMAGE_ENGINE ä¼ é€’ç»™unified_pipelineçš„æ•°æ®è°ƒè¯•")
                                    logger.info(f"reranked_resultsæ•°é‡: {len(reranked_results)}")
                                    
                                    for i, result in enumerate(reranked_results):  # åªæ£€æŸ¥å‰3ä¸ª
                                        # logger.info(f"reranked_results[{i}]:")
                                        # logger.info(f"  - ç±»å‹: {type(result)}")
                                        if isinstance(result, dict):
                                            # logger.info(f"  - æ‰€æœ‰å­—æ®µ: {list(result.keys())}")
                                            # logger.info(f"  - document_name: {result.get('document_name', 'N/A')}")
                                            # logger.info(f"  - page_number: {result.get('page_number', 'N/A')}")
                                            # logger.info(f"  - chunk_type: {result.get('chunk_type', 'N/A')}")
                                            # logger.info(f"  - image_path: {result.get('image_path', 'N/A')}")
                                            # logger.info(f"  - caption: {result.get('caption', 'N/A')}")
                                            # logger.info(f"  - enhanced_description: {result.get('enhanced_description', 'N/A')}")
                                            # logger.info(f"  - llm_context: {result.get('llm_context', 'N/A')}")
                                            
                                            # æ£€æŸ¥docå¯¹è±¡
                                            doc = result.get('doc')
                                            if doc:
                                                logger.info(f"  - docç±»å‹: {type(doc)}")
                                                if hasattr(doc, 'metadata') and doc.metadata:
                                            #         logger.info(f"  - doc.metadataå­—æ®µ: {list(doc.metadata.keys())}")
                                            #         # logger.info(f"  - doc.metadata.enhanced_description: {doc.metadata.get('enhanced_description', 'N/A')}")
                                                    logger.info(f"  - doc.metadata.img_caption: {doc.metadata.get('img_caption', 'N/A')}")
                                            #     if hasattr(doc, 'page_content'):
                                            #         logger.info(f"  - doc.page_contenté•¿åº¦: {len(doc.page_content) if doc.page_content else 0}")
                                            # else:
                                            #     logger.info(f"  - doc: None")
                                        else:
                                            logger.info(f"  - éå­—å…¸ç±»å‹: {result}")
                                    
                                    logger.info("=" * 50)
                                    
                                    # å¢å¼ºreranked_resultsï¼šæå–doc.metadataä¸­çš„å…³é”®å­—æ®µåˆ°é¡¶å±‚
                                    enhanced_reranked_results = self._enhance_reranked_results(reranked_results)
                                    
                                    # åªä¿ç•™å…³é”®è°ƒè¯•ä¿¡æ¯
                                    logger.info(f"ğŸ” IMAGE_ENGINE: å¢å¼ºå®Œæˆï¼Œç»“æœæ•°é‡: {len(enhanced_reranked_results)}")
                                    
                                    # æ‰§è¡Œç»Ÿä¸€Pipeline
                                    pipeline_result = unified_pipeline.process(query, enhanced_reranked_results, query_type='image')
                                    
                                    if pipeline_result.success:
                                        logger.info("ç»Ÿä¸€Pipelineæ‰§è¡ŒæˆåŠŸ")
                                        final_results = pipeline_result.filtered_sources
                                        # æ·»åŠ Pipelineå…ƒæ•°æ®
                                        pipeline_metadata = {
                                            'pipeline': 'unified_pipeline',
                                            'llm_answer': pipeline_result.llm_answer,
                                            'pipeline_metrics': pipeline_result.pipeline_metrics
                                        }
                                        # å°†LLMç­”æ¡ˆä¹Ÿæ·»åŠ åˆ°metadataä¸­ï¼Œä¾›HybridEngineä½¿ç”¨
                                        if pipeline_result.llm_answer:
                                            logger.info(f"ç»Ÿä¸€Pipelineç”ŸæˆLLMç­”æ¡ˆï¼Œé•¿åº¦: {len(pipeline_result.llm_answer)}")
                                    else:
                                        logger.warning(f"ç»Ÿä¸€Pipelineæ‰§è¡Œå¤±è´¥: {pipeline_result.error_message}")
                                        final_results = self._final_ranking_and_limit(query, reranked_results)
                                        pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
                                else:
                                    logger.warning("ç»Ÿä¸€Pipelineæœªå¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ’åº")
                                    final_results = self._final_ranking_and_limit(query, reranked_results)
                                    pipeline_metadata = {'pipeline': 'traditional_ranking'}
                                    
                            except Exception as e:
                                logger.error(f"ç»Ÿä¸€Pipelineæ‰§è¡Œå¤±è´¥: {e}ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ’åº")
                                final_results = self._final_ranking_and_limit(query, reranked_results)
                                pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
                        else:
                            logger.info("ä½¿ç”¨ä¼ ç»Ÿæ’åºæ–¹å¼")
                            final_results = self._final_ranking_and_limit(query, reranked_results)
                            pipeline_metadata = {'pipeline': 'traditional_ranking'}
                    else:
                        logger.warning("ImageRerankingServiceåˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
                        final_results = recall_results
                        pipeline_metadata = {'pipeline': 'no_reranking'}
                except Exception as e:
                    logger.error(f"ImageRerankingæ‰§è¡Œå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
                    final_results = recall_results
                    pipeline_metadata = {'pipeline': 'reranking_failed'}
            else:
                logger.info("å¢å¼ºRerankingæœªå¯ç”¨ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
                final_results = recall_results
                pipeline_metadata = {'pipeline': 'no_reranking'}
            
            processing_time = time.time() - start_time
            
            return QueryResult(
                success=True,
                query=query,
                query_type=QueryType.IMAGE,
                results=final_results,
                total_count=len(final_results),
                processing_time=processing_time,
                engine_name=self.name,
                metadata={
                    'total_images': len(self.image_docs), 
                    'pipeline': 'five_layer_recall_with_reranking',
                    'pipeline_metadata': pipeline_metadata
                }
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"å›¾ç‰‡æŸ¥è¯¢å¤±è´¥: {e}")
            
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.IMAGE,
                results=[],
                total_count=0,
                processing_time=processing_time,
                engine_name=self.name,
                metadata={},
                error_message=str(e)
            )
    
    def _search_images_with_five_layer_recall(self, query: str) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œäº”å±‚å¬å›ç­–ç•¥çš„å›¾ç‰‡æœç´¢
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æ‰§è¡Œäº”å±‚å¬å›ç­–ç•¥ï¼ŒæŸ¥è¯¢: {query}")
        
        # è·å–é…ç½®å‚æ•°
        max_results = getattr(self.config, 'max_results', 10)
        max_recall_results = getattr(self.config, 'max_recall_results', 150)
        
        all_candidates = []
        
        try:
            # ç¬¬ä¸€å±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢
            logger.info("ç¬¬ä¸€å±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢")
            vector_results = self._vector_search(query, max_recall_results // 3)
            all_candidates.extend(vector_results)
            logger.info(f"ç¬¬ä¸€å±‚å¬å›ç»“æœæ•°é‡: {len(vector_results)}")
            
            # # ç¬¬äºŒå±‚ï¼šè¯­ä¹‰å…³é”®è¯åŒ¹é…
            # logger.info("ç¬¬äºŒå±‚ï¼šè¯­ä¹‰å…³é”®è¯åŒ¹é…")
            # keyword_results = self._keyword_search(query, max_recall_results // 3)
            # all_candidates.extend(keyword_results)
            # logger.info(f"ç¬¬äºŒå±‚å¬å›ç»“æœæ•°é‡: {len(keyword_results)}")
            
            # # ç¬¬ä¸‰å±‚ï¼šæ··åˆå¬å›ç­–ç•¥
            # logger.info("ç¬¬ä¸‰å±‚ï¼šæ··åˆå¬å›ç­–ç•¥")
            # # ä¼ å…¥ç¬¬ä¸€å±‚ç»“æœï¼Œé¿å…é‡å¤è°ƒç”¨
            # hybrid_results = self._hybrid_search(query, max_recall_results // 3, vector_candidates=vector_results)
            # all_candidates.extend(hybrid_results)
            # logger.info(f"ç¬¬ä¸‰å±‚å¬å›ç»“æœæ•°é‡: {len(hybrid_results)}")
            
            # # ç¬¬å››å±‚ï¼šæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…
            # logger.info("ç¬¬å››å±‚ï¼šæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…")
            # fuzzy_results = self._fuzzy_search(query, max_recall_results // 6)
            # all_candidates.extend(fuzzy_results)
            # logger.info(f"ç¬¬å››å±‚å¬å›ç»“æœæ•°é‡: {len(fuzzy_results)}")
            
            # # ç¬¬äº”å±‚ï¼šæŸ¥è¯¢æ‰©å±•å¬å›
            # logger.info("ç¬¬äº”å±‚ï¼šæŸ¥è¯¢æ‰©å±•å¬å›")
            # expansion_results = self._expansion_search(query, max_recall_results // 6)
            # all_candidates.extend(expansion_results)
            # logger.info(f"ç¬¬äº”å±‚å¬å›ç»“æœæ•°é‡: {len(expansion_results)}")
            
            # logger.info(f"äº”å±‚å¬å›æ€»ç»“æœæ•°é‡: {len(all_candidates)}")
            
            # å»é‡å’Œæ’åº
            final_results = self._deduplicate_and_sort_results(all_candidates)
            
            # é™åˆ¶ç»“æœæ•°é‡
            return final_results[:max_results]
            
        except Exception as e:
            logger.error(f"äº”å±‚å¬å›æœç´¢å¤±è´¥: {e}")
            return []
    
    def _vector_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸€å±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢ - ä½¿ç”¨FAISS filterç­–ç•¥
        
        ç»è¿‡è¯Šæ–­ç¡®è®¤ï¼ŒFAISS filterå®Œå…¨æ”¯æŒchunk_typeè¿‡æ»¤ï¼š
        1. ç­–ç•¥1ï¼šä½¿ç”¨filteræœç´¢image_text chunksï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        2. ç­–ç•¥2ï¼šä½¿ç”¨filteræœç´¢image chunksï¼ˆè§†è§‰ç‰¹å¾ç›¸ä¼¼åº¦ï¼‰
        3. æœ€ååº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        if not self.vector_store or not getattr(self.config, 'enable_vector_search', True):
            logger.info("å‘é‡æœç´¢æœªå¯ç”¨æˆ–å‘é‡æ•°æ®åº“ä¸å¯ç”¨")
            return results
        
        try:
            # ç­–ç•¥1ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
            semantic_threshold = getattr(self.config, 'semantic_similarity_threshold', 0.3)
            logger.info(f"ç¬¬ä¸€å±‚å‘é‡æœç´¢ - æŸ¥è¯¢: {query}, è¯­ä¹‰é˜ˆå€¼: {semantic_threshold}, æœ€å¤§ç»“æœæ•°: {max_results}")
            
            # ä½¿ç”¨post-filteringç­–ç•¥ï¼šå…ˆæœç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤
            search_k = max(max_results * 3, 50)  # æœç´¢æ›´å¤šå€™é€‰ç»“æœç”¨äºåè¿‡æ»¤
            logger.info(f"æœç´¢å€™é€‰ç»“æœæ•°é‡: {search_k}")
            
            # ç­–ç•¥1ï¼šæœç´¢image_text chunksï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
            logger.info("ç­–ç•¥1ï¼šæœç´¢image_text chunksï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰")
            try:
                # ä½¿ç”¨åè¿‡æ»¤æ–¹æ¡ˆï¼šå…ˆæœç´¢æ›´å¤šå€™é€‰ç»“æœï¼Œç„¶åè¿‡æ»¤å‡ºimage_textç±»å‹
                all_candidates = self.vector_store.similarity_search(
                    query, 
                    k=200  # å¢åŠ æœç´¢èŒƒå›´ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°è¶³å¤Ÿçš„image_textæ–‡æ¡£
                )
                logger.info(f"ç­–ç•¥1æœç´¢è¿”å› {len(all_candidates)} ä¸ªå€™é€‰ç»“æœ")
                
                # åè¿‡æ»¤ï¼šç­›é€‰å‡ºimage_textç±»å‹çš„æ–‡æ¡£
                image_text_candidates = []
                for doc in all_candidates:
                    if (hasattr(doc, 'metadata') and doc.metadata and 
                        doc.metadata.get('chunk_type') == 'image_text'):
                        image_text_candidates.append(doc)
                
                logger.info(f"åè¿‡æ»¤åæ‰¾åˆ° {len(image_text_candidates)} ä¸ªimage_textæ–‡æ¡£")
                
                # æ·»åŠ åˆ†æ•°åˆ†å¸ƒæ—¥å¿—ï¼Œå¸®åŠ©è¯Šæ–­é˜ˆå€¼é—®é¢˜
                if image_text_candidates:
                    scores = []
                    for doc in image_text_candidates:
                        score = self._calculate_content_relevance(query, doc.page_content)
                        scores.append(score)
                    
                    if scores:
                        logger.info(f"ç­–ç•¥1ï¼šæ‰¾åˆ° {len(scores)} ä¸ªimage_textå€™é€‰ç»“æœ")
                
                # å¤„ç†image_textæœç´¢ç»“æœ
                for doc in image_text_candidates:
                    # è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°ï¼ˆæ›¿ä»£FAISSåˆ†æ•°ï¼‰
                    score = self._calculate_content_relevance(query, doc.page_content)
                    
                    # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                    if score >= semantic_threshold:
                        # é€šè¿‡related_image_idæ‰¾åˆ°å¯¹åº”çš„image chunk
                        related_image_id = doc.metadata.get('related_image_id')
                        if related_image_id:
                            # æŸ¥æ‰¾å¯¹åº”çš„image chunk
                            image_doc = self._find_image_chunk_by_id(related_image_id)
                            if image_doc:
                                results.append({
                                    'doc': image_doc,  # è¿”å›image chunkï¼Œä¸æ˜¯image_text chunk
                                    'score': score * 1.2,  # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡æ›´é«˜
                                    'source': 'vector_search',
                                    'layer': 1,
                                    'search_method': 'semantic_similarity',
                                    'semantic_score': score,
                                    'related_image_text_id': doc.metadata.get('image_id'),
                                    'enhanced_description': doc.metadata.get('enhanced_description', ''),
                                    
                                    # æ–°å¢ï¼šä¼ é€’å®Œæ•´çš„æ¥æºä¿¡æ¯ï¼ˆä¸å½±å“å…¶ä»–æŸ¥è¯¢æ¨¡å¼ï¼‰
                                    'document_name': doc.metadata.get('document_name', ''),
                                    'page_number': doc.metadata.get('page_number', ''),
                                    'chunk_type': doc.metadata.get('chunk_type', ''),
                                    
                                    # æ–°å¢ï¼šå›¾ç‰‡å±•ç¤ºå¿…éœ€å­—æ®µï¼ˆå‰ç«¯éœ€è¦ï¼‰
                                    'image_path': image_doc.metadata.get('image_path', '') if hasattr(image_doc, 'metadata') and image_doc.metadata else '',
                                    'caption': image_doc.metadata.get('img_caption', []) if hasattr(image_doc, 'metadata') and image_doc.metadata else [],
                                    

                                })
                
                logger.info(f"ç­–ç•¥1é€šè¿‡é˜ˆå€¼æ£€æŸ¥çš„ç»“æœæ•°é‡: {len(results)}")
                
            except Exception as e:
                logger.error(f"ç­–ç•¥1æœç´¢å¤±è´¥: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # ç­–ç•¥2ï¼šè·¨æ¨¡æ€æœç´¢image chunksï¼ˆè§†è§‰ç‰¹å¾ç›¸ä¼¼åº¦ï¼‰
            logger.info("ç­–ç•¥2ï¼šè·¨æ¨¡æ€æœç´¢image chunksï¼ˆè§†è§‰ç‰¹å¾ç›¸ä¼¼åº¦ï¼‰")
            try:
                # ç­–ç•¥2ä½¿ç”¨è·¨æ¨¡æ€ç›¸ä¼¼åº¦é˜ˆå€¼
                cross_modal_threshold = getattr(self.config, 'cross_modal_similarity_threshold', 0.5)
                logger.info(f"ç­–ç•¥2ï¼šè·¨æ¨¡æ€é˜ˆå€¼è®¾ç½®ä¸º: {cross_modal_threshold}")
                
                # ä½¿ç”¨multimodal-embedding-one-peace-v1å°†æ–‡æœ¬æŸ¥è¯¢è½¬æ¢ä¸ºå¤šæ¨¡æ€å‘é‡
                logger.info("ç­–ç•¥2ï¼šä½¿ç”¨multimodal-embedding-one-peace-v1è¿›è¡Œè·¨æ¨¡æ€å‘é‡åŒ–")
                try:
                    from dashscope import MultiModalEmbedding
                    import dashscope
                    
                    # è®¾ç½®APIå¯†é’¥ - ä½¿ç”¨ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†å™¨
                    from config.api_key_manager import APIKeyManager
                    
                    # ä»é…ç½®ä¸­è·å–å¯†é’¥ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œç„¶åé€šè¿‡APIå¯†é’¥ç®¡ç†å™¨ç»Ÿä¸€å¤„ç†
                    config_api_key = getattr(self.config, 'api_key', None)
                    api_key = APIKeyManager.get_dashscope_api_key(config_api_key)
                    
                    if not api_key:
                        raise Exception("æœªæ‰¾åˆ°DashScope APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡MY_DASHSCOPE_API_KEY")
                    
                    dashscope.api_key = api_key
                    
                    # è°ƒç”¨multimodal-embedding APIï¼Œä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
                    image_embedding_model = getattr(self.config, 'image_embedding_model', 'multimodal-embedding-one-peace-v1')
                    
                    # å°†æ¨¡å‹åç§°ä¸­çš„è¿å­—ç¬¦è½¬æ¢ä¸ºä¸‹åˆ’çº¿ï¼Œä»¥åŒ¹é…MultiModalEmbedding.Modelsä¸­çš„å±æ€§å
                    model_name = image_embedding_model.replace('-', '_')
                    
                    result = MultiModalEmbedding.call(
                        model=model_name,
                        input=[{'text': query}]
                    )
                    
                    if result.status_code == 200:
                        # æå–æŸ¥è¯¢å‘é‡ - å…¼å®¹ä¸¤ç§è¾“å‡ºæ ¼å¼
                        if "embedding" in result.output:
                            query_embedding = result.output["embedding"]
                        elif "embeddings" in result.output and len(result.output["embeddings"]) > 0:
                            query_embedding = result.output["embeddings"][0]["embedding"]
                        else:
                            raise Exception(f"æ— æ³•è¯†åˆ«çš„è¾“å‡ºæ ¼å¼: {result.output}")
                        
                        logger.info(f"ç­–ç•¥2ï¼šè·¨æ¨¡æ€å‘é‡åŒ–æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(query_embedding)}")
                        
                        # éªŒè¯å‘é‡ç»´åº¦å…¼å®¹æ€§
                        if len(query_embedding) != 1536:
                            logger.warning(f"ç­–ç•¥2ï¼šæŸ¥è¯¢å‘é‡ç»´åº¦ä¸åŒ¹é…ï¼ŒæœŸæœ›1536ï¼Œå®é™…{len(query_embedding)}")
                            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè·³è¿‡æ­¤ç­–ç•¥
                            raise Exception("å‘é‡ç»´åº¦ä¸åŒ¹é…")
                        
                        # æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨FAISSåº•å±‚APIè¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
                        try:
                            logger.info("ç­–ç•¥2ï¼šä½¿ç”¨FAISSåº•å±‚APIè¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢")
                            
                            # è·å–FAISSç´¢å¼•
                            if hasattr(self.vector_store, 'index') and hasattr(self.vector_store.index, 'search'):
                                # å‡†å¤‡æŸ¥è¯¢å‘é‡
                                import numpy as np
                                query_vector = np.array([query_embedding], dtype=np.float32)
                                
                                # ä½¿ç”¨FAISSè¿›è¡Œå‘é‡æœç´¢
                                # æœç´¢æ‰€æœ‰imageç±»å‹çš„æ–‡æ¡£
                                search_k = min(max_results * 3, 100)  # æœç´¢æ›´å¤šå€™é€‰ç»“æœ
                                
                                # è·å–æ‰€æœ‰imageæ–‡æ¡£çš„ç´¢å¼•ä½ç½®
                                image_indices = []
                                docstore_dict = self.vector_store.docstore._dict
                                
                                for doc_id, doc in docstore_dict.items():
                                    metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                                    if metadata.get('chunk_type') == 'image':
                                        # æ‰¾åˆ°æ–‡æ¡£åœ¨FAISSç´¢å¼•ä¸­çš„ä½ç½®
                                        if hasattr(self.vector_store, 'index_to_docstore_id'):
                                            for idx, stored_doc_id in self.vector_store.index_to_docstore_id.items():
                                                if str(stored_doc_id) == str(doc_id):
                                                    image_indices.append(idx)
                                                    break
                                
                                logger.info(f"ç­–ç•¥2ï¼šæ‰¾åˆ° {len(image_indices)} ä¸ªimageæ–‡æ¡£çš„ç´¢å¼•ä½ç½®")
                                
                                if image_indices:
                                    # ä½¿ç”¨FAISSæœç´¢è¿™äº›ç‰¹å®šä½ç½®
                                    # ç”±äºFAISSçš„é™åˆ¶ï¼Œæˆ‘ä»¬éœ€è¦æœç´¢æ‰€æœ‰å‘é‡ç„¶åè¿‡æ»¤
                                    distances, indices = self.vector_store.index.search(query_vector, search_k)
                                    
                                    # è¿‡æ»¤å‡ºimageç±»å‹çš„æ–‡æ¡£
                                    image_candidates = []
                                    for i, idx in enumerate(indices[0]):
                                        if idx < len(self.vector_store.index_to_docstore_id):
                                            doc_id = self.vector_store.index_to_docstore_id[idx]
                                            doc = docstore_dict.get(doc_id)
                                            if doc and (hasattr(doc, 'metadata') and doc.metadata and 
                                                      doc.metadata.get('chunk_type') == 'image'):
                                                # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
                                                distance = distances[0][i]
                                                similarity_score = self._convert_l2_distance_to_similarity(distance)
                                                
                                                image_candidates.append({
                                                    'doc': doc,
                                                    'score': similarity_score,
                                                    'distance': distance,
                                                    'index': idx
                                                })
                                
                                logger.info(f"ç­–ç•¥2ï¼šFAISSæœç´¢è¿”å› {len(image_candidates)} ä¸ªimageå€™é€‰ç»“æœ")
                                
                                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                                image_candidates.sort(key=lambda x: x['score'], reverse=True)
                                
                                # è®°å½•ç­–ç•¥2æœç´¢ç»“æœæ•°é‡
                                if image_candidates:
                                    logger.info(f"ç­–ç•¥2ï¼šæ‰¾åˆ° {len(image_candidates)} ä¸ªimageå€™é€‰ç»“æœ")
                                
                                # å¤„ç†æœç´¢ç»“æœ
                                for candidate in image_candidates[:max_results]:
                                    score = candidate['score']
                                    

                                    
                                    # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                                    if score >= cross_modal_threshold:
                                        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ç»“æœä¸­ï¼ˆé¿å…é‡å¤ï¼‰
                                        doc_id = self._get_doc_id(candidate['doc'])
                                        if not any(r['doc'] == candidate['doc'] for r in results):
                                            results.append({
                                                'doc': candidate['doc'],
                                                'score': score * 0.8,  # è§†è§‰ç‰¹å¾ç›¸ä¼¼åº¦æƒé‡ç¨ä½
                                                'source': 'vector_search',
                                                'layer': 1,
                                                'search_method': 'cross_modal_similarity',
                                                'cross_modal_score': score,
                                                'query_embedding_dim': len(query_embedding),
                                                'faiss_distance': candidate['distance'],
                                                'faiss_index': candidate['index']
                                            })
                                
                                logger.info(f"ç­–ç•¥2é€šè¿‡é˜ˆå€¼æ£€æŸ¥çš„ç»“æœæ•°é‡: {len([r for r in results if r['search_method'] == 'cross_modal_similarity'])}")
                                
                            else:
                                logger.warning("ç­–ç•¥2ï¼šFAISSç´¢å¼•ä¸æ”¯æŒsearchæ–¹æ³•")
                                raise Exception("FAISSç´¢å¼•ä¸æ”¯æŒsearchæ–¹æ³•")
                                
                        except Exception as faiss_error:
                            logger.warning(f"ç­–ç•¥2 FAISSåº•å±‚æœç´¢å¤±è´¥: {faiss_error}")
                            logger.info("ç­–ç•¥2ï¼šé™çº§åˆ°ä¼ ç»Ÿæœç´¢æ–¹æ³•")
                            
                            # é™çº§ï¼šä½¿ç”¨ä¼ ç»Ÿæ–¹æ³• + filter
                            try:
                                image_candidates = self.vector_store.similarity_search(
                                    query, 
                                    k=max_results * 2,
                                    filter={'chunk_type': 'image'}
                                )
                                logger.info(f"ç­–ç•¥2é™çº§filteræœç´¢è¿”å› {len(image_candidates)} ä¸ªimageå€™é€‰ç»“æœ")
                                
                                # è®°å½•é™çº§æœç´¢ç»“æœæ•°é‡
                                if image_candidates:
                                    logger.info(f"ç­–ç•¥2é™çº§æœç´¢ï¼šæ‰¾åˆ° {len(image_candidates)} ä¸ªimageå€™é€‰ç»“æœ")
                                
                                # å¤„ç†imageæœç´¢ç»“æœ
                                for doc in image_candidates:
                                    score = getattr(doc, 'score', 0.5)
                                    if score >= cross_modal_threshold:
                                        doc_id = self._get_doc_id(doc)
                                        if not any(r['doc'] == doc for r in results):
                                            results.append({
                                                'doc': doc,
                                                'score': score * 0.6,  # é™çº§æœç´¢æƒé‡æ›´ä½
                                                'source': 'vector_search_fallback',
                                                'layer': 1,
                                                'search_method': 'traditional_similarity',
                                                'traditional_score': score
                                            })
                                
                                logger.info(f"ç­–ç•¥2é™çº§æœç´¢é€šè¿‡é˜ˆå€¼æ£€æŸ¥çš„ç»“æœæ•°é‡: {len([r for r in results if r['search_method'] == 'traditional_similarity'])}")
                            except Exception as fallback_error:
                                logger.error(f"ç­–ç•¥2é™çº§æœç´¢å¤±è´¥: {fallback_error}")
                        
                    else:
                        raise Exception(f"multimodal-embedding-one-peace-v1 APIè°ƒç”¨å¤±è´¥: {result}")
                        
                except Exception as multimodal_error:
                    logger.error(f"ç­–ç•¥2ï¼šmultimodal-embedding-one-peace-v1è°ƒç”¨å¤±è´¥: {multimodal_error}")
                    logger.info("ç­–ç•¥2ï¼šé™çº§åˆ°ä¼ ç»Ÿæœç´¢æ–¹æ³•")
                    
                    # é™çº§åˆ°ä¼ ç»Ÿæ–¹æ³•ï¼šä½¿ç”¨text-embedding-v1 + filter
                    try:
                        image_candidates = self.vector_store.similarity_search(
                            query, 
                            k=max_results * 2,
                            filter={'chunk_type': 'image'}
                        )
                        logger.info(f"ç­–ç•¥2é™çº§filteræœç´¢è¿”å› {len(image_candidates)} ä¸ªimageå€™é€‰ç»“æœ")
                        
                        # è®°å½•é™çº§æœç´¢ç»“æœæ•°é‡
                        if image_candidates:
                            logger.info(f"ç­–ç•¥2é™çº§æœç´¢ï¼šæ‰¾åˆ° {len(image_candidates)} ä¸ªimageå€™é€‰ç»“æœ")
                        
                        # å¤„ç†imageæœç´¢ç»“æœ
                        for doc in image_candidates:
                            score = getattr(doc, 'score', 0.5)
                            if score >= cross_modal_threshold:
                                doc_id = self._get_doc_id(doc)
                                if not any(r['doc'] == doc for r in results):
                                    results.append({
                                        'doc': doc,
                                        'score': score * 0.6,  # é™çº§æœç´¢æƒé‡æ›´ä½
                                        'source': 'vector_search_fallback',
                                        'layer': 1,
                                        'search_method': 'traditional_similarity',
                                        'traditional_score': score
                                    })
                            
                            logger.info(f"ç­–ç•¥2é™çº§æœç´¢é€šè¿‡é˜ˆå€¼æ£€æŸ¥çš„ç»“æœæ•°é‡: {len([r for r in results if r['search_method'] == 'traditional_similarity'])}")
                    except Exception as traditional_error:
                        logger.error(f"ç­–ç•¥2ä¼ ç»Ÿæœç´¢ä¹Ÿå¤±è´¥: {traditional_error}")
                
                except Exception as e:
                    logger.error(f"ç­–ç•¥2å®Œå…¨å¤±è´¥: {e}")
                    logger.info("ç­–ç•¥2ï¼šæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè·³è¿‡æ­¤ç­–ç•¥")
            except Exception as strategy2_error:
                logger.error(f"ç­–ç•¥2æ•´ä½“æ‰§è¡Œå¤±è´¥: {strategy2_error}")
                logger.info("ç­–ç•¥2ï¼šè·³è¿‡æ­¤ç­–ç•¥")
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            final_results = results[:max_results]
            
            logger.info(f"ç¬¬ä¸€å±‚å‘é‡æœç´¢å®Œæˆï¼Œæ€»ç»“æœæ•°é‡: {len(final_results)}")
            logger.info(f"è¯­ä¹‰ç›¸ä¼¼åº¦ç»“æœ: {len([r for r in final_results if r['search_method'] == 'semantic_similarity'])}")
            logger.info(f"è·¨æ¨¡æ€ç›¸ä¼¼åº¦ç»“æœ: {len([r for r in final_results if r['search_method'] == 'cross_modal_similarity'])}")
            logger.info(f"è·¨æ¨¡æ€é™çº§ç»“æœ: {len([r for r in final_results if r['search_method'] == 'cross_modal_similarity_fallback'])}")
            logger.info(f"ä¼ ç»Ÿç›¸ä¼¼åº¦ç»“æœ: {len([r for r in final_results if r['search_method'] == 'traditional_similarity'])}")
            
            return final_results
            
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            logger.info("å‘é‡æœç´¢å¤±è´¥ï¼Œé™çº§åˆ°å…³é”®è¯æœç´¢ä½œä¸ºç¬¬ä¸€å±‚å¬å›")
            
            # é™çº§ç­–ç•¥ï¼šä½¿ç”¨å…³é”®è¯æœç´¢ä½œä¸ºç¬¬ä¸€å±‚å¬å›
            try:
                keyword_fallback = self._keyword_search(query, max_results)
                logger.info(f"ç¬¬ä¸€å±‚é™çº§å…³é”®è¯æœç´¢è¿”å› {len(keyword_fallback)} ä¸ªç»“æœ")
                
                # ä¸ºé™çº§ç»“æœæ·»åŠ æ ‡è¯†
                for result in keyword_fallback:
                    result['source'] = 'vector_search_fallback'
                    result['layer'] = 1
                    result['search_method'] = 'keyword_fallback'
                    result['fallback_reason'] = str(e)
                
                return keyword_fallback
            except Exception as fallback_error:
                logger.error(f"ç¬¬ä¸€å±‚é™çº§å…³é”®è¯æœç´¢ä¹Ÿå¤±è´¥: {fallback_error}")
                return []
    
    def _keyword_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬äºŒå±‚ï¼šè¯­ä¹‰å…³é”®è¯åŒ¹é…
        
        åœ¨å›¾ç‰‡å…ƒæ•°æ®çš„æ–‡æœ¬å­—æ®µä¸­è¿›è¡Œå…³é”®è¯åŒ¹é…ï¼Œ
        åŒ…æ‹¬enhanced_descriptionã€img_captionã€image_typeç­‰
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        if not getattr(self.config, 'enable_keyword_search', True):
            logger.info("å…³é”®è¯æœç´¢æœªå¯ç”¨")
            return results
        
        try:
            # ä½¿ç”¨jiebaåˆ†è¯æå–æŸ¥è¯¢å…³é”®è¯ï¼Œå‚è€ƒTextEngineçš„å®ç°
            query_keywords = self._extract_semantic_keywords_from_query(query)
            query_words = set(query_keywords)
            logger.info(f"ç¬¬äºŒå±‚å…³é”®è¯æœç´¢ - æŸ¥è¯¢è¯: {query_words}")
            
            for doc in self.image_docs:
                if not hasattr(doc, 'metadata') or not doc.metadata:
                    continue
                
                score = 0.0
                metadata = doc.metadata
                
                # å›¾ç‰‡æ ‡é¢˜åŒ¹é… (æƒé‡æœ€é«˜)
                if 'img_caption' in metadata and metadata['img_caption']:
                    caption_text = ' '.join(metadata['img_caption']).lower()
                    caption_score = self._calculate_text_match_score(query_words, caption_text, 0.8)
                    score += caption_score
                    if caption_score > 0:
                        logger.debug(f"æ ‡é¢˜åŒ¹é…å¾—åˆ†: {caption_score}, æ ‡é¢˜: {metadata['img_caption']}")
                
                # å¢å¼ºæè¿°åŒ¹é… (æƒé‡æ¬¡ä¹‹)
                if 'enhanced_description' in metadata and metadata['enhanced_description']:
                    enhanced_text = metadata['enhanced_description'].lower()
                    enhanced_score = self._calculate_text_match_score(query_words, enhanced_text, 0.7)
                    score += enhanced_score
                    if enhanced_score > 0:
                        logger.debug(f"å¢å¼ºæè¿°åŒ¹é…å¾—åˆ†: {enhanced_score}")
                
                # å›¾ç‰‡ç±»å‹åŒ¹é…
                if 'image_type' in metadata and metadata['image_type']:
                    image_type = metadata['image_type'].lower()
                    type_score = self._calculate_text_match_score(query_words, image_type, 0.6)
                    score += type_score
                    if type_score > 0:
                        logger.debug(f"å›¾ç‰‡ç±»å‹åŒ¹é…å¾—åˆ†: {type_score}, ç±»å‹: {metadata['image_type']}")
                
                # æ–‡æ¡£åç§°åŒ¹é…
                if 'document_name' in metadata and metadata['document_name']:
                    doc_name = metadata['document_name'].lower()
                    doc_name_score = self._calculate_text_match_score(query_words, doc_name, 0.5)
                    score += doc_name_score
                    if doc_name_score > 0:
                        logger.debug(f"æ–‡æ¡£åç§°åŒ¹é…å¾—åˆ†: {doc_name_score}")
                
                # å…¶ä»–å…ƒæ•°æ®åŒ¹é…
                for key in ['title', 'description', 'content']:
                    if key in metadata and metadata[key]:
                        text = str(metadata[key]).lower()
                        other_score = self._calculate_text_match_score(query_words, text, 0.4)
                        score += other_score
                
                if score > 0:
                    results.append({
                        'doc': doc,
                        'score': score,
                        'source': 'keyword_search',
                        'layer': 2,
                        'search_method': 'metadata_text_matching'
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            logger.info(f"ç¬¬äºŒå±‚å…³é”®è¯æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç»“æœ")
            return results[:max_results]
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            return []
    
    def _hybrid_search(self, query: str, max_results: int, vector_candidates: List[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸‰å±‚ï¼šæ··åˆå¬å›ç­–ç•¥
        
        ç»“åˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢çš„ç»“æœï¼Œ
        é€šè¿‡åŠ æƒèåˆæå‡å¬å›è´¨é‡
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :param vector_candidates: ç¬¬ä¸€å±‚çš„å‘é‡æœç´¢ç»“æœï¼ˆé¿å…é‡å¤è°ƒç”¨ï¼‰
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ç¬¬ä¸‰å±‚æ··åˆå¬å› - å¼€å§‹èåˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢ç»“æœ")
            
            # ä½¿ç”¨ä¼ å…¥çš„å‘é‡æœç´¢ç»“æœï¼Œé¿å…é‡å¤è°ƒç”¨ç¬¬ä¸€å±‚
            if vector_candidates is None:
                logger.info("æœªä¼ å…¥å‘é‡æœç´¢ç»“æœï¼Œé‡æ–°è°ƒç”¨ç¬¬ä¸€å±‚æœç´¢")
                vector_candidates = self._vector_search(query, max_results // 2)
            else:
                logger.info(f"ä½¿ç”¨ä¼ å…¥çš„å‘é‡æœç´¢ç»“æœï¼Œæ•°é‡: {len(vector_candidates)}")
            
            keyword_candidates = self._keyword_search(query, max_results // 2)
            
            logger.info(f"å‘é‡æœç´¢å€™é€‰: {len(vector_candidates)}, å…³é”®è¯æœç´¢å€™é€‰: {len(keyword_candidates)}")
            
            # èåˆç­–ç•¥ï¼šåŠ æƒå¹³å‡ + å¤šæ ·æ€§ä¿è¯
            all_candidates = {}
            
            # å¤„ç†å‘é‡æœç´¢ç»“æœ (æƒé‡0.7)
            for candidate in vector_candidates:
                doc_id = self._get_doc_id(candidate['doc'])
                if doc_id not in all_candidates:
                    all_candidates[doc_id] = candidate.copy()
                    all_candidates[doc_id]['hybrid_score'] = candidate['score'] * 0.7  # å‘é‡æœç´¢æƒé‡
                    all_candidates[doc_id]['vector_score'] = candidate['score']
                    all_candidates[doc_id]['keyword_score'] = 0.0
                else:
                    # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°å‘é‡åˆ†æ•°
                    all_candidates[doc_id]['vector_score'] = candidate['score']
                    all_candidates[doc_id]['hybrid_score'] = max(
                        all_candidates[doc_id]['hybrid_score'],
                        candidate['score'] * 0.7
                    )
            
            # å¤„ç†å…³é”®è¯æœç´¢ç»“æœ (æƒé‡0.8)
            for candidate in keyword_candidates:
                doc_id = self._get_doc_id(candidate['doc'])
                if doc_id not in all_candidates:
                    all_candidates[doc_id] = candidate.copy()
                    all_candidates[doc_id]['hybrid_score'] = candidate['score'] * 0.8  # å…³é”®è¯æœç´¢æƒé‡
                    all_candidates[doc_id]['keyword_score'] = candidate['score']
                    all_candidates[doc_id]['vector_score'] = 0.0
                else:
                    # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°å…³é”®è¯åˆ†æ•°å¹¶é‡æ–°è®¡ç®—æ··åˆåˆ†æ•°
                    all_candidates[doc_id]['keyword_score'] = candidate['score']
                    # æ··åˆåˆ†æ•° = å‘é‡åˆ†æ•°*0.7 + å…³é”®è¯åˆ†æ•°*0.8
                    all_candidates[doc_id]['hybrid_score'] = (
                        all_candidates[doc_id]['vector_score'] * 0.7 + 
                        candidate['score'] * 0.8
                    )
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ··åˆåˆ†æ•°æ’åº
            results = list(all_candidates.values())
            results.sort(key=lambda x: x['hybrid_score'], reverse=True)
            
            # æ·»åŠ æ··åˆæœç´¢æ ‡è¯†
            for result in results:
                result['source'] = 'hybrid_search'
                result['layer'] = 3
                result['search_method'] = 'vector_keyword_fusion'
                # è®°å½•èåˆè¯¦æƒ…
                result['fusion_details'] = {
                    'vector_weight': 0.7,
                    'keyword_weight': 0.8,
                    'vector_score': result.get('vector_score', 0.0),
                    'keyword_score': result.get('keyword_score', 0.0)
                }
                
                # å¢å¼ºï¼šç¡®ä¿æ··åˆæœç´¢ç»“æœä¹Ÿæœ‰å®Œæ•´çš„metadataå­—æ®µ
                if 'document_name' not in result:
                    doc = result.get('doc')
                    if doc and hasattr(doc, 'metadata') and doc.metadata:
                        result['document_name'] = doc.metadata.get('document_name', '')
                        result['page_number'] = doc.metadata.get('page_number', '')
                        result['chunk_type'] = doc.metadata.get('chunk_type', '')
                        result['enhanced_description'] = doc.metadata.get('enhanced_description', '')


            
            logger.info(f"ç¬¬ä¸‰å±‚æ··åˆå¬å›å®Œæˆï¼Œèåˆåç»“æœæ•°é‡: {len(results)}")
            return results[:max_results]
            
        except Exception as e:
            logger.error(f"æ··åˆæœç´¢å¤±è´¥: {e}")
            return []
    
    def _fuzzy_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬å››å±‚ï¼šæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…
        
        åœ¨å›¾ç‰‡å…ƒæ•°æ®ä¸­è¿›è¡Œæ¨¡ç³ŠåŒ¹é…ï¼Œå¤„ç†æ‹¼å†™é”™è¯¯ã€åŒä¹‰è¯ç­‰ï¼Œ
        æå‡å¬å›ç‡
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            # ä½¿ç”¨jiebaåˆ†è¯æå–æŸ¥è¯¢å…³é”®è¯ï¼Œå‚è€ƒTextEngineçš„å®ç°
            query_keywords = self._extract_semantic_keywords_from_query(query)
            query_words = query_keywords
            logger.info(f"ç¬¬å››å±‚æ¨¡ç³ŠåŒ¹é… - æŸ¥è¯¢è¯: {query_words}")
            
            for doc in self.image_docs:
                if not hasattr(doc, 'metadata') or not doc.metadata:
                    continue
                
                score = 0.0
                metadata = doc.metadata
                
                # éå†æ‰€æœ‰æ–‡æœ¬å­—æ®µè¿›è¡Œæ¨¡ç³ŠåŒ¹é…
                for key, text in metadata.items():
                    if isinstance(text, str) and text:
                        text_lower = text.lower()
                        
                        # 1. å®Œå…¨åŒ…å«åŒ¹é… (æƒé‡æœ€é«˜)
                        if query.lower() in text_lower:
                             score += 0.6
                             logger.debug(f"å®Œå…¨åŒ…å«åŒ¹é…: {query.lower()} in {text_lower[:50]}...")
                        
                        # 2. å•è¯çº§åŒ¹é…ï¼ˆä½¿ç”¨jiebaåˆ†è¯ç»“æœï¼‰
                        text_keywords = self._extract_semantic_keywords_from_text(text_lower, set())
                        text_words_set = set(text_keywords)
                        common_words = set(query_words) & text_words_set
                        if common_words:
                            word_score = len(common_words) * 0.3
                            score += word_score
                            logger.debug(f"å•è¯åŒ¹é…: {common_words}, å¾—åˆ†: {word_score}")
                        
                        # 3. å­—ç¬¦çº§ç›¸ä¼¼åº¦ (ç”¨äºå¤„ç†æ‹¼å†™é”™è¯¯)
                        if len(query.lower()) > 3 and len(text_lower) > 3:
                            similarity = self._calculate_string_similarity(query.lower(), text_lower)
                            char_score = similarity * 0.4
                            score += char_score
                            if similarity > 0.5:
                                logger.debug(f"å­—ç¬¦ç›¸ä¼¼åº¦: {similarity:.3f}, å¾—åˆ†: {char_score:.3f}")
                        
                        # 4. éƒ¨åˆ†å•è¯åŒ¹é… (å¤„ç†ç¼©å†™ã€å˜ä½“ç­‰)
                        for query_word in query_words:
                            if len(query_word) > 2:  # åªå¤„ç†é•¿åº¦å¤§äº2çš„è¯
                                for text_word in text_keywords:
                                    if len(text_word) > 2:
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æˆ–ç›¸ä¼¼
                                        if query_word in text_word or text_word in query_word:
                                            score += 0.2
                                        elif self._calculate_string_similarity(query_word, text_word) > 0.7:
                                            score += 0.3
                
                # åº”ç”¨æ¨¡ç³ŠåŒ¹é…çš„è¾ƒä½é˜ˆå€¼
                if score > 0.3:
                    results.append({
                        'doc': doc,
                        'score': score,
                        'source': 'fuzzy_search',
                        'layer': 4,
                        'search_method': 'fuzzy_text_matching'
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            logger.info(f"ç¬¬å››å±‚æ¨¡ç³ŠåŒ¹é…å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªæ¨¡ç³ŠåŒ¹é…ç»“æœ")
            return results[:max_results]
            
        except Exception as e:
            logger.error(f"æ¨¡ç³Šæœç´¢å¤±è´¥: {e}")
            return []
    
    def _expansion_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬äº”å±‚ï¼šæŸ¥è¯¢æ‰©å±•å¬å›
        
        é€šè¿‡æŸ¥è¯¢æ‰©å±•å¢åŠ å¬å›èŒƒå›´ï¼Œå¤„ç†åŒä¹‰è¯ã€ç›¸å…³æ¦‚å¿µç­‰ï¼Œ
        æå‡é•¿å°¾æŸ¥è¯¢çš„å¬å›ç‡
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ç¬¬äº”å±‚æŸ¥è¯¢æ‰©å±• - åŸå§‹æŸ¥è¯¢: {query}")
            
            # æŸ¥è¯¢æ‰©å±•ç­–ç•¥
            expanded_queries = self._expand_query(query)
            logger.info(f"æ‰©å±•æŸ¥è¯¢åˆ—è¡¨: {expanded_queries}")
            
            if not expanded_queries:
                logger.info("æ²¡æœ‰ç”Ÿæˆæ‰©å±•æŸ¥è¯¢ï¼Œè·³è¿‡ç¬¬äº”å±‚")
                return results
            
            # ä¸ºæ¯ä¸ªæ‰©å±•æŸ¥è¯¢åˆ†é…ç»“æœæ•°é‡
            results_per_query = max(1, max_results // len(expanded_queries))
            
            for expanded_query in expanded_queries:
                logger.debug(f"å¤„ç†æ‰©å±•æŸ¥è¯¢: {expanded_query}")
                
                # ä½¿ç”¨æ‰©å±•æŸ¥è¯¢è¿›è¡Œå…³é”®è¯æœç´¢
                expansion_results = self._keyword_search(expanded_query, results_per_query)
                
                for result in expansion_results:
                    # é™ä½æ‰©å±•æŸ¥è¯¢çš„åˆ†æ•°æƒé‡ (0.7)
                    original_score = result['score']
                    result['score'] *= 0.7
                    
                    # æ›´æ–°ç»“æœä¿¡æ¯
                    result['source'] = 'expansion_search'
                    result['layer'] = 5
                    result['search_method'] = 'query_expansion'
                    result['expanded_query'] = expanded_query
                    result['original_score'] = original_score
                    result['expansion_penalty'] = 0.7
                    
                    logger.debug(f"æ‰©å±•æŸ¥è¯¢ç»“æœ: {expanded_query} -> åˆ†æ•°: {original_score:.3f} -> {result['score']:.3f}")
                
                results.extend(expansion_results)
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            logger.info(f"ç¬¬äº”å±‚æŸ¥è¯¢æ‰©å±•å®Œæˆï¼Œæ‰©å±•åç»“æœæ•°é‡: {len(results)}")
            return results[:max_results]
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ‰©å±•æœç´¢å¤±è´¥: {e}")
            return []
    
    def _enhance_reranked_results(self, reranked_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¢å¼ºreranked_resultsï¼šä»doc.metadataä¸­æå–å…³é”®å­—æ®µåˆ°é¡¶å±‚
        
        :param reranked_results: é‡æ’åºåçš„ç»“æœ
        :return: å¢å¼ºåçš„ç»“æœ
        """
        enhanced_results = []
        
        for result in reranked_results:
            if isinstance(result, dict) and 'doc' in result:
                # åˆ›å»ºå¢å¼ºåçš„ç»“æœ
                enhanced_result = result.copy()
                
                # ä»doc.metadataä¸­æå–å…³é”®å­—æ®µ
                doc = result['doc']
                if hasattr(doc, 'metadata') and doc.metadata:
                    metadata = doc.metadata
                    
                    # æå–åŸºæœ¬ä¿¡æ¯
                    enhanced_result['document_name'] = metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£')
                    enhanced_result['page_number'] = metadata.get('page_number', 'æœªçŸ¥é¡µ')
                    enhanced_result['chunk_type'] = metadata.get('chunk_type', 'å›¾ç‰‡')
                    enhanced_result['image_path'] = metadata.get('image_path', '')
                    
                    # æå–å›¾ç‰‡ç›¸å…³å­—æ®µ
                    img_caption = metadata.get('img_caption', [])
                    if isinstance(img_caption, list):
                        enhanced_result['caption'] = img_caption
                    else:
                        enhanced_result['caption'] = [str(img_caption)] if img_caption else ['æ— æ ‡é¢˜']
                    
                    # æå–å¢å¼ºæè¿°
                    enhanced_result['enhanced_description'] = metadata.get('enhanced_description', '')
                    
                    # æ„å»ºLLMä¸Šä¸‹æ–‡
                    llm_context_parts = []
                    if metadata.get('enhanced_description'):
                        llm_context_parts.append(metadata['enhanced_description'])
                    
                    if hasattr(doc, 'page_content') and doc.page_content:
                        llm_context_parts.append(doc.page_content)
                    
                    if not llm_context_parts and img_caption:
                        llm_context_parts.append(' '.join(img_caption))
                    
                    enhanced_result['llm_context'] = "\n\n".join(llm_context_parts) if llm_context_parts else "æ— å¯ç”¨å†…å®¹"
                    
                    # åªä¿ç•™å…³é”®è°ƒè¯•ä¿¡æ¯
                    logger.info(f"æ„å»ºllm_context: {len(enhanced_result['llm_context'])}å­—ç¬¦")
                    
                    # ç”Ÿæˆformatted_source
                    try:
                        from ..api.v2_routes import _format_source_display
                        enhanced_result['formatted_source'] = _format_source_display(
                            enhanced_result['document_name'],
                            enhanced_result['llm_context'],
                            enhanced_result['page_number'],
                            enhanced_result['chunk_type']
                        )
                    except ImportError:
                        enhanced_result['formatted_source'] = f"{enhanced_result['document_name']} - ç¬¬{enhanced_result['page_number']}é¡µ"
                
                enhanced_results.append(enhanced_result)
            else:
                # å¦‚æœä¸æ˜¯æ ‡å‡†æ ¼å¼ï¼Œä¿æŒåŸæ ·
                enhanced_results.append(result)
        
        logger.info(f"å¢å¼ºå®Œæˆï¼šè¾“å…¥ {len(reranked_results)} ä¸ªç»“æœï¼Œè¾“å‡º {len(enhanced_results)} ä¸ªç»“æœ")
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥å¢å¼ºåçš„æ•°æ®
        logger.info("ğŸ” å¢å¼ºåçš„reranked_resultsæ•°æ®:")
        for i, result in enumerate(enhanced_results):  #
            # logger.info(f"enhanced_reranked_results[{i}]:")
            # logger.info(f"  - ç±»å‹: {type(result)}")
            if isinstance(result, dict):
                # logger.info(f"  - æ‰€æœ‰å­—æ®µ: {list(result.keys())}")
                logger.info(f"  - document_name: {result.get('document_name', 'N/A')}")
                # logger.info(f"  - page_number: {result.get('page_number', 'N/A')}")
                # logger.info(f"  - chunk_type: {result.get('chunk_type', 'N/A')}")
                # logger.info(f"  - image_path: {result.get('image_path', 'N/A')}")
                logger.info(f"  - caption: {result.get('caption', 'N/A')}")
                # logger.info(f"  - enhanced_description: {result.get('enhanced_description', 'N/A')}")
                # logger.info(f"  - llm_context: {result.get('llm_context', 'N/A')}")
                # logger.info(f"  - formatted_source: {result.get('formatted_source', 'N/A')}")
            else:
                logger.info(f"  - éå­—å…¸ç±»å‹: {result}")
        
        return enhanced_results
    
    def _expand_query(self, query: str) -> List[str]:
        """
        æŸ¥è¯¢æ‰©å±•
        
        åŸºäºé¢†åŸŸçŸ¥è¯†å’Œè¯­ä¹‰ç›¸å…³æ€§è¿›è¡Œæ™ºèƒ½æŸ¥è¯¢æ‰©å±•ï¼Œ
        æå‡é•¿å°¾æŸ¥è¯¢çš„å¬å›æ•ˆæœ
        
        :param query: åŸå§‹æŸ¥è¯¢
        :return: æ‰©å±•åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        expanded_queries = []
        query_lower = query.lower()
        
        # 1. åŒä¹‰è¯æ‰©å±•
        synonyms = {
            # å›¾è¡¨ç›¸å…³
            'å›¾': ['å›¾ç‰‡', 'å›¾è¡¨', 'å›¾åƒ', 'figure', 'chart', 'graph'],
            'è¡¨': ['è¡¨æ ¼', 'å›¾è¡¨', 'table', 'chart'],
            'å›¾è¡¨': ['å›¾', 'è¡¨', 'å›¾åƒ', 'figure', 'chart'],
            
            # æ•°æ®ç±»å‹
            'æ•°æ®': ['æ•°æ®', 'ç»Ÿè®¡', 'æ•°å­—', 'data', 'statistics'],
            'ç»Ÿè®¡': ['ç»Ÿè®¡', 'æ•°æ®', 'æ•°å­—', 'statistics', 'data'],
            'æ•°å­—': ['æ•°å­—', 'æ•°æ®', 'ç»Ÿè®¡', 'number', 'data'],
            
            # åˆ†æç›¸å…³
            'åˆ†æ': ['åˆ†æ', 'ç ”ç©¶', 'åˆ†ææŠ¥å‘Š', 'analysis', 'research'],
            'ç ”ç©¶': ['ç ”ç©¶', 'åˆ†æ', 'ç ”ç©¶æŠ¥å‘Š', 'research', 'analysis'],
            'æŠ¥å‘Š': ['æŠ¥å‘Š', 'æ–‡æ¡£', 'æŠ¥å‘Šä¹¦', 'report', 'document'],
            
            # è´¢åŠ¡ç›¸å…³
            'åˆ©æ¶¦': ['åˆ©æ¶¦', 'å‡€åˆ©æ¶¦', 'æ”¶å…¥', 'è´¢åŠ¡', 'ä¸šç»©', 'profit'],
            'æ”¶å…¥': ['æ”¶å…¥', 'è¥æ”¶', 'é”€å”®é¢', 'revenue', 'sales'],
            'è´¢åŠ¡': ['è´¢åŠ¡', 'è´¢åŠ¡æ•°æ®', 'è´¢åŠ¡æŠ¥è¡¨', 'financial'],
            'ä¸šç»©': ['ä¸šç»©', 'è¡¨ç°', 'è´¢åŠ¡è¡¨ç°', 'performance'],
            
            # æ—¶é—´ç›¸å…³
            'å­£åº¦': ['å­£åº¦', 'Q1', 'Q2', 'Q3', 'Q4', 'quarter'],
            'å¹´åº¦': ['å¹´åº¦', 'å¹´', 'å…¨å¹´', 'annual', 'year'],
            'æœˆåº¦': ['æœˆåº¦', 'æœˆ', 'monthly', 'month']
        }
        
        # åº”ç”¨åŒä¹‰è¯æ‰©å±•
        for word, syns in synonyms.items():
            if word in query_lower:
                for syn in syns:
                    expanded_query = query_lower.replace(word, syn)
                    if expanded_query != query_lower and expanded_query not in expanded_queries:
                        expanded_queries.append(expanded_query)
        
        # 2. é¢†åŸŸç›¸å…³æ¦‚å¿µæ‰©å±•
        domain_concepts = {
            # åŠå¯¼ä½“è¡Œä¸š
            'ä¸­èŠ¯å›½é™…': ['èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'IC', 'æ™¶åœ†', 'åˆ¶é€ '],
            'èŠ¯ç‰‡': ['åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'IC', 'æ™¶åœ†', 'åˆ¶é€ ', 'ä¸­èŠ¯å›½é™…'],
            'åŠå¯¼ä½“': ['èŠ¯ç‰‡', 'é›†æˆç”µè·¯', 'IC', 'æ™¶åœ†', 'åˆ¶é€ ', 'ä¸­èŠ¯å›½é™…'],
            
            # è´¢åŠ¡æŒ‡æ ‡
            'å‡€åˆ©æ¶¦': ['åˆ©æ¶¦', 'æ”¶å…¥', 'è´¢åŠ¡', 'ä¸šç»©', 'ç›ˆåˆ©', 'æ”¶ç›Š'],
            'è¥æ”¶': ['æ”¶å…¥', 'é”€å”®é¢', 'è´¢åŠ¡', 'ä¸šç»©', 'revenue'],
            'æ¯›åˆ©ç‡': ['åˆ©æ¶¦', 'æˆæœ¬', 'è´¢åŠ¡', 'ç›ˆåˆ©èƒ½åŠ›', 'margin'],
            
            # æŠ€æœ¯æŒ‡æ ‡
            'è‰¯ç‡': ['è´¨é‡', 'åˆæ ¼ç‡', 'æŠ€æœ¯', 'åˆ¶é€ ', 'yield'],
            'äº§èƒ½': ['äº§é‡', 'ç”Ÿäº§èƒ½åŠ›', 'åˆ¶é€ ', 'æŠ€æœ¯', 'capacity'],
            'å·¥è‰º': ['æŠ€æœ¯', 'åˆ¶ç¨‹', 'åˆ¶é€ ', 'å·¥è‰ºæ°´å¹³', 'process']
        }
        
        # åº”ç”¨é¢†åŸŸæ¦‚å¿µæ‰©å±•
        for concept, related in domain_concepts.items():
            if concept in query:
                for related_concept in related:
                    if related_concept not in query_lower:
                        expanded_queries.append(related_concept)
        
        # 3. æŸ¥è¯¢ç»“æ„æ‰©å±•
        # å¦‚æœæŸ¥è¯¢åŒ…å«"ä¸­èŠ¯å›½é™…"ï¼Œæ·»åŠ "ä¸­èŠ¯å›½é™…"ä½œä¸ºç‹¬ç«‹æŸ¥è¯¢
        if 'ä¸­èŠ¯å›½é™…' in query and 'ä¸­èŠ¯å›½é™…' not in expanded_queries:
            expanded_queries.append('ä¸­èŠ¯å›½é™…')
        
        # 4. æ—¶é—´èŒƒå›´æ‰©å±•
        time_patterns = {
            r'(\d{4})å¹´': [r'\1å¹´Q1', r'\1å¹´Q2', r'\1å¹´Q3', r'\1å¹´Q4'],
            r'Q(\d)': [r'Q\1å­£åº¦', r'\1å­£åº¦'],
            r'(\d{4})å¹´(\d{1,2})æœˆ': [r'\1å¹´\2æœˆ', r'\1å¹´\2å­£åº¦']
        }
        
        import re
        for pattern, expansions in time_patterns.items():
            match = re.search(pattern, query)
            if match:
                for expansion in expansions:
                    expanded_query = re.sub(pattern, expansion, query)
                    if expanded_query != query and expanded_query not in expanded_queries:
                        expanded_queries.append(expanded_query)
        
        # 5. å»é‡å’Œé™åˆ¶æ•°é‡
        unique_expansions = list(set(expanded_queries))
        logger.info(f"æŸ¥è¯¢æ‰©å±•ç”Ÿæˆ {len(unique_expansions)} ä¸ªæ‰©å±•æŸ¥è¯¢")
        
        # é™åˆ¶æ‰©å±•æŸ¥è¯¢æ•°é‡ï¼Œä¼˜å…ˆä¿ç•™æœ€ç›¸å…³çš„
        return unique_expansions[:8]  # å¢åŠ æ‰©å±•æŸ¥è¯¢æ•°é‡ä¸Šé™
    
    def _calculate_text_match_score(self, query_words: Set[str], text: str, base_score: float) -> float:
        """
        è®¡ç®—æ–‡æœ¬åŒ¹é…åˆ†æ•° - å‚è€ƒTextEngineçš„å®ç°ï¼Œä½¿ç”¨jiebaåˆ†è¯å’ŒJaccardç›¸ä¼¼åº¦
        
        :param query_words: æŸ¥è¯¢è¯é›†åˆ
        :param text: ç›®æ ‡æ–‡æœ¬
        :param base_score: åŸºç¡€åˆ†æ•°
        :return: åŒ¹é…åˆ†æ•°
        """
        if not text or not query_words:
            return 0.0
        
        try:
            # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯ï¼Œå‚è€ƒTextEngineçš„å®ç°
            import jieba
            import jieba.analyse
            
            # åœç”¨è¯åˆ—è¡¨ï¼ˆä¸TextEngineä¿æŒä¸€è‡´ï¼‰
            stop_words = {
                'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å¦‚æœ', 'é‚£ä¹ˆ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•',
                'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'ä¸€ä¸ª', 'ä¸€äº›', 'å¯ä»¥', 'åº”è¯¥', 'èƒ½å¤Ÿ', 'éœ€è¦', 'å¿…é¡»', 'å¯èƒ½', 'ä¹Ÿè®¸',
                'å¤§æ¦‚', 'å¤§çº¦', 'å·¦å³', 'æ ¹æ®', 'æ˜¾ç¤º', 'è¡¨æ˜', 'è¯´æ˜', 'æŒ‡å‡º', 'æåˆ°', 'åŒ…æ‹¬', 'æ¶‰åŠ', 'å…³äº', 'å¯¹äº',
                'å‘¢', 'å—', 'å•Š', 'å§', 'äº†', 'ç€', 'è¿‡', 'æ¥', 'å»', 'ä¸Š', 'ä¸‹', 'é‡Œ', 'å¤–', 'å‰', 'å', 'å·¦', 'å³'
            }
            
            # æå–æ–‡æœ¬å…³é”®è¯
            text_keywords = self._extract_semantic_keywords_from_text(text, stop_words)
            
            if not text_keywords:
                logger.debug(f"æ–‡æœ¬å…³é”®è¯æå–å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†è¯")
                # é™çº§åˆ°åŸºæœ¬åˆ†è¯
                text_words_set = set(text.lower().split())
            else:
                text_words_set = set(text_keywords)
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦ï¼ˆä¸TextEngineä¿æŒä¸€è‡´ï¼‰
            intersection = query_words.intersection(text_words_set)
            union = query_words.union(text_words_set)
            
            if union:
                jaccard_score = len(intersection) / len(union)
                logger.debug(f"Jaccardç›¸ä¼¼åº¦è®¡ç®—: æŸ¥è¯¢è¯={query_words}, æ–‡æœ¬è¯={text_words_set}, äº¤é›†={intersection}, å¹¶é›†={union}, ç›¸ä¼¼åº¦={jaccard_score:.3f}")
                
                # åº”ç”¨åŸºç¡€åˆ†æ•°æƒé‡
                final_score = base_score * jaccard_score
                return final_score
            else:
                return 0.0
                
        except Exception as e:
            logger.warning(f"jiebaåˆ†è¯å¤±è´¥ï¼Œé™çº§åˆ°åŸºæœ¬æ–‡æœ¬åŒ¹é…: {e}")
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨åŸºæœ¬çš„è¯æ±‡é‡å 
            text_lower = text.lower()
            text_words_set = set(text_lower.split())
            
            intersection = query_words.intersection(text_words_set)
            total_query_words = len(query_words)
            
            if total_query_words > 0:
                match_ratio = len(intersection) / total_query_words
                return base_score * match_ratio
            else:
                return 0.0
    
    def _extract_semantic_keywords_from_text(self, text: str, stop_words: set) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–è¯­ä¹‰å…³é”®è¯ - å‚è€ƒTextEngineçš„å®ç°"""
        try:
            # å¯¼å…¥jiebaåˆ†è¯å·¥å…·
            import jieba
            import jieba.analyse
            
            # æ·»åŠ é¢†åŸŸä¸“ä¸šè¯æ±‡åˆ°jiebaè¯å…¸
            domain_words = [
                'ä¸­èŠ¯å›½é™…', 'SMIC', 'æ™¶åœ†ä»£å·¥', 'åŠå¯¼ä½“åˆ¶é€ ', 'é›†æˆç”µè·¯', 'IC', 'å¾®å¤„ç†å™¨',
                'è‰¯ç‡', 'yield', 'åˆ¶ç¨‹', 'å·¥è‰º', 'å°è£…', 'æµ‹è¯•', 'æ™¶åœ†', 'ç¡…ç‰‡', 'åŸºæ¿'
            ]
            for word in domain_words:
                jieba.add_word(word)
            
            # æ–¹æ³•1ï¼šä½¿ç”¨jieba.lcutè¿›è¡Œç²¾ç¡®åˆ†è¯ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼Œä¿è¯å®Œæ•´æ€§ï¼‰
            try:
                words = jieba.lcut(text, cut_all=False)
                keywords = [word for word in words if word not in stop_words and len(word) > 1]
                if keywords:
                    logger.debug(f"jiebaç²¾ç¡®åˆ†è¯æˆåŠŸï¼Œæå–åˆ° {len(keywords)} ä¸ªå…³é”®è¯")
                    return keywords
            except Exception as e:
                logger.warning(f"jiebaç²¾ç¡®åˆ†è¯å¤±è´¥: {e}")
            
            # æ–¹æ³•2ï¼šä½¿ç”¨jieba.analyse.extract_tagsæå–å…³é”®è¯ï¼ˆåŸºäºTF-IDFï¼Œä½œä¸ºè¡¥å……ï¼‰
            try:
                keywords_tfidf = jieba.analyse.extract_tags(text, topK=15, allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an'))
                if keywords_tfidf:
                    filtered_keywords = [word for word in keywords_tfidf if word not in stop_words and len(word) > 1]
                    if filtered_keywords:
                        logger.debug(f"jieba TF-IDFæå–æˆåŠŸï¼Œæå–åˆ° {len(filtered_keywords)} ä¸ªå…³é”®è¯")
                        return filtered_keywords
            except Exception as e:
                logger.warning(f"jieba TF-IDFæå–å¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šä½¿ç”¨jieba.analyse.textrankæå–å…³é”®è¯ï¼ˆåŸºäºTextRankç®—æ³•ï¼Œä½œä¸ºè¡¥å……ï¼‰
            try:
                keywords_textrank = jieba.analyse.textrank(text, topK=15, allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an'))
                if keywords_textrank:
                    filtered_keywords = [word for word in keywords_textrank if word not in stop_words and len(word) > 1]
                    if filtered_keywords:
                        logger.debug(f"jieba TextRankæå–æˆåŠŸï¼Œæå–åˆ° {len(filtered_keywords)} ä¸ªå…³é”®è¯")
                        return filtered_keywords
            except Exception as e:
                logger.warning(f"jieba TextRankæå–å¤±è´¥: {e}")
            
            # æ–¹æ³•4ï¼šé™çº§åˆ°æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¦‚æœjiebaéƒ½å¤±è´¥äº†ï¼‰
            try:
                import re
                words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', text.lower())
                keywords = [word for word in words if word not in stop_words and len(word) > 1]
                if keywords:
                    return keywords
            except Exception as e:
                logger.warning(f"æ­£åˆ™è¡¨è¾¾å¼æå–å¤±è´¥: {e}")
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›æœ€åŸºæœ¬çš„è¯
            basic_words = [word.strip() for word in text.split() if len(word.strip()) > 1]
            return basic_words[:10]  # æœ€å¤šè¿”å›10ä¸ªè¯
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æå–å®Œå…¨å¤±è´¥: {e}")
            # æœ€åçš„é™çº§æ–¹æ¡ˆ
            return [word.strip() for word in text.split() if len(word.strip()) > 1]
    
    def _extract_semantic_keywords_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–è¯­ä¹‰å…³é”®è¯ - å‚è€ƒTextEngineçš„å®ç°"""
        try:
            # å¯¼å…¥jiebaåˆ†è¯å·¥å…·
            import jieba
            import jieba.analyse
            
            # æ·»åŠ é¢†åŸŸä¸“ä¸šè¯æ±‡åˆ°jiebaè¯å…¸
            domain_words = [
                'ä¸­èŠ¯å›½é™…', 'SMIC', 'æ™¶åœ†ä»£å·¥', 'åŠå¯¼ä½“åˆ¶é€ ', 'é›†æˆç”µè·¯', 'IC', 'å¾®å¤„ç†å™¨',
                'è‰¯ç‡', 'yield', 'åˆ¶ç¨‹', 'å·¥è‰º', 'å°è£…', 'æµ‹è¯•', 'æ™¶åœ†', 'ç¡…ç‰‡', 'åŸºæ¿'
            ]
            for word in domain_words:
                jieba.add_word(word)
            
            # åœç”¨è¯åˆ—è¡¨ï¼ˆä¸TextEngineä¿æŒä¸€è‡´ï¼‰
            stop_words = {
                'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å¦‚æœ', 'é‚£ä¹ˆ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•',
                'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'ä¸€ä¸ª', 'ä¸€äº›', 'å¯ä»¥', 'åº”è¯¥', 'èƒ½å¤Ÿ', 'éœ€è¦', 'å¿…é¡»', 'å¯èƒ½', 'ä¹Ÿè®¸',
                'å¤§æ¦‚', 'å¤§çº¦', 'å·¦å³', 'æ ¹æ®', 'æ˜¾ç¤º', 'è¡¨æ˜', 'è¯´æ˜', 'æŒ‡å‡º', 'æåˆ°', 'åŒ…æ‹¬', 'æ¶‰åŠ', 'å…³äº', 'å¯¹äº',
                'å‘¢', 'å—', 'å•Š', 'å§', 'äº†', 'ç€', 'è¿‡', 'æ¥', 'å»', 'ä¸Š', 'ä¸‹', 'é‡Œ', 'å¤–', 'å‰', 'å', 'å·¦', 'å³'
            }
            
            # æ–¹æ³•1ï¼šä½¿ç”¨jieba.lcutè¿›è¡Œç²¾ç¡®åˆ†è¯ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼Œä¿è¯å®Œæ•´æ€§ï¼‰
            try:
                words = jieba.lcut(query, cut_all=False)
                keywords = [word for word in words if word not in stop_words and len(word) > 1]
                if keywords:
                    logger.debug(f"jiebaç²¾ç¡®åˆ†è¯æˆåŠŸï¼Œæå–åˆ° {len(keywords)} ä¸ªæŸ¥è¯¢å…³é”®è¯")
                    return keywords
            except Exception as e:
                logger.warning(f"jiebaç²¾ç¡®åˆ†è¯å¤±è´¥: {e}")
            
            # æ–¹æ³•2ï¼šä½¿ç”¨jieba.analyse.extract_tagsæå–å…³é”®è¯ï¼ˆåŸºäºTF-IDFï¼Œä½œä¸ºè¡¥å……ï¼‰
            try:
                keywords_tfidf = jieba.analyse.extract_tags(query, topK=10, allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an'))
                if keywords_tfidf:
                    filtered_keywords = [word for word in keywords_tfidf if word not in stop_words and len(word) > 1]
                    if filtered_keywords:
                        logger.debug(f"jieba TF-IDFæå–æˆåŠŸï¼Œæå–åˆ° {len(filtered_keywords)} ä¸ªæŸ¥è¯¢å…³é”®è¯")
                        return filtered_keywords
            except Exception as e:
                logger.warning(f"jieba TF-IDFæå–å¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šä½¿ç”¨jieba.analyse.textrankæå–å…³é”®è¯ï¼ˆåŸºäºTextRankç®—æ³•ï¼Œä½œä¸ºè¡¥å……ï¼‰
            try:
                keywords_textrank = jieba.analyse.textrank(query, topK=10, allowPOS=('n', 'nr', 'nt', 'nz', 'v', 'vn', 'a', 'an'))
                if keywords_textrank:
                    filtered_keywords = [word for word in keywords_textrank if word not in stop_words and len(word) > 1]
                    if filtered_keywords:
                        logger.debug(f"jieba TextRankæå–æˆåŠŸï¼Œæå–åˆ° {len(filtered_keywords)} ä¸ªæŸ¥è¯¢å…³é”®è¯")
                        return filtered_keywords
            except Exception as e:
                logger.warning(f"jieba TextRankæå–å¤±è´¥: {e}")
            
            # æ–¹æ³•4ï¼šé™çº§åˆ°æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¦‚æœjiebaéƒ½å¤±è´¥äº†ï¼‰
            try:
                import re
                words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', query.lower())
                keywords = [word for word in words if word not in stop_words and len(word) > 1]
                if keywords:
                    return keywords
            except Exception as e:
                logger.warning(f"æ­£åˆ™è¡¨è¾¾å¼æå–å¤±è´¥: {e}")
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›æœ€åŸºæœ¬çš„è¯
            basic_words = [word.strip() for word in query.split() if len(word.strip()) > 1]
            return basic_words[:5]  # æŸ¥è¯¢å…³é”®è¯æœ€å¤šè¿”å›5ä¸ªè¯
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å…³é”®è¯æå–å®Œå…¨å¤±è´¥: {e}")
            # æœ€åçš„é™çº§æ–¹æ¡ˆ
            return [word.strip() for word in query.split() if len(word.strip()) > 1]
    
    def _calculate_string_similarity(self, str1: str, str2: str) -> float:
        """
        è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
        
        :param str1: å­—ç¬¦ä¸²1
        :param str2: å­—ç¬¦ä¸²2
        :return: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not str1 or not str2:
            return 0.0
        
        # ç®€å•çš„å­—ç¬¦çº§ç›¸ä¼¼åº¦è®¡ç®—
        set1 = set(str1)
        set2 = set(str2)
        
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        if union == 0:
            return 0.0
        
        return intersection / union
    
    def _get_doc_id(self, doc) -> str:
        """
        è·å–æ–‡æ¡£ID
        
        :param doc: æ–‡æ¡£å¯¹è±¡
        :return: æ–‡æ¡£IDå­—ç¬¦ä¸²
        """
        if hasattr(doc, 'metadata') and doc.metadata:
            return doc.metadata.get('image_id', str(id(doc)))  # ä¿®æ­£ï¼šä½¿ç”¨image_idè€Œä¸æ˜¯id
        return str(id(doc))
    
    def _find_image_chunk_by_id(self, image_id: str):
        """
        æ ¹æ®image_idæŸ¥æ‰¾å¯¹åº”çš„image chunk
        
        :param image_id: å›¾ç‰‡ID
        :return: image chunkæ–‡æ¡£å¯¹è±¡ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        if not self.vector_store:
            return None
        
        try:
            # åœ¨å‘é‡å­˜å‚¨ä¸­æœç´¢image chunk
            results = self.vector_store.similarity_search(
                "",  # ç©ºæŸ¥è¯¢ï¼Œåªç”¨äºè¿‡æ»¤
                k=1,
                filter={'chunk_type': 'image', 'image_id': image_id}
            )
            
            if results:
                return results[0]
            
            # å¦‚æœå‘é‡æœç´¢æ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨docstoreä¸­æŸ¥æ‰¾
            if hasattr(self.vector_store, 'docstore') and hasattr(self.vector_store.docstore, '_dict'):
                for doc_id, doc in self.vector_store.docstore._dict.items():
                    metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                    if (metadata.get('chunk_type') == 'image' and 
                        metadata.get('image_id') == image_id):
                        return doc
            
            logger.warning(f"æœªæ‰¾åˆ°image_idä¸º {image_id} çš„image chunk")
            return None
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾image chunkå¤±è´¥: {e}")
            return None
    
    def _deduplicate_and_sort_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é‡å’Œæ’åºç»“æœ
        
        :param results: åŸå§‹ç»“æœåˆ—è¡¨
        :return: å»é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        # å»é‡ï¼ˆåŸºäºæ–‡æ¡£IDï¼‰
        seen_docs = set()
        unique_results = []
        
        for result in results:
            doc_id = self._get_doc_id(result['doc'])
            if doc_id not in seen_docs:
                seen_docs.add(doc_id)
                unique_results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        unique_results.sort(key=lambda x: x['score'], reverse=True)
        
        logger.info(f"å»é‡åç»“æœæ•°é‡: {len(unique_results)}")
        return unique_results
    
    def _final_ranking_and_limit(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æœ€ç»ˆæ’åºå’Œé™åˆ¶ç»“æœæ•°é‡
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param results: å€™é€‰ç»“æœåˆ—è¡¨
        :return: æœ€ç»ˆç»“æœåˆ—è¡¨
        """
        if not results:
            return []
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_results = sorted(results, key=lambda x: x.get('score', 0.0), reverse=True)
        
        # é™åˆ¶ç»“æœæ•°é‡
        max_results = getattr(self.config, 'max_results', 20)
        final_results = sorted_results[:max_results]
        
        logger.info(f"æœ€ç»ˆæ’åºå®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
        return final_results
    
    def clear_cache(self):
        """æ¸…ç†å›¾ç‰‡å¼•æ“ç¼“å­˜"""
        self.image_docs = []
        self._docs_loaded = False
        logger.info("å›¾ç‰‡å¼•æ“ç¼“å­˜å·²æ¸…ç†")
    
    def _calculate_content_relevance(self, query: str, content: str) -> float:
        """
        è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°ï¼ˆåŸºäºæµ‹è¯•éªŒè¯çš„ç®—æ³•ï¼‰
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param content: æ–‡æ¡£å†…å®¹
        :return: ç›¸å…³æ€§åˆ†æ•° [0, 1]
        """
        try:
            # é¢„å¤„ç†ï¼šè½¬æ¢ä¸ºå°å†™å¹¶åˆ†è¯
            query_lower = query.lower()
            content_lower = content.lower()
            
            # æ–¹æ³•1ï¼šç›´æ¥å­—ç¬¦ä¸²åŒ…å«åŒ¹é…
            if query_lower in content_lower:
                return 0.8  # å®Œå…¨åŒ…å«ç»™é«˜åˆ†
            
            # æ–¹æ³•2ï¼šåˆ†è¯åŒ¹é…
            query_words = [word for word in query_lower.split() if len(word) > 1]  # è¿‡æ»¤å•å­—ç¬¦
            if not query_words:
                return 0.0
            
            content_words = content_lower.split()
            
            # è®¡ç®—åŒ¹é…è¯æ•°
            matched_words = 0
            total_score = 0.0
            
            for query_word in query_words:
                if query_word in content_words:
                    matched_words += 1
                    # è®¡ç®—è¯é¢‘åˆ†æ•°
                    word_count = content_lower.count(query_word)
                    word_score = min(word_count / len(content_words), 0.3)  # é™åˆ¶å•ä¸ªè¯çš„æœ€å¤§åˆ†æ•°
                    total_score += word_score
            
            # è®¡ç®—åŒ¹é…ç‡
            match_rate = matched_words / len(query_words) if query_words else 0
            
            # ç»¼åˆåˆ†æ•°ï¼šåŒ¹é…ç‡ + è¯é¢‘åˆ†æ•°
            final_score = (match_rate * 0.7 + total_score * 0.3)
            
            return min(final_score, 1.0)
            
        except Exception as e:
            logger.warning(f"è®¡ç®—å†…å®¹ç›¸å…³æ€§å¤±è´¥: {e}")
            return 0.0
    
    def _load_image_docs(self):
        """ä»å‘é‡æ•°æ®åº“åŠ è½½å›¾ç‰‡æ–‡æ¡£"""
        try:
            # ä»å‘é‡æ•°æ®åº“åŠ è½½imageæ–‡æ¡£
            image_docs = self.vector_store.search_by_type('image', limit=1000)
            self.image_docs.extend(image_docs)
            
            # ä»å‘é‡æ•°æ®åº“åŠ è½½image_textæ–‡æ¡£
            image_text_docs = self.vector_store.search_by_type('image_text', limit=1000)
            self.image_docs.extend(image_text_docs)
            
            self.logger.info(f"å›¾ç‰‡å¼•æ“åŠ è½½å®Œæˆ: {len(self.image_docs)} ä¸ªæ–‡æ¡£")
            
        except Exception as e:
            self.logger.error(f"ä»å‘é‡æ•°æ®åº“åŠ è½½å›¾ç‰‡æ–‡æ¡£å¤±è´¥: {e}")
            self.logger.info(f"ä»å‘é‡æ•°æ®åº“åŠ è½½äº† {len(self.image_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
    
    def _execute_reranking(self, recall_results: List[Any], query: str, **kwargs) -> List[Any]:
        """æ‰§è¡Œé‡æ’åº"""
        if not recall_results:
            return []
        
        try:
            # ä½¿ç”¨å›¾ç‰‡é‡æ’åºå¼•æ“
            if self.reranking_engine:
                self.logger.info(f"å¼€å§‹å›¾ç‰‡é‡æ’åºï¼Œå€™é€‰æ–‡æ¡£: {len(recall_results)}")
                reranked_results = self.reranking_engine.rerank(query, recall_results, **kwargs)
                self.logger.info(f"é‡æ’åºå®Œæˆï¼Œç»“æœæ•°é‡: {len(reranked_results)}")
                return reranked_results
            else:
                self.logger.warning("é‡æ’åºå¼•æ“ä¸å¯ç”¨ï¼Œè·³è¿‡é‡æ’åº")
                return recall_results
                
        except Exception as e:
            self.logger.error(f"é‡æ’åºæ‰§è¡Œå¤±è´¥: {e}")
            return recall_results
    
    def _convert_l2_distance_to_similarity(self, distance: float) -> float:
        """
        æ”¹è¿›çš„L2è·ç¦»åˆ°ç›¸ä¼¼åº¦è½¬æ¢å‡½æ•°
        
        ä½¿ç”¨æŒ‡æ•°è¡°å‡å…¬å¼ï¼šexp(-distance / 2.0)
        ç›¸æ¯”åŸæ¥çš„ 1/(1+distance) å…¬å¼ï¼Œè¿™ä¸ªå…¬å¼èƒ½æä¾›æ›´å¥½çš„åˆ†æ•°åˆ†å¸ƒ
        
        :param distance: L2è·ç¦»å€¼
        :return: ç›¸ä¼¼åº¦åˆ†æ•° (0-1ä¹‹é—´)
        """
        import numpy as np
        return np.exp(-distance / 2.0)

