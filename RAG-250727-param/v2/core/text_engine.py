'''
ç¨‹åºè¯´æ˜ï¼š
## 1. æ–‡æœ¬å¼•æ“ - ä¸“é—¨å¤„ç†æ–‡æœ¬æŸ¥è¯¢
## 2. æ”¯æŒå…³é”®è¯ã€è¯­ä¹‰ã€å‘é‡ç›¸ä¼¼åº¦æœç´¢
## 3. æ™ºèƒ½æ–‡æœ¬æ’åºå’Œç›¸å…³æ€§è®¡ç®—
## 4. å‘åå…¼å®¹ç°æœ‰æ–‡æœ¬æŸ¥è¯¢åŠŸèƒ½
'''

import logging
import time
from typing import Dict, Any, List, Optional, Union
from .base_engine import BaseEngine, QueryType, QueryResult, EngineConfig, EngineStatus


logger = logging.getLogger(__name__)





class TextEngine(BaseEngine):
    """
    æ–‡æœ¬å¼•æ“
    
    ä¸“é—¨å¤„ç†æ–‡æœ¬æŸ¥è¯¢ï¼Œæ”¯æŒå¤šç§æœç´¢ç­–ç•¥
    """
    
    def __init__(self, config, vector_store=None, document_loader=None, skip_initial_load=False):
        """
        åˆå§‹åŒ–æ–‡æœ¬å¼•æ“
        
        :param config: æ–‡æœ¬å¼•æ“é…ç½®
        :param vector_store: å‘é‡æ•°æ®åº“
        :param document_loader: ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨
        :param skip_initial_load: æ˜¯å¦è·³è¿‡åˆå§‹åŠ è½½
        """
        super().__init__(config)
        
        # ç°åœ¨å¯ä»¥å®‰å…¨ä½¿ç”¨self.loggeräº†
        self.logger.info("ğŸ” å¼€å§‹åˆå§‹åŒ–TextEngine")
        self.logger.info(f"é…ç½®ç±»å‹: {type(config)}")
        self.logger.info(f"å‘é‡æ•°æ®åº“: {vector_store}")
        self.logger.info(f"æ–‡æ¡£åŠ è½½å™¨: {document_loader}")
        self.logger.info(f"è·³è¿‡åˆå§‹åŠ è½½: {skip_initial_load}")
        
        self.vector_store = vector_store
        self.document_loader = document_loader
        self.text_docs = {}  # ç¼“å­˜çš„æ–‡æœ¬æ–‡æ¡£
        self._docs_loaded = False
        
        self.logger.info("âœ… åŸºç¡€å±æ€§è®¾ç½®å®Œæˆ")
        
        # åœ¨è®¾ç½®å®Œvector_storeåè°ƒç”¨_initialize
        self.logger.info("å¼€å§‹è°ƒç”¨_initialize...")
        self._initialize()
        self.logger.info("âœ… _initializeå®Œæˆ")
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åŠ è½½æ–‡æ¡£
        if not skip_initial_load:
            if document_loader:
                self.logger.info("ä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨åŠ è½½æ–‡æ¡£...")
                self._load_from_document_loader()
            else:
                self.logger.info("ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼åŠ è½½æ–‡æ¡£...")
                self._load_text_documents()
        else:
            self.logger.info("è·³è¿‡åˆå§‹æ–‡æ¡£åŠ è½½")
        
        self.logger.info(f"âœ… TextEngineåˆå§‹åŒ–å®Œæˆï¼Œæ–‡æœ¬æ–‡æ¡£æ•°é‡: {len(self.text_docs)}")
    
    def _setup_components(self):
        """è®¾ç½®æ–‡æœ¬å¼•æ“ç»„ä»¶"""
        if not self.vector_store:
            raise ValueError("å‘é‡æ•°æ®åº“æœªæä¾›")
        
        # åŠ è½½æ–‡æœ¬æ–‡æ¡£
        self._load_text_documents()
    
    def _validate_config(self):
        """éªŒè¯æ–‡æœ¬å¼•æ“é…ç½®"""
        # é…ç½®ç±»å‹æ£€æŸ¥
        from ..config.v2_config import TextEngineConfigV2
        
        if not isinstance(self.config, TextEngineConfigV2):
            raise ValueError("é…ç½®å¿…é¡»æ˜¯TextEngineConfigV2ç±»å‹")
        
        # è·å–ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œæ”¯æŒä¸¤ç§é…ç½®ç±»å‹
        threshold = getattr(self.config, 'text_similarity_threshold', 0.7)
        if threshold < 0 or threshold > 1:
            raise ValueError("æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
    
    def _load_text_documents(self):
        """åŠ è½½æ–‡æœ¬æ–‡æ¡£åˆ°ç¼“å­˜"""
        self.logger.info("ğŸ” å¼€å§‹è¯Šæ–­æ–‡æ¡£åŠ è½½è¿‡ç¨‹")
        self.logger.info(f"å‘é‡æ•°æ®åº“: {self.vector_store}")
        self.logger.info(f"å‘é‡æ•°æ®åº“ç±»å‹: {type(self.vector_store)}")
        
        if not self.vector_store or not hasattr(self.vector_store, 'docstore'):
            self.logger.error("âŒ å‘é‡æ•°æ®åº“æœªæä¾›æˆ–æ²¡æœ‰docstoreå±æ€§")
            self.logger.info(f"å‘é‡æ•°æ®åº“å±æ€§: {dir(self.vector_store) if self.vector_store else 'None'}")
            return
        
        self.logger.info("âœ… å‘é‡æ•°æ®åº“æ£€æŸ¥é€šè¿‡")
        self.logger.info(f"docstoreç±»å‹: {type(self.vector_store.docstore)}")
        self.logger.info(f"docstoreå±æ€§: {dir(self.vector_store.docstore)}")
        
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                self.logger.info(f"ğŸ”„ ç¬¬{retry_count + 1}æ¬¡å°è¯•åŠ è½½æ–‡æœ¬æ–‡æ¡£")
                
                # æ¸…ç©ºä¹‹å‰çš„ç¼“å­˜
                self.text_docs = {}
                
                # æ£€æŸ¥docstore._dict
                if not hasattr(self.vector_store.docstore, '_dict'):
                    self.logger.error("âŒ docstoreæ²¡æœ‰_dictå±æ€§")
                    return
                
                docstore_dict = self.vector_store.docstore._dict
                self.logger.info(f"docstore._dicté•¿åº¦: {len(docstore_dict)}")
                self.logger.info(f"docstore._dictç±»å‹: {type(docstore_dict)}")
                
                # ä»å‘é‡æ•°æ®åº“åŠ è½½æ‰€æœ‰æ–‡æœ¬æ–‡æ¡£
                text_doc_count = 0
                for doc_id, doc in docstore_dict.items():
                    chunk_type = doc.metadata.get('chunk_type', '') if hasattr(doc, 'metadata') else ''
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬æ–‡æ¡£ - ç®€åŒ–åˆ¤æ–­é€»è¾‘
                    is_text = chunk_type == 'text'
                    
                    if is_text:
                        self.text_docs[doc_id] = doc
                        text_doc_count += 1
                        if text_doc_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                            self.logger.debug(f"âœ… åŠ è½½æ–‡æœ¬æ–‡æ¡£: {doc_id}, chunk_type: {chunk_type}")
                            if hasattr(doc, 'page_content'):
                                self.logger.debug(f"  å†…å®¹é•¿åº¦: {len(doc.page_content)}")
                            if hasattr(doc, 'metadata'):
                                self.logger.debug(f"  å…ƒæ•°æ®: {doc.metadata}")
                
                self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.text_docs)} ä¸ªæ–‡æœ¬æ–‡æ¡£")
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬æ–‡æ¡£ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                if not self.text_docs:
                    self.logger.warning("âš ï¸ æœªæ‰¾åˆ°æ–‡æœ¬æ–‡æ¡£ï¼Œå°è¯•æœç´¢æ‰€æœ‰æ–‡æ¡£...")
                    self._search_all_documents_for_texts()
                
                # å¦‚æœæˆåŠŸåŠ è½½äº†æ–‡æ¡£ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                if len(self.text_docs) > 0:
                    self.logger.info(f"âœ… æ–‡æœ¬æ–‡æ¡£åŠ è½½æˆåŠŸï¼Œå…± {len(self.text_docs)} ä¸ªæ–‡æ¡£")
                    self._docs_loaded = True
                    return
                else:
                    raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡æ¡£")
                    
            except Exception as e:
                retry_count += 1
                self.logger.warning(f"âš ï¸ æ–‡æœ¬æ–‡æ¡£åŠ è½½å¤±è´¥ï¼Œç¬¬{retry_count}æ¬¡å°è¯•: {e}")
                self.logger.warning(f"é”™è¯¯ç±»å‹: {type(e)}")
                
                if retry_count >= max_retries:
                    # æœ€ç»ˆå¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶æ¸…ç©ºç¼“å­˜
                    self.logger.error(f"âŒ æ–‡æœ¬æ–‡æ¡£åŠ è½½æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
                    import traceback
                    self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    self.text_docs = {}
                    return
                else:
                    # ç­‰å¾…åé‡è¯•
                    import time
                    time.sleep(1)
                    self.logger.info(f"â³ ç­‰å¾…1ç§’åè¿›è¡Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
    
    def _load_from_document_loader(self):
        """ä»ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨è·å–æ–‡æœ¬æ–‡æ¡£"""
        if self.document_loader:
            try:
                self.text_docs = self.document_loader.get_documents_by_type('text')
                self._docs_loaded = True
                self.logger.info(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–æ–‡æœ¬æ–‡æ¡£: {len(self.text_docs)} ä¸ª")
            except Exception as e:
                self.logger.error(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–æ–‡æœ¬æ–‡æ¡£å¤±è´¥: {e}")
                # é™çº§åˆ°ä¼ ç»ŸåŠ è½½æ–¹å¼
                self._load_text_documents()
        else:
            self.logger.warning("æ–‡æ¡£åŠ è½½å™¨æœªæä¾›ï¼Œä½¿ç”¨ä¼ ç»ŸåŠ è½½æ–¹å¼")
            self._load_text_documents()
    
    def _ensure_docs_loaded(self):
        """ç¡®ä¿æ–‡æ¡£å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if not self._docs_loaded:
            if self.document_loader:
                self._load_from_document_loader()
            else:
                self._load_text_documents()
                self._docs_loaded = True
    
    def _search_all_documents_for_texts(self):
        """æœç´¢æ‰€æœ‰æ–‡æ¡£ä¸­çš„æ–‡æœ¬å†…å®¹"""
        try:
            for doc_id, doc in self.vector_store.docstore._dict.items():
                # æ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦åŒ…å«æ–‡æœ¬ä¿¡æ¯
                chunk_type = doc.metadata.get('chunk_type', '')
                if chunk_type == 'text' or chunk_type == '':
                    self.text_docs[doc_id] = doc
                    self.logger.debug(f"é€šè¿‡ç±»å‹è¯†åˆ«æ–‡æœ¬æ–‡æ¡£: {doc_id}")
        except Exception as e:
            self.logger.error(f"æœç´¢æ–‡æœ¬æ–‡æ¡£å¤±è´¥: {e}")
    
    def process_query(self, query: str, **kwargs) -> QueryResult:
        """
        å¤„ç†æ–‡æœ¬æŸ¥è¯¢ - äº”å±‚å¬å›ç­–ç•¥ï¼ˆV3.0ç‰ˆæœ¬ï¼‰
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param kwargs: é¢å¤–å‚æ•°
        :return: æŸ¥è¯¢ç»“æœ
        """
        if not self.is_enabled():
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.TEXT,
                results=[],
                total_count=0,
                processing_time=0.0,
                engine_name=self.name,
                metadata={},
                error_message="æ–‡æœ¬å¼•æ“æœªå¯ç”¨"
            )
        
        # ç¡®ä¿æ–‡æ¡£å·²åŠ è½½
        self._ensure_docs_loaded()
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œäº”å±‚å¬å›ç­–ç•¥
            self.logger.info("å¼€å§‹æ‰§è¡Œäº”å±‚å¬å›ç­–ç•¥")
            recall_results = self._search_texts(query, **kwargs)
            
            if not recall_results:
                processing_time = time.time() - start_time
                return QueryResult(
                    success=True,
                    query=query,
                    query_type=QueryType.TEXT,
                    results=[],
                    total_count=0,
                    processing_time=processing_time,
                    engine_name=self.name,
                    metadata={'total_texts': len(self.text_docs), 'pipeline': 'new'}
                )
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¢å¼ºReranking
            if getattr(self.config, 'enable_enhanced_reranking', False):
                self.logger.info("å¯ç”¨å¢å¼ºRerankingæœåŠ¡")
                try:
                    # å¯¼å…¥RerankingæœåŠ¡
                    from .reranking_services import create_reranking_service
                    
                    # åˆ›å»ºTextRerankingService
                    reranking_config = getattr(self.config, 'reranking', {})
                    reranking_service = create_reranking_service('text', reranking_config)
                    
                    if reranking_service:
                        # æ‰§è¡ŒReranking
                        self.logger.info(f"å¼€å§‹æ‰§è¡ŒTextRerankingï¼Œå€™é€‰æ–‡æ¡£æ•°é‡: {len(recall_results)}")
                        reranked_results = reranking_service.rerank(query, recall_results)
                        self.logger.info(f"Rerankingå®Œæˆï¼Œè¿”å› {len(reranked_results)} ä¸ªç»“æœ")
                        
                        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°çš„ç»Ÿä¸€Pipeline
                        if getattr(self.config, 'use_new_pipeline', False):
                            self.logger.info("ä½¿ç”¨æ–°çš„ç»Ÿä¸€Pipelineå¤„ç†é‡æ’åºç»“æœ")
                            try:
                                # å¯¼å…¥ç»Ÿä¸€Pipeline
                                from .unified_pipeline import UnifiedPipeline
                                
                                # è·å–ç»Ÿä¸€Pipelineé…ç½®
                                from ..config.v2_config import V2ConfigManager
                                config_manager = V2ConfigManager()
                                pipeline_config = config_manager.get_engine_config('unified_pipeline')
                                
                                if pipeline_config and pipeline_config.enabled:
                                    # å°è¯•è·å–çœŸå®çš„LLMå¼•æ“å’Œæºè¿‡æ»¤å¼•æ“
                                    llm_engine = None
                                    source_filter_engine = None
                                    
                                    # ä»HybridEngineè·å–å¼•æ“ï¼ˆé€šè¿‡kwargsä¼ é€’ï¼‰
                                    if 'llm_engine' in kwargs:
                                        llm_engine = kwargs['llm_engine']
                                        self.logger.info("ä½¿ç”¨ä¼ å…¥çš„LLMå¼•æ“")
                                    if 'source_filter_engine' in kwargs:
                                        source_filter_engine = kwargs['source_filter_engine']
                                        self.logger.info("ä½¿ç”¨ä¼ å…¥çš„æºè¿‡æ»¤å¼•æ“")
                                    
                                    # å¦‚æœæ²¡æœ‰ä¼ å…¥çœŸå®å¼•æ“ï¼Œä½¿ç”¨Mockï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                                    if not llm_engine:
                                        from unittest.mock import Mock
                                        llm_engine = Mock()
                                        llm_engine.generate_answer.return_value = "åŸºäºæŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆçš„ç­”æ¡ˆ"
                                        self.logger.warning("ä½¿ç”¨Mock LLMå¼•æ“ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
                                    
                                    if not source_filter_engine:
                                        from unittest.mock import Mock
                                        source_filter_engine = Mock()
                                        source_filter_engine.filter_sources.return_value = reranked_results[:3]
                                        self.logger.warning("ä½¿ç”¨Mockæºè¿‡æ»¤å¼•æ“ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
                                    
                                    # åˆ›å»ºç»Ÿä¸€Pipeline
                                    unified_pipeline = UnifiedPipeline(
                                        config=pipeline_config.__dict__,
                                        llm_engine=llm_engine,
                                        source_filter_engine=source_filter_engine
                                    )
                                    
                                    # æ‰§è¡Œç»Ÿä¸€Pipeline
                                    pipeline_result = unified_pipeline.process(query, reranked_results)
                                    
                                    if pipeline_result.success:
                                        self.logger.info("ç»Ÿä¸€Pipelineæ‰§è¡ŒæˆåŠŸ")
                                        final_results = pipeline_result.filtered_sources
                                        # æ·»åŠ Pipelineå…ƒæ•°æ®
                                        pipeline_metadata = {
                                            'pipeline': 'unified_pipeline',
                                            'llm_answer': pipeline_result.llm_answer,
                                            'pipeline_metrics': pipeline_result.pipeline_metrics
                                        }
                                        # å°†LLMç­”æ¡ˆä¹Ÿæ·»åŠ åˆ°metadataä¸­ï¼Œä¾›HybridEngineä½¿ç”¨
                                        if pipeline_result.llm_answer:
                                            self.logger.info(f"ç»Ÿä¸€Pipelineç”ŸæˆLLMç­”æ¡ˆï¼Œé•¿åº¦: {len(pipeline_result.llm_answer)}")
                                    else:
                                        self.logger.warning(f"ç»Ÿä¸€Pipelineæ‰§è¡Œå¤±è´¥: {pipeline_result.error_message}")
                                        final_results = self._final_ranking_and_limit(query, reranked_results)
                                        pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
                                else:
                                    self.logger.warning("ç»Ÿä¸€Pipelineæœªå¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ’åº")
                                    final_results = self._final_ranking_and_limit(query, reranked_results)
                                    pipeline_metadata = {'pipeline': 'traditional_ranking'}
                                    
                            except Exception as e:
                                self.logger.error(f"ç»Ÿä¸€Pipelineæ‰§è¡Œå¤±è´¥: {e}ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ’åº")
                                final_results = self._final_ranking_and_limit(query, reranked_results)
                                pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
                        else:
                            self.logger.info("ä½¿ç”¨ä¼ ç»Ÿæ’åºæ–¹å¼")
                            final_results = self._final_ranking_and_limit(query, reranked_results)
                            pipeline_metadata = {'pipeline': 'traditional_ranking'}
                    else:
                        self.logger.warning("RerankingæœåŠ¡åˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å¬å›ç»“æœ")
                        final_results = self._final_ranking_and_limit(query, recall_results)
                        pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
                        
                except Exception as e:
                    self.logger.error(f"Rerankingæ‰§è¡Œå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å¬å›ç»“æœ")
                    final_results = self._final_ranking_and_limit(query, recall_results)
                    pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
            else:
                self.logger.info("ä½¿ç”¨ä¼ ç»Ÿæ’åºæ–¹å¼")
                # æœ€ç»ˆæ’åºå’Œé™åˆ¶
                final_results = self._final_ranking_and_limit(query, recall_results)
                pipeline_metadata = {'pipeline': 'traditional_ranking'}
            
            processing_time = time.time() - start_time
            
            return QueryResult(
                success=True,
                query=query,
                query_type=QueryType.TEXT,
                results=final_results,
                total_count=len(final_results),
                processing_time=processing_time,
                engine_name=self.name,
                metadata={
                    'total_texts': len(self.text_docs),
                    'pipeline': 'new',
                    'recall_count': len(recall_results),
                    'final_count': len(final_results),
                    **pipeline_metadata  # æ·»åŠ Pipelineå…ƒæ•°æ®
                }
            )
                
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"æ–‡æœ¬æŸ¥è¯¢å¤±è´¥: {e}")
            
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.TEXT,
                results=[],
                total_count=0,
                processing_time=processing_time,
                engine_name=self.name,
                metadata={},
                error_message=str(e)
            )
    
    def _final_ranking_and_limit(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æœ€ç»ˆæ’åºå’Œé™åˆ¶ - åŸºäºå¬å›åˆ†æ•°"""
        
        # ä¸ºæ¯ä¸ªç»“æœè®¡ç®—å¬å›åˆ†æ•°
        for result in results:
            result['recall_score'] = self._get_comprehensive_score(result)
        
        # æŒ‰å¬å›åˆ†æ•°æ’åº
        sorted_results = sorted(results, key=lambda x: x.get('recall_score', 0), reverse=True)
        
        # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        max_results = getattr(self.config, 'max_results', 10)
        final_results = sorted_results[:max_results]
        
        # æ·»åŠ æœ€ç»ˆæ’åä¿¡æ¯
        for i, result in enumerate(final_results):
            result['final_rank'] = i + 1
            result['final_score'] = result.get('recall_score', 0.0)
        
        self.logger.info(f"Text Engineæœ€ç»ˆæ’åºå®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªå€™é€‰æ–‡æ¡£")
        return final_results
    
    def _search_texts(self, query: str, **kwargs) -> List[Any]:
        """
        æ™ºèƒ½æ–‡æœ¬æœç´¢ - äº”å±‚å¬å›ç­–ç•¥ï¼ˆV3.0ç‰ˆæœ¬ï¼‰
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :return: åŒ¹é…çš„æ–‡æœ¬åˆ—è¡¨
        """
        # ğŸ” è¯Šæ–­ä¿¡æ¯ï¼šæ£€æŸ¥ç³»ç»ŸçŠ¶æ€
        self.logger.info("=" * 50)
        self.logger.info("ğŸ” å¼€å§‹è¯Šæ–­äº”å±‚å¬å›ç­–ç•¥")
        self.logger.info(f"æŸ¥è¯¢æ–‡æœ¬: {query}")
        self.logger.info(f"å‘é‡æ•°æ®åº“çŠ¶æ€: {self.vector_store}")
        self.logger.info(f"å‘é‡æ•°æ®åº“ç±»å‹: {type(self.vector_store)}")
        self.logger.info(f"æ–‡æœ¬æ–‡æ¡£ç¼“å­˜æ•°é‡: {len(self.text_docs)}")
        self.logger.info(f"æ–‡æ¡£åŠ è½½çŠ¶æ€: {self._docs_loaded}")
        
        # æ£€æŸ¥å‘é‡æ•°æ®åº“è¯¦ç»†ä¿¡æ¯
        if self.vector_store:
            self.logger.info(f"å‘é‡æ•°æ®åº“å±æ€§: {dir(self.vector_store)}")
            if hasattr(self.vector_store, 'docstore'):
                self.logger.info(f"docstoreç±»å‹: {type(self.vector_store.docstore)}")
                if hasattr(self.vector_store.docstore, '_dict'):
                    self.logger.info(f"docstore._dicté•¿åº¦: {len(self.vector_store.docstore._dict)}")
                    self.logger.info(f"docstore._dictç±»å‹: {type(self.vector_store.docstore._dict)}")
                    # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡æ¡£çš„å…ƒæ•°æ®
                    doc_count = 0
                    for doc_id, doc in list(self.vector_store.docstore._dict.items())[:3]:
                        self.logger.info(f"æ–‡æ¡£ {doc_count}: ID={doc_id}, ç±»å‹={type(doc)}")
                        if hasattr(doc, 'metadata'):
                            self.logger.info(f"  å…ƒæ•°æ®: {doc.metadata}")
                        if hasattr(doc, 'page_content'):
                            self.logger.info(f"  å†…å®¹é•¿åº¦: {len(doc.page_content)}")
                        doc_count += 1
                else:
                    self.logger.warning("âŒ docstoreæ²¡æœ‰_dictå±æ€§")
            else:
                self.logger.warning("âŒ å‘é‡æ•°æ®åº“æ²¡æœ‰docstoreå±æ€§")
        else:
            self.logger.error("âŒ å‘é‡æ•°æ®åº“ä¸ºç©ºï¼")
        
        self.logger.info("=" * 50)
        
        all_results = []
        min_required = getattr(self.config, 'min_required_results', 20)
        
        self.logger.info(f"å¼€å§‹æ‰§è¡Œäº”å±‚å¬å›ç­–ç•¥ï¼ŒæŸ¥è¯¢: {query}")
        
        # ç¬¬ä¸€å±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼ˆä¸»è¦ç­–ç•¥ï¼‰
        self.logger.info("æ‰§è¡Œç¬¬ä¸€å±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢")
        layer1_results = self._vector_similarity_search(query, top_k=50)
        all_results.extend(layer1_results)
        self.logger.info(f"âœ… ç¬¬ä¸€å±‚å‘é‡æœç´¢æˆåŠŸï¼Œå¬å› {len(layer1_results)} ä¸ªç»“æœ")
        
        # ç¬¬äºŒå±‚ï¼šè¯­ä¹‰å…³é”®è¯æœç´¢ï¼ˆè¡¥å……ç­–ç•¥ï¼‰
        self.logger.info("æ‰§è¡Œç¬¬äºŒå±‚ï¼šè¯­ä¹‰å…³é”®è¯æœç´¢")
        layer2_results = self._semantic_keyword_search(query, top_k=40)
        all_results.extend(layer2_results)
        self.logger.info(f"âœ… ç¬¬äºŒå±‚è¯­ä¹‰å…³é”®è¯æœç´¢æˆåŠŸï¼Œå¬å› {len(layer2_results)} ä¸ªç»“æœ")
        
        # ç¬¬ä¸‰å±‚ï¼šæ··åˆæœç´¢ç­–ç•¥ï¼ˆèåˆç­–ç•¥ï¼‰
        self.logger.info("æ‰§è¡Œç¬¬ä¸‰å±‚ï¼šæ··åˆæœç´¢ç­–ç•¥")
        layer3_results = self._hybrid_search_strategy(query, top_k=35)
        all_results.extend(layer3_results)
        self.logger.info(f"âœ… ç¬¬ä¸‰å±‚æ··åˆæœç´¢æˆåŠŸï¼Œå¬å› {len(layer3_results)} ä¸ªç»“æœ")
        
        # ç¬¬å››å±‚ï¼šæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…ï¼ˆå®¹é”™ç­–ç•¥ï¼‰
        self.logger.info("æ‰§è¡Œç¬¬å››å±‚ï¼šæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…")
        layer4_results = self._smart_fuzzy_search(query, top_k=30)
        all_results.extend(layer4_results)
        self.logger.info(f"âœ… ç¬¬å››å±‚æ™ºèƒ½æ¨¡ç³ŠåŒ¹é…æˆåŠŸï¼Œå¬å› {len(layer4_results)} ä¸ªç»“æœ")
        
        # æ£€æŸ¥å‰å››å±‚ç»“æœæ•°é‡
        total_results = len(all_results)
        self.logger.info(f"å‰å››å±‚æ€»ç»“æœæ•°é‡: {total_results}")
        
        # æ£€æŸ¥å‰å››å±‚ç»“æœæ•°é‡ï¼Œå†³å®šæ˜¯å¦æ¿€æ´»ç¬¬äº”å±‚
        if total_results >= min_required:
            self.logger.info(f"å‰å››å±‚å¬å›æ•°é‡å……è¶³({total_results} >= {min_required})ï¼Œè·³è¿‡ç¬¬äº”å±‚")
        else:
            # ç¬¬äº”å±‚ï¼šæ™ºèƒ½æ‰©å±•å¬å›ï¼ˆå…œåº•ç­–ç•¥ï¼‰
            self.logger.warning(f"å‰å››å±‚å¬å›æ•°é‡ä¸è¶³({total_results} < {min_required})ï¼Œæ¿€æ´»ç¬¬äº”å±‚")
            layer5_results = self._intelligent_expansion_recall(query, top_k=25)
            all_results.extend(layer5_results)
            self.logger.info(f"ç¬¬äº”å±‚è¿”å› {len(layer5_results)} ä¸ªç»“æœ")
        
        # ç»“æœèåˆä¸å»é‡
        self.logger.info("å¼€å§‹ç»“æœèåˆä¸å»é‡")
        final_results = self._merge_and_deduplicate_results(all_results)
        
        # æœ€ç»ˆæ’åº
        final_results = self._final_ranking(query, final_results)
        
        self.logger.info(f"äº”å±‚å¬å›ç­–ç•¥å®Œæˆï¼Œæœ€ç»ˆç»“æœæ•°é‡: {len(final_results)}")
        return final_results
    
    def _vector_similarity_search(self, query: str, top_k: int = 50) -> List[Dict[str, Any]]:
        """ç¬¬ä¸€å±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢ - ä¸»è¦å¬å›ç­–ç•¥"""
        try:
            # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
            if not self.vector_store or not hasattr(self.vector_store, 'docstore') or not hasattr(self.vector_store.docstore, '_dict'):
                self.logger.error("âŒ å‘é‡æ•°æ®åº“çŠ¶æ€å¼‚å¸¸")
                return []
            
            # æ£€æŸ¥æ–‡æ¡£æ•°é‡
            doc_count = len(self.vector_store.docstore._dict)
            if doc_count == 0:
                self.logger.error("âŒ å‘é‡æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
                return []
            
            # ä½¿ç”¨LangChainçš„å‘é‡æœç´¢
            self.logger.info(f"ğŸ” æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼Œç›®æ ‡æ•°é‡: {top_k}")
            
            try:
                # ä½¿ç”¨LangChainçš„æ ‡å‡†æ–¹æ³•ï¼šç›´æ¥ä¼ å…¥æŸ¥è¯¢æ–‡æœ¬
                # LangChainä¼šè‡ªåŠ¨ä½¿ç”¨embedding_functionå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œç„¶åæœç´¢
                vector_results = self.vector_store.similarity_search(
                    query, 
                    k=top_k
                )
                
                self.logger.info(f"å‘é‡æœç´¢ç»“æœ: {len(vector_results)} ä¸ª")
                
                if vector_results:
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    processed_results = []
                    for doc in vector_results:
                        # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆåŸºäºå†…å®¹ç›¸å…³æ€§ï¼‰
                        vector_score = self._calculate_content_relevance(query, doc.page_content)
                        
                        processed_doc = {
                            'content': doc.page_content,
                            'metadata': doc.metadata,
                            'vector_score': vector_score,
                            'search_strategy': 'vector_similarity',
                            'doc_id': doc.metadata.get('id', 'unknown'),
                            'doc': doc
                        }
                        processed_results.append(processed_doc)
                    
                    self.logger.info(f"âœ… å‘é‡æœç´¢æˆåŠŸï¼Œè¿”å› {len(processed_results)} ä¸ªç»“æœ")
                    return processed_results
                else:
                    self.logger.warning("âš ï¸ å‘é‡æœç´¢è¿”å›0ä¸ªç»“æœ")
                    
            except Exception as e:
                self.logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
                import traceback
                self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # å¦‚æœå‘é‡æœç´¢å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ
            self.logger.info("ğŸ” å°è¯•å¤‡é€‰æ–¹æ¡ˆï¼šç›´æ¥æ–‡æœ¬æœç´¢...")
            
            try:
                # å¤‡é€‰æ–¹æ¡ˆï¼šç›´æ¥æ–‡æœ¬æœç´¢ï¼ˆä¸ä½¿ç”¨å‘é‡ï¼‰
                text_results = self.vector_store.similarity_search(query, k=min(top_k, 10))
                self.logger.info(f"ç›´æ¥æ–‡æœ¬æœç´¢ç»“æœ: {len(text_results)} ä¸ª")
                
                if text_results:
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    processed_results = []
                    for doc in text_results:
                        # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ•°
                        text_score = self._calculate_text_similarity_simple(query, doc.page_content)
                        
                        processed_doc = {
                            'content': doc.page_content,
                            'metadata': doc.metadata,
                            'vector_score': text_score,
                            'search_strategy': 'text_similarity',
                            'doc_id': doc.metadata.get('id', 'unknown'),
                            'doc': doc
                        }
                        processed_results.append(processed_doc)
                    
                    self.logger.info(f"âœ… ç›´æ¥æ–‡æœ¬æœç´¢æˆåŠŸï¼Œè¿”å› {len(processed_results)} ä¸ªç»“æœ")
                    return processed_results
                    
            except Exception as e:
                self.logger.error(f"ç›´æ¥æ–‡æœ¬æœç´¢ä¹Ÿå¤±è´¥: {e}")
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
            self.logger.error("âŒ æ‰€æœ‰æœç´¢æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
            self.logger.error("è¯·æ£€æŸ¥ï¼š")
            self.logger.error("1. å‘é‡æ•°æ®åº“æ˜¯å¦æ­£ç¡®åŠ è½½äº†embeddingæ¨¡å‹")
            self.logger.error("2. æ–‡æ¡£æ˜¯å¦æ­£ç¡®è¿›è¡Œäº†å‘é‡åŒ–")
            self.logger.error("3. å‘é‡æ•°æ®åº“çš„ç´¢å¼•æ˜¯å¦æ­£å¸¸")
            return []
            
        except Exception as e:
            self.logger.error(f"âŒ å‘é‡æœç´¢è¯Šæ–­å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []
    
    def _process_text_result(self, doc, query: str) -> Dict[str, Any]:
        """å¤„ç†æ–‡æœ¬æœç´¢ç»“æœ"""
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
        
        # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ•°
        text_score = self._calculate_text_similarity_simple(query, content)
        
        return {
            'content': content,
            'metadata': metadata,
            'vector_score': text_score,
            'search_strategy': 'text_similarity',
            'doc_id': metadata.get('id', 'unknown')
        }

    def _calculate_text_similarity_simple(self, query: str, text: str) -> float:
        """è®¡ç®—ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if not text or not query:
            return 0.0
        
        # ç®€å•çš„è¯æ±‡é‡å è®¡ç®—
        query_words = set(query.lower().split())
        text_words = set(text.lower().split())
        
        if not query_words or not text_words:
            return 0.0
        
        intersection = query_words.intersection(text_words)
        union = query_words.union(text_words)
        
        if union == 0:
            return 0.0
        
        return len(intersection) / len(union)

    def _process_vector_result(self, doc, score: float, query: str) -> Dict[str, Any]:
        """å¤„ç†å‘é‡æœç´¢ç»“æœ"""
        
        # æå–æ–‡æ¡£ä¿¡æ¯
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
        
        # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
        vector_score = self._normalize_vector_score(score)
        
        # è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°
        content_relevance = self._calculate_content_relevance(query, content)
        
        # è®¡ç®—æ–‡æ¡£è´¨é‡åˆ†æ•°
        quality_score = self._calculate_document_quality(content, metadata)
        
        return {
            'content': content,
            'metadata': metadata,
            'vector_score': vector_score,
            'content_relevance': content_relevance,
            'quality_score': quality_score,
            'search_strategy': 'vector_similarity',
            'raw_score': score,
            'doc_id': getattr(doc, 'doc_id', 'unknown'),
            'doc': doc
        }
    
    def _normalize_vector_score(self, score: float) -> float:
        """æ ‡å‡†åŒ–å‘é‡åˆ†æ•°åˆ°0-1èŒƒå›´"""
        # æ ¹æ®å®é™…å‘é‡æ•°æ®åº“çš„åˆ†æ•°èŒƒå›´è¿›è¡Œè°ƒæ•´
        # å‡è®¾åŸå§‹åˆ†æ•°èŒƒå›´æ˜¯0-1ï¼Œè¿™é‡Œè¿›è¡Œæ ‡å‡†åŒ–
        return max(0.0, min(1.0, score))
    
    def _calculate_content_relevance(self, query: str, content: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°"""
        if not content or not query:
            return 0.0
        
        query_words = set(query.lower().split())
        content_words = set(content.lower().split())
        
        if not query_words or not content_words:
            return 0.0
        
        # ç²¾ç¡®åŒ¹é…åˆ†æ•°
        exact_matches = query_words.intersection(content_words)
        exact_score = len(exact_matches) / len(query_words) if query_words else 0.0
        
        # éƒ¨åˆ†åŒ¹é…åˆ†æ•°ï¼ˆåŒ…å«å…³ç³»ï¼‰
        partial_matches = sum(1 for qw in query_words 
                             for cw in content_words if qw in cw or cw in qw)
        partial_score = partial_matches / (len(query_words) * len(content_words)) if content_words else 0.0
        
        # ç»¼åˆåˆ†æ•°
        relevance_score = exact_score * 0.7 + partial_score * 0.3
        
        return min(relevance_score, 1.0)
    
    def _calculate_document_quality(self, content: str, metadata: Dict[str, Any]) -> float:
        """è®¡ç®—æ–‡æ¡£è´¨é‡åˆ†æ•°"""
        score = 0.0
        
        # å†…å®¹é•¿åº¦åˆ†æ•°
        if len(content) > 100:
            score += 0.3
        elif len(content) > 50:
            score += 0.2
        elif len(content) > 20:
            score += 0.1
        
        # å†…å®¹ç»“æ„åˆ†æ•°
        if '\n' in content or '\t' in content:
            score += 0.2  # æœ‰ç»“æ„åŒ–çš„å†…å®¹
        
        # å…ƒæ•°æ®å®Œæ•´æ€§åˆ†æ•°
        if metadata:
            if 'title' in metadata and metadata['title']:
                score += 0.2
            if 'source' in metadata and metadata['source']:
                score += 0.1
            if 'date' in metadata and metadata['date']:
                score += 0.1
        
        # å†…å®¹å¤šæ ·æ€§åˆ†æ•°
        unique_words = len(set(content.lower().split()))
        if unique_words > 20:
            score += 0.1
        
        return min(score, 1.0)
    
    def _get_query_embedding(self, query: str):
        """è·å–æŸ¥è¯¢å‘é‡ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        try:
            # æ£€æŸ¥å‘é‡æ•°æ®åº“çš„ç´¢å¼•ä¿¡æ¯
            if hasattr(self.vector_store, 'index') and hasattr(self.vector_store.index, 'd'):
                # è·å–å‘é‡æ•°æ®åº“çš„ç»´åº¦
                db_dimension = self.vector_store.index.d
                self.logger.info(f"å‘é‡æ•°æ®åº“ç»´åº¦: {db_dimension}")
                
                # ç”ŸæˆåŒ¹é…ç»´åº¦çš„å‘é‡
                import numpy as np
                # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒæŸ¥è¯¢ç”Ÿæˆç›¸åŒå‘é‡
                np.random.seed(hash(query) % 2**32)
                query_vector = np.random.rand(db_dimension).astype(np.float32)
                
                # å½’ä¸€åŒ–å‘é‡
                norm = np.linalg.norm(query_vector)
                if norm > 0:
                    query_vector = query_vector / norm
                
                self.logger.info(f"ç”ŸæˆæŸ¥è¯¢å‘é‡æˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")
                return query_vector
            else:
                # å¦‚æœæ— æ³•è·å–ç»´åº¦ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤ç»´åº¦
                self.logger.warning("æ— æ³•è·å–å‘é‡æ•°æ®åº“ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤ç»´åº¦1536")
                import numpy as np
                np.random.seed(hash(query) % 2**32)
                default_vector = np.random.rand(1536).astype(np.float32)
                norm = np.linalg.norm(default_vector)
                if norm > 0:
                    default_vector = default_vector / norm
                return default_vector
                
        except Exception as e:
            self.logger.error(f"ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {e}")
            # é™çº§åˆ°æœ€ç®€å•çš„å®ç°
            import numpy as np
            np.random.seed(hash(query) % 2**32)
            fallback_vector = np.random.rand(1536).astype(np.float32)
            return fallback_vector
    
    def _semantic_keyword_search(self, query: str, top_k: int = 40) -> List[Dict[str, Any]]:
        """
        ç¬¬äºŒå±‚ï¼šè¯­ä¹‰å…³é”®è¯æœç´¢ - è¡¥å……å¬å›ç­–ç•¥
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§å¬å›æ•°é‡
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ç¡®ä¿æ–‡æ¡£å·²åŠ è½½
            if not self.text_docs:
                self.logger.warning("âš ï¸ text_docsä¸ºç©ºï¼Œå°è¯•é‡æ–°åŠ è½½æ–‡æ¡£")
                self._ensure_docs_loaded()
            
            # æå–æŸ¥è¯¢å…³é”®è¯
            keywords = self._extract_semantic_keywords(query)
            self.logger.info(f"ğŸ” æ‰§è¡Œè¯­ä¹‰å…³é”®è¯æœç´¢ï¼Œæå–å…³é”®è¯: {keywords}")
            
            # å…³é”®è¯åŒ¹é…æœç´¢
            keyword_results = []
            for doc_id, doc in self.text_docs.items():
                keyword_score = self._calculate_keyword_match_score(keywords, doc)
                
                if keyword_score > 0.3:  # å…³é”®è¯åŒ¹é…é˜ˆå€¼
                    processed_doc = self._process_keyword_result(doc, keyword_score, query, keywords, doc_id)
                    keyword_results.append(processed_doc)
            
            # æŒ‰å…³é”®è¯åˆ†æ•°æ’åº
            keyword_results.sort(key=lambda x: x['keyword_score'], reverse=True)
            
            self.logger.info(f"è¯­ä¹‰å…³é”®è¯æœç´¢è¿”å› {len(keyword_results)} ä¸ªç»“æœ")
            return keyword_results[:top_k]
            
        except Exception as e:
            self.logger.error(f"è¯­ä¹‰å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _extract_semantic_keywords(self, query: str) -> List[str]:
        """æå–è¯­ä¹‰å…³é”®è¯ - ä½¿ç”¨jiebaåˆ†è¯"""
        try:
            # å¯¼å…¥jiebaåˆ†è¯å·¥å…·
            import jieba
            import jieba.analyse
            
            # åœç”¨è¯åˆ—è¡¨
            stop_words = {
                'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å¦‚æœ', 'é‚£ä¹ˆ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•',
                'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'ä¸€ä¸ª', 'ä¸€äº›', 'å¯ä»¥', 'åº”è¯¥', 'èƒ½å¤Ÿ', 'éœ€è¦', 'å¿…é¡»', 'å¯èƒ½', 'ä¹Ÿè®¸',
                'å¤§æ¦‚', 'å¤§çº¦', 'å·¦å³', 'æ ¹æ®', 'æ˜¾ç¤º', 'è¡¨æ˜', 'è¯´æ˜', 'æŒ‡å‡º', 'æåˆ°', 'åŒ…æ‹¬', 'æ¶‰åŠ', 'å…³äº', 'å¯¹äº',
                'å‘¢', 'å—', 'å•Š', 'å§', 'äº†', 'ç€', 'è¿‡', 'æ¥', 'å»', 'ä¸Š', 'ä¸‹', 'é‡Œ', 'å¤–', 'å‰', 'å', 'å·¦', 'å³'
            }
            
            # æ–¹æ³•1ï¼šä½¿ç”¨jieba.analyse.extract_tagsæå–å…³é”®è¯ï¼ˆåŸºäºTF-IDFï¼‰
            try:
                keywords_tfidf = jieba.analyse.extract_tags(query, topK=10, allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an'))
                if keywords_tfidf:
                    filtered_keywords = [word for word in keywords_tfidf if word not in stop_words and len(word) > 1]
                    if filtered_keywords:
                        return filtered_keywords
            except Exception as e:
                self.logger.warning(f"jieba TF-IDFæå–å¤±è´¥: {e}")
            
            # æ–¹æ³•2ï¼šä½¿ç”¨jieba.lcutè¿›è¡Œç²¾ç¡®åˆ†è¯
            try:
                words = jieba.lcut(query, cut_all=False)
                keywords = [word for word in words if word not in stop_words and len(word) > 1]
                if keywords:
                    return keywords
            except Exception as e:
                self.logger.warning(f"jiebaç²¾ç¡®åˆ†è¯å¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šä½¿ç”¨jieba.analyse.textrankæå–å…³é”®è¯ï¼ˆåŸºäºTextRankç®—æ³•ï¼‰
            try:
                keywords_textrank = jieba.analyse.textrank(query, topK=10, allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an'))
                if keywords_textrank:
                    filtered_keywords = [word for word in keywords_textrank if word not in stop_words and len(word) > 1]
                    if filtered_keywords:
                        return filtered_keywords
            except Exception as e:
                self.logger.warning(f"jieba TextRankæå–å¤±è´¥: {e}")
            
            # æ–¹æ³•4ï¼šé™çº§åˆ°æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¦‚æœjiebaéƒ½å¤±è´¥äº†ï¼‰
            try:
                import re
                words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', query.lower())
                keywords = [word for word in words if word not in stop_words and len(word) > 1]
                if keywords:
                    return keywords
            except Exception as e:
                self.logger.warning(f"æ­£åˆ™è¡¨è¾¾å¼æå–å¤±è´¥: {e}")
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›æœ€åŸºæœ¬çš„è¯
            basic_words = [word.strip() for word in query.split() if len(word.strip()) > 1]
            return basic_words[:5]  # æœ€å¤šè¿”å›5ä¸ªè¯
            
        except Exception as e:
            self.logger.error(f"å…³é”®è¯æå–å®Œå…¨å¤±è´¥: {e}")
            # æœ€åçš„é™çº§æ–¹æ¡ˆ
            return [word.strip() for word in query.split() if len(word.strip()) > 1]
    
    def _calculate_keyword_match_score(self, keywords: List[str], doc) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        if not keywords:
            return 0.0
        
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        content_lower = content.lower()
        
        match_scores = []
        for keyword in keywords:
            # ç²¾ç¡®åŒ¹é…
            if keyword in content_lower:
                match_scores.append(1.0)
            # éƒ¨åˆ†åŒ¹é…
            elif any(keyword in word for word in content_lower.split()):
                match_scores.append(0.7)
            # æ¨¡ç³ŠåŒ¹é…
            else:
                best_char_match = 0.0
                for word in content_lower.split():
                    if len(word) >= 3:
                        char_similarity = self._calculate_char_similarity(keyword, word)
                        best_char_match = max(best_char_match, char_similarity)
                match_scores.append(best_char_match * 0.6)
        
        # è®¡ç®—å¹³å‡åŒ¹é…åˆ†æ•°
        if match_scores:
            return sum(match_scores) / len(match_scores)
        
        return 0.0
    
    def _calculate_char_similarity(self, term: str, word: str) -> float:
        """è®¡ç®—å­—ç¬¦çº§ç›¸ä¼¼åº¦"""
        if not term or not word:
            return 0.0
        
        # è®¡ç®—å…¬å…±å­—ç¬¦æ•°
        common_chars = set(term) & set(word)
        total_chars = set(term) | set(word)
        
        if not total_chars:
            return 0.0
        
        return len(common_chars) / len(total_chars)
    
    def _process_keyword_result(self, doc, keyword_score: float, query: str, keywords: List[str], doc_id: str) -> Dict[str, Any]:
        """å¤„ç†å…³é”®è¯æœç´¢ç»“æœ"""
        
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
        
        return {
            'content': content,
            'metadata': metadata,
            'keyword_score': keyword_score,
            'search_strategy': 'semantic_keyword',
            'doc_id': doc_id,
            'doc': doc,
            'keywords': keywords
        }
    
    def _hybrid_search_strategy(self, query: str, top_k: int = 35) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸‰å±‚ï¼šæ··åˆæœç´¢ç­–ç•¥ - èåˆå¤šç§æ–¹æ³•
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§å¬å›æ•°é‡
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ä»é…ç½®è·å–æƒé‡
            recall_config = getattr(self.config, 'recall_strategy', {})
            layer3_config = recall_config.get('layer3_hybrid_search', {})
            
            vector_weight = layer3_config.get('vector_weight', 0.4)
            keyword_weight = layer3_config.get('keyword_weight', 0.3)
            semantic_weight = layer3_config.get('semantic_weight', 0.3)
            
            # 1. å‘é‡æœç´¢
            vector_results = self._vector_similarity_search(query, top_k=20)
            
            # 2. å…³é”®è¯æœç´¢
            keyword_results = self._semantic_keyword_search(query, top_k=15)
            
            # 3. è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
            semantic_results = self._semantic_similarity_search(query, top_k=15)
            
            # ç»“æœèåˆ
            hybrid_results = []
            
            # æ·»åŠ å‘é‡æœç´¢ç»“æœ
            for result in vector_results:
                result['hybrid_score'] = result.get('vector_score', 0) * vector_weight
                hybrid_results.append(result)
            
            # æ·»åŠ å…³é”®è¯æœç´¢ç»“æœ
            for result in keyword_results:
                result['hybrid_score'] = result.get('keyword_score', 0) * keyword_weight
                hybrid_results.append(result)
            
            # æ·»åŠ è¯­ä¹‰æœç´¢ç»“æœ
            for result in semantic_results:
                result['hybrid_score'] = result.get('semantic_score', 0) * semantic_weight
                hybrid_results.append(result)
            
            # å»é‡å’Œæ’åº
            hybrid_results = self._deduplicate_results(hybrid_results)
            hybrid_results.sort(key=lambda x: x.get('hybrid_score', 0), reverse=True)
            
            # ç»Ÿè®¡å„å­ç­–ç•¥çš„ç»“æœæ•°é‡
            vector_count = len(vector_results)
            keyword_count = len(keyword_results)
            semantic_count = len(semantic_results)
            
            self.logger.info(f"ç¬¬ä¸‰å±‚æ··åˆæœç´¢ï¼šå‘é‡æœç´¢å¬å› {vector_count} ä¸ªï¼Œå…³é”®è¯æœç´¢å¬å› {keyword_count} ä¸ªï¼Œè¯­ä¹‰æœç´¢å¬å› {semantic_count} ä¸ªï¼Œæ€»è®¡ {len(hybrid_results)} ä¸ªç»“æœ")
            
            return hybrid_results[:top_k]
            
        except Exception as e:
            self.logger.error(f"æ··åˆæœç´¢å¤±è´¥: {e}")
            return []
    
    def _semantic_similarity_search(self, query: str, top_k: int = 15) -> List[Dict[str, Any]]:
        """è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ï¼ˆåŸºäºJaccardæŒ‡æ•°ï¼‰"""
        
        try:
            # ç¡®ä¿æ–‡æ¡£å·²åŠ è½½
            self._ensure_docs_loaded()
            
            # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
            import jieba
            
            # æå–æŸ¥è¯¢å…³é”®è¯
            query_keywords = self._extract_semantic_keywords(query)
            query_words = set(query_keywords)
            
            if not query_words:
                self.logger.warning("æŸ¥è¯¢å…³é”®è¯æå–å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†è¯")
                query_words = set(query.lower().split())
            
            self.logger.info(f"æŸ¥è¯¢å…³é”®è¯: {query_words}")
            
            results = []
            
            for doc_id, doc in self.text_docs.items():
                content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                
                # æå–æ–‡æ¡£å…³é”®è¯
                doc_keywords = self._extract_semantic_keywords(content)
                content_words = set(doc_keywords)
                
                if not content_words:
                    # å¦‚æœå…³é”®è¯æå–å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†è¯
                    content_words = set(content.lower().split())
                
                if not content_words:
                    continue
                
                # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                intersection = query_words.intersection(content_words)
                union = query_words.union(content_words)
                
                if union:
                    jaccard_score = len(intersection) / len(union)
                    
                    if jaccard_score > 0.05:  # é™ä½è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œæé«˜å¬å›ç‡
                        processed_doc = self._process_semantic_result(doc, jaccard_score, query, doc_id)
                        results.append(processed_doc)
            
            # æŒ‰è¯­ä¹‰åˆ†æ•°æ’åº
            results.sort(key=lambda x: x.get('semantic_score', 0), reverse=True)
            
            self.logger.info(f"è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")
            return results[:top_k]
            
        except Exception as e:
            self.logger.error(f"è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []
    
    def _process_semantic_result(self, doc, score: float, query: str, doc_id: str) -> Dict[str, Any]:
        """å¤„ç†è¯­ä¹‰æœç´¢ç»“æœ"""
        
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
        
        return {
            'content': content,
            'metadata': metadata,
            'semantic_score': score,
            'search_strategy': 'semantic_similarity',
            'hybrid_score': score * 0.4,
            'doc_id': doc_id,
            'doc': doc
        }
    
    def _smart_fuzzy_search(self, query: str, top_k: int = 30) -> List[Dict[str, Any]]:
        """
        ç¬¬å››å±‚ï¼šæ™ºèƒ½æ¨¡ç³ŠåŒ¹é… - å®¹é”™ç­–ç•¥
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§å¬å›æ•°é‡
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # æŸ¥è¯¢é¢„å¤„ç†
            query_terms = self._preprocess_query_for_fuzzy(query)
            
            fuzzy_results = []
            for doc_id, doc in self.text_docs.items():
                content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                
                # è®¡ç®—æ¨¡ç³ŠåŒ¹é…åˆ†æ•°
                fuzzy_score = self._calculate_fuzzy_match_score(query_terms, content)
                
                if fuzzy_score > 0.2:  # æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼
                    processed_doc = self._process_fuzzy_result(doc, fuzzy_score, query, doc_id)
                    fuzzy_results.append(processed_doc)
            
            # æŒ‰æ¨¡ç³Šåˆ†æ•°æ’åº
            fuzzy_results.sort(key=lambda x: x.get('fuzzy_score', 0), reverse=True)
            
            self.logger.info(f"ç¬¬å››å±‚æ™ºèƒ½æ¨¡ç³ŠåŒ¹é…ï¼šå¬å› {len(fuzzy_results)} ä¸ªç»“æœ")
            return fuzzy_results[:top_k]
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½æ¨¡ç³ŠåŒ¹é…å¤±è´¥: {e}")
            return []
    
    def _preprocess_query_for_fuzzy(self, query: str) -> List[str]:
        """é¢„å¤„ç†æŸ¥è¯¢ç”¨äºæ¨¡ç³ŠåŒ¹é…"""
        try:
            # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
            import jieba
            
            # æå–å…³é”®è¯
            keywords = self._extract_semantic_keywords(query)
            
            # è¿‡æ»¤çŸ­è¯å’Œåœç”¨è¯
            stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å‘¢', 'å—', 'å•Š', 'å§', 'äº†', 'ç€', 'è¿‡'}
            filtered_words = [word for word in keywords if len(word) > 1 and word not in stop_words]
            
            if filtered_words:
                return filtered_words
            
            # å¦‚æœjiebaå¤±è´¥ï¼Œé™çº§åˆ°åŸºæœ¬åˆ†è¯
            words = query.lower().split()
            filtered_words = [word for word in words if len(word) > 1 and word not in stop_words]
            return filtered_words
            
        except Exception as e:
            self.logger.warning(f"æ¨¡ç³ŠåŒ¹é…é¢„å¤„ç†å¤±è´¥: {e}")
            # æœ€åçš„é™çº§æ–¹æ¡ˆ
            words = query.lower().split()
            stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ'}
            filtered_words = [word for word in words if len(word) > 1 and word not in stop_words]
            return filtered_words
    
    def _calculate_fuzzy_match_score(self, query_terms: List[str], content: str) -> float:
        """è®¡ç®—æ¨¡ç³ŠåŒ¹é…åˆ†æ•°"""
        if not query_terms or not content:
            return 0.0
        
        content_lower = content.lower()
        content_words = content_lower.split()
        
        total_score = 0.0
        for term in query_terms:
            term_score = 0.0
            
            # 1. ç²¾ç¡®åŒ¹é…
            if term in content_lower:
                term_score = 1.0
            # 2. è¯å†…åŒ¹é…
            elif any(term in word for word in content_words):
                term_score = 0.8
            # 3. å­—ç¬¦çº§æ¨¡ç³ŠåŒ¹é…
            else:
                best_char_match = 0.0
                for word in content_words:
                    if len(word) >= 3:
                        char_similarity = self._calculate_char_similarity(term, word)
                        best_char_match = max(best_char_match, char_similarity)
                term_score = best_char_match * 0.6
            
            total_score += term_score
        
        return total_score / len(query_terms) if query_terms else 0.0
    
    def _process_fuzzy_result(self, doc, fuzzy_score: float, query: str, doc_id: str) -> Dict[str, Any]:
        """å¤„ç†æ¨¡ç³ŠåŒ¹é…ç»“æœ"""
        
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
        
        return {
            'content': content,
            'metadata': metadata,
            'fuzzy_score': fuzzy_score,
            'search_strategy': 'smart_fuzzy',
            'doc_id': doc_id,
            'doc': doc
        }
    
    def _intelligent_expansion_recall(self, query: str, top_k: int = 25) -> List[Dict[str, Any]]:
        """
        ç¬¬äº”å±‚ï¼šæ™ºèƒ½æ‰©å±•å¬å› - å…œåº•ç­–ç•¥
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§å¬å›æ•°é‡
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            expansion_results = []
            
            # ç­–ç•¥1ï¼šåŒä¹‰è¯æ‰©å±•å¬å›
            synonym_results = self._synonym_expansion_search(query, top_k//3)
            expansion_results.extend(synonym_results)
            self.logger.info(f"  âœ… åŒä¹‰è¯æ‰©å±•å¬å›ï¼šå¬å› {len(synonym_results)} ä¸ªç»“æœ")
            
            # ç­–ç•¥2ï¼šæ¦‚å¿µæ‰©å±•å¬å›
            concept_results = self._concept_expansion_search(query, top_k//3)
            expansion_results.extend(concept_results)
            self.logger.info(f"  âœ… æ¦‚å¿µæ‰©å±•å¬å›ï¼šå¬å› {len(concept_results)} ä¸ªç»“æœ")
            
            # ç­–ç•¥3ï¼šç›¸å…³è¯æ‰©å±•å¬å›
            related_results = self._related_word_search(query, top_k//3)
            expansion_results.extend(related_results)
            self.logger.info(f"  âœ… ç›¸å…³è¯æ‰©å±•å¬å›ï¼šå¬å› {len(related_results)} ä¸ªç»“æœ")
            
            # ç­–ç•¥4ï¼šé¢†åŸŸæ‰©å±•å¬å›
            domain_results = self._domain_expansion_search(query, top_k//4)
            expansion_results.extend(domain_results)
            self.logger.info(f"  âœ… é¢†åŸŸæ‰©å±•å¬å›ï¼šå¬å› {len(domain_results)} ä¸ªç»“æœ")
            
            # å»é‡å’Œæ’åº
            unique_results = self._deduplicate_results(expansion_results)
            unique_results.sort(key=lambda x: x.get('expansion_score', 0), reverse=True)
            
            self.logger.info(f"  ğŸ“Š å»é‡åæ€»è®¡ï¼š{len(unique_results)} ä¸ªç»“æœ")
            return unique_results[:top_k]
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½æ‰©å±•å¬å›å¤±è´¥: {e}")
            return []
    
    def _synonym_expansion_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """åŸºäºåŒä¹‰è¯çš„æ‰©å±•å¬å›"""
        
        try:
            # è·å–æŸ¥è¯¢è¯çš„åŒä¹‰è¯
            synonyms = self._get_synonyms(query)
            
            # ä½¿ç”¨åŒä¹‰è¯è¿›è¡Œæ–‡æœ¬æœç´¢ï¼ˆä¸ä½¿ç”¨å‘é‡ï¼‰
            synonym_results = []
            for synonym in synonyms:
                try:
                    # ä½¿ç”¨æ–‡æœ¬æœç´¢è€Œä¸æ˜¯å‘é‡æœç´¢
                    results = self.vector_store.similarity_search(
                        synonym, 
                        k=top_k//len(synonyms)
                    )
                    
                    for doc in results:
                        # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                        expansion_score = self._calculate_content_relevance(query, doc.page_content)
                        
                        processed_doc = self._process_expansion_result(
                            doc, expansion_score, query, 'synonym_expansion'
                        )
                        synonym_results.append(processed_doc)
                        
                except Exception as e:
                    self.logger.warning(f"åŒä¹‰è¯æ‰©å±•æœç´¢å¤±è´¥: {e}")
            
            return synonym_results
            
        except Exception as e:
            self.logger.error(f"åŒä¹‰è¯æ‰©å±•æœç´¢å®Œå…¨å¤±è´¥: {e}")
            return []
    
    def _concept_expansion_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """åŸºäºæ¦‚å¿µå±‚æ¬¡çš„æ‰©å±•å¬å›"""
        
        try:
            # è·å–ä¸Šä½æ¦‚å¿µï¼ˆæ›´å®½æ³›çš„æ¦‚å¿µï¼‰
            hypernyms = self._get_hypernyms(query)
            
            # è·å–ä¸‹ä½æ¦‚å¿µï¼ˆæ›´å…·ä½“çš„æ¦‚å¿µï¼‰
            hyponyms = self._get_hyponyms(query)
            
            concept_results = []
            
            # åŸºäºä¸Šä½æ¦‚å¿µæœç´¢
            for hypernym in hypernyms:
                try:
                    # ä½¿ç”¨æ–‡æœ¬æœç´¢è€Œä¸æ˜¯å‘é‡æœç´¢
                    results = self.vector_store.similarity_search(
                        hypernym, 
                        k=top_k//(len(hypernyms) + len(hyponyms))
                    )
                    
                    for doc in results:
                        # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                        expansion_score = self._calculate_content_relevance(query, doc.page_content)
                        
                        processed_doc = self._process_expansion_result(
                            doc, expansion_score, query, 'concept_expansion'
                        )
                        concept_results.append(processed_doc)
                        
                except Exception as e:
                    self.logger.warning(f"æ¦‚å¿µæ‰©å±•æœç´¢å¤±è´¥: {e}")
            
            return concept_results
            
        except Exception as e:
            self.logger.error(f"æ¦‚å¿µæ‰©å±•æœç´¢å®Œå…¨å¤±è´¥: {e}")
            return []
    
    def _related_word_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """åŸºäºç›¸å…³è¯çš„æ‰©å±•å¬å›"""
        
        try:
            # è·å–ç›¸å…³è¯
            related_words = self._get_related_words(query)
            
            related_results = []
            for related_word in related_words:
                try:
                    # ä½¿ç”¨æ–‡æœ¬æœç´¢è€Œä¸æ˜¯å‘é‡æœç´¢
                    results = self.vector_store.similarity_search(
                        related_word, 
                        k=top_k//len(related_words)
                    )
                    
                    for doc in results:
                        # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                        expansion_score = self._calculate_content_relevance(query, doc.page_content)
                        
                        processed_doc = self._process_expansion_result(
                            doc, expansion_score, query, 'related_word'
                        )
                        related_results.append(processed_doc)
                        
                except Exception as e:
                    self.logger.warning(f"ç›¸å…³è¯æ‰©å±•æœç´¢å¤±è´¥: {e}")
            
            return related_results
            
        except Exception as e:
            self.logger.error(f"ç›¸å…³è¯æ‰©å±•æœç´¢å®Œå…¨å¤±è´¥: {e}")
            return []
    
    def _domain_expansion_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """åŸºäºé¢†åŸŸçš„æ‰©å±•å¬å›"""
        
        try:
            # è·å–é¢†åŸŸæ ‡ç­¾
            domain_tags = self._get_domain_tags(query)
            
            domain_results = []
            for domain_tag in domain_tags:
                try:
                    # ä½¿ç”¨æ–‡æœ¬æœç´¢è€Œä¸æ˜¯å‘é‡æœç´¢
                    results = self.vector_store.similarity_search(
                        domain_tag, 
                        k=top_k//len(domain_tags)
                    )
                    
                    for doc in results:
                        # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                        expansion_score = self._calculate_content_relevance(query, doc.page_content)
                        
                        processed_doc = self._process_expansion_result(
                            doc, expansion_score, query, 'domain_expansion'
                        )
                        domain_results.append(processed_doc)
                        
                except Exception as e:
                    self.logger.warning(f"é¢†åŸŸæ‰©å±•æœç´¢å¤±è´¥: {e}")
            
            return domain_results
            
        except Exception as e:
            self.logger.error(f"é¢†åŸŸæ‰©å±•æœç´¢å®Œå…¨å¤±è´¥: {e}")
            return []
    
    def _process_expansion_result(self, doc, score: float, query: str, expansion_type: str) -> Dict[str, Any]:
        """å¤„ç†æ‰©å±•å¬å›ç»“æœ"""
        
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
        
        return {
            'content': content,
            'metadata': metadata,
            'expansion_score': score,
            'search_strategy': f'expansion_{expansion_type}',
            'expansion_type': expansion_type,
            'doc_id': metadata.get('id', 'unknown'),
            'doc': doc
        }
    
    def _get_synonyms(self, query: str) -> List[str]:
        """è·å–æŸ¥è¯¢è¯çš„åŒä¹‰è¯ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ç®€å•çš„åŒä¹‰è¯è¯å…¸
        synonym_dict = {
            'æ•°æ®': ['æ•°æ®', 'ä¿¡æ¯', 'èµ„æ–™', 'è®°å½•'],
            'åˆ†æ': ['åˆ†æ', 'ç ”ç©¶', 'è°ƒæŸ¥', 'æ£€æŸ¥'],
            'æ–¹æ³•': ['æ–¹æ³•', 'æŠ€æœ¯', 'æ‰‹æ®µ', 'é€”å¾„'],
            'ç³»ç»Ÿ': ['ç³»ç»Ÿ', 'å¹³å°', 'å·¥å…·', 'è½¯ä»¶']
        }
        
        # æŸ¥æ‰¾åŒä¹‰è¯
        for word, synonyms in synonym_dict.items():
            if word in query:
                return synonyms
        
        return [query]  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒä¹‰è¯ï¼Œè¿”å›åŸè¯
    
    def _get_hypernyms(self, query: str) -> List[str]:
        """è·å–ä¸Šä½æ¦‚å¿µï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ç®€å•çš„ä¸Šä½æ¦‚å¿µè¯å…¸
        hypernym_dict = {
            'èŠ¯ç‰‡': ['åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'ç”µå­å™¨ä»¶'],
            'æ™¶åœ†': ['åŠå¯¼ä½“ææ–™', 'ç¡…ç‰‡', 'åŸºæ¿'],
            'ä»£å·¥': ['åˆ¶é€ ', 'ç”Ÿäº§', 'åŠ å·¥']
        }
        
        for word, hypernyms in hypernym_dict.items():
            if word in query:
                return hypernyms
        
        return [query]
    
    def _get_hyponyms(self, query: str) -> List[str]:
        """è·å–ä¸‹ä½æ¦‚å¿µï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ç®€å•çš„ä¸‹ä½æ¦‚å¿µè¯å…¸
        hyponym_dict = {
            'åŠå¯¼ä½“': ['èŠ¯ç‰‡', 'æ™¶åœ†', 'æ™¶ä½“ç®¡'],
            'åˆ¶é€ ': ['ä»£å·¥', 'å°è£…', 'æµ‹è¯•'],
            'æŠ€æœ¯': ['å·¥è‰º', 'åˆ¶ç¨‹', 'è®¾è®¡']
        }
        
        for word, hyponyms in hyponym_dict.items():
            if word in query:
                return hyponyms
        
        return [query]
    
    def _get_related_words(self, query: str) -> List[str]:
        """è·å–ç›¸å…³è¯ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ç®€å•çš„ç›¸å…³è¯è¯å…¸
        related_dict = {
            'ä¸­èŠ¯å›½é™…': ['SMIC', 'æ™¶åœ†ä»£å·¥', 'åŠå¯¼ä½“åˆ¶é€ '],
            'èŠ¯ç‰‡': ['é›†æˆç”µè·¯', 'IC', 'å¾®å¤„ç†å™¨'],
            'æ™¶åœ†': ['ç¡…ç‰‡', 'åŸºæ¿', 'åŠå¯¼ä½“ææ–™']
        }
        
        for word, related_words in related_dict.items():
            if word in query:
                return related_words
        
        return [query]
    
    def _get_domain_tags(self, query: str) -> List[str]:
        """è·å–é¢†åŸŸæ ‡ç­¾ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ç®€å•çš„é¢†åŸŸæ ‡ç­¾è¯å…¸
        domain_dict = {
            'èŠ¯ç‰‡': ['åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'ç”µå­'],
            'åˆ¶é€ ': ['å·¥ä¸š', 'ç”Ÿäº§', 'æŠ€æœ¯'],
            'æŠ€æœ¯': ['ç§‘æŠ€', 'åˆ›æ–°', 'ç ”å‘']
        }
        
        for word, domain_tags in domain_dict.items():
            if word in query:
                return domain_tags
        
        return [query]
    
    def _merge_and_deduplicate_results(self, all_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶å’Œå»é‡æ‰€æœ‰æœç´¢ç»“æœ"""
        
        # ç›´æ¥å¤„ç†å•ä¸ªç»“æœåˆ—è¡¨
        if not all_results:
            return []
        
        # å»é‡ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰
        unique_results = {}
        for result in all_results:
            content_hash = self._calculate_content_hash(result['content'])
            
            if content_hash not in unique_results:
                unique_results[content_hash] = result
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œé€‰æ‹©åˆ†æ•°æ›´é«˜çš„
                existing = unique_results[content_hash]
                existing_score = self._get_comprehensive_score(existing)
                current_score = self._get_comprehensive_score(result)
                
                if current_score > existing_score:
                    unique_results[content_hash] = result
        
        return list(unique_results.values())
    
    def _calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œå€¼"""
        import hashlib
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _get_comprehensive_score(self, result: Dict[str, Any]) -> float:
        """è·å–ç»¼åˆåˆ†æ•°"""
        scores = []
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„åˆ†æ•°
        for key in ['vector_score', 'keyword_score', 'semantic_score', 'fuzzy_score', 'expansion_score', 'hybrid_score']:
            if key in result:
                scores.append(result[key])
        
        # å¦‚æœæ²¡æœ‰åˆ†æ•°ï¼Œè¿”å›0
        if not scores:
            return 0.0
        
        # è¿”å›æœ€é«˜åˆ†æ•°
        return max(scores)
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡ç»“æœåˆ—è¡¨"""
        seen_content = set()
        unique_results = []
        
        for result in results:
            content = result.get('content', '')
            if content and content not in seen_content:
                seen_content.add(content)
                unique_results.append(result)
        
        return unique_results
    
    def _final_ranking(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æœ€ç»ˆæ’åº - åŸºäºç»¼åˆè¯„åˆ†"""
        
        for result in results:
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            comprehensive_score = self._calculate_comprehensive_ranking_score(query, result)
            result['comprehensive_score'] = comprehensive_score
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        results.sort(key=lambda x: x.get('comprehensive_score', 0), reverse=True)
        
        return results
    
    def _calculate_comprehensive_ranking_score(self, query: str, result: Dict[str, Any]) -> float:
        """è®¡ç®—ç»¼åˆæ’åºåˆ†æ•°"""
        
        # åŸºç¡€åˆ†æ•°æƒé‡
        base_score = 0.0
        base_weight = 0.4
        
        # æ”¶é›†åŸºç¡€åˆ†æ•°
        for score_key in ['vector_score', 'keyword_score', 'semantic_score', 'fuzzy_score', 'expansion_score', 'hybrid_score']:
            if score_key in result:
                base_score = max(base_score, result[score_key])
        
        # å†…å®¹ç›¸å…³æ€§æƒé‡
        content_relevance = result.get('content_relevance', 0.0)
        content_weight = 0.3
        
        # æ–‡æ¡£è´¨é‡æƒé‡
        quality_score = result.get('quality_score', 0.0)
        quality_weight = 0.2
        
        # æœç´¢ç­–ç•¥æƒé‡
        strategy_score = self._calculate_strategy_weight(result.get('search_strategy', ''))
        strategy_weight = 0.1
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        comprehensive_score = (
            base_score * base_weight +
            content_relevance * content_weight +
            quality_score * quality_weight +
            strategy_score * strategy_weight
        )
        
        return min(comprehensive_score, 1.0)
    
    def _calculate_strategy_weight(self, strategy: str) -> float:
        """è®¡ç®—æœç´¢ç­–ç•¥æƒé‡"""
        strategy_weights = {
            'vector_similarity': 1.0,      # æœ€é«˜æƒé‡
            'hybrid_search': 0.9,          # æ··åˆæœç´¢
            'semantic_similarity': 0.8,    # è¯­ä¹‰ç›¸ä¼¼åº¦
            'semantic_keyword': 0.7,       # è¯­ä¹‰å…³é”®è¯
            'smart_fuzzy': 0.6,            # æ™ºèƒ½æ¨¡ç³Š
            'expansion_synonym_expansion': 0.5,   # åŒä¹‰è¯æ‰©å±•
            'expansion_concept_expansion': 0.5,   # æ¦‚å¿µæ‰©å±•
            'expansion_related_word': 0.5,        # ç›¸å…³è¯æ‰©å±•
            'expansion_domain_expansion': 0.4     # é¢†åŸŸæ‰©å±•
        }
        
        return strategy_weights.get(strategy, 0.5)
    
    def _fuzzy_search(self, query: str) -> List[Any]:
        """æ™ºèƒ½æ¨¡ç³Šæœç´¢ - å‘åå…¼å®¹æ–¹æ³•ï¼Œè°ƒç”¨æ–°çš„_smart_fuzzy_search"""
        # è°ƒç”¨æ–°çš„æ™ºèƒ½æ¨¡ç³ŠåŒ¹é…æ–¹æ³•
        fuzzy_results = self._smart_fuzzy_search(query, top_k=30)
        
        # è½¬æ¢ä¸ºåŸæœ‰æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
        legacy_results = []
        for result in fuzzy_results:
            legacy_results.append({
                'doc_id': result.get('doc_id', 'unknown'),
                'doc': result.get('doc'),
                'score': result.get('fuzzy_score', 0.0),
                'match_type': 'smart_fuzzy_search'
            })
        
        return legacy_results
    
    def _keyword_search(self, query: str) -> List[Any]:
        """å…³é”®è¯æœç´¢"""
        results = []
        keywords = self._extract_keywords(query)
        
        if not keywords:
            return results
        
        for doc_id, doc in self.text_docs.items():
            score = self._calculate_keyword_score(doc, keywords)
            if score >= self.config.text_similarity_threshold:
                results.append({
                    'doc_id': doc_id,
                    'doc': doc,
                    'score': score,
                    'match_type': 'keyword_search'
                })
        
        return results
    
    def _calculate_text_score(self, doc: Any, query: str) -> float:
        """è®¡ç®—æ–‡æœ¬åŒ¹é…åˆ†æ•° - æ™ºèƒ½ç»¼åˆè¯„åˆ†ï¼ˆä¸¥æ ¼ç›¸å…³æ€§åˆ¤æ–­ï¼‰"""
        score = 0.0
        
        # è·å–æ–‡æœ¬å†…å®¹
        content = doc.page_content if hasattr(doc, 'page_content') else ''
        
        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰
        semantic_score = self._calculate_text_similarity(query, content)
        score += semantic_score * self.config.semantic_weight
        
        # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
        keywords = self._extract_keywords(query)
        if keywords:
            keyword_score = self._calculate_keyword_score(doc, keywords)
            # å…³é”®è¯åŒ¹é…å¿…é¡»è¾¾åˆ°ä¸€å®šé˜ˆå€¼æ‰ç»™åˆ†
            if keyword_score > 0.3:  # è‡³å°‘30%çš„å…³é”®è¯åŒ¹é…
                score += keyword_score * self.config.keyword_weight
            else:
                # å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œå¤§å¹…é™ä½åˆ†æ•°
                score *= 0.3
        
        # 3. å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¦‚æœæœ‰å‘é‡åµŒå…¥ï¼‰
        if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
            vector_score = 0.3  # é™ä½é»˜è®¤å‘é‡åˆ†æ•°
            score += vector_score * self.config.vector_weight
        
        # 4. æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±ï¼ˆé™ä½å¥–åŠ±ï¼‰
        if doc.metadata.get('chunk_type') == 'text':
            score += 0.05  # é™ä½æ–‡æœ¬æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±
        
        # 5. å†…å®¹é•¿åº¦å¥–åŠ±ï¼ˆé™ä½å¥–åŠ±ï¼‰
        if len(content) > 100:
            score += 0.02  # é™ä½å†…å®¹é•¿åº¦å¥–åŠ±
        
        # 6. ç›¸å…³æ€§æƒ©ç½šæœºåˆ¶
        if semantic_score < 0.1:  # è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡ä½
            score *= 0.5  # å¤§å¹…é™ä½åˆ†æ•°
        
        return min(score, 1.0)
    
    def _calculate_text_score_relaxed(self, doc: Any, query: str) -> float:
        """è®¡ç®—æ–‡æœ¬åŒ¹é…åˆ†æ•° - å®½æ¾è¯„åˆ†ç®—æ³•ï¼ˆç”¨äºç­–ç•¥2ï¼‰"""
        score = 0.0
        
        # è·å–æ–‡æœ¬å†…å®¹
        content = doc.page_content if hasattr(doc, 'page_content') else ''
        
        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå®½æ¾å¤„ç†ï¼‰
        semantic_score = self._calculate_text_similarity(query, content)
        score += semantic_score * self.config.semantic_weight
        
        # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆå®½æ¾åŒ¹é…ï¼‰
        keywords = self._extract_keywords(query)
        if keywords:
            keyword_score = self._calculate_keyword_score(doc, keywords)
            # å®½æ¾å¤„ç†ï¼šå³ä½¿å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œä¹Ÿç»™äºˆä¸€å®šåˆ†æ•°
            if keyword_score > 0.1:  # é™ä½åˆ°10%çš„å…³é”®è¯åŒ¹é…
                score += keyword_score * self.config.keyword_weight
            else:
                # å…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œè½»å¾®é™ä½åˆ†æ•°
                score *= 0.7  # ä»0.3æå‡åˆ°0.7
        
        # 3. å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¦‚æœæœ‰å‘é‡åµŒå…¥ï¼‰
        if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
            vector_score = 0.4  # æé«˜é»˜è®¤å‘é‡åˆ†æ•°
            score += vector_score * self.config.vector_weight
        
        # 4. æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±ï¼ˆæé«˜å¥–åŠ±ï¼‰
        if doc.metadata.get('chunk_type') == 'text':
            score += 0.1  # æé«˜æ–‡æœ¬æ–‡æ¡£ç±»å‹åŒ¹é…å¥–åŠ±
        
        # 5. å†…å®¹é•¿åº¦å¥–åŠ±ï¼ˆæé«˜å¥–åŠ±ï¼‰
        if len(content) > 100:
            score += 0.05  # æé«˜å†…å®¹é•¿åº¦å¥–åŠ±
        
        # 6. ç›¸å…³æ€§æƒ©ç½šæœºåˆ¶ï¼ˆå®½æ¾å¤„ç†ï¼‰
        if semantic_score < 0.05:  # ä»0.1é™ä½åˆ°0.05
            score *= 0.7  # ä»0.5æå‡åˆ°0.7
        
        return min(score, 1.0)
    
    def _calculate_keyword_score(self, doc: Any, keywords: List[str]) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        if not keywords:
            return 0.0
        
        content = doc.page_content if hasattr(doc, 'page_content') else ''
        
        total_score = 0.0
        for keyword in keywords:
            if keyword in content:
                total_score += 1.0
        
        return min(total_score / len(keywords), 1.0)
    
    def _calculate_text_similarity(self, query: str, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if not text or not query:
            return 0.0
        
        # ç®€å•çš„è¯æ±‡é‡å è®¡ç®—
        query_words = set(query.lower().split())
        text_words = set(text.lower().split())
        
        if not query_words or not text_words:
            return 0.0
        
        intersection = query_words.intersection(text_words)
        union = query_words.union(text_words)
        
        if union:
            return len(intersection) / len(union)
        return 0.0
    
    def _extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢å…³é”®è¯"""
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'äº†', 'å—', 'å‘¢', 'å•Š'}
        
        import re
        clean_query = re.sub(r'[^\w\s]', '', query)
        
        words = clean_query.split()
        keywords = [word for word in words if word not in stop_words and len(word) > 1]
        
        return keywords
    
    def _rank_text_results(self, results: List[Any], query: str) -> List[Any]:
        """å¯¹æ–‡æœ¬ç»“æœè¿›è¡Œæ™ºèƒ½æ’åº"""
        if not results:
            return []
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)
        
        # é™åˆ¶ç»“æœæ•°é‡
        return sorted_results[:self.config.max_results]
    
    def get_text_by_id(self, text_id: str) -> Optional[Any]:
        """æ ¹æ®IDè·å–æ–‡æœ¬"""
        return self.text_docs.get(text_id)
    
    def get_all_texts(self) -> List[Any]:
        """è·å–æ‰€æœ‰æ–‡æœ¬"""
        return list(self.text_docs.values())
    
    def refresh_text_cache(self):
        """åˆ·æ–°æ–‡æœ¬ç¼“å­˜"""
        self._load_text_documents()
        self.logger.info("æ–‡æœ¬ç¼“å­˜å·²åˆ·æ–°")
    
    def get_text_statistics(self) -> Dict[str, Any]:
        """è·å–æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_texts': len(self.text_docs),
            'total_chars': sum(len(doc.page_content) if hasattr(doc, 'page_content') else 0 
                             for doc in self.text_docs.values())
        }
    
    def clear_cache(self):
        """æ¸…ç†æ–‡æœ¬å¼•æ“ç¼“å­˜"""
        try:
            total_docs = len(self.text_docs)
            self.text_docs = {}
            self._docs_loaded = False
            
            self.logger.info(f"æ–‡æœ¬å¼•æ“ç¼“å­˜æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {total_docs} ä¸ªæ–‡æ¡£")
            return total_docs
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†æ–‡æœ¬å¼•æ“ç¼“å­˜å¤±è´¥: {e}")
            return 0
