#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š

## 1. è¡¨æ ¼å¼•æ“æ ¸å¿ƒå®ç°
## 2. æ”¯æŒè¡¨æ ¼æŸ¥è¯¢çš„ä¸“ç”¨å¼•æ“
## 3. é›†æˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢
## 4. æ”¯æŒè¡¨æ ¼ç»“æ„è¯†åˆ«å’ŒæŸ¥è¯¢ä¼˜åŒ–
"""

import logging
import time
from typing import List, Dict, Any, Optional
from ..core.base_engine import BaseEngine
from ..core.base_engine import EngineConfig
from ..core.base_engine import QueryResult, QueryType
try:
    from .reranking_services import TableRerankingService
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from v2.core.reranking_services import TableRerankingService
    except ImportError:
        TableRerankingService = None
import jieba
import jieba.analyse
import re

# åˆå§‹åŒ–jiebaåˆ†è¯å™¨
jieba.initialize()

# æ·»åŠ è‡ªå®šä¹‰è¯å…¸
custom_words = [
    'è´¢åŠ¡æŠ¥è¡¨', 'æ”¶å…¥æƒ…å†µ', 'å‘˜å·¥è–ªèµ„', 'éƒ¨é—¨åˆ†å¸ƒ', 'äº§å“åº“å­˜', 'æ•°é‡ç»Ÿè®¡',
    'è¯¦ç»†æ˜ç»†', 'æ±‡æ€»ç»Ÿè®¡', 'å¯¹æ¯”åˆ†æ', 'å¢é•¿è¶‹åŠ¿', 'æˆæœ¬æ§åˆ¶', 'åˆ©æ¶¦åˆ†æ',
    'åº“å­˜ç®¡ç†', 'é”€å”®ä¸šç»©', 'è´¢åŠ¡æŒ‡æ ‡', 'ä¸šåŠ¡æ•°æ®', 'è¿è¥æŠ¥è¡¨', 'ç»©æ•ˆè¯„ä¼°'
]

for word in custom_words:
    jieba.add_word(word)

# å®šä¹‰åœç”¨è¯
stop_words = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
}

logger = logging.getLogger(__name__)


class TableEngine(BaseEngine):
    """
    è¡¨æ ¼å¼•æ“
    
    ä¸“é—¨å¤„ç†è¡¨æ ¼æŸ¥è¯¢ï¼Œæ”¯æŒå¤šç§æœç´¢ç­–ç•¥
    """
    
    def __init__(self, config, vector_store=None, document_loader=None, skip_initial_load=False, 
                 llm_engine=None, source_filter_engine=None):
        """
        åˆå§‹åŒ–è¡¨æ ¼å¼•æ“ - é‡æ„ç‰ˆæœ¬ï¼Œæ”¯æŒæ›´å¥½çš„é…ç½®éªŒè¯å’Œæ–‡æ¡£åŠ è½½
        
        :param config: è¡¨æ ¼å¼•æ“é…ç½®
        :param vector_store: å‘é‡æ•°æ®åº“
        :param document_loader: æ–‡æ¡£åŠ è½½å™¨
        :param skip_initial_load: æ˜¯å¦è·³è¿‡åˆå§‹æ–‡æ¡£åŠ è½½
        :param llm_engine: LLMå¼•æ“ï¼ˆç”¨äºæ–°Pipelineï¼‰
        :param source_filter_engine: æºè¿‡æ»¤å¼•æ“ï¼ˆç”¨äºæ–°Pipelineï¼‰
        """
        super().__init__(config)
        
        logger.info("ğŸ” å¼€å§‹åˆå§‹åŒ–TableEngine")
        
        self.vector_store = vector_store
        self.document_loader = document_loader
        self.table_docs = []  # è¡¨æ ¼æ–‡æ¡£ç¼“å­˜
        self._docs_loaded = False
        
        # æ–°Pipelineç›¸å…³å¼•æ“
        self.llm_engine = llm_engine
        self.source_filter_engine = source_filter_engine
        
        # åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡
        self.table_reranking_service = None
        
        # éªŒè¯é…ç½®
        self._validate_config()
        
        # åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡
        self._initialize_table_reranking_service()
        
        # åˆå§‹åŒ–äº”å±‚å¬å›ç­–ç•¥
        self._initialize_recall_strategy()
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åŠ è½½æ–‡æ¡£
        if not skip_initial_load:
            self._load_documents()
        
        logger.info(f"âœ… TableEngineåˆå§‹åŒ–å®Œæˆï¼Œè¡¨æ ¼æ–‡æ¡£æ•°é‡: {len(self.table_docs)}")
    
    def _load_documents(self):
        """åŠ è½½è¡¨æ ¼æ–‡æ¡£ - é‡æ„ç‰ˆæœ¬ï¼Œæ”¯æŒé‡è¯•å’Œé™çº§ç­–ç•¥"""
        if self._docs_loaded:
            return
            
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨
                if self.document_loader:
                    self.table_docs = self.document_loader.get_documents_by_type('table')
                    if self.table_docs:
                        self._docs_loaded = True
                        return
                    else:
                        pass
                
                # å¤‡é€‰æ–¹æ¡ˆï¼šä»å‘é‡æ•°æ®åº“åŠ è½½
                if self.vector_store:
                    self.table_docs = self._load_from_vector_store()
                    if self.table_docs:
                        self._docs_loaded = True
                        return
                    else:
                        pass
                
                # å¦‚æœä¸¤ç§æ–¹å¼éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise ValueError("æ— æ³•é€šè¿‡ä»»ä½•æ–¹å¼åŠ è½½è¡¨æ ¼æ–‡æ¡£")
                    
            except Exception as e:
                retry_count += 1
                
                if retry_count >= max_retries:
                    # æœ€ç»ˆå¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶æ¸…ç©ºç¼“å­˜
                    logger.error(f"âŒ è¡¨æ ¼æ–‡æ¡£åŠ è½½æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
                    self.table_docs = []
                    self._docs_loaded = False
                    return
                else:
                    # ç­‰å¾…åé‡è¯•
                    import time
                    time.sleep(1)
    
    def _load_from_vector_store(self):
        """ä»å‘é‡æ•°æ®åº“åŠ è½½è¡¨æ ¼æ–‡æ¡£"""
        try:
            if hasattr(self.vector_store, 'get_table_documents'):
                # ä½¿ç”¨ä¸“é—¨çš„è¡¨æ ¼æ–‡æ¡£è·å–æ–¹æ³•
                return self.vector_store.get_table_documents()
            elif hasattr(self.vector_store, 'docstore') and hasattr(self.vector_store.docstore, '_dict'):
                # ä»docstoreä¸­ç­›é€‰è¡¨æ ¼æ–‡æ¡£
                table_docs = []
                docstore_dict = self.vector_store.docstore._dict
                
                for doc_id, doc in docstore_dict.items():
                    # ä¸¥æ ¼æ£€æŸ¥æ–‡æ¡£ç±»å‹
                    if not hasattr(doc, 'metadata'):
                        continue
                    
                    chunk_type = doc.metadata.get('chunk_type', '')
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºè¡¨æ ¼æ–‡æ¡£
                    if chunk_type == 'table':
                        # éªŒè¯æ–‡æ¡£ç»“æ„
                        if hasattr(doc, 'page_content') and hasattr(doc, 'metadata'):
                            table_docs.append(doc)
                    else:
                        pass
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¡¨æ ¼æ–‡æ¡£ï¼Œå°è¯•å…¶ä»–ç±»å‹
                if not table_docs:
                    for doc_id, doc in docstore_dict.items():
                        if hasattr(doc, 'metadata') and hasattr(doc, 'page_content'):
                            content = doc.page_content.lower()
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«è¡¨æ ¼ç‰¹å¾
                            if any(keyword in content for keyword in ['è¡¨æ ¼', 'è¡¨', 'è¡Œ', 'åˆ—', 'æ•°æ®', 'ç»Ÿè®¡']):
                                table_docs.append(doc)
                return table_docs
            else:
                return []
                
        except Exception as e:
            logger.error(f"ä»å‘é‡æ•°æ®åº“åŠ è½½è¡¨æ ¼æ–‡æ¡£å¤±è´¥: {e}")
            return []
    
    def _ensure_docs_loaded(self):
        """ç¡®ä¿æ–‡æ¡£å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if not self._docs_loaded:
            logger.info("ğŸ” å¼€å§‹åŠ è½½æ–‡æ¡£...")
            if self.document_loader:
                logger.info("ğŸ” ä½¿ç”¨document_loaderåŠ è½½æ–‡æ¡£")
                self._load_from_document_loader()
            else:
                logger.info("ğŸ” ä½¿ç”¨vector_storeåŠ è½½æ–‡æ¡£")
                self.table_docs = self._load_from_vector_store()
                self._docs_loaded = True
            
            logger.info(f"ğŸ” æ–‡æ¡£åŠ è½½å®Œæˆï¼Œtable_docsæ•°é‡: {len(self.table_docs)}")
            
            # è¯¦ç»†æ£€æŸ¥åŠ è½½çš„æ–‡æ¡£ç»“æ„
            if self.table_docs:
                logger.info("ğŸ” å¼€å§‹æ£€æŸ¥åŠ è½½çš„æ–‡æ¡£ç»“æ„...")
                for i, doc in enumerate(self.table_docs[:3]):  # åªæ£€æŸ¥å‰3ä¸ª
                    logger.info(f"ğŸ” æ–‡æ¡£ {i+1} ç±»å‹: {type(doc)}")
                    logger.info(f"ğŸ” æ–‡æ¡£ {i+1} å±æ€§: {[attr for attr in dir(doc) if not attr.startswith('_')]}")
                    
                    # æ£€æŸ¥page_contentå­—æ®µ
                    if hasattr(doc, 'page_content'):
                        page_content = doc.page_content
                        logger.info(f"ğŸ” æ–‡æ¡£ {i+1} page_contentå­˜åœ¨ï¼Œç±»å‹: {type(page_content)}")
                        logger.info(f"ğŸ” æ–‡æ¡£ {i+1} page_contenté•¿åº¦: {len(page_content) if page_content else 0}")
                        if page_content and len(page_content) > 100:
                            logger.info(f"ğŸ” æ–‡æ¡£ {i+1} page_contentå‰100å­—ç¬¦: {page_content[:100]}")
                        else:
                            logger.info(f"ğŸ” æ–‡æ¡£ {i+1} page_contentå†…å®¹: {page_content}")
                    else:
                        logger.warning(f"ğŸ” æ–‡æ¡£ {i+1} æ²¡æœ‰page_contentå±æ€§ï¼")
                    
                    # æ£€æŸ¥metadataå­—æ®µ
                    if hasattr(doc, 'metadata'):
                        metadata = doc.metadata
                        logger.info(f"ğŸ” æ–‡æ¡£ {i+1} metadataå­˜åœ¨ï¼Œç±»å‹: {type(metadata)}")
                        if isinstance(metadata, dict):
                            logger.info(f"ğŸ” æ–‡æ¡£ {i+1} metadataé”®: {list(metadata.keys())}")
                            
                            # æ£€æŸ¥metadataä¸­çš„page_content
                            if 'page_content' in metadata:
                                meta_page_content = metadata['page_content']
                                logger.info(f"ğŸ” æ–‡æ¡£ {i+1} metadata['page_content']å­˜åœ¨ï¼Œç±»å‹: {type(meta_page_content)}")
                                logger.info(f"ğŸ” æ–‡æ¡£ {i+1} metadata['page_content']é•¿åº¦: {len(meta_page_content) if meta_page_content else 0}")
                                if meta_page_content and len(meta_page_content) > 100:
                                    logger.info(f"ğŸ” æ–‡æ¡£ {i+1} metadata['page_content']å‰100å­—ç¬¦: {meta_page_content[:100]}")
                                else:
                                    logger.info(f"ğŸ” æ–‡æ¡£ {i+1} metadata['page_content']å†…å®¹: {meta_page_content}")
                            else:
                                logger.warning(f"ğŸ” æ–‡æ¡£ {i+1} metadataä¸­æ²¡æœ‰page_contentå­—æ®µ")
                        else:
                            logger.warning(f"ğŸ” æ–‡æ¡£ {i+1} metadataä¸æ˜¯å­—å…¸ç±»å‹: {type(metadata)}")
                    else:
                        logger.warning(f"ğŸ” æ–‡æ¡£ {i+1} æ²¡æœ‰metadataå±æ€§ï¼")
                    
                    # æ£€æŸ¥å…¶ä»–é‡è¦å­—æ®µ
                    important_fields = ['document_name', 'page_number', 'chunk_type', 'table_id']
                    for field in important_fields:
                        if hasattr(doc, field):
                            value = getattr(doc, field)
                            logger.info(f"ğŸ” æ–‡æ¡£ {i+1} {field}: {value}")
                        elif hasattr(doc, 'metadata') and isinstance(doc.metadata, dict) and field in doc.metadata:
                            value = doc.metadata[field]
                            logger.info(f"ğŸ” æ–‡æ¡£ {i+1} {field} (ä»metadata): {value}")
                        else:
                            logger.warning(f"ğŸ” æ–‡æ¡£ {i+1} {field}å­—æ®µä¸å­˜åœ¨")
                    
                    logger.info(f"ğŸ” æ–‡æ¡£ {i+1} æ£€æŸ¥å®Œæˆ")
                    logger.info("-" * 50)
            else:
                logger.warning("ğŸ” table_docsä¸ºç©ºï¼")
            
            # éªŒè¯åŠ è½½çš„æ–‡æ¡£
            logger.info("ğŸ” å¼€å§‹éªŒè¯åŠ è½½çš„æ–‡æ¡£...")
            self._validate_loaded_documents()
            logger.info(f"ğŸ” æ–‡æ¡£éªŒè¯å®Œæˆï¼Œæœ€ç»ˆtable_docsæ•°é‡: {len(self.table_docs)}")
    
    def _validate_loaded_documents(self):
        """éªŒè¯å·²åŠ è½½çš„æ–‡æ¡£"""
        try:
            if not self.table_docs:
                return
            
            valid_docs = []
            invalid_docs = []
            
            for i, doc in enumerate(self.table_docs):
                # æ£€æŸ¥æ–‡æ¡£ç»“æ„
                if not hasattr(doc, 'metadata'):
                    invalid_docs.append(i)
                    continue
                
                if not hasattr(doc, 'page_content'):
                    invalid_docs.append(i)
                    continue
                
                # æ£€æŸ¥å…ƒæ•°æ®å®Œæ•´æ€§
                metadata = doc.metadata
                if not isinstance(metadata, dict):
                    invalid_docs.append(i)
                    continue
                
                # æ£€æŸ¥å†…å®¹
                content = doc.page_content
                if not isinstance(content, str):
                    invalid_docs.append(i)
                    continue
                
                if len(content.strip()) == 0:
                    invalid_docs.append(i)
                    continue
                
                valid_docs.append(doc)
            
            # æ›´æ–°æ–‡æ¡£åˆ—è¡¨
            if invalid_docs:
                self.table_docs = valid_docs
                
        except Exception as e:
            logger.error(f"æ–‡æ¡£éªŒè¯å¤±è´¥: {e}")
    
    def _load_from_document_loader(self):
        """ä»ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨è·å–è¡¨æ ¼æ–‡æ¡£"""
        if self.document_loader:
            try:
                self.table_docs = self.document_loader.get_documents_by_type('table')
                self._docs_loaded = True
            except Exception as e:
                logger.error(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–è¡¨æ ¼æ–‡æ¡£å¤±è´¥: {e}")
                # é™çº§åˆ°å‘é‡æ•°æ®åº“åŠ è½½æ–¹å¼
                self.table_docs = self._load_from_vector_store()
                self._docs_loaded = True
        else:
            self.table_docs = self._load_from_vector_store()
            self._docs_loaded = True
    
    def _validate_config(self):
        """éªŒè¯è¡¨æ ¼å¼•æ“é…ç½® - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒTableä¸“ç”¨é…ç½®éªŒè¯"""
        # é…ç½®ç±»å‹æ£€æŸ¥
        from ..config.v2_config import TableEngineConfigV2
        
        if not isinstance(self.config, TableEngineConfigV2):
            raise ValueError("é…ç½®å¿…é¡»æ˜¯TableEngineConfigV2ç±»å‹")
        
        # è·å–ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œæ”¯æŒä¸¤ç§é…ç½®ç±»å‹
        threshold = getattr(self.config, 'table_similarity_threshold', 0.7)
        if not isinstance(threshold, (int, float)) or threshold < 0 or threshold > 1:
            raise ValueError("è¡¨æ ¼ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
        
        # éªŒè¯Tableä¸“ç”¨é…ç½®å‚æ•°
        self._validate_table_specific_config()
        
        # éªŒè¯äº”å±‚å¬å›ç­–ç•¥é…ç½®
        self._validate_recall_strategy_config()
        
        # éªŒè¯é‡æ’åºé…ç½®
        self._validate_reranking_config()
        
        pass
    
    def _validate_table_specific_config(self):
        """éªŒè¯Tableä¸“ç”¨é…ç½®å‚æ•°"""
        try:
            # éªŒè¯è¡¨æ ¼å¤„ç†ç›¸å…³é…ç½®
            table_configs = [
                ('max_table_rows', int, 'è¡¨æ ¼æœ€å¤§è¡Œæ•°'),
                ('header_weight', float, 'è¡¨å¤´æƒé‡'),
                ('content_weight', float, 'å†…å®¹æƒé‡'),
                ('structure_weight', float, 'ç»“æ„æƒé‡'),
                ('enable_structure_search', bool, 'å¯ç”¨ç»“æ„æœç´¢'),
                ('enable_content_search', bool, 'å¯ç”¨å†…å®¹æœç´¢')
            ]
            
            for config_name, expected_type, description in table_configs:
                if hasattr(self.config, config_name):
                    value = getattr(self.config, config_name)
                    if not isinstance(value, expected_type):
                        pass
                    else:
                        pass
                else:
                    pass
            
            # éªŒè¯æƒé‡é…ç½®çš„åˆç†æ€§
            if hasattr(self.config, 'header_weight') and hasattr(self.config, 'content_weight') and hasattr(self.config, 'structure_weight'):
                total_weight = self.config.header_weight + self.config.content_weight + self.config.structure_weight
                if abs(total_weight - 1.0) > 0.01:
                    pass
            
        except Exception as e:
            logger.error(f"éªŒè¯Tableä¸“ç”¨é…ç½®å¤±è´¥: {e}")
    
    def _validate_recall_strategy_config(self):
        """éªŒè¯äº”å±‚å¬å›ç­–ç•¥é…ç½®"""
        try:
            if not hasattr(self.config, 'recall_strategy'):
                return
            
            strategy = self.config.recall_strategy
            required_layers = [
                'layer1_structure_search',    # ç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„æœç´¢
                'layer2_vector_search',       # ç¬¬äºŒå±‚ï¼šå‘é‡è¯­ä¹‰æœç´¢
                'layer3_keyword_search',      # ç¬¬ä¸‰å±‚ï¼šå…³é”®è¯åŒ¹é…
                'layer4_hybrid_search',       # ç¬¬å››å±‚ï¼šæ··åˆæ™ºèƒ½æœç´¢
                'layer5_expansion_search'     # ç¬¬äº”å±‚ï¼šå®¹é”™æ‰©å±•æœç´¢
            ]
            
            for layer in required_layers:
                if layer not in strategy:
                    pass
                else:
                    layer_config = strategy[layer]
                    # ä¿®å¤ï¼šæ”¯æŒå¯¹è±¡å’Œå­—å…¸ä¸¤ç§æ ¼å¼
                    if hasattr(layer_config, 'enabled'):
                        # å¯¹è±¡æ ¼å¼ï¼ˆé€šè¿‡_convert_recall_strategy_to_objectsè½¬æ¢åï¼‰
                        enabled = layer_config.enabled
                        top_k = getattr(layer_config, 'top_k', 50)
                    elif isinstance(layer_config, dict):
                        # å­—å…¸æ ¼å¼ï¼ˆåŸå§‹é…ç½®ï¼‰
                        enabled = layer_config.get('enabled', True)
                        top_k = layer_config.get('top_k', 50)
                    else:
                        pass
            
        except Exception as e:
            logger.error(f"éªŒè¯å¬å›ç­–ç•¥é…ç½®å¤±è´¥: {e}")
    
    def _validate_reranking_config(self):
        """éªŒè¯é‡æ’åºé…ç½®"""
        try:
            if not hasattr(self.config, 'reranking'):
                return
            
            reranking = self.config.reranking
            reranking_configs = [
                ('target_count', int, 'ç›®æ ‡ç»“æœæ•°é‡'),
                ('use_llm_enhancement', bool, 'ä½¿ç”¨LLMå¢å¼º'),
                ('model_name', str, 'æ¨¡å‹åç§°'),
                ('similarity_threshold', float, 'ç›¸ä¼¼åº¦é˜ˆå€¼')
            ]
            
            for config_name, expected_type, description in reranking_configs:
                if config_name in reranking:
                    value = reranking[config_name]
                    if not isinstance(value, expected_type):
                        pass
                    else:
                        pass
                else:
                    pass
            
        except Exception as e:
            logger.error(f"éªŒè¯é‡æ’åºé…ç½®å¤±è´¥: {e}")
    
    def _initialize_table_reranking_service(self):
        """åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡"""
        try:
            if not hasattr(self.config, 'reranking'):
                return
            
            reranking_config = self.config.reranking
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨LLMå¢å¼º
            if not reranking_config.get('use_llm_enhancement', False):
                return
            
            # åˆ›å»ºè¡¨æ ¼é‡æ’åºæœåŠ¡å®ä¾‹
            self.table_reranking_service = TableRerankingService(reranking_config)
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡å¤±è´¥: {e}")
            self.table_reranking_service = None
    
    def _rerank_table_results(self, query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨è¡¨æ ¼é‡æ’åºæœåŠ¡å¯¹ç»“æœè¿›è¡Œé‡æ’åº
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param candidates: å€™é€‰ç»“æœåˆ—è¡¨
        :return: é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            if not self.table_reranking_service:
                return candidates
            
            if not candidates:
                return candidates
            
            start_time = time.time()
            
            # å‡†å¤‡é‡æ’åºæ•°æ®æ ¼å¼
            rerank_candidates = []
            for candidate in candidates:
                # ä»docå¯¹è±¡ä¸­æå–å†…å®¹
                doc = candidate.get('doc')
                if doc and hasattr(doc, 'page_content') and hasattr(doc, 'metadata'):
                    rerank_candidate = {
                        'content': getattr(doc, 'page_content', ''),
                        'metadata': getattr(doc, 'metadata', {}),
                        'original_candidate': candidate  # ä¿å­˜åŸå§‹å€™é€‰æ–‡æ¡£çš„å¼•ç”¨
                    }
                    rerank_candidates.append(rerank_candidate)
                else:
                    logger.warning(f"å€™é€‰æ–‡æ¡£ç¼ºå°‘å¿…è¦å±æ€§ï¼Œè·³è¿‡é‡æ’åº")
            
            if not rerank_candidates:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„é‡æ’åºå€™é€‰æ–‡æ¡£ï¼Œè¿”å›åŸå§‹ç»“æœ")
                return candidates
            
            # è°ƒç”¨è¡¨æ ¼é‡æ’åºæœåŠ¡
            reranked_results = self.table_reranking_service.rerank(query, rerank_candidates)
            
            # è°ƒè¯•ï¼šæŸ¥çœ‹é‡æ’åºç»“æœçš„æ ¼å¼
            pass
            
            # ä¿®å¤ï¼šç¡®ä¿è¿”å›ç»“æœæ ¼å¼ä¸€è‡´
            final_results = []
            for i, reranked_result in enumerate(reranked_results):
                if isinstance(reranked_result, dict):
                    # å¦‚æœé‡æ’åºç»“æœåŒ…å«docå­—æ®µ
                    if 'doc' in reranked_result:
                        # æ£€æŸ¥docå­—æ®µæ˜¯å¦åŒ…å«original_candidateå¼•ç”¨
                        doc_data = reranked_result['doc']
                        if isinstance(doc_data, dict) and 'original_candidate' in doc_data:
                            # ä½¿ç”¨åŸå§‹å€™é€‰æ–‡æ¡£å¼•ç”¨
                            original_candidate = doc_data['original_candidate']
                        else:
                            # ç›´æ¥ä½¿ç”¨docå­—æ®µ
                            original_candidate = doc_data
                        
                        # éªŒè¯åŸå§‹å€™é€‰æ–‡æ¡£çš„å†…å®¹
                        if 'doc' in original_candidate and original_candidate['doc']:
                            doc = original_candidate['doc']
                            content = getattr(doc, 'page_content', '')
                        
                        final_results.append({
                            'doc': original_candidate,
                            'score': reranked_result.get('score', 0.5),
                            'source': reranked_result.get('source', 'rerank'),
                            'layer': reranked_result.get('layer', 1)
                        })
                    else:
                        # å¦åˆ™ï¼Œæ„é€ æ ‡å‡†æ ¼å¼
                        original_candidate = candidates[i] if i < len(candidates) else candidates[0]
                        final_results.append({
                            'doc': original_candidate['doc'],
                            'score': reranked_result.get('score', original_candidate.get('score', 0.5)),
                            'source': reranked_result.get('source', 'rerank'),
                            'layer': reranked_result.get('layer', original_candidate.get('layer', 1))
                        })
                else:
                    # éå­—å…¸ç±»å‹ï¼Œä½¿ç”¨åŸå§‹å€™é€‰ç»“æœ
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„é‡æ’åºç»“æœç±»å‹: {type(reranked_result)}")
                    if i < len(candidates):
                        final_results.append({
                            'doc': candidates[i]['doc'],
                            'score': candidates[i].get('score', 0.5),
                            'source': candidates[i].get('source', 'unknown'),
                            'layer': candidates[i].get('layer', 1)
                        })
            
            rerank_time = time.time() - start_time
            pass
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ è¡¨æ ¼é‡æ’åºå¤±è´¥: {e}")
            # è¿”å›åŸå§‹ç»“æœ
            return candidates
    
    def _setup_components(self):
        """è®¾ç½®å¼•æ“ç»„ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•ï¼Œä½¿ç”¨æ–°çš„æ–‡æ¡£åŠ è½½æœºåˆ¶"""
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²åŠ è½½ï¼Œå¦‚æœæ²¡æœ‰åˆ™åŠ è½½
        if not self._docs_loaded:
            try:
                self._ensure_docs_loaded()
            except Exception as e:
                logger.error(f"âŒ è¡¨æ ¼å¼•æ“åœ¨_setup_componentsä¸­åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
                raise
    
    def _analyze_query_intent(self, query: str) -> Dict[str, Any]:
        """
        åˆ†ææŸ¥è¯¢æ„å›¾
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :return: æŸ¥è¯¢æ„å›¾åˆ†æç»“æœ
        """
        try:
            intent = {
                'query_type': 'unknown',
                'business_domain': 'unknown',
                'data_requirement': 'unknown',
                'time_range': 'unknown',
                'comparison_type': 'unknown'
            }
            
            query_lower = query.lower()
            
            # åˆ†ææŸ¥è¯¢ç±»å‹
            if any(word in query_lower for word in ['è¶‹åŠ¿', 'å˜åŒ–', 'å¢é•¿', 'ä¸‹é™', 'æ³¢åŠ¨']):
                intent['query_type'] = 'trend_analysis'
            elif any(word in query_lower for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚', 'é«˜ä½']):
                intent['query_type'] = 'comparison'
            elif any(word in query_lower for word in ['æ’å', 'æ’åº', 'å‰å‡ ', 'åå‡ ']):
                intent['query_type'] = 'ranking'
            elif any(word in query_lower for word in ['ç»Ÿè®¡', 'æ±‡æ€»', 'æ€»è®¡', 'å¹³å‡']):
                intent['query_type'] = 'statistics'
            
            # åˆ†æä¸šåŠ¡é¢†åŸŸ
            if any(word in query_lower for word in ['è´¢åŠ¡', 'æ”¶å…¥', 'åˆ©æ¶¦', 'æˆæœ¬', 'èµ„äº§']):
                intent['business_domain'] = 'finance'
            elif any(word in query_lower for word in ['é”€å”®', 'å¸‚åœº', 'å®¢æˆ·', 'äº§å“']):
                intent['business_domain'] = 'sales'
            elif any(word in query_lower for word in ['æŠ€æœ¯', 'ç ”å‘', 'åˆ›æ–°', 'ä¸“åˆ©']):
                intent['business_domain'] = 'technology'
            
            # åˆ†ææ•°æ®è¦æ±‚
            if any(word in query_lower for word in ['è¯¦ç»†', 'å…·ä½“', 'å®Œæ•´']):
                intent['data_requirement'] = 'detailed'
            elif any(word in query_lower for word in ['æ¦‚è§ˆ', 'æ€»ç»“', 'ç®€è¦']):
                intent['data_requirement'] = 'overview'
            
            # åˆ†ææ—¶é—´èŒƒå›´
            if any(word in query_lower for word in ['å¹´', 'å­£åº¦', 'æœˆ', 'æ—¥']):
                intent['time_range'] = 'time_series'
            
            return intent
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ„å›¾åˆ†æå¤±è´¥: {e}")
            return {'query_type': 'unknown', 'business_domain': 'unknown', 'data_requirement': 'unknown', 'time_range': 'unknown', 'comparison_type': 'unknown'}
    
    def _analyze_structure_requirements(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢å¯¹è¡¨æ ¼ç»“æ„çš„è¦æ±‚"""
        try:
            requirements = {
                'min_rows': 1,
                'max_rows': 1000,
                'min_columns': 1,
                'max_columns': 20,
                'preferred_structure': 'unknown'
            }
            
            query_lower = query.lower()
            
            # åˆ†æè¡Œæ•°è¦æ±‚
            if any(word in query_lower for word in ['è¯¦ç»†', 'å®Œæ•´', 'æ‰€æœ‰']):
                requirements['min_rows'] = 10
                requirements['max_rows'] = 1000
            elif any(word in query_lower for word in ['æ¦‚è§ˆ', 'æ€»ç»“', 'ä¸»è¦']):
                requirements['min_rows'] = 1
                requirements['max_rows'] = 20
            
            # åˆ†æåˆ—æ•°è¦æ±‚
            if any(word in query_lower for word in ['å¤šç»´åº¦', 'å…¨é¢', 'ç»¼åˆ']):
                requirements['min_columns'] = 3
                requirements['max_columns'] = 20
            elif any(word in query_lower for word in ['ç®€å•', 'åŸºç¡€']):
                requirements['min_columns'] = 1
                requirements['max_columns'] = 5
            
            return requirements
            
        except Exception as e:
            logger.error(f"ç»“æ„è¦æ±‚åˆ†æå¤±è´¥: {e}")
            return {'min_rows': 1, 'max_rows': 1000, 'min_columns': 1, 'max_columns': 20, 'preferred_structure': 'unknown'}
    
    def process_query(self, query: str, **kwargs) -> QueryResult:
        """
        å¤„ç†è¡¨æ ¼æŸ¥è¯¢è¯·æ±‚ - ä¿®å¤ç‰ˆæœ¬ï¼Œä¸text_engine.pyä¿æŒä¸€è‡´
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param kwargs: é¢å¤–å‚æ•°ï¼ˆåŒ…æ‹¬query_typeç­‰ï¼‰
        :return: QueryResultå¯¹è±¡
        """
        if not self.is_enabled():
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.TABLE,
                results=[],
                total_count=0,
                processing_time=0.0,
                engine_name=self.name,
                metadata={},
                error_message="è¡¨æ ¼å¼•æ“æœªå¯ç”¨"
            )
        
        start_time = time.time()
        
        try:
            # ç¡®ä¿æ–‡æ¡£å·²åŠ è½½
            self._ensure_docs_loaded()
            
            # å¦‚æœæ–‡æ¡£æ•°é‡ä¸º0ï¼Œå°è¯•é‡æ–°åŠ è½½
            if len(self.table_docs) == 0:
                self._docs_loaded = False
                self._ensure_docs_loaded()
            
            # åˆ†ææŸ¥è¯¢æ„å›¾
            intent_analysis = self._analyze_query_intent(query)
            
            # æ‰§è¡Œæœç´¢
            search_results = self._search_tables(query)
            
            # æ ¹æ®æ„å›¾è°ƒæ•´ç»“æœ
            if intent_analysis['query_type'] == 'detail_view' and intent_analysis['data_requirement'] == 'detailed':
                # å¦‚æœç”¨æˆ·æ„å›¾æ˜¯æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼Œå°è¯•è·å–å®Œæ•´è¡¨æ ¼
                if search_results and len(search_results) > 0:
                    top_result = search_results[0]
                    table_id = top_result['doc'].metadata.get('table_id', 'unknown')
                    full_table_result = self.get_full_table(table_id)
                    if full_table_result['status'] == 'success':
                        search_results[0]['full_content'] = full_table_result['content']
                        search_results[0]['full_metadata'] = full_table_result['metadata']
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¢å¼ºReranking
            logger.info(f"ğŸ” é…ç½®æ£€æŸ¥ - enable_enhanced_reranking: {getattr(self.config, 'enable_enhanced_reranking', False)}")
            logger.info(f"ğŸ” é…ç½®æ£€æŸ¥ - use_new_pipeline: {getattr(self.config, 'use_new_pipeline', False)}")
            
            if getattr(self.config, 'enable_enhanced_reranking', False):
                try:
                    # å¯¼å…¥RerankingæœåŠ¡
                    from .reranking_services import create_reranking_service
                    
                    # åˆ›å»ºTableRerankingService
                    reranking_config = getattr(self.config, 'reranking', {})
                    reranking_service = create_reranking_service('table', reranking_config)
                    
                    if reranking_service:
                        # æ‰§è¡ŒReranking
                        reranked_results = reranking_service.rerank(query, search_results)
                        
                        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°çš„ç»Ÿä¸€Pipeline
                        logger.info(f"ğŸ” Pipelineæ£€æŸ¥ - è¿›å…¥use_new_pipelineåˆ†æ”¯")
                        if getattr(self.config, 'use_new_pipeline', False):
                            try:
                                # å¯¼å…¥ç»Ÿä¸€Pipeline
                                from .unified_pipeline import UnifiedPipeline
                                
                                # è·å–ç»Ÿä¸€Pipelineé…ç½®
                                from ..config.v2_config import V2ConfigManager
                                config_manager = V2ConfigManager()
                                pipeline_config = config_manager.get_engine_config('unified_pipeline')
                                
                                if pipeline_config and pipeline_config.enabled:
                                    # å°è¯•è·å–çœŸå®çš„LLMå¼•æ“å’Œæºè¿‡æ»¤å¼•æ“
                                    llm_engine = None
                                    source_filter_engine = None
                                    
                                    # ä»HybridEngineè·å–å¼•æ“ï¼ˆé€šè¿‡kwargsä¼ é€’ï¼‰
                                    if 'llm_engine' in kwargs:
                                        llm_engine = kwargs['llm_engine']
                                    if 'source_filter_engine' in kwargs:
                                        source_filter_engine = kwargs['source_filter_engine']
                                    
                                    # å¦‚æœæ²¡æœ‰ä¼ å…¥çœŸå®å¼•æ“ï¼Œä½¿ç”¨Mockï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                                    if not llm_engine:
                                        from unittest.mock import Mock
                                        llm_engine = Mock()
                                        llm_engine.generate_answer.return_value = "åŸºäºæŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆçš„ç­”æ¡ˆ"
                                    
                                    if not source_filter_engine:
                                        from unittest.mock import Mock
                                        source_filter_engine = Mock()
                                        source_filter_engine.filter_sources.return_value = reranked_results[:3]
                                    
                                    # åˆ›å»ºç»Ÿä¸€Pipeline
                                    unified_pipeline = UnifiedPipeline(
                                        config=pipeline_config.__dict__,
                                        llm_engine=llm_engine,
                                        source_filter_engine=source_filter_engine
                                    )
                                    
                                    # ğŸ” æ£€æŸ¥reranked_resultsçš„å­—æ®µå†…å®¹
                                    logger.info(f"ğŸ” è°ƒç”¨Pipelineå‰ - reranked_resultså­—æ®µæ£€æŸ¥:")
                                    logger.info(f"ğŸ” reranked_resultsæ•°é‡: {len(reranked_results)}")
                                    
                                    for i, result in enumerate(reranked_results[:3]):  # æ£€æŸ¥å‰3ä¸ª
                                        logger.info(f"ğŸ” ç»“æœ {i+1} å­—æ®µæ£€æŸ¥:")
                                        logger.info(f"  - ç»“æœç±»å‹: {type(result)}")
                                        logger.info(f"  - ç»“æœé”®: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                                        
                                        if 'doc' in result and hasattr(result['doc'], 'metadata'):
                                            doc = result['doc']
                                            metadata = doc.metadata
                                            logger.info(f"  - docç±»å‹: {type(doc)}")
                                            logger.info(f"  - metadataç±»å‹: {type(metadata)}")
                                            logger.info(f"  - metadataé”®: {list(metadata.keys()) if metadata else 'None'}")
                                            logger.info(f"  - chunk_type: {metadata.get('chunk_type', 'None')}")
                                            
                                            # æ£€æŸ¥è¡¨æ ¼ç›¸å…³å­—æ®µ
                                            if metadata.get('chunk_type') == 'table':
                                                logger.info(f"  - è¡¨æ ¼å­—æ®µæ£€æŸ¥:")
                                                logger.info(f"    * processed_table_content: {metadata.get('processed_table_content', 'None')}")
                                                logger.info(f"    * table_summary: {metadata.get('table_summary', 'None')}")
                                                logger.info(f"    * table_title: {metadata.get('table_title', 'None')}")
                                                logger.info(f"    * table_headers: {metadata.get('table_headers', 'None')}")
                                                logger.info(f"    * page_contenté•¿åº¦: {len(doc.page_content) if doc.page_content else 0}")
                                                logger.info(f"    * page_contenté¢„è§ˆ: {doc.page_content[:100] if doc.page_content else 'None'}...")
                                            else:
                                                logger.info(f"  - éè¡¨æ ¼ç±»å‹ï¼Œchunk_type: {metadata.get('chunk_type', 'None')}")
                                        else:
                                            logger.info(f"  - ç¼ºå°‘docæˆ–metadataå­—æ®µ")
                                    
                                    # æ‰§è¡Œç»Ÿä¸€Pipeline
                                    pipeline_result = unified_pipeline.process(query, reranked_results, query_type='table')
                                    
                                    if pipeline_result.success:
                                        logger.info("ğŸ” Pipelineå¤„ç†æˆåŠŸï¼Œå¼€å§‹å¤„ç†è¿”å›ç»“æœ")
                                        final_results = pipeline_result.filtered_sources
                                        logger.info(f"ğŸ” Pipelineè¿”å›ç»“æœæ•°é‡: {len(final_results)}")
                                        
                                        # æ£€æŸ¥Pipelineè¿”å›çš„ç»“æœæ ¼å¼
                                        logger.info("ğŸ” Pipelineè¿”å›ç»“æœæ ¼å¼æ£€æŸ¥:")
                                        for i, result in enumerate(final_results[:2]):  # åªæ£€æŸ¥å‰2ä¸ª
                                            logger.info(f"ğŸ” Pipelineç»“æœ {i+1} - ç±»å‹: {type(result)}")
                                            logger.info(f"ğŸ” Pipelineç»“æœ {i+1} - é”®: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                                            if isinstance(result, dict):
                                                for key, value in result.items():
                                                    if isinstance(value, dict):
                                                        logger.info(f"ğŸ” Pipelineç»“æœ {i+1} - {key}: {list(value.keys()) if isinstance(value, dict) else value}")
                                                    else:
                                                        logger.info(f"ğŸ” Pipelineç»“æœ {i+1} - {key}: {value}")
                                        
                                        # ğŸ”‘ ä¿®å¤ï¼šå°†Pipelineè¿”å›çš„å­—å…¸æ ¼å¼docè½¬æ¢ä¸ºå¯¹è±¡æ ¼å¼
                                        logger.info("ğŸ” å¼€å§‹ä¿®å¤Pipelineè¿”å›çš„docæ ¼å¼")
                                        for i, result in enumerate(final_results):
                                            if 'doc' in result and isinstance(result['doc'], dict):
                                                logger.info(f"ğŸ” ä¿®å¤Pipelineç»“æœ {i+1} - åŸå§‹docç±»å‹: {type(result['doc'])}")
                                                logger.info(f"ğŸ” ä¿®å¤Pipelineç»“æœ {i+1} - åŸå§‹docé”®: {list(result['doc'].keys())}")
                                                
                                                # æ„é€ ä¸€ä¸ªåŒ…å«page_contentå’Œmetadataå±æ€§çš„å¯¹è±¡
                                                class MockDoc:
                                                    def __init__(self, content, metadata):
                                                        self.page_content = content
                                                        self.metadata = metadata
                                                
                                                # ä»Pipelineçš„docå­—å…¸ä¸­æå–contentå’Œmetadata
                                                doc_dict = result['doc']
                                                content = doc_dict.get('content', '')
                                                metadata = doc_dict.get('metadata', {})
                                                
                                                logger.info(f"ğŸ” ä¿®å¤Pipelineç»“æœ {i+1} - æå–çš„contenté•¿åº¦: {len(content) if content else 0}")
                                                logger.info(f"ğŸ” ä¿®å¤Pipelineç»“æœ {i+1} - æå–çš„metadata: {metadata}")
                                                
                                                # æ›¿æ¢ä¸ºMockDocå¯¹è±¡
                                                result['doc'] = MockDoc(content, metadata)
                                                logger.info(f"ğŸ” ä¿®å¤Pipelineç»“æœ {i+1} - ä¿®å¤å®Œæˆ")
                                        
                                        logger.info("ğŸ” Pipelineè¿”å›çš„docæ ¼å¼ä¿®å¤å®Œæˆ")
                                        
                                        # æ·»åŠ Pipelineå…ƒæ•°æ®
                                        pipeline_metadata = {
                                            'pipeline': 'unified_pipeline',
                                            'llm_answer': pipeline_result.llm_answer,
                                            'pipeline_metrics': pipeline_result.pipeline_metrics
                                        }
                                        # å°†LLMç­”æ¡ˆä¹Ÿæ·»åŠ åˆ°metadataä¸­ï¼Œä¾›HybridEngineä½¿ç”¨
                                        if pipeline_result.llm_answer:
                                            pass
                                    else:
                                        final_results = self._final_ranking_and_limit(query, reranked_results)
                                        pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
                                else:
                                    final_results = self._final_ranking_and_limit(query, reranked_results)
                                    pipeline_metadata = {'pipeline': 'traditional_ranking'}
                                    
                            except Exception as e:
                                final_results = self._final_ranking_and_limit(query, reranked_results)
                                pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
                        else:
                            final_results = self._final_ranking_and_limit(query, reranked_results)
                            pipeline_metadata = {'pipeline': 'traditional_ranking'}
                    else:
                        final_results = self._final_ranking_and_limit(query, search_results)
                        pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
                        
                except Exception as e:
                    final_results = self._final_ranking_and_limit(query, search_results)
                    pipeline_metadata = {'pipeline': 'fallback_to_ranking'}
            else:
                # æœ€ç»ˆæ’åºå’Œé™åˆ¶
                final_results = self._final_ranking_and_limit(query, search_results)
                pipeline_metadata = {'pipeline': 'traditional_ranking'}
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            logger.info(f"ğŸ” å¼€å§‹æ ¼å¼åŒ– {len(final_results)} ä¸ªç»“æœ")
            
            for i, result in enumerate(final_results):
                logger.info(f"ğŸ” å¤„ç†ç»“æœ {i+1}: {type(result)}")
                logger.info(f"ğŸ” ç»“æœ {i+1} çš„é”®: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                logger.info(f"ğŸ” ç»“æœ {i+1} çš„å®Œæ•´ç»“æ„: {result}")
                
                # ğŸ”‘ ä¿®å¤ï¼šå¤„ç†ç»Ÿä¸€Pipelineè¿”å›çš„ç‰¹æ®Šæ ¼å¼
                if 'doc' in result and isinstance(result['doc'], list):
                    logger.info(f"ğŸ” æ£€æµ‹åˆ°ç»Ÿä¸€Pipelineç»“æœæ ¼å¼ï¼Œdocæ˜¯åˆ—è¡¨: {result['doc']}")
                    
                    # ä»åˆ—è¡¨ä¸­æå–å®é™…çš„docå¯¹è±¡
                    if len(result['doc']) > 0:
                        # æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„docå¯¹è±¡
                        class MockDoc:
                            def __init__(self, content, metadata):
                                self.page_content = content
                                self.metadata = metadata
                        
                        # ğŸ”‘ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨HTMLæ ¼å¼çš„page_contentï¼Œç¡®ä¿table_htmlå­—æ®µæ­£ç¡®
                        # ä»resultä¸­æå–contentå’Œmetadata
                        content = result.get('content', '')
                        metadata = result.get('metadata', {})
                        
                        # ğŸ”‘ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨HTMLæ ¼å¼çš„page_content
                        html_content = ''
                        if metadata and 'page_content' in metadata:
                            html_content = metadata['page_content']
                            logger.info(f"ğŸ” ä»metadata.page_contentè·å–HTMLå†…å®¹ï¼Œé•¿åº¦: {len(html_content)}")
                        elif hasattr(result, 'page_content'):
                            html_content = result.page_content
                            logger.info(f"ğŸ” ä»result.page_contentè·å–HTMLå†…å®¹ï¼Œé•¿åº¦: {len(html_content)}")
                        elif 'page_content' in result:
                            html_content = result['page_content']
                            logger.info(f"ğŸ” ä»result['page_content']è·å–HTMLå†…å®¹ï¼Œé•¿åº¦: {len(html_content)}")
                        else:
                            # å¦‚æœæ²¡æœ‰HTMLå†…å®¹ï¼Œä½¿ç”¨contentä½œä¸ºå¤‡ç”¨
                            html_content = content
                            logger.info(f"ğŸ” ä½¿ç”¨contentä½œä¸ºå¤‡ç”¨ï¼Œé•¿åº¦: {len(html_content)}")
                        
                        if not metadata and 'metadata' in result:
                            metadata = result['metadata']
                        
                        logger.info(f"ğŸ” æœ€ç»ˆä½¿ç”¨çš„HTMLå†…å®¹é•¿åº¦: {len(html_content)}")
                        logger.info(f"ğŸ” æå–çš„metadata: {metadata}")
                        
                        # ğŸ”‘ ä½¿ç”¨HTMLå†…å®¹æ„é€ MockDoc
                        mock_doc = MockDoc(html_content, metadata)
                        result['doc'] = mock_doc
                        logger.info(f"ğŸ” ä¿®å¤Pipelineç»“æœ {i+1} - ä¿®å¤å®Œæˆ")
                    else:
                        logger.warning(f"ğŸ” Pipelineç»“æœ {i+1} - docåˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                
                # ğŸ”‘ æ–°å¢ï¼šç¡®ä¿æ¯ä¸ªç»“æœéƒ½æœ‰æœ‰æ•ˆçš„docå¯¹è±¡å’Œmetadata
                if 'doc' not in result or not hasattr(result['doc'], 'metadata') or not result['doc'].metadata:
                    logger.warning(f"ğŸ” ç»“æœ {i+1} ç¼ºå°‘æœ‰æ•ˆçš„docå¯¹è±¡æˆ–metadataï¼Œå°è¯•ä»åŸå§‹æ•°æ®æ¢å¤")
                    logger.info(f"ğŸ” ç»“æœ {i+1} çš„å®Œæ•´ç»“æ„: {result}")
                    
                    # å°è¯•ä»resultæœ¬èº«æ¢å¤metadata
                    if isinstance(result, dict):
                        # ğŸ”‘ å…³é”®ä¿®å¤ï¼šä»resultä¸­æå–æ‰€æœ‰å¯èƒ½çš„å­—æ®µ
                        recovered_metadata = {
                            'document_name': result.get('document_name', result.get('title', 'æœªçŸ¥æ–‡æ¡£')),
                            'page_number': result.get('page_number', result.get('page_idx', 'æœªçŸ¥é¡µ')),
                            'chunk_type': 'table',
                            'table_type': result.get('table_type', 'æ•°æ®è¡¨æ ¼'),
                            'table_id': result.get('id', result.get('table_id', f'table_{i+1}')),
                            'chunk_index': result.get('chunk_index', i),
                            'page_content': result.get('table_html', result.get('page_content', '')),
                            'processed_table_content': result.get('table_content', ''),
                            'table_headers': result.get('table_headers', []),
                            'table_row_count': result.get('table_row_count', 0),
                            'table_column_count': result.get('table_column_count', 0),
                            'table_summary': result.get('table_summary', '')
                        }
                        
                        # ğŸ”‘ å°è¯•ä»åµŒå¥—ç»“æ„ä¸­æå–æ›´å¤šä¿¡æ¯
                        if 'original_result' in result and isinstance(result['original_result'], dict):
                            orig = result['original_result']
                            if 'doc' in orig and hasattr(orig['doc'], 'metadata'):
                                orig_metadata = orig['doc'].metadata
                                logger.info(f"ğŸ” ä»original_result.doc.metadataæå–ä¿¡æ¯: {orig_metadata}")
                                # ä½¿ç”¨åŸå§‹metadataè¡¥å……ç¼ºå¤±çš„å­—æ®µ
                                for key in ['document_name', 'page_number', 'page_content']:
                                    if key in orig_metadata and not recovered_metadata.get(key):
                                        recovered_metadata[key] = orig_metadata[key]
                                        logger.info(f"ğŸ” è¡¥å……å­—æ®µ {key}: {orig_metadata[key]}")
                        
                        # ğŸ”‘ æ–°å¢ï¼šå°è¯•ä»Pipelineç»“æœæœ¬èº«æå–æ›´å¤šä¿¡æ¯
                        if 'table_info' in result and isinstance(result['table_info'], dict):
                            table_info = result['table_info']
                            logger.info(f"ğŸ” ä»table_infoæå–ä¿¡æ¯: {table_info}")
                            # ä½¿ç”¨table_infoè¡¥å……ç¼ºå¤±çš„å­—æ®µ
                            if 'table_type' in table_info and not recovered_metadata.get('table_type'):
                                recovered_metadata['table_type'] = table_info['table_type']
                            if 'business_domain' in table_info and not recovered_metadata.get('business_domain'):
                                recovered_metadata['business_domain'] = table_info['business_domain']
                        
                        # ğŸ”‘ æ–°å¢ï¼šå°è¯•ä»docå¯¹è±¡æœ¬èº«æå–ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                        if 'doc' in result and isinstance(result['doc'], dict):
                            doc_obj = result['doc']
                            logger.info(f"ğŸ” ä»docå¯¹è±¡æå–ä¿¡æ¯: {doc_obj}")
                            # å¦‚æœdocå¯¹è±¡æœ‰metadataï¼Œå°è¯•ä½¿ç”¨å®ƒ
                            if hasattr(doc_obj, 'metadata') and doc_obj.metadata:
                                doc_metadata = doc_obj.metadata
                                logger.info(f"ğŸ” ä»doc.metadataæå–ä¿¡æ¯: {doc_metadata}")
                                # ä½¿ç”¨doc.metadataè¡¥å……ç¼ºå¤±çš„å­—æ®µ
                                for key in ['document_name', 'page_number', 'page_content', 'processed_table_content']:
                                    if key in doc_metadata and not recovered_metadata.get(key):
                                        recovered_metadata[key] = doc_metadata[key]
                                        logger.info(f"ğŸ” ä»doc.metadataè¡¥å……å­—æ®µ {key}: {doc_metadata[key]}")
                        
                        logger.info(f"ğŸ” æ¢å¤çš„metadata: {recovered_metadata}")
                        
                        # æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„docå¯¹è±¡
                        class MockDoc:
                            def __init__(self, content, metadata):
                                self.page_content = content
                                self.metadata = metadata
                        
                        # ğŸ”‘ ä¼˜å…ˆä½¿ç”¨HTMLå†…å®¹ï¼Œç¡®ä¿table_htmlå­—æ®µæ­£ç¡®
                        # ğŸ”‘ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨SourceFilterEngineè¿”å›çš„contentå­—æ®µ
                        html_content = result.get('content', '')  # ä¼˜å…ˆä½¿ç”¨contentå­—æ®µ
                        if not html_content:
                            html_content = result.get('table_html', '')  # å…¶æ¬¡ä½¿ç”¨table_html
                        if not html_content:
                            html_content = result.get('page_content', '')  # å†æ¬¡ä½¿ç”¨page_content
                        if not html_content:
                            html_content = result.get('table_content', '')  # æœ€åä½¿ç”¨table_content
                        
                        # ğŸ”‘ æ–°å¢ï¼šå°è¯•ä»docå¯¹è±¡ä¸­æå–HTMLå†…å®¹
                        if not html_content and 'doc' in result:
                            doc_obj = result['doc']
                            if hasattr(doc_obj, 'page_content'):
                                html_content = doc_obj.page_content
                                logger.info(f"ğŸ” ä»doc.page_contentæå–HTMLå†…å®¹ï¼Œé•¿åº¦: {len(html_content)}")
                            elif isinstance(doc_obj, dict) and 'page_content' in doc_obj:
                                html_content = doc_obj['page_content']
                                logger.info(f"ğŸ” ä»doc['page_content']æå–HTMLå†…å®¹ï¼Œé•¿åº¦: {len(html_content)}")
                        
                        logger.info(f"ğŸ” æ¢å¤çš„HTMLå†…å®¹é•¿åº¦: {len(html_content)}")
                        
                        mock_doc = MockDoc(html_content, recovered_metadata)
                        result['doc'] = mock_doc
                        result['score'] = result.get('score', 0.5)
                        logger.info(f"ğŸ” å·²æ¢å¤ç»“æœ {i+1} çš„metadataå’Œdocå¯¹è±¡")
                    else:
                        logger.warning(f"ğŸ” ç»“æœ {i+1} æ— æ³•æ¢å¤ï¼Œè·³è¿‡")
                        continue
                
                # ä¿®å¤ï¼šå¤„ç†é‡æ’åºåå¯èƒ½æ²¡æœ‰'doc'é”®çš„æƒ…å†µ
                if 'doc' not in result:
                    logger.warning(f"ğŸ” ç»“æœ {i+1} ç¼ºå°‘'doc'é”®ï¼Œå°è¯•ä¿®å¤")
                    
                    # å°è¯•ä¿®å¤ç»Ÿä¸€Pipelineçš„ç»“æœæ ¼å¼
                    if 'original_result' in result and 'doc' in result['original_result']:
                        logger.info("æ£€æµ‹åˆ°ç»Ÿä¸€Pipelineç»“æœæ ¼å¼ï¼Œå°è¯•ä¿®å¤...")
                        original_doc = result['original_result']['doc']
                        
                        # å¤„ç†åµŒå¥—çš„doc.docç»“æ„
                        if isinstance(original_doc, dict) and 'doc' in original_doc:
                            actual_doc = original_doc['doc']
                            # ğŸ”‘ ä¿®å¤ï¼šä½¿ç”¨actual_doc.metadataè€Œä¸æ˜¯original_doc.get('metadata', {})
                            actual_metadata = actual_doc.metadata if hasattr(actual_doc, 'metadata') else {}
                            
                            # æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„docå¯¹è±¡
                            class MockDoc:
                                def __init__(self, content, metadata):
                                    self.page_content = content
                                    self.metadata = metadata
                            
                            # ğŸ”‘ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨HTMLæ ¼å¼çš„page_contentï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨processed_table_content
                            html_content = actual_doc.metadata.get('page_content', '') if hasattr(actual_doc, 'metadata') else ''
                            if not html_content and hasattr(actual_doc, 'page_content'):
                                html_content = actual_doc.page_content
                            
                            mock_doc = MockDoc(html_content, actual_metadata)
                            result['doc'] = mock_doc
                            result['score'] = result.get('score', 0.5)
                            result['source'] = result.get('source', 'unknown')
                            result['layer'] = result.get('layer', 1)
                            logger.info("å·²ä¿®å¤ç»Ÿä¸€Pipelineç»“æœæ ¼å¼")
                        else:
                            # ç›´æ¥ä½¿ç”¨original_doc
                            if hasattr(original_doc, 'page_content') and hasattr(original_doc, 'metadata'):
                                result['doc'] = original_doc
                                result['score'] = result.get('score', 0.5)
                                result['source'] = result.get('source', 'unknown')
                                result['layer'] = result.get('layer', 1)
                                logger.info("å·²ä¿®å¤ç»Ÿä¸€Pipelineç»“æœæ ¼å¼ï¼ˆç›´æ¥ä½¿ç”¨ï¼‰")
                            else:
                                logger.warning("æ— æ³•ä¿®å¤ç»Ÿä¸€Pipelineç»“æœæ ¼å¼ï¼Œè·³è¿‡")
                                continue
                    # å°è¯•ä¿®å¤å…¶ä»–æ ¼å¼
                    elif isinstance(result, dict) and 'content' in result and 'metadata' in result:
                        # æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„docå¯¹è±¡
                        class MockDoc:
                            def __init__(self, content, metadata):
                                self.page_content = content
                                self.metadata = metadata
                        
                        mock_doc = MockDoc(result['content'], result['metadata'])
                        result['doc'] = mock_doc
                        result['score'] = result.get('score', 0.5)
                        result['source'] = result.get('source', 'unknown')
                        result['layer'] = result.get('layer', 1)
                        logger.info("å·²ä¿®å¤ç»“æœæ ¼å¼")
                    else:
                        logger.warning(f"ğŸ” ç»“æœ {i+1} æ— æ³•ä¿®å¤ï¼Œè·³è¿‡")
                        continue
                
                doc = result['doc']
                metadata = getattr(doc, 'metadata', {})
                structure_analysis = result.get('structure_analysis', {})

                # è°ƒè¯•ï¼šæ£€æŸ¥æ ¼å¼åŒ–æ—¶çš„metadata
                logger.info(f"ğŸ” æ ¼å¼åŒ– - metadata: {metadata}")
                logger.info(f"ğŸ” æ ¼å¼åŒ– - document_name: '{metadata.get('document_name', 'æœªæ‰¾åˆ°')}'")     
                logger.info(f"ğŸ” æ ¼å¼åŒ– - page_number: {metadata.get('page_number', 'æœªæ‰¾åˆ°')}")

                # å¼€å§‹æ ¼å¼åŒ–è¡¨æ ¼å†…å®¹
                
                # æ–¹æ¡ˆAï¼šä¿ç•™ç°æœ‰å­—æ®µï¼ŒåŒæ—¶è¡¥å……é¡¶å±‚é”®ï¼Œç¡®ä¿Webç«¯å…¼å®¹æ€§
                logger.info(f"ğŸ” ä¼ ç»Ÿæ ¼å¼åŒ– - å¼€å§‹å¤„ç†ç»“æœ")
                logger.info(f"ğŸ” ä¼ ç»Ÿæ ¼å¼åŒ– - metadata: {metadata}")
                logger.info(f"ğŸ” ä¼ ç»Ÿæ ¼å¼åŒ– - document_name: '{metadata.get('document_name', 'æœªæ‰¾åˆ°')}'")
                logger.info(f"ğŸ” ä¼ ç»Ÿæ ¼å¼åŒ– - page_number: {metadata.get('page_number', 'æœªæ‰¾åˆ°')}")
                
                # ğŸ”‘ ä¿®å¤ï¼šä½¿ç”¨ä¸æ–¹æ¡ˆå®Œå…¨ä¸€è‡´çš„å­—æ®µæ˜ å°„
                formatted_result = {
                    'id': metadata.get('table_id', 'unknown'),
                    'table_type': metadata.get('table_type', 'æ•°æ®è¡¨æ ¼'),
                    'table_title': metadata.get('table_title', ''),
                    # ğŸ”‘ å…³é”®ä¿®å¤ï¼šç¡®ä¿table_htmlåŒ…å«çœŸæ­£çš„HTMLå†…å®¹
                    'table_html': metadata.get('page_content', ''),              # ä»metadataè·å–HTMLå†…å®¹
                    'table_content': metadata.get('processed_table_content', ''), # ä»metadataè·å–æ–‡æœ¬å†…å®¹
                    'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                    'page_number': metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                    'score': result.get('score', 0.0),
                    'chunk_type': 'table',
                    'table_headers': metadata.get('table_headers', []),
                    'table_row_count': metadata.get('table_row_count', 0),
                    'table_column_count': metadata.get('table_column_count', 0),
                    'table_summary': metadata.get('table_summary', ''),
                    'chunk_index': metadata.get('chunk_index', 0)
                }
                
                # ğŸ”‘ æ–°å¢ï¼šå¦‚æœmetadataä¸­æ²¡æœ‰HTMLå†…å®¹ï¼Œå°è¯•ä»doc.page_contentè·å–
                if not formatted_result['table_html'] and hasattr(doc, 'page_content'):
                    # æ£€æŸ¥page_contentæ˜¯å¦åŒ…å«HTMLæ ‡ç­¾
                    page_content = getattr(doc, 'page_content', '')
                    if '<table' in page_content and '</table>' in page_content:
                        formatted_result['table_html'] = page_content
                        logger.info(f"ğŸ” ä»doc.page_contentè·å–åˆ°HTMLè¡¨æ ¼å†…å®¹ï¼Œé•¿åº¦: {len(page_content)}")
                    else:
                        logger.warning(f"ğŸ” doc.page_contentä¸åŒ…å«æœ‰æ•ˆçš„HTMLè¡¨æ ¼æ ‡ç­¾")
                
                # ğŸ”‘ æ–°å¢ï¼šå¦‚æœä»ç„¶æ²¡æœ‰HTMLå†…å®¹ï¼Œå°è¯•ä»resultä¸­è·å–
                if not formatted_result['table_html']:
                    # å°è¯•ä»resultæœ¬èº«è·å–HTMLå†…å®¹
                    html_content = result.get('table_html', '') or result.get('page_content', '')
                    if html_content and ('<table' in html_content and '</table>' in html_content):
                        formatted_result['table_html'] = html_content
                        logger.info(f"ğŸ” ä»resultè·å–åˆ°HTMLè¡¨æ ¼å†…å®¹ï¼Œé•¿åº¦: {len(html_content)}")
                    else:
                        logger.warning(f"ğŸ” resultä¸­ä¹Ÿæ²¡æœ‰æœ‰æ•ˆçš„HTMLè¡¨æ ¼å†…å®¹")
                
                logger.info(f"ğŸ” æœ€ç»ˆtable_htmlé•¿åº¦: {len(formatted_result['table_html'])}")
                logger.info(f"ğŸ” æœ€ç»ˆtable_contenté•¿åº¦: {len(formatted_result['table_content'])}")
                
                logger.info(f"ğŸ” ä¼ ç»Ÿæ ¼å¼åŒ– - æ„é€ çš„formatted_result: {formatted_result}")
                logger.info(f"ğŸ” ä¼ ç»Ÿæ ¼å¼åŒ– - æ„é€ çš„document_name: '{formatted_result['document_name']}'")
                logger.info(f"ğŸ” ä¼ ç»Ÿæ ¼å¼åŒ– - æ„é€ çš„page_number: '{formatted_result['page_number']}'")
                
                # å¦‚æœæœ‰å®Œæ•´è¡¨æ ¼å†…å®¹ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
                if 'full_content' in result:
                    formatted_result['full_content'] = result['full_content']
                    formatted_result['full_metadata'] = result['full_metadata']
                
                formatted_results.append(formatted_result)
            
            processing_time = time.time() - start_time
            
            # è¿”å›QueryResultå¯¹è±¡ï¼Œä¸text_engine.pyä¿æŒä¸€è‡´
            return QueryResult(
                success=True,
                query=query,
                query_type=QueryType.TABLE,
                results=formatted_results,
                total_count=len(formatted_results),
                processing_time=processing_time,
                engine_name=self.name,
                metadata={
                    'total_tables': len(self.table_docs),
                    'pipeline': pipeline_metadata.get('pipeline', 'traditional_ranking'),
                    'intent_analysis': intent_analysis,
                    'search_strategy': 'five_layer_recall',
                    'docs_loaded': self._docs_loaded,
                    'vector_store_available': self.vector_store is not None,
                    'document_loader_available': self.document_loader is not None,
                    'llm_answer': pipeline_metadata.get('llm_answer', 'åŸºäºæŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆçš„ç­”æ¡ˆ'),
                    'recall_count': len(search_results),  # å¬å›æ•°é‡
                    'final_count': len(formatted_results),  # æœ€ç»ˆç»“æœæ•°é‡
                    'pipeline_metrics': pipeline_metadata.get('pipeline_metrics', {})  # PipelineæŒ‡æ ‡
                }
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"å¤„ç†è¡¨æ ¼æŸ¥è¯¢å¤±è´¥: {e}")
            
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.TABLE,
                results=[],
                total_count=0,
                processing_time=processing_time,
                engine_name=self.name,
                metadata={},
                error_message=str(e)
            )
    
    def _process_with_new_pipeline(self, query: str, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨æ–°çš„ç»Ÿä¸€Pipelineå¤„ç†æœç´¢ç»“æœ
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param search_results: æœç´¢ç»“æœ
        :return: å¤„ç†åçš„ç»“æœ
        """
        try:
            logger.info("å¼€å§‹ä½¿ç”¨æ–°Pipelineå¤„ç†è¡¨æ ¼æœç´¢ç»“æœ")
            
            # 1. é¦–å…ˆè¿›è¡Œé‡æ’åº
            reranked_results = self._rerank_table_results(query, search_results)
            logger.info(f"é‡æ’åºå®Œæˆï¼Œç»“æœæ•°é‡: {len(reranked_results)}")
            
            # éªŒè¯é‡æ’åºç»“æœçŠ¶æ€
            logger.info(f"ğŸ” é‡æ’åºç»“æœéªŒè¯ - ç»“æœæ•°é‡: {len(reranked_results)}")
            for i, result in enumerate(reranked_results[:3]):  # åªæ£€æŸ¥å‰3ä¸ª
                logger.info(f"ğŸ” é‡æ’åºç»“æœ {i+1} - ç±»å‹: {type(result)}")
                logger.info(f"ğŸ” é‡æ’åºç»“æœ {i+1} - é”®: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                if 'doc' in result and result['doc']:
                    doc = result['doc']
                    logger.info(f"ğŸ” é‡æ’åºç»“æœ {i+1} - docç±»å‹: {type(doc)}")
                    if hasattr(doc, 'metadata'):
                        logger.info(f"ğŸ” é‡æ’åºç»“æœ {i+1} - doc.metadata: {doc.metadata}")
                        logger.info(f"ğŸ” é‡æ’åºç»“æœ {i+1} - document_name: '{doc.metadata.get('document_name', 'æœªæ‰¾åˆ°')}'")
                        logger.info(f"ğŸ” é‡æ’åºç»“æœ {i+1} - page_number: {doc.metadata.get('page_number', 'æœªæ‰¾åˆ°')}")
                    else:
                        logger.warning(f"âŒ é‡æ’åºç»“æœ {i+1} - docæ²¡æœ‰metadataå±æ€§")
                else:
                    logger.warning(f"âŒ é‡æ’åºç»“æœ {i+1} - ç¼ºå°‘docå­—æ®µ")
            
            # 2. ä½¿ç”¨ç»Ÿä¸€Pipelineå¤„ç†
            from v2.core.unified_pipeline import UnifiedPipeline
            
            # è·å–Pipelineé…ç½®
            pipeline_config = {
                'enable_llm_generation': True,
                'enable_source_filtering': True,
                'max_context_results': 10,
                'max_content_length': 1000
            }
            
            # åˆ›å»ºç»Ÿä¸€Pipelineå®ä¾‹
            # ä½¿ç”¨æ³¨å…¥çš„å¼•æ“
            if not self.llm_engine:
                logger.warning("LLMå¼•æ“æœªæ³¨å…¥ï¼Œä½¿ç”¨Mockå¼•æ“")
                # åˆ›å»ºMock LLMå¼•æ“
                class MockLLMEngine:
                    def generate_answer(self, query, context):
                        return f"åŸºäºæŸ¥è¯¢'{query}'ç”Ÿæˆçš„Mockç­”æ¡ˆï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)}"
                
                llm_engine = MockLLMEngine()
            else:
                llm_engine = self.llm_engine
            
            if not self.source_filter_engine:
                logger.warning("æºè¿‡æ»¤å¼•æ“æœªæ³¨å…¥ï¼Œä½¿ç”¨Mockå¼•æ“")
                # åˆ›å»ºMockæºè¿‡æ»¤å¼•æ“
                class MockSourceFilterEngine:
                    def filter_sources(self, llm_answer, sources, query):
                        return sources[:5]  # ç®€å•è¿”å›å‰5ä¸ªæº
                
                source_filter_engine = MockSourceFilterEngine()
            else:
                source_filter_engine = self.source_filter_engine
            
            unified_pipeline = UnifiedPipeline(
                config=pipeline_config,
                llm_engine=llm_engine,
                source_filter_engine=source_filter_engine
            )
            
            # è½¬æ¢ç»“æœæ ¼å¼ä¸ºPipelineæœŸæœ›çš„æ ¼å¼
            # å¢åŠ è¾“å…¥æ•°é‡é™åˆ¶ï¼Œè®©LLMèƒ½çœ‹åˆ°æ›´å¤šä¸Šä¸‹æ–‡
            max_pipeline_inputs = min(8, len(reranked_results))  # æœ€å¤š8ä¸ªè¾“å…¥
            pipeline_input = []
            logger.info(f"å¼€å§‹è½¬æ¢ {len(reranked_results)} ä¸ªé‡æ’åºç»“æœä¸ºPipelineè¾“å…¥æ ¼å¼ï¼Œé™åˆ¶ä¸º {max_pipeline_inputs} ä¸ª")
            
            for i, result in enumerate(reranked_results[:max_pipeline_inputs]):
                logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - å¼€å§‹å¤„ç†")
                logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - ç»“æœç±»å‹: {type(result)}")
                logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - ç»“æœé”®: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                
                # å¤„ç†ä¸åŒçš„ç»“æœæ ¼å¼
                if 'doc' in result and result['doc']:
                    doc = result['doc']
                    logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - docç±»å‹: {type(doc)}")
                    
                    # æ£€æŸ¥docæ˜¯å¦æ˜¯åŒ…å«docå­—æ®µçš„å­—å…¸ï¼ˆé‡æ’åºç»“æœæ ¼å¼ï¼‰
                    if isinstance(doc, dict) and 'doc' in doc and doc['doc']:
                        # é‡æ’åºç»“æœæ ¼å¼ï¼š{'doc': {'doc': doc_object, ...}, ...}
                        actual_doc = doc['doc']
                        content = getattr(actual_doc, 'page_content', '')
                        metadata = getattr(actual_doc, 'metadata', {})
                        logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - åµŒå¥—docæ ¼å¼ï¼Œactual_docç±»å‹: {type(actual_doc)}")
                        logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - actual_doc.metadata: {metadata}")
                    else:
                        # ç›´æ¥åŒ…å«docå¯¹è±¡çš„æƒ…å†µ
                        content = getattr(doc, 'page_content', '')
                        metadata = getattr(doc, 'metadata', {})
                        logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - ç›´æ¥docæ ¼å¼ï¼Œdoc.metadata: {metadata}")
                elif 'content' in result:
                    # ç›´æ¥åŒ…å«contentçš„æƒ…å†µ
                    content = result['content']
                    metadata = result.get('metadata', {})
                    logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - ç›´æ¥contentæ ¼å¼ï¼Œmetadata: {metadata}")
                else:
                    logger.warning(f"ç»“æœ {i} æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡")
                    continue
                
                logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - æœ€ç»ˆmetadata: {metadata}")
                logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - document_name: '{metadata.get('document_name', 'æœªæ‰¾åˆ°')}'")
                logger.info(f"ğŸ” Pipelineè¾“å…¥è½¬æ¢ {i+1} - page_number: {metadata.get('page_number', 'æœªæ‰¾åˆ°')}")
                
                # æ„é€ Pipelineè¾“å…¥
                pipeline_item = {
                    'content': content,
                    'metadata': metadata,
                    'score': result.get('score', 0.5),
                    'source': result.get('source', 'unknown'),
                    'layer': result.get('layer', 1)
                }
                pipeline_input.append(pipeline_item)
                logger.debug(f"ç»“æœ {i} è½¬æ¢å®Œæˆ")
            
            logger.info(f"Pipelineè¾“å…¥è½¬æ¢å®Œæˆï¼Œå…± {len(pipeline_input)} ä¸ªæœ‰æ•ˆè¾“å…¥")
            
            # æ‰§è¡ŒPipelineå¤„ç†
            pipeline_result = unified_pipeline.process(query, pipeline_input)
            
            logger.info(f"ğŸ” Pipelineå¤„ç†ç»“æœæ£€æŸ¥:")
            logger.info(f"ğŸ” Pipelineå¤„ç†ç»“æœç±»å‹: {type(pipeline_result)}")
            logger.info(f"ğŸ” Pipelineå¤„ç†ç»“æœå±æ€§: {[attr for attr in dir(pipeline_result) if not attr.startswith('_')]}")
            logger.info(f"ğŸ” Pipelineå¤„ç†ç»“æœsuccess: {pipeline_result.success}")
            logger.info(f"ğŸ” Pipelineå¤„ç†ç»“æœfiltered_sourcesæ•°é‡: {len(pipeline_result.filtered_sources) if hasattr(pipeline_result, 'filtered_sources') else 'N/A'}")
            
            if pipeline_result.success:
                logger.info("æ–°Pipelineå¤„ç†æˆåŠŸ")
                logger.info(f"Pipelineè¿”å›ç»“æœ: filtered_sourcesæ•°é‡={len(pipeline_result.filtered_sources)}")
                
                # æ£€æŸ¥Pipelineè¿”å›çš„filtered_sourcesç»“æ„
                logger.info(f"ğŸ” Pipeline filtered_sourcesç»“æ„æ£€æŸ¥:")
                for i, source in enumerate(pipeline_result.filtered_sources[:2]):  # åªæ£€æŸ¥å‰2ä¸ª
                    logger.info(f"ğŸ” Pipelineæº {i+1} - ç±»å‹: {type(source)}")
                    logger.info(f"ğŸ” Pipelineæº {i+1} - é”®: {list(source.keys()) if isinstance(source, dict) else 'N/A'}")
                    if isinstance(source, dict):
                        for key, value in source.items():
                            if isinstance(value, dict):
                                logger.info(f"ğŸ” Pipelineæº {i+1} - {key}: {list(value.keys()) if isinstance(value, dict) else value}")
                            else:
                                logger.info(f"ğŸ” Pipelineæº {i+1} - {key}: {value}")
                
                # å°†Pipelineç»“æœè½¬æ¢ä¸ºTableEngineæœŸæœ›çš„æ ¼å¼
                formatted_results = []
                logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ - å¼€å§‹å¤„ç† {len(pipeline_result.filtered_sources)} ä¸ªæº")
                
                for i, source in enumerate(pipeline_result.filtered_sources):
                    logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - æºç±»å‹: {type(source)}")
                    logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - æºé”®: {list(source.keys()) if isinstance(source, dict) else 'N/A'}")
                    
                    # æ£€æŸ¥æºçš„metadata
                    source_metadata = source.get('metadata', {})
                    logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - source.metadata: {source_metadata}")
                    logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - document_name: '{source_metadata.get('document_name', 'æœªæ‰¾åˆ°')}'")
                    logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - page_number: {source_metadata.get('page_number', 'æœªæ‰¾åˆ°')}")
                    
                    # æ·±å…¥æ£€æŸ¥sourceå¯¹è±¡çš„æ‰€æœ‰å­—æ®µ
                    logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - å®Œæ•´sourceå¯¹è±¡: {source}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¯èƒ½åŒ…å«metadataçš„å­—æ®µ
                    for key, value in source.items():
                        if isinstance(value, dict) and 'document_name' in value:
                            logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - åœ¨å­—æ®µ'{key}'ä¸­æ‰¾åˆ°document_name: {value.get('document_name')}")
                        if isinstance(value, dict) and 'page_number' in value:
                            logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - åœ¨å­—æ®µ'{key}'ä¸­æ‰¾åˆ°page_number: {value.get('page_number')}")
                    
                    # æ„é€ æ ‡å‡†æ ¼å¼
                    formatted_result = {
                        'id': source_metadata.get('table_id', f'table_{i}'),
                        'content': source.get('content', ''),
                        'score': source.get('score', 0.5),
                        'source': source.get('source', 'pipeline'),
                        'layer': source.get('layer', 1),
                        
                        # é¡¶å±‚å­—æ®µæ˜ å°„
                        'page_content': source.get('content', ''),
                        'document_name': source_metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                        'page_number': source_metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                        'chunk_type': 'table',
                        'table_type': source_metadata.get('table_type', 'unknown'),
                        'doc_id': source_metadata.get('table_id', f'table_{i}'),
                        
                        'metadata': source_metadata
                    }
                    
                    logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - æ„é€ çš„formatted_result.metadata: {formatted_result['metadata']}")
                    logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - æ„é€ çš„document_name: '{formatted_result['document_name']}'")
                    logger.info(f"ğŸ” Pipelineç»“æœè½¬æ¢ {i+1} - æ„é€ çš„page_number: '{formatted_result['page_number']}'")
                    formatted_results.append(formatted_result)
                
                # ä¿å­˜Pipelineç»“æœåˆ°å®ä¾‹å˜é‡ï¼Œä¾›åç»­ä½¿ç”¨
                self._last_pipeline_result = {
                    'llm_answer': pipeline_result.llm_answer,
                    'filtered_sources': pipeline_result.filtered_sources,
                    'pipeline_metrics': pipeline_result.pipeline_metrics
                }
                
                logger.info(f"æ–°Pipelineå¤„ç†å®Œæˆï¼Œè¿”å› {len(formatted_results)} ä¸ªç»“æœ")
                return formatted_results
            else:
                logger.warning(f"æ–°Pipelineå¤„ç†å¤±è´¥: {pipeline_result.error_message}")
                # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
                return self._format_results_traditional(search_results)
                
        except Exception as e:
            logger.error(f"æ–°Pipelineå¤„ç†å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
            return self._format_results_traditional(search_results)
    
    def _format_results_traditional(self, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä¼ ç»Ÿæ–¹å¼æ ¼å¼åŒ–ç»“æœï¼ˆä½œä¸ºæ–°Pipelineçš„å›é€€æ–¹æ¡ˆï¼‰
        
        :param search_results: æœç´¢ç»“æœ
        :return: æ ¼å¼åŒ–åçš„ç»“æœ
        """
        logger.info("ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ ¼å¼åŒ–ç»“æœ")
        formatted_results = []
        
        for result in search_results:
            # ç°åœ¨æ‰€æœ‰ç»“æœéƒ½åº”è¯¥æœ‰æ­£ç¡®çš„docç»“æ„
            if 'doc' not in result:
                logger.warning(f"è·³è¿‡æ— æ•ˆç»“æœï¼Œç¼ºå°‘'doc'é”®")
                continue
                
            doc = result['doc']
            metadata = getattr(doc, 'metadata', {})
            structure_analysis = result.get('structure_analysis', {})
            
            # ä½¿ç”¨æ˜ç¡®çš„å­—æ®µæ˜ å°„å…³ç³»
            logger.info(f"ğŸ” æ˜ç¡®å­—æ®µæ˜ å°„æ ¼å¼åŒ– - å¼€å§‹å¤„ç†ç»“æœ")
            logger.info(f"ğŸ” æ˜ç¡®å­—æ®µæ˜ å°„æ ¼å¼åŒ– - metadata: {metadata}")
            logger.info(f"ğŸ” æ˜ç¡®å­—æ®µæ˜ å°„æ ¼å¼åŒ– - document_name: '{metadata.get('document_name', 'æœªæ‰¾åˆ°')}'")
            logger.info(f"ğŸ” æ˜ç¡®å­—æ®µæ˜ å°„æ ¼å¼åŒ– - page_number: {metadata.get('page_number', 'æœªæ‰¾åˆ°')}")
            
            formatted_result = {
                # åŸºç¡€å­—æ®µ
                'id': metadata.get('table_id', 'unknown'),
                'score': result['score'],
                'source': result.get('source', 'unknown'),
                'layer': result.get('layer', 1),
                
                # é€šç”¨å­—æ®µ - æ˜ç¡®å¯¹åº”
                'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                'page_number': metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                'chunk_type': 'table',
                
                # è¡¨æ ¼å­—æ®µ - æ˜ç¡®å¯¹åº”
                'table_id': metadata.get('table_id', ''),                    # æ˜ç¡®ä»table_idè·å–
                'table_type': metadata.get('table_type', ''),                # æ˜ç¡®ä»table_typeè·å–
                'table_title': metadata.get('table_title', ''),              # æ˜ç¡®ä»table_titleè·å–
                'table_summary': metadata.get('table_summary', ''),          # æ˜ç¡®ä»table_summaryè·å–
                'table_headers': metadata.get('table_headers', []),          # æ˜ç¡®ä»table_headersè·å–
                'table_row_count': metadata.get('table_row_count', 0),      # æ˜ç¡®ä»table_row_countè·å–
                'table_column_count': metadata.get('table_column_count', 0), # æ˜ç¡®ä»table_column_countè·å–
                'html_content': getattr(doc, 'page_content', ''),           # æ˜ç¡®ä»page_contentè·å–ï¼ˆHTMLæ ¼å¼ï¼‰
                'processed_content': metadata.get('processed_table_content', ''), # æ˜ç¡®ä»processed_table_contentè·å–
                'related_text': metadata.get('related_text', ''),            # æ˜ç¡®ä»related_textè·å–
                'chunk_index': metadata.get('chunk_index', 0),              # æ˜ç¡®ä»chunk_indexè·å–
                
                # å…¼å®¹æ€§å­—æ®µï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
                'content': getattr(doc, 'page_content', ''),
                'page_content': getattr(doc, 'page_content', ''),
                'doc_id': metadata.get('table_id') or metadata.get('doc_id') or metadata.get('id', 'unknown'),
                
                # ç»“æ„åˆ†æå­—æ®µ
                'metadata': {
                    'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                    'page_number': metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                    'table_type': structure_analysis.get('table_type', 'unknown'),
                    'business_domain': structure_analysis.get('business_domain', 'unknown'),
                    'quality_score': structure_analysis.get('quality_score', 0.0),
                    'is_truncated': structure_analysis.get('is_truncated', False),
                    'truncation_type': structure_analysis.get('truncation_type', 'none'),
                    'truncated_rows': structure_analysis.get('truncated_rows', 0),
                    'current_rows': structure_analysis.get('row_count', 0),
                    'original_rows': structure_analysis.get('original_row_count', 0)
                }
            }
            
            # å¦‚æœæœ‰å®Œæ•´è¡¨æ ¼å†…å®¹ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if 'full_content' in result:
                formatted_result['full_content'] = result['full_content']
                formatted_result['full_metadata'] = result['full_metadata']
            
            formatted_results.append(formatted_result)
            
            logger.info(f"ğŸ” æ˜ç¡®å­—æ®µæ˜ å°„æ ¼å¼åŒ– - æ„é€ çš„formatted_result.metadata: {formatted_result['metadata']}")
            logger.info(f"ğŸ” æ˜ç¡®å­—æ®µæ˜ å°„æ ¼å¼åŒ– - æ„é€ çš„document_name: '{formatted_result['document_name']}'")
            logger.info(f"ğŸ” æ˜ç¡®å­—æ®µæ˜ å°„æ ¼å¼åŒ– - æ„é€ çš„page_number: '{formatted_result['page_number']}'")
        
        return formatted_results
    
    def _search_tables(self, query: str) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œè¡¨æ ¼æœç´¢ - æ–°çš„äº”å±‚å¬å›ç­–ç•¥ï¼ˆä¸Text/Image Engineä¿æŒä¸€è‡´ï¼‰
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        # ğŸ” è¯Šæ–­ä¿¡æ¯ï¼šæ£€æŸ¥ç³»ç»ŸçŠ¶æ€
        logger.info("=" * 50)
        logger.info("ğŸ” å¼€å§‹è¯Šæ–­äº”å±‚å¬å›ç­–ç•¥")
        logger.info(f"æŸ¥è¯¢æ–‡æœ¬: {query}")
        logger.info(f"å‘é‡æ•°æ®åº“çŠ¶æ€: {self.vector_store}")
        logger.info(f"è¡¨æ ¼æ–‡æ¡£ç¼“å­˜æ•°é‡: {len(self.table_docs)}")
        logger.info(f"æ–‡æ¡£åŠ è½½çŠ¶æ€: {self._docs_loaded}")
        
        # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
        if self.vector_store:
            if hasattr(self.vector_store, 'docstore') and hasattr(self.vector_store.docstore, '_dict'):
                logger.info(f"å‘é‡æ•°æ®åº“å¯ç”¨ï¼Œæ–‡æ¡£æ•°é‡: {len(self.vector_store.docstore._dict)}")
            else:
                logger.info("å‘é‡æ•°æ®åº“å¯ç”¨ï¼Œä½†docstoreç»“æ„å¼‚å¸¸")
        else:
            logger.error("âŒ å‘é‡æ•°æ®åº“ä¸ºç©ºï¼")
        
        logger.info("=" * 50)
        
        all_results = []
        min_required = getattr(self.config, 'min_required_results', 20)
        max_recall_results = getattr(self.config, 'max_recall_results', 150)
        
        logger.info(f"å¼€å§‹æ‰§è¡Œäº”å±‚å¬å›ç­–ç•¥ï¼ŒæŸ¥è¯¢: {query}")
        
        # ç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„ç²¾ç¡®åŒ¹é…ï¼ˆé«˜ç²¾åº¦ï¼Œä½å¬å›ï¼‰
        logger.info("æ‰§è¡Œç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„ç²¾ç¡®åŒ¹é…")
        layer1_results = self._table_structure_precise_search(query, top_k=30)
        all_results.extend(layer1_results)
        logger.info(f"âœ… ç¬¬ä¸€å±‚ç»“æ„æœç´¢æˆåŠŸï¼Œå¬å› {len(layer1_results)} ä¸ªç»“æœ")
        
        # ç¬¬äºŒå±‚ï¼šå‘é‡è¯­ä¹‰æœç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œä¸­ç­‰å¬å›ï¼‰
        logger.info("æ‰§è¡Œç¬¬äºŒå±‚ï¼šå‘é‡è¯­ä¹‰æœç´¢")
        layer2_results = self._enhanced_vector_search(query, top_k=40)
        all_results.extend(layer2_results)
        logger.info(f"âœ… ç¬¬äºŒå±‚å‘é‡æœç´¢æˆåŠŸï¼Œå¬å› {len(layer2_results)} ä¸ªç»“æœ")
        
        # # ç¬¬ä¸‰å±‚ï¼šè¡¨æ ¼å†…å®¹å…³é”®è¯åŒ¹é…ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œé«˜å¬å›ï¼‰
        # logger.info("æ‰§è¡Œç¬¬ä¸‰å±‚ï¼šè¡¨æ ¼å†…å®¹å…³é”®è¯åŒ¹é…")
        # layer3_results = self._enhanced_content_keyword_search(query, top_k=35)
        # all_results.extend(layer3_results)
        # logger.info(f"âœ… ç¬¬ä¸‰å±‚å…³é”®è¯æœç´¢æˆåŠŸï¼Œå¬å› {len(layer3_results)} ä¸ªç»“æœ")
        
        # # ç¬¬å››å±‚ï¼šæ··åˆæ™ºèƒ½æœç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œé«˜å¬å›ï¼‰
        # logger.info("æ‰§è¡Œç¬¬å››å±‚ï¼šæ··åˆæ™ºèƒ½æœç´¢")
        # layer4_results = self._enhanced_hybrid_search(query, top_k=30)
        # all_results.extend(layer4_results)
        # logger.info(f"âœ… ç¬¬å››å±‚æ··åˆæœç´¢æˆåŠŸï¼Œå¬å› {len(layer4_results)} ä¸ªç»“æœ")
        
        # æ£€æŸ¥å‰å››å±‚ç»“æœæ•°é‡ï¼Œå†³å®šæ˜¯å¦æ¿€æ´»ç¬¬äº”å±‚
        total_results = len(all_results)
        logger.info(f"å‰å››å±‚æ€»ç»“æœæ•°é‡: {total_results}")
        
        if total_results >= min_required:
            logger.info(f"å‰å››å±‚å¬å›æ•°é‡å……è¶³({total_results} >= {min_required})ï¼Œè·³è¿‡ç¬¬äº”å±‚")
        else:
            # ç¬¬äº”å±‚ï¼šå®¹é”™æ‰©å±•æœç´¢ï¼ˆå…œåº•ç­–ç•¥ï¼‰
            logger.warning(f"å‰å››å±‚å¬å›æ•°é‡ä¸è¶³({total_results} < {min_required})ï¼Œæ¿€æ´»ç¬¬äº”å±‚")
            layer5_results = self._fault_tolerant_expansion_search(query, top_k=25)
            all_results.extend(layer5_results)
            logger.info(f"ç¬¬äº”å±‚è¿”å› {len(layer5_results)} ä¸ªç»“æœ")
        
        # ç»“æœèåˆä¸å»é‡
        logger.info("å¼€å§‹ç»“æœèåˆä¸å»é‡")
        final_results = self._merge_and_deduplicate_results(all_results)
        
        # æœ€ç»ˆæ’åº
        final_results = self._final_ranking(query, final_results)
        
        # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        max_results = getattr(self.config, 'max_results', 10)
        final_results = final_results[:max_results]
        
        logger.info(f"äº”å±‚å¬å›ç­–ç•¥å®Œæˆï¼Œæœ€ç»ˆç»“æœæ•°é‡: {len(final_results)}")
        return final_results
    
    def _table_structure_precise_search(self, query: str, top_k: int = 30) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„ç²¾ç¡®åŒ¹é…ï¼ˆé«˜ç²¾åº¦ï¼Œä½å¬å›ï¼‰
        
        åŸºäºè¡¨æ ¼çš„ç»“æ„ç‰¹å¾è¿›è¡Œç²¾ç¡®åŒ¹é…ï¼š
        1. è¡¨æ ¼æ ‡é¢˜åŒ¹é…
        2. åˆ—åç²¾ç¡®åŒ¹é…
        3. è¡¨æ ¼ç±»å‹åŒ¹é…
        4. è¡¨æ ¼å†…å®¹ç»“æ„åˆ†æ
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ğŸ” ç¬¬ä¸€å±‚ç»“æ„æœç´¢ - æŸ¥è¯¢: {query}, ç›®æ ‡æ•°é‡: {top_k}")
            logger.info(f"ğŸ” ç¬¬ä¸€å±‚ç»“æ„æœç´¢ - table_docsæ•°é‡: {len(self.table_docs)}")
            
            # ğŸ”‘ æ–°å¢ï¼šæ£€æŸ¥å‰3ä¸ªtable_docsçš„çŠ¶æ€
            if self.table_docs:
                logger.info("ğŸ” æ£€æŸ¥å‰3ä¸ªtable_docsçš„çŠ¶æ€...")
                for i, doc in enumerate(self.table_docs[:3]):
                    logger.info(f"ğŸ” table_docs[{i}] ç±»å‹: {type(doc)}")
                    if hasattr(doc, 'page_content'):
                        page_content = doc.page_content
                        logger.info(f"ğŸ” table_docs[{i}] page_contenté•¿åº¦: {len(page_content) if page_content else 0}")
                        if page_content and len(page_content) > 50:
                            logger.info(f"ğŸ” table_docs[{i}] page_contentå‰50å­—ç¬¦: {page_content[:50]}")
                    else:
                        logger.warning(f"ğŸ” table_docs[{i}] æ²¡æœ‰page_contentå±æ€§ï¼")
                    
                    if hasattr(doc, 'metadata') and doc.metadata and 'page_content' in doc.metadata:
                        meta_page_content = doc.metadata['page_content']
                        logger.info(f"ğŸ” table_docs[{i}] metadata['page_content']é•¿åº¦: {len(meta_page_content) if meta_page_content else 0}")
                    else:
                        logger.warning(f"ğŸ” table_docs[{i}] metadataä¸­æ²¡æœ‰page_contentå­—æ®µ")
            else:
                logger.warning("ğŸ” table_docsä¸ºç©ºï¼")
            
            logger.info("ç¬¬ä¸€å±‚ç»“æ„æœç´¢ - æŸ¥è¯¢: {query}, ç›®æ ‡æ•°é‡: {top_k}")
            
            # 1. è¡¨æ ¼æ ‡é¢˜ç²¾ç¡®åŒ¹é…
            title_matches = self._search_by_table_title(query, top_k // 3)
            results.extend(title_matches)
            logger.info(f"æ ‡é¢˜åŒ¹é…ç»“æœ: {len(title_matches)} ä¸ª")
            
            # 2. åˆ—åç²¾ç¡®åŒ¹é…
            column_matches = self._search_by_column_names(query, top_k // 3)
            results.extend(column_matches)
            logger.info(f"åˆ—ååŒ¹é…ç»“æœ: {len(column_matches)} ä¸ª")
            
            # 3. è¡¨æ ¼ç±»å‹åŒ¹é…
            type_matches = self._search_by_table_type(query, top_k // 3)
            results.extend(type_matches)
            logger.info(f"ç±»å‹åŒ¹é…ç»“æœ: {len(type_matches)} ä¸ª")
            
            # 4. è¡¨æ ¼å†…å®¹ç»“æ„åˆ†æ
            structure_matches = self._search_by_table_structure(query, top_k // 3)
            results.extend(structure_matches)
            logger.info(f"ç»“æ„åŒ¹é…ç»“æœ: {len(structure_matches)} ä¸ª")
            
            # å»é‡å’Œæ’åº
            unique_results = self._deduplicate_by_doc_id(results)
            sorted_results = sorted(unique_results, key=lambda x: x.get('structure_score', 0), reverse=True)
            
            # é™åˆ¶ç»“æœæ•°é‡
            final_results = sorted_results[:top_k]
            

            
            logger.info(f"âœ… ç¬¬ä¸€å±‚ç»“æ„æœç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬ä¸€å±‚ç»“æ„æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_by_table_title(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """åŸºäºè¡¨æ ¼æ ‡é¢˜æœç´¢"""
        results = []
        
        try:
            logger.debug(f"ğŸ” æ ‡é¢˜æœç´¢ - å¼€å§‹æœç´¢ï¼ŒæŸ¥è¯¢: {query}, æœ€å¤§ç»“æœæ•°: {max_results}")
            logger.debug(f"ğŸ” æ ‡é¢˜æœç´¢ - table_docsæ•°é‡: {len(self.table_docs)}")
            
            # æå–æŸ¥è¯¢ä¸­çš„å…³é”®æ¦‚å¿µ
            query_keywords = self._extract_keywords(query)
            
            for i, table_doc in enumerate(self.table_docs):
                if not hasattr(table_doc, 'metadata'):
                    logger.debug(f"ğŸ” æ ‡é¢˜æœç´¢ - è·³è¿‡æ–‡æ¡£ {i}ï¼šæ²¡æœ‰metadataå±æ€§")
                    continue
                
                metadata = table_doc.metadata
                table_title = metadata.get('table_title', '')
                
                if not table_title:
                    logger.debug(f"ğŸ” æ ‡é¢˜æœç´¢ - è·³è¿‡æ–‡æ¡£ {i}ï¼šæ²¡æœ‰table_title")
                    continue
                
                # ğŸ”‘ æ–°å¢ï¼šæ£€æŸ¥Documentå¯¹è±¡çš„çŠ¶æ€
                if hasattr(table_doc, 'page_content'):
                    page_content = table_doc.page_content
                    logger.debug(f"ğŸ” æ ‡é¢˜æœç´¢ - æ–‡æ¡£ {i} page_contenté•¿åº¦: {len(page_content) if page_content else 0}")
                else:
                    logger.warning(f"ğŸ” æ ‡é¢˜æœç´¢ - æ–‡æ¡£ {i} æ²¡æœ‰page_contentå±æ€§ï¼")
                
                if 'page_content' in metadata:
                    meta_page_content = metadata['page_content']
                    logger.debug(f"ğŸ” æ ‡é¢˜æœç´¢ - æ–‡æ¡£ {i} metadata['page_content']é•¿åº¦: {len(meta_page_content) if meta_page_content else 0}")
                else:
                    logger.debug(f"ğŸ” æ ‡é¢˜æœç´¢ - æ–‡æ¡£ {i} metadataä¸­æ²¡æœ‰page_contentå­—æ®µ")
                
                # è®¡ç®—æ ‡é¢˜åŒ¹é…åˆ†æ•°
                title_score = self._calculate_title_similarity(query_keywords, table_title)
                
                if title_score > 0.6:  # æ ‡é¢˜åŒ¹é…é˜ˆå€¼
                    logger.debug(f"ğŸ” æ ‡é¢˜æœç´¢ - æ–‡æ¡£ {i} åŒ¹é…æˆåŠŸï¼Œåˆ†æ•°: {title_score}")
                    results.append({
                        'doc': table_doc,
                        'content': table_doc.page_content,
                        'metadata': table_doc.metadata,
                        'score': title_score,
                        'source': 'structure_search',
                        'layer': 1,
                        'search_method': 'title_match',
                        'structure_score': title_score,
                        'match_details': f"æ ‡é¢˜åŒ¹é…: {table_title}"
                    })
            
            logger.debug(f"ğŸ” æ ‡é¢˜æœç´¢ - æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç»“æœ")
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            

            
            return sorted_results[:max_results]
            
        except Exception as e:
            logger.error(f"æ ‡é¢˜æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_by_column_names(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """åŸºäºåˆ—åæœç´¢"""
        results = []
        
        try:
            # æå–æŸ¥è¯¢ä¸­çš„å…³é”®æ¦‚å¿µ
            query_keywords = self._extract_keywords(query)
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                table_headers = metadata.get('table_headers', [])
                
                if not table_headers:
                    continue
                
                # è®¡ç®—åˆ—ååŒ¹é…åˆ†æ•°
                column_score = self._calculate_column_similarity(query_keywords, table_headers)
                
                if column_score > 0.5:  # åˆ—ååŒ¹é…é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'content': table_doc.page_content,
                        'metadata': table_doc.metadata,
                        'score': column_score,
                        'source': 'structure_search',
                        'layer': 1,
                        'search_method': 'column_match',
                        'structure_score': column_score,
                        'match_details': f"åˆ—ååŒ¹é…: {', '.join(table_headers[:3])}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            

            
            return sorted_results[:max_results]
            
        except Exception as e:
            logger.error(f"åˆ—åæœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_by_table_type(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """åŸºäºè¡¨æ ¼ç±»å‹æœç´¢"""
        results = []
        
        try:
            # åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œåˆ¤æ–­è¡¨æ ¼ç±»å‹
            query_intent = self._analyze_query_intent(query)
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                table_type = metadata.get('table_type', '')
                
                if not table_type:
                    continue
                
                # è®¡ç®—ç±»å‹åŒ¹é…åˆ†æ•°
                type_score = self._calculate_type_similarity(query_intent, table_type)
                
                if type_score > 0.4:  # ç±»å‹åŒ¹é…é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'content': table_doc.page_content,
                        'metadata': table_doc.metadata,
                        'score': type_score,
                        'source': 'structure_search',
                        'layer': 1,
                        'search_method': 'type_match',
                        'structure_score': type_score,
                        'match_details': f"ç±»å‹åŒ¹é…: {table_type}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            

            
            return sorted_results[:max_results]
            
        except Exception as e:
            logger.error(f"ç±»å‹æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_by_table_structure(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """åŸºäºè¡¨æ ¼ç»“æ„æœç´¢"""
        results = []
        
        try:
            # åˆ†ææŸ¥è¯¢å¯¹è¡¨æ ¼ç»“æ„çš„è¦æ±‚
            structure_requirements = self._analyze_structure_requirements(query)
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                row_count = metadata.get('table_row_count', 0)
                column_count = metadata.get('table_column_count', 0)
                
                # è®¡ç®—ç»“æ„åŒ¹é…åˆ†æ•°
                structure_score = self._calculate_structure_similarity(structure_requirements, row_count, column_count)
                
                if structure_score > 0.3:  # ç»“æ„åŒ¹é…é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'content': table_doc.page_content,
                        'metadata': table_doc.metadata,
                        'score': structure_score,
                        'source': 'structure_search',
                        'layer': 1,
                        'search_method': 'structure_match',
                        'structure_score': structure_score,
                        'match_details': f"ç»“æ„åŒ¹é…: {row_count}è¡ŒÃ—{column_count}åˆ—"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            return sorted_results[:max_results]
            
        except Exception as e:
            logger.error(f"ç»“æ„æœç´¢å¤±è´¥: {e}")
            return []
    
    def _calculate_search_k(self, target_k: int, layer_config) -> int:
        """
        æ™ºèƒ½è®¡ç®—æœç´¢èŒƒå›´ï¼Œç”¨äºpost-filterç­–ç•¥
        
        :param target_k: ç›®æ ‡ç»“æœæ•°é‡
        :param layer_config: å±‚çº§é…ç½®å¯¹è±¡
        :return: æœç´¢èŒƒå›´
        """
        try:
            if hasattr(layer_config, 'top_k'):
                base_top_k = layer_config.top_k
            else:
                base_top_k = 40
            if hasattr(layer_config, 'similarity_threshold'):
                similarity_threshold = layer_config.similarity_threshold
            else:
                similarity_threshold = 0.65
            
            # æ ¹æ®é˜ˆå€¼åŠ¨æ€è°ƒæ•´æœç´¢èŒƒå›´
            if similarity_threshold < 0.3:
                # ä½é˜ˆå€¼ï¼Œéœ€è¦æœç´¢æ›´å¤šå€™é€‰ç»“æœ
                search_k = max(target_k * 4, base_top_k * 2)
            elif similarity_threshold < 0.6:
                # ä¸­ç­‰é˜ˆå€¼
                search_k = max(target_k * 3, base_top_k * 1.5)
            else:
                # é«˜é˜ˆå€¼ï¼Œå¯ä»¥æœç´¢è¾ƒå°‘å€™é€‰ç»“æœ
                search_k = max(target_k * 2, base_top_k)
            
            # è®¾ç½®ä¸Šé™é¿å…è¿‡åº¦æœç´¢
            search_k = min(search_k, 150)
            
            logger.debug(f"æ™ºèƒ½è®¡ç®—search_k: ç›®æ ‡{target_k}, åŸºç¡€top_k{base_top_k}, é˜ˆå€¼{similarity_threshold}, æœ€ç»ˆsearch_k{search_k}")
            return search_k
            
        except Exception as e:
            logger.error(f"è®¡ç®—search_kå¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            return max(target_k * 3, 80)

    def _enhanced_vector_search(self, query: str, top_k: int = 40) -> List[Dict[str, Any]]:
        """
        ç¬¬äºŒå±‚ï¼šå¢å¼ºçš„å‘é‡è¯­ä¹‰æœç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œä¸­ç­‰å¬å›ï¼‰ï¼Œæ”¯æŒpost-filterç­–ç•¥
        
        åˆ©ç”¨å¤šç§å‘é‡åŒ–ç­–ç•¥è¿›è¡Œè¡¨æ ¼å¬å›ï¼š
        1. ç­–ç•¥1ï¼šå°è¯•ä½¿ç”¨FAISS filterç›´æ¥æœç´¢
        2. ç­–ç•¥2ï¼šä½¿ç”¨post-filterç­–ç•¥ï¼ˆå…ˆæœç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤ï¼‰
        3. ç­–ç•¥3ï¼šå¤‡é€‰æ–¹æ¡ˆï¼ˆå¦‚æœå‰ä¸¤ç§éƒ½å¤±è´¥ï¼‰
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        if not self.vector_store or not getattr(self.config, 'enable_vector_search', True):
            logger.info("å‘é‡æœç´¢æœªå¯ç”¨æˆ–å‘é‡æ•°æ®åº“ä¸å¯ç”¨")
            return results
        
        try:
            # è·å–ç¬¬äºŒå±‚é…ç½®
            layer2_config = getattr(self.config, 'recall_strategy', {}).get('layer2_vector_search', {})
            if hasattr(layer2_config, 'similarity_threshold'):
                threshold = layer2_config.similarity_threshold
            else:
                threshold = 0.65
            if hasattr(layer2_config, 'top_k'):
                base_top_k = layer2_config.top_k
            else:
                base_top_k = 40
            
            # æ™ºèƒ½è®¡ç®—æœç´¢èŒƒå›´
            search_k = self._calculate_search_k(top_k, layer2_config)
            
            logger.info(f"ç¬¬äºŒå±‚å‘é‡æœç´¢ - æŸ¥è¯¢: {query}, é˜ˆå€¼: {threshold}, ç›®æ ‡æ•°é‡: {top_k}, æœç´¢èŒƒå›´: {search_k}")
            
            # ç­–ç•¥1ï¼šä½¿ç”¨FAISS filterç›´æ¥æœç´¢tableç±»å‹æ–‡æ¡£
            logger.info("ç­–ç•¥1ï¼šä½¿ç”¨FAISS filterç›´æ¥æœç´¢tableç±»å‹æ–‡æ¡£")
            try:
                # ä½¿ç”¨æ­£ç¡®çš„FAISS filterè¯­æ³•ï¼Œå¢åŠ æœç´¢èŒƒå›´ä»¥æé«˜å¬å›ç‡
                content_results = []
                
                # å°è¯•ä½¿ç”¨æ›´å¤§çš„æœç´¢èŒƒå›´æ¥æ‰¾åˆ°æ›´å¤šç›¸å…³æ–‡æ¡£
                filter_search_k = min(search_k * 2, 200)  # æ‰©å¤§æœç´¢èŒƒå›´ï¼Œä½†ä¸è¶…è¿‡200
                
                try:
                    logger.info(f"ä½¿ç”¨filteræœç´¢ï¼Œk={filter_search_k}")
                    content_results = self.vector_store.similarity_search(
                        query, 
                        k=filter_search_k,
                        filter={'chunk_type': 'table'}  # æ ‡å‡†FAISS filteræ ¼å¼
                    )
                    
                    # ğŸ”‘ æ‰‹åŠ¨è¡¥å……page_contentå­—æ®µ
                    for doc in content_results:
                        if hasattr(doc, 'metadata') and doc.metadata and 'page_content' in doc.metadata:
                            # ä»metadataä¸­æ¢å¤page_content
                            doc.page_content = doc.metadata.get('page_content', '')
                            logger.info(f"ğŸ” ç­–ç•¥1 - å·²è¡¥å……Documentå¯¹è±¡çš„page_contentå­—æ®µï¼Œé•¿åº¦: {len(doc.page_content)}")
                        else:
                            logger.warning(f"ğŸ” ç­–ç•¥1 - Documentå¯¹è±¡ç¼ºå°‘page_contentå­—æ®µï¼Œæ— æ³•è¡¥å……")
                    
                    if len(content_results) > 0:
                        logger.info(f"âœ… FAISS filteræˆåŠŸï¼Œè¿”å› {len(content_results)} ä¸ªtableæ–‡æ¡£")
                    else:
                        logger.info(f"âš ï¸ FAISS filterè¿”å›0ä¸ªç»“æœï¼Œå¯èƒ½æŸ¥è¯¢ä¸tableæ–‡æ¡£ç›¸ä¼¼åº¦å¤ªä½")
                        
                        # FAISS filteræœ‰ä¸¥æ ¼çš„å†…éƒ¨ç›¸ä¼¼åº¦é™åˆ¶ï¼Œæ— æ³•é€šè¿‡æ‰©å¤§æœç´¢èŒƒå›´çªç ´
                        logger.info("âš ï¸ FAISS filteræœ‰ä¸¥æ ¼çš„å†…éƒ¨ç›¸ä¼¼åº¦é™åˆ¶ï¼Œæ— æ³•çªç ´")
                        logger.info("ç›´æ¥è¿›å…¥ç­–ç•¥2ï¼ˆpost-filterï¼‰ä»¥è·å¾—æ›´å¥½çš„å¬å›æ•ˆæœ")
                        
                except Exception as filter_e:
                    logger.warning(f"FAISS filterå¤±è´¥: {filter_e}")
                
                logger.info(f"âœ… ç­–ç•¥1æœ€ç»ˆè¿”å› {len(content_results)} ä¸ªç»“æœ")
                
                # å¤„ç†filteræœç´¢ç»“æœ
                processed_results = []
                for doc in content_results:
                    if not hasattr(doc, 'metadata'):
                        logger.warning(f"âŒ ç­–ç•¥1 - æ–‡æ¡£ç¼ºå°‘metadataå±æ€§: {type(doc)}")
                        continue
                    
                    # ä½¿ç”¨å†…å®¹ç›¸å…³æ€§åˆ†æ•°ï¼ˆå‚è€ƒtext_engineçš„æ–¹æ³•ï¼‰
                    vector_score = self._calculate_content_relevance(query, doc.page_content)
                    
                    # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                    if vector_score >= threshold:
                        # è°ƒè¯•ï¼šæ£€æŸ¥ç­–ç•¥1çš„metadata
                        logger.info(f"ğŸ” ç­–ç•¥1 - doc.metadata: {doc.metadata}")
                        logger.info(f"ğŸ” ç­–ç•¥1 - document_name: '{doc.metadata.get('document_name', 'æœªæ‰¾åˆ°')}'")
                        logger.info(f"ğŸ” ç­–ç•¥1 - page_number: {doc.metadata.get('page_number', 'æœªæ‰¾åˆ°')}")
                        logger.info(f"ğŸ” ç­–ç•¥1 - docç±»å‹: {type(doc)}")
                        logger.info(f"ğŸ” ç­–ç•¥1 - docå±æ€§: {[attr for attr in dir(doc) if not attr.startswith('_')]}")

                        processed_doc = {
                            'doc': doc,
                            'content': doc.page_content,
                            'metadata': doc.metadata,
                            'score': vector_score,
                            'source': 'vector_search',
                            'layer': 2,
                            'search_method': 'content_semantic_similarity_filter',
                            'vector_score': vector_score,
                            'match_details': 'processed_table_contentè¯­ä¹‰åŒ¹é…(filter)'
                        }
                        logger.info(f"ğŸ” ç­–ç•¥1 - æ„é€ processed_doc: {list(processed_doc.keys())}")
                        processed_results.append(processed_doc)
                
                logger.info(f"ç­–ç•¥1é€šè¿‡é˜ˆå€¼æ£€æŸ¥çš„ç»“æœæ•°é‡: {len(processed_results)}")
                
                # å¦‚æœç­–ç•¥1è¿”å›è¶³å¤Ÿçš„ç»“æœï¼Œç›´æ¥è¿”å›
                if len(processed_results) >= top_k * 0.8:  # 80%çš„ç›®æ ‡æ•°é‡
                    logger.info(f"âœ… ç­–ç•¥1æˆåŠŸï¼Œè¿”å› {len(processed_results)} ä¸ªç»“æœ")
                    return processed_results[:top_k]
                else:
                    logger.info(f"âš ï¸ ç­–ç•¥1ç»“æœä¸è¶³ï¼Œåªæœ‰ {len(processed_results)} ä¸ªï¼Œéœ€è¦é™çº§åˆ°ç­–ç•¥2")
                    
            except Exception as e:
                logger.warning(f"ç­–ç•¥1å®Œå…¨å¤±è´¥: {e}")
                logger.info("é™çº§åˆ°post-filterç­–ç•¥")
            
            # ç­–ç•¥2ï¼šä½¿ç”¨post-filterç­–ç•¥ï¼ˆå…ˆæœç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤ï¼‰
            logger.info("ç­–ç•¥2ï¼šä½¿ç”¨post-filterç­–ç•¥ï¼ˆå…ˆæœç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤ï¼‰")
            
            # æœç´¢æ›´å¤šå€™é€‰ç»“æœç”¨äºåè¿‡æ»¤
            all_candidates = self.vector_store.similarity_search(
                query, 
                k=search_k
            )
            
            logger.info(f"ç­–ç•¥2æœç´¢è¿”å› {len(all_candidates)} ä¸ªå€™é€‰ç»“æœ")
            
            # åè¿‡æ»¤ï¼šç­›é€‰å‡ºtableç±»å‹çš„æ–‡æ¡£
            table_candidates = []
            for doc in all_candidates:
                if (hasattr(doc, 'metadata') and doc.metadata and 
                    doc.metadata.get('chunk_type') == 'table'):
                    table_candidates.append(doc)
            
            logger.info(f"åè¿‡æ»¤åæ‰¾åˆ° {len(table_candidates)} ä¸ªtableæ–‡æ¡£")
            
            # ğŸ”‘ ä¼˜åŒ–ï¼šåœ¨post_filterä¹‹åå†è¡¥å……å­—æ®µï¼Œåªå¯¹éœ€è¦çš„ç»“æœæ“ä½œ
            logger.info("å¼€å§‹å¯¹post-filteråçš„ç»“æœè¡¥å……å­—æ®µ...")
            for doc in table_candidates:
                if hasattr(doc, 'metadata') and doc.metadata and 'page_content' in doc.metadata:
                    # ä»metadataä¸­æ¢å¤page_content
                    doc.page_content = doc.metadata.get('page_content', '')
                    logger.info(f"ğŸ” ç­–ç•¥2 - å·²è¡¥å……Documentå¯¹è±¡çš„page_contentå­—æ®µï¼Œé•¿åº¦: {len(doc.page_content)}")
                else:
                    logger.warning(f"ğŸ” ç­–ç•¥2 - Documentå¯¹è±¡ç¼ºå°‘page_contentå­—æ®µï¼Œæ— æ³•è¡¥å……")
            
            # å¤„ç†tableæœç´¢ç»“æœï¼Œåº”ç”¨é˜ˆå€¼è¿‡æ»¤
            processed_results = []
            for doc in table_candidates:
                # ä½¿ç”¨å†…å®¹ç›¸å…³æ€§åˆ†æ•°ï¼ˆå‚è€ƒtext_engineçš„æ–¹æ³•ï¼‰
                vector_score = self._calculate_content_relevance(query, doc.page_content)
                
                # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                if vector_score >= threshold:
                    processed_doc = {
                        'doc': doc,
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'score': vector_score,
                        'source': 'vector_search',
                        'layer': 2,
                        'search_method': 'content_semantic_similarity_post_filter',
                        'vector_score': vector_score,
                        'match_details': 'processed_table_contentè¯­ä¹‰åŒ¹é…(post-filter)'
                    }
                    processed_results.append(processed_doc)
            
            logger.info(f"ç­–ç•¥2é€šè¿‡é˜ˆå€¼æ£€æŸ¥çš„ç»“æœæ•°é‡: {len(processed_results)}")
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            processed_results.sort(key=lambda x: x['score'], reverse=True)
            final_results = processed_results[:top_k]
            
            logger.info(f"âœ… ç­–ç•¥2 post-filteræˆåŠŸï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬äºŒå±‚å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def _enhanced_content_keyword_search(self, query: str, top_k: int = 35) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸‰å±‚ï¼šè¡¨æ ¼å†…å®¹å…³é”®è¯åŒ¹é…ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œé«˜å¬å›ï¼‰
        
        åŸºäºè¡¨æ ¼å†…å®¹çš„å…³é”®è¯åŒ¹é…ç­–ç•¥ï¼š
        1. è¡¨æ ¼æ ‡é¢˜å…³é”®è¯åŒ¹é…
        2. åˆ—åå…³é”®è¯åŒ¹é…
        3. è¡¨æ ¼å†…å®¹å…³é”®è¯åŒ¹é…
        4. è¡¨æ ¼æ‘˜è¦å…³é”®è¯åŒ¹é…
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ç¬¬ä¸‰å±‚å…³é”®è¯æœç´¢ - æŸ¥è¯¢: {query}, ç›®æ ‡æ•°é‡: {top_k}")
            
            # æå–æŸ¥è¯¢å…³é”®è¯
            query_keywords = self._extract_keywords(query)
            logger.debug(f"æå–çš„æŸ¥è¯¢å…³é”®è¯: {query_keywords}")
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                
                # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
                keyword_score = self._calculate_keyword_match_score(query_keywords, metadata)
                
                if keyword_score > 0.3:  # å…³é”®è¯åŒ¹é…é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'content': table_doc.page_content,
                        'metadata': table_doc.metadata,
                        'score': keyword_score,
                        'source': 'keyword_search',
                        'layer': 3,
                        'search_method': 'keyword_match',
                        'keyword_score': keyword_score,
                        'match_details': f"å…³é”®è¯åŒ¹é…åˆ†æ•°: {keyword_score:.2f}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            final_results = sorted_results[:top_k]
            

            
            logger.info(f"âœ… ç¬¬ä¸‰å±‚å…³é”®è¯æœç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬ä¸‰å±‚å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            return []
    
    def _enhanced_hybrid_search(self, query: str, top_k: int = 30) -> List[Dict[str, Any]]:
        """
        ç¬¬å››å±‚ï¼šæ··åˆæ™ºèƒ½æœç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œé«˜å¬å›ï¼‰
        
        ç»“åˆå¤šç§æœç´¢ç­–ç•¥çš„æ··åˆæ–¹æ³•ï¼š
        1. ç»“æ„ç‰¹å¾ + å†…å®¹ç‰¹å¾çš„ç»„åˆè¯„åˆ†
        2. è¡¨æ ¼è´¨é‡è¯„ä¼°
        3. æŸ¥è¯¢æ„å›¾åˆ†æ
        4. åŠ¨æ€æƒé‡è°ƒæ•´
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ç¬¬å››å±‚æ··åˆæœç´¢ - æŸ¥è¯¢: {query}, ç›®æ ‡æ•°é‡: {top_k}")
            
            # åˆ†ææŸ¥è¯¢æ„å›¾
            query_intent = self._analyze_query_intent(query)
            logger.debug(f"æŸ¥è¯¢æ„å›¾åˆ†æ: {query_intent}")
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                
                # è®¡ç®—æ··åˆè¯„åˆ†
                hybrid_score = self._calculate_hybrid_score(query, query_intent, metadata)
                
                if hybrid_score > 0.2:  # æ··åˆæœç´¢é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'content': table_doc.page_content,
                        'metadata': table_doc.metadata,
                        'score': hybrid_score,
                        'source': 'hybrid_search',
                        'layer': 4,
                        'search_method': 'hybrid_intelligent',
                        'hybrid_score': hybrid_score,
                        'match_details': f"æ··åˆè¯„åˆ†: {hybrid_score:.2f}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            final_results = sorted_results[:top_k]
            

            
            logger.info(f"âœ… ç¬¬å››å±‚æ··åˆæœç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬å››å±‚æ··åˆæœç´¢å¤±è´¥: {e}")
            return []
    
    def _fault_tolerant_expansion_search(self, query: str, top_k: int = 25) -> List[Dict[str, Any]]:
        """
        ç¬¬äº”å±‚ï¼šå®¹é”™æ‰©å±•æœç´¢ï¼ˆå…œåº•ç­–ç•¥ï¼‰
        
        å½“å…¶ä»–å±‚å¬å›ä¸è¶³æ—¶çš„å…œåº•ç­–ç•¥ï¼š
        1. æ¨¡ç³ŠåŒ¹é…
        2. éƒ¨åˆ†å…³é”®è¯åŒ¹é…
        3. è¡¨æ ¼ç±»å‹æ³›åŒ–
        4. é™çº§é˜ˆå€¼åŒ¹é…
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ç¬¬äº”å±‚å®¹é”™æ‰©å±•æœç´¢ - æŸ¥è¯¢: {query}, ç›®æ ‡æ•°é‡: {top_k}")
            
            # æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆæ›´å®½æ¾çš„æå–ï¼‰
            query_keywords = self._extract_keywords_relaxed(query)
            logger.debug(f"å®½æ¾æå–çš„æŸ¥è¯¢å…³é”®è¯: {query_keywords}")
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                
                # è®¡ç®—å®¹é”™è¯„åˆ†ï¼ˆæ›´å®½æ¾çš„é˜ˆå€¼ï¼‰
                fault_tolerant_score = self._calculate_fault_tolerant_score(query_keywords, metadata)
                
                if fault_tolerant_score > 0.1:  # å®¹é”™æœç´¢é˜ˆå€¼ï¼ˆå¾ˆä½ï¼‰
                    results.append({
                        'doc': table_doc,
                        'content': table_doc.page_content,
                        'metadata': table_doc.metadata,
                        'score': fault_tolerant_score,
                        'source': 'fault_tolerant_search',
                        'layer': 5,
                        'search_method': 'fault_tolerant_expansion',
                        'fault_tolerant_score': fault_tolerant_score,
                        'match_details': f"å®¹é”™è¯„åˆ†: {fault_tolerant_score:.2f}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            final_results = sorted_results[:top_k]
            

            
            logger.info(f"âœ… ç¬¬äº”å±‚å®¹é”™æ‰©å±•æœç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬äº”å±‚å®¹é”™æ‰©å±•æœç´¢å¤±è´¥: {e}")
            return []
    
    def _merge_and_deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ç»“æœèåˆä¸å»é‡
        
        :param results: åŸå§‹ç»“æœåˆ—è¡¨
        :return: å»é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        if not results:
            return []
        
        try:
            # å»é‡ï¼ˆåŸºäºæ–‡æ¡£IDï¼‰
            seen_docs = set()
            unique_results = []
            
            for result in results:
                doc_id = result['doc'].metadata.get('id', id(result['doc']))
                if doc_id not in seen_docs:
                    seen_docs.add(doc_id)
                    unique_results.append(result)
                else:
                    # å¦‚æœæ–‡æ¡£å·²å­˜åœ¨ï¼Œé€‰æ‹©åˆ†æ•°æ›´é«˜çš„ç»“æœ
                    existing_result = next(r for r in unique_results if r['doc'].metadata.get('id', id(r['doc'])) == doc_id)
                    if result['score'] > existing_result['score']:
                        # æ›¿æ¢ä¸ºåˆ†æ•°æ›´é«˜çš„ç»“æœ
                        unique_results.remove(existing_result)
                        unique_results.append(result)
            
            # å±‚é—´æƒé‡è°ƒæ•´
            for result in unique_results:
                layer = result.get('layer', 1)
                # æ ¹æ®å±‚çº§è°ƒæ•´åˆ†æ•°ï¼Œå±‚çº§è¶Šä½åˆ†æ•°è¶Šé«˜
                layer_weight = 1.0 / layer
                result['adjusted_score'] = result['score'] * layer_weight
            
            # æŒ‰è°ƒæ•´åçš„åˆ†æ•°æ’åº
            unique_results.sort(key=lambda x: x.get('adjusted_score', x['score']), reverse=True)
            
            logger.info(f"å»é‡åå‰©ä½™ {len(unique_results)} ä¸ªå”¯ä¸€ç»“æœ")
            return unique_results
            
        except Exception as e:
            logger.error(f"å»é‡å’Œæ’åºå¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šç®€å•æ’åº
            return sorted(results, key=lambda x: x['score'], reverse=True)
    
    def _final_ranking(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æœ€ç»ˆæ’åº
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param results: åŸå§‹ç»“æœåˆ—è¡¨
        :return: æœ€ç»ˆæ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            if not results:
                return []
            
            # ç®€å•çš„æ’åºï¼šæŒ‰åˆ†æ•°æ’åºå³å¯
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            
            # æ·»åŠ æ’åä¿¡æ¯
            for i, result in enumerate(sorted_results):
                result['final_rank'] = i + 1
                result['final_score'] = result.get('score', 0.0)
            
            logger.info(f"æœ€ç»ˆæ’åºå®Œæˆï¼Œç»“æœæ•°é‡: {len(sorted_results)}")
            return sorted_results
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆæ’åºå¤±è´¥: {e}")
            return results
    

    
    def _initialize_recall_strategy(self):
        """åˆå§‹åŒ–äº”å±‚å¬å›ç­–ç•¥"""
        try:
            # æ£€æŸ¥å¿…è¦çš„é…ç½®é¡¹
            if not hasattr(self.config, 'use_new_pipeline'):
                logger.warning("æœªé…ç½®use_new_pipelineï¼Œé»˜è®¤å¯ç”¨")
            
            if not hasattr(self.config, 'enable_enhanced_reranking'):
                logger.warning("æœªé…ç½®enable_enhanced_rerankingï¼Œé»˜è®¤å¯ç”¨")
            
            logger.info("äº”å±‚å¬å›ç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¬å›ç­–ç•¥å¤±è´¥: {e}")
    
    def clear_cache(self):
        """æ¸…ç†è¡¨æ ¼å¼•æ“ç¼“å­˜"""
        self.table_docs = []
        self._docs_loaded = False
        logger.info("è¡¨æ ¼å¼•æ“ç¼“å­˜å·²æ¸…ç†")

    def _analyze_table_structure(self, doc):
        """
        ç®€åŒ–ç‰ˆè¡¨æ ¼ç»“æ„åˆ†æ
        
        :param doc: è¡¨æ ¼æ–‡æ¡£
        :return: è¡¨æ ¼ç»“æ„åˆ†æç»“æœ
        """
        try:
            analysis = {
                'table_type': 'unknown',
                'columns': [],
                'row_count': 0,
                'column_count': 0,
                'quality_score': 0.0
            }
            
            # ä»å…ƒæ•°æ®ä¸­æå–åŸºæœ¬ä¿¡æ¯
            metadata = getattr(doc, 'metadata', {})
            if metadata:
                analysis['columns'] = metadata.get('table_headers', [])
                analysis['row_count'] = metadata.get('table_row_count', 0)
                analysis['column_count'] = metadata.get('table_column_count', 0)
                analysis['table_type'] = metadata.get('table_type', 'unknown')
            
            # è®¡ç®—ç®€å•çš„è´¨é‡è¯„åˆ†
            if analysis['row_count'] > 0 and analysis['column_count'] > 0:
                analysis['quality_score'] = min(1.0, (analysis['row_count'] + analysis['column_count']) / 20.0)
            
            return analysis
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {
                'table_type': 'unknown',
                'columns': [],
                'row_count': 0,
                'column_count': 0,
                'quality_score': 0.0
            }
    


    def get_full_table(self, table_id: str) -> Dict[str, Any]:
        """
        è·å–å®Œæ•´è¡¨æ ¼å†…å®¹ï¼ˆæœªæˆªæ–­ç‰ˆæœ¬ï¼‰
        
        :param table_id: è¡¨æ ¼ID
        :return: å®Œæ•´è¡¨æ ¼å†…å®¹å­—å…¸
        """
        try:
            # é¦–å…ˆå°è¯•ä»æ–‡æ¡£åŠ è½½å™¨è·å–å®Œæ•´è¡¨æ ¼
            if self.document_loader:
                full_doc = self.document_loader.get_full_document_by_id(table_id, chunk_type='table')
                if full_doc:
                    logger.info(f"ä»æ–‡æ¡£åŠ è½½å™¨è·å–å®Œæ•´è¡¨æ ¼ {table_id} æˆåŠŸ")
                    return {
                        'status': 'success',
                        'table_id': table_id,
                        'content': getattr(full_doc, 'page_content', ''),
                        'metadata': getattr(full_doc, 'metadata', {}),
                        'message': 'å®Œæ•´è¡¨æ ¼å†…å®¹ä»æ–‡æ¡£åŠ è½½å™¨è·å–æˆåŠŸ'
                    }
            
            # å¦‚æœæ²¡æœ‰æ–‡æ¡£åŠ è½½å™¨æˆ–è·å–å¤±è´¥ï¼Œå°è¯•ä»å‘é‡å­˜å‚¨è·å–
            if self.vector_store:
                full_doc = self.vector_store.get_full_document_by_id(table_id)
                if full_doc:
                    logger.info(f"ä»å‘é‡å­˜å‚¨è·å–å®Œæ•´è¡¨æ ¼ {table_id} æˆåŠŸ")
                    return {
                        'status': 'success',
                        'table_id': table_id,
                        'content': getattr(full_doc, 'page_content', ''),
                        'metadata': getattr(full_doc, 'metadata', {}),
                        'message': 'å®Œæ•´è¡¨æ ¼å†…å®¹ä»å‘é‡å­˜å‚¨è·å–æˆåŠŸ'
                    }
            
            # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            logger.warning(f"æ— æ³•è·å–å®Œæ•´è¡¨æ ¼ {table_id}")
            return {
                'status': 'error',
                'table_id': table_id,
                'content': '',
                'metadata': {},
                'message': f'æ— æ³•è·å–å®Œæ•´è¡¨æ ¼ {table_id}ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰å®Œæ•´æ•°æ®æˆ–æ²¡æœ‰é…ç½®æ–‡æ¡£åŠ è½½å™¨/å‘é‡å­˜å‚¨'
            }
            
        except Exception as e:
            logger.error(f"è·å–å®Œæ•´è¡¨æ ¼ {table_id} å¤±è´¥: {e}")
            return {
                'status': 'error',
                'table_id': table_id,
                'content': '',
                'metadata': {},
                'message': f'è·å–å®Œæ•´è¡¨æ ¼å¤±è´¥: {str(e)}'
            }

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬å…³é”®è¯"""
        try:
            if not text:
                return []
            
            # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥åç»­ä¼˜åŒ–ä¸ºæ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
            text_lower = text.lower()
            
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·
            text_clean = re.sub(r'[^\w\s]', ' ', text_lower)
            
            # åˆ†è¯
            words = text_clean.split()
            
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å¦‚æœ', 'é‚£ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'å“ªäº›', 'ä»€ä¹ˆ', 'å¤šå°‘', 'å‡ ', 'ä¸ª', 'å¹´', 'æœˆ', 'æ—¥', 'æ—¶', 'åˆ†', 'ç§’'}
            keywords = [word for word in words if len(word) > 1 and word not in stop_words]
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_keywords = list(set(keywords))[:20]
            
            return unique_keywords
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    def _extract_keywords_relaxed(self, text: str) -> List[str]:
        """å®½æ¾çš„å…³é”®è¯æå–ï¼ˆç”¨äºå®¹é”™æœç´¢ï¼‰"""
        try:
            if not text:
                return []
            
            # æ›´å®½æ¾çš„å…³é”®è¯æå–
            text_lower = text.lower()
            
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·
            text_clean = re.sub(r'[^\w\s]', ' ', text_lower)
            
            # åˆ†è¯
            words = text_clean.split()
            
            # åªè¿‡æ»¤éå¸¸çŸ­çš„è¯
            keywords = [word for word in words if len(word) > 0]
            
            # å»é‡
            unique_keywords = list(set(keywords))
            
            return unique_keywords
            
        except Exception as e:
            logger.error(f"å®½æ¾å…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    def _calculate_title_similarity(self, query_keywords: List[str], table_title: str) -> float:
        """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not table_title or not query_keywords:
                return 0.0
            
            title_lower = table_title.lower()
            title_words = set(title_lower.split())
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            matched_keywords = sum(1 for kw in query_keywords if kw.lower() in title_words)
            
            if matched_keywords == 0:
                return 0.0
            
            # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
            similarity = min(1.0, matched_keywords / len(query_keywords))
            
            return similarity
            
        except Exception as e:
            logger.error(f"æ ‡é¢˜ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_column_similarity(self, query_keywords: List[str], table_headers: List[str]) -> float:
        """è®¡ç®—åˆ—åç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not table_headers or not query_keywords:
                return 0.0
            
            # è®¡ç®—æ¯ä¸ªåˆ—åçš„åŒ¹é…åˆ†æ•°
            column_scores = []
            
            for header in table_headers:
                if not isinstance(header, str):
                    continue
                
                header_lower = header.lower()
                header_words = set(header_lower.split())
                
                # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                matched_keywords = sum(1 for kw in query_keywords if kw.lower() in header_words)
                
                if matched_keywords > 0:
                    similarity = min(1.0, matched_keywords / len(query_keywords))
                    column_scores.append(similarity)
            
            if not column_scores:
                return 0.0
            
            # è¿”å›æœ€é«˜åˆ†æ•°
            return max(column_scores)
            
        except Exception as e:
            logger.error(f"åˆ—åç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_type_similarity(self, query_intent: Dict[str, Any], table_type: str) -> float:
        """è®¡ç®—ç±»å‹ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not table_type or not query_intent:
                return 0.0
            
            table_type_lower = table_type.lower()
            query_type = query_intent.get('query_type', 'unknown')
            business_domain = query_intent.get('business_domain', 'unknown')
            
            score = 0.0
            
            # æŸ¥è¯¢ç±»å‹åŒ¹é…
            if query_type != 'unknown':
                if query_type in table_type_lower:
                    score += 0.5
                elif any(word in table_type_lower for word in query_type.split('_')):
                    score += 0.3
            
            # ä¸šåŠ¡é¢†åŸŸåŒ¹é…
            if business_domain != 'unknown':
                if business_domain in table_type_lower:
                    score += 0.3
                elif any(word in table_type_lower for word in business_domain.split('_')):
                    score += 0.2
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"ç±»å‹ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_structure_similarity(self, requirements: Dict[str, Any], row_count: int, column_count: int) -> float:
        """è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not requirements:
                return 0.0
            
            score = 0.0
            
            # è¡Œæ•°åŒ¹é…
            min_rows = requirements.get('min_rows', 1)
            max_rows = requirements.get('max_rows', 1000)
            
            if min_rows <= row_count <= max_rows:
                score += 0.5
            elif row_count > 0:
                # éƒ¨åˆ†åŒ¹é…
                if row_count >= min_rows * 0.5:
                    score += 0.3
                elif row_count <= max_rows * 1.5:
                    score += 0.2
            
            # åˆ—æ•°åŒ¹é…
            min_columns = requirements.get('min_columns', 1)
            max_columns = requirements.get('max_columns', 20)
            
            if min_columns <= column_count <= max_columns:
                score += 0.5
            elif column_count > 0:
                # éƒ¨åˆ†åŒ¹é…
                if column_count >= min_columns * 0.5:
                    score += 0.3
                elif column_count <= max_columns * 1.5:
                    score += 0.2
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"ç»“æ„ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_keyword_match_score(self, query_keywords: List[str], metadata: Dict[str, Any]) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        try:
            if not query_keywords or not metadata:
                return 0.0
            
            score = 0.0
            
            # è¡¨æ ¼æ ‡é¢˜å…³é”®è¯åŒ¹é…
            table_title = metadata.get('table_title', '')
            if table_title:
                title_score = self._calculate_title_similarity(query_keywords, table_title)
                score += title_score * 0.4  # æ ‡é¢˜æƒé‡40%
            
            # åˆ—åå…³é”®è¯åŒ¹é…
            table_headers = metadata.get('table_headers', [])
            if table_headers:
                column_score = self._calculate_column_similarity(query_keywords, table_headers)
                score += column_score * 0.4  # åˆ—åæƒé‡40%
            
            # è¡¨æ ¼æ‘˜è¦å…³é”®è¯åŒ¹é…
            table_summary = metadata.get('table_summary', '')
            if table_summary:
                summary_score = self._calculate_title_similarity(query_keywords, table_summary)
                score += summary_score * 0.2  # æ‘˜è¦æƒé‡20%
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"å…³é”®è¯åŒ¹é…åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_hybrid_score(self, query: str, query_intent: Dict[str, Any], metadata: Dict[str, Any]) -> float:
        """è®¡ç®—æ··åˆè¯„åˆ†"""
        try:
            if not metadata:
                return 0.0
            
            score = 0.0
            
            # ç»“æ„ç‰¹å¾è¯„åˆ†
            structure_score = 0.0
            
            # è¡¨æ ¼ç±»å‹åŒ¹é…
            table_type = metadata.get('table_type', '')
            if table_type:
                type_score = self._calculate_type_similarity(query_intent, table_type)
                structure_score += type_score * 0.3
            
            # è¡¨æ ¼ç»“æ„åŒ¹é…
            row_count = metadata.get('table_row_count', 0)
            column_count = metadata.get('table_column_count', 0)
            if row_count > 0 and column_count > 0:
                structure_requirements = self._analyze_structure_requirements(query)
                struct_score = self._calculate_structure_similarity(structure_requirements, row_count, column_count)
                structure_score += struct_score * 0.3
            
            # è¡¨æ ¼è´¨é‡è¯„åˆ†
            quality_score = 0.0
            if row_count > 5 and column_count > 2:
                quality_score = 0.4  # åŸºç¡€è´¨é‡åˆ†æ•°
            
            # æœ€ç»ˆæ··åˆåˆ†æ•°
            score = (structure_score * 0.6) + (quality_score * 0.4)
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"æ··åˆè¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_fault_tolerant_score(self, query_keywords: List[str], metadata: Dict[str, Any]) -> float:
        """è®¡ç®—å®¹é”™è¯„åˆ†ï¼ˆæ›´å®½æ¾çš„é˜ˆå€¼ï¼‰"""
        try:
            if not query_keywords or not metadata:
                return 0.0
            
            score = 0.0
            
            # éå¸¸å®½æ¾çš„æ ‡é¢˜åŒ¹é…
            table_title = metadata.get('table_title', '')
            if table_title:
                title_lower = table_title.lower()
                for kw in query_keywords:
                    if kw.lower() in title_lower:
                        score += 0.2
                        break
            
            # éå¸¸å®½æ¾çš„åˆ—ååŒ¹é…
            table_headers = metadata.get('table_headers', [])
            if table_headers:
                for header in table_headers:
                    if not isinstance(header, str):
                        continue
                    header_lower = header.lower()
                    for kw in query_keywords:
                        if kw.lower() in header_lower:
                            score += 0.2
                            break
                    if score > 0.2:
                        break
            
            # è¡¨æ ¼å­˜åœ¨æ€§åŠ åˆ†
            if metadata.get('table_id'):
                score += 0.1
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"å®¹é”™è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_text_similarity_simple(self, query: str, content: str) -> float:
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
        try:
            if not query or not content:
                return 0.0
            
            query_lower = query.lower()
            content_lower = content.lower()
            
            # è®¡ç®—è¯åŒ¹é…åº¦
            query_words = set(query_lower.split())
            content_words = set(content_lower.split())
            
            if not query_words:
                return 0.0
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            intersection = len(query_words & content_words)
            union = len(query_words | content_words)
            
            if union == 0:
                return 0.0
            
            similarity = intersection / union
            return similarity
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _deduplicate_by_doc_id(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ ¹æ®æ–‡æ¡£IDå»é‡"""
        try:
            seen_doc_ids = set()
            unique_results = []
            
            for result in results:
                doc = result.get('doc')
                if not doc:
                    continue
                
                # è·å–æ–‡æ¡£ID
                doc_id = None
                if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict):
                    doc_id = doc.metadata.get('id') or doc.metadata.get('doc_id') or doc.metadata.get('table_id')
                elif hasattr(doc, 'id'):
                    doc_id = doc.id
                
                if doc_id and doc_id not in seen_doc_ids:
                    seen_doc_ids.add(doc_id)
                    unique_results.append(result)
                elif not doc_id:
                    # å¦‚æœæ²¡æœ‰IDï¼Œç›´æ¥æ·»åŠ 
                    unique_results.append(result)
            
            logger.info(f"å»é‡å‰: {len(results)} ä¸ªç»“æœï¼Œå»é‡å: {len(unique_results)} ä¸ªç»“æœ")
            return unique_results
            
        except Exception as e:
            logger.error(f"ç»“æœå»é‡å¤±è´¥: {e}")
            return results
    
    def _merge_and_deduplicate_results(self, all_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶å’Œå»é‡æ‰€æœ‰ç»“æœ"""
        try:
            if not all_results:
                return []
            
            # å»é‡
            unique_results = self._deduplicate_by_doc_id(all_results)
            
            # ç®€å•çš„å±‚é—´æƒé‡è°ƒæ•´
            for result in unique_results:
                layer = result.get('layer', 1)
                # æ ¹æ®å±‚çº§è°ƒæ•´åˆ†æ•°ï¼Œå±‚çº§è¶Šä½åˆ†æ•°è¶Šé«˜
                layer_weight = 1.0 / layer
                result['adjusted_score'] = result['score'] * layer_weight
            
            # æŒ‰è°ƒæ•´åçš„åˆ†æ•°æ’åº
            unique_results.sort(key=lambda x: x.get('adjusted_score', x['score']), reverse=True)
            
            logger.info(f"ç»“æœåˆå¹¶å®Œæˆï¼Œæœ€ç»ˆç»“æœæ•°é‡: {len(unique_results)}")
            return unique_results
            
        except Exception as e:
            logger.error(f"ç»“æœåˆå¹¶å¤±è´¥: {e}")
            return all_results
    

    
    def _get_doc_id(self, doc) -> str:
        """è·å–æ–‡æ¡£ID"""
        try:
            if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict):
                return (doc.metadata.get('id') or 
                        doc.metadata.get('doc_id') or 
                        doc.metadata.get('table_id') or 
                        str(id(doc)))
            elif hasattr(doc, 'id'):
                return str(doc.id)
            else:
                return str(id(doc))
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£IDå¤±è´¥: {e}")
            return str(id(doc))
    
    def _final_ranking(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æœ€ç»ˆæ’åº"""
        try:
            if not results:
                return []
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            
            # æ·»åŠ æ’åä¿¡æ¯
            for i, result in enumerate(sorted_results):
                result['final_rank'] = i + 1
                result['final_score'] = result.get('score', 0.0)
            
            logger.info(f"æœ€ç»ˆæ’åºå®Œæˆï¼Œç»“æœæ•°é‡: {len(sorted_results)}")
            return sorted_results
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆæ’åºå¤±è´¥: {e}")
            return results
    
    def _final_ranking_and_limit(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æœ€ç»ˆæ’åºå’Œé™åˆ¶ - åŸºäºå¬å›åˆ†æ•°"""
        
        # ä¸ºæ¯ä¸ªç»“æœè®¡ç®—å¬å›åˆ†æ•°
        for result in results:
            result['recall_score'] = self._get_comprehensive_score(result)
        
        # æŒ‰å¬å›åˆ†æ•°æ’åº
        sorted_results = sorted(results, key=lambda x: x.get('recall_score', 0), reverse=True)
        
        # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        max_results = getattr(self.config, 'max_results', 15)
        final_results = sorted_results[:max_results]
        
        # æ·»åŠ æœ€ç»ˆæ’åä¿¡æ¯
        for i, result in enumerate(final_results):
            result['final_rank'] = i + 1
            result['final_score'] = result.get('recall_score', 0.0)
        
        logger.info(f"Table Engineæœ€ç»ˆæ’åºå®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªå€™é€‰æ–‡æ¡£")
        return final_results
    
    def _get_comprehensive_score(self, result: Dict[str, Any]) -> float:
        """è·å–ç»¼åˆåˆ†æ•°"""
        scores = []
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„åˆ†æ•°
        for key in ['vector_score', 'keyword_score', 'semantic_score', 'fuzzy_score', 'expansion_score', 'hybrid_score', 'score']:
            if key in result:
                scores.append(result[key])
        
        # å¦‚æœæ²¡æœ‰åˆ†æ•°ï¼Œè¿”å›0
        if not scores:
            return 0.0
        
        # è¿”å›æœ€é«˜åˆ†æ•°
        return max(scores)

    def _calculate_content_relevance(self, query: str, content: str) -> float:
        """
        è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°ï¼ˆå‚è€ƒtext_engineçš„å®ç°ï¼‰
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param content: æ–‡æ¡£å†…å®¹
        :return: ç›¸å…³æ€§åˆ†æ•° [0, 1]
        """
        try:
            if not content or not query:
                return 0.0
            
            query_lower = query.lower()
            content_lower = content.lower()
            
            # ç›´æ¥åŒ…å«æ£€æŸ¥
            if query_lower in content_lower:
                return 0.8
            
            try:
                import jieba
                query_keywords = jieba.lcut(query_lower, cut_all=False)
                query_words = [word for word in query_keywords if len(word) > 1]
                if not query_words:
                    query_words = [word for word in query_lower.split() if len(word) > 1]
                
                content_keywords = jieba.lcut(content_lower, cut_all=False)
                content_words = [word for word in content_keywords if len(word) > 1]
                if not content_words:
                    content_words = [word for word in content_lower.split() if len(word) > 1]
            except Exception as e:
                query_words = [word for word in query_lower.split() if len(word) > 1]
                content_words = [word for word in content_lower.split() if len(word) > 1]
            
            if not query_words or not content_words:
                return 0.0
            
            matched_words = 0
            total_score = 0.0
            
            for query_word in query_words:
                if query_word in content_words:
                    matched_words += 1
                    word_count = content_lower.count(query_word)
                    word_score = min(word_count / len(content_words), 0.3)
                    total_score += word_score
            
            match_rate = matched_words / len(query_words) if query_words else 0
            final_score = (match_rate * 0.7 + total_score * 0.3)
            
            return min(final_score, 1.0)
            
        except Exception as e:
            logger.warning(f"è®¡ç®—å†…å®¹ç›¸å…³æ€§å¤±è´¥: {e}")
            return 0.0

    def _format_table_results(self, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ ¼å¼åŒ–è¡¨æ ¼ç»“æœï¼šä½¿ç”¨æ˜ç¡®çš„å­—æ®µæ˜ å°„å…³ç³»
        
        :param search_results: æœç´¢ç»“æœåˆ—è¡¨
        :return: æ ¼å¼åŒ–åçš„è¡¨æ ¼ç»“æœåˆ—è¡¨
        """
        formatted_results = []
        
        for result in search_results:
            if isinstance(result, dict) and 'doc' in result:
                doc = result['doc']
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                
                # ä½¿ç”¨æ˜ç¡®çš„å­—æ®µæ˜ å°„
                formatted_result = {
                    'id': metadata.get('table_id', 'unknown'),
                    'table_type': metadata.get('table_type', 'æ•°æ®è¡¨æ ¼'),
                    'table_title': metadata.get('table_title', ''),
                    'table_summary': metadata.get('table_summary', ''),
                    'table_headers': metadata.get('table_headers', []),
                    'table_row_count': metadata.get('table_row_count', 0),
                    'table_column_count': metadata.get('table_column_count', 0),
                    'html_content': metadata.get('page_content', ''),     # æ˜ç¡®ä»page_contentè·å–
                    'processed_content': metadata.get('processed_table_content', ''), # æ˜ç¡®ä»processed_table_contentè·å–
                    'related_text': metadata.get('related_text', ''),
                    'chunk_index': metadata.get('chunk_index', 0),
                    'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                    'page_number': metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                    'chunk_type': 'table',
                    'score': result.get('score', 0.0)
                }
                
                formatted_results.append(formatted_result)
        
        logger.info(f"è¡¨æ ¼ç»“æœæ ¼å¼åŒ–å®Œæˆï¼šè¾“å…¥ {len(search_results)} ä¸ªç»“æœï¼Œè¾“å‡º {len(formatted_results)} ä¸ªç»“æœ")
        return formatted_results
