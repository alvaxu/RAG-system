#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š

## 1. è¡¨æ ¼å¼•æ“æ ¸å¿ƒå®ç°
## 2. æ”¯æŒè¡¨æ ¼æŸ¥è¯¢çš„ä¸“ç”¨å¼•æ“
## 3. é›†æˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢
## 4. æ”¯æŒè¡¨æ ¼ç»“æ„è¯†åˆ«å’ŒæŸ¥è¯¢ä¼˜åŒ–
"""

import logging
import time
from typing import List, Dict, Any, Optional
from ..core.base_engine import BaseEngine
from ..core.base_engine import EngineConfig
from ..core.base_engine import QueryResult, QueryType
try:
from .reranking_services import TableRerankingService
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from v2.core.reranking_services import TableRerankingService
    except ImportError:
        TableRerankingService = None
import jieba
import jieba.analyse

# åˆå§‹åŒ–jiebaåˆ†è¯å™¨
jieba.initialize()

# æ·»åŠ è‡ªå®šä¹‰è¯å…¸
custom_words = [
    'è´¢åŠ¡æŠ¥è¡¨', 'æ”¶å…¥æƒ…å†µ', 'å‘˜å·¥è–ªèµ„', 'éƒ¨é—¨åˆ†å¸ƒ', 'äº§å“åº“å­˜', 'æ•°é‡ç»Ÿè®¡',
    'è¯¦ç»†æ˜ç»†', 'æ±‡æ€»ç»Ÿè®¡', 'å¯¹æ¯”åˆ†æ', 'å¢é•¿è¶‹åŠ¿', 'æˆæœ¬æ§åˆ¶', 'åˆ©æ¶¦åˆ†æ',
    'åº“å­˜ç®¡ç†', 'é”€å”®ä¸šç»©', 'è´¢åŠ¡æŒ‡æ ‡', 'ä¸šåŠ¡æ•°æ®', 'è¿è¥æŠ¥è¡¨', 'ç»©æ•ˆè¯„ä¼°'
]

for word in custom_words:
    jieba.add_word(word)

# å®šä¹‰åœç”¨è¯
stop_words = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
}

logger = logging.getLogger(__name__)


class TableEngine(BaseEngine):
    """
    è¡¨æ ¼å¼•æ“
    
    ä¸“é—¨å¤„ç†è¡¨æ ¼æŸ¥è¯¢ï¼Œæ”¯æŒå¤šç§æœç´¢ç­–ç•¥
    """
    
    def __init__(self, config, vector_store=None, document_loader=None, skip_initial_load=False):
        """
        åˆå§‹åŒ–è¡¨æ ¼å¼•æ“ - é‡æ„ç‰ˆæœ¬ï¼Œæ”¯æŒæ›´å¥½çš„é…ç½®éªŒè¯å’Œæ–‡æ¡£åŠ è½½
        
        :param config: è¡¨æ ¼å¼•æ“é…ç½®
        :param vector_store: å‘é‡æ•°æ®åº“
        :param document_loader: æ–‡æ¡£åŠ è½½å™¨
        :param skip_initial_load: æ˜¯å¦è·³è¿‡åˆå§‹æ–‡æ¡£åŠ è½½
        """
        super().__init__(config)
        
        logger.info("ğŸ” å¼€å§‹åˆå§‹åŒ–TableEngine")
        logger.info(f"é…ç½®ç±»å‹: {type(config)}")
        logger.info(f"å‘é‡æ•°æ®åº“: {vector_store}")
        logger.info(f"æ–‡æ¡£åŠ è½½å™¨: {document_loader}")
        logger.info(f"è·³è¿‡åˆå§‹åŠ è½½: {skip_initial_load}")
        
        self.vector_store = vector_store
        self.document_loader = document_loader
        self.table_docs = []  # è¡¨æ ¼æ–‡æ¡£ç¼“å­˜
        self._docs_loaded = False
        
        # åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡
        self.table_reranking_service = None
        
        logger.info("âœ… åŸºç¡€å±æ€§è®¾ç½®å®Œæˆ")
        
        # éªŒè¯é…ç½®
        logger.info("å¼€å§‹éªŒè¯é…ç½®...")
        self._validate_config()
        logger.info("âœ… é…ç½®éªŒè¯å®Œæˆ")
        
        # åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡
        logger.info("å¼€å§‹åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡...")
        self._initialize_table_reranking_service()
        logger.info("âœ… è¡¨æ ¼é‡æ’åºæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–äº”å±‚å¬å›ç­–ç•¥
        logger.info("å¼€å§‹åˆå§‹åŒ–å…­å±‚å¬å›ç­–ç•¥...")
        self._initialize_recall_strategy()
        logger.info("âœ… å…­å±‚å¬å›ç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åŠ è½½æ–‡æ¡£
        if not skip_initial_load:
            logger.info("å¼€å§‹åˆå§‹æ–‡æ¡£åŠ è½½...")
            self._load_documents()
        else:
            logger.info("è·³è¿‡åˆå§‹æ–‡æ¡£åŠ è½½")
        
        logger.info(f"âœ… TableEngineåˆå§‹åŒ–å®Œæˆï¼Œè¡¨æ ¼æ–‡æ¡£æ•°é‡: {len(self.table_docs)}")
    
    def _load_documents(self):
        """åŠ è½½è¡¨æ ¼æ–‡æ¡£ - é‡æ„ç‰ˆæœ¬ï¼Œæ”¯æŒé‡è¯•å’Œé™çº§ç­–ç•¥"""
        if self._docs_loaded:
            return
            
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                logger.info(f"ğŸ”„ ç¬¬{retry_count + 1}æ¬¡å°è¯•åŠ è½½è¡¨æ ¼æ–‡æ¡£")
                
                # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨
                if self.document_loader:
                    logger.info("ä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨åŠ è½½è¡¨æ ¼æ–‡æ¡£")
                    self.table_docs = self.document_loader.get_documents_by_type('table')
                    if self.table_docs:
                        logger.info(f"âœ… ä»ç»Ÿä¸€åŠ è½½å™¨æˆåŠŸåŠ è½½ {len(self.table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
                        self._docs_loaded = True
                        return
                    else:
                        logger.warning("ç»Ÿä¸€åŠ è½½å™¨æœªè¿”å›è¡¨æ ¼æ–‡æ¡£ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ")
                
                # å¤‡é€‰æ–¹æ¡ˆï¼šä»å‘é‡æ•°æ®åº“åŠ è½½
                if self.vector_store:
                    logger.info("ä»å‘é‡æ•°æ®åº“åŠ è½½è¡¨æ ¼æ–‡æ¡£")
                    self.table_docs = self._load_from_vector_store()
                    if self.table_docs:
                        logger.info(f"âœ… ä»å‘é‡æ•°æ®åº“æˆåŠŸåŠ è½½ {len(self.table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
                        self._docs_loaded = True
                        return
                    else:
                        logger.warning("å‘é‡æ•°æ®åº“æœªè¿”å›è¡¨æ ¼æ–‡æ¡£")
                
                # å¦‚æœä¸¤ç§æ–¹å¼éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise ValueError("æ— æ³•é€šè¿‡ä»»ä½•æ–¹å¼åŠ è½½è¡¨æ ¼æ–‡æ¡£")
                    
            except Exception as e:
                retry_count += 1
                logger.warning(f"âš ï¸ è¡¨æ ¼æ–‡æ¡£åŠ è½½å¤±è´¥ï¼Œç¬¬{retry_count}æ¬¡å°è¯•: {e}")
                logger.warning(f"é”™è¯¯ç±»å‹: {type(e)}")
                
                if retry_count >= max_retries:
                    # æœ€ç»ˆå¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶æ¸…ç©ºç¼“å­˜
                    logger.error(f"âŒ è¡¨æ ¼æ–‡æ¡£åŠ è½½æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    self.table_docs = []
                    self._docs_loaded = False
                    return
                else:
                    # ç­‰å¾…åé‡è¯•
                    import time
                    time.sleep(1)
                    logger.info(f"â³ ç­‰å¾…1ç§’åè¿›è¡Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
    
    def _load_from_vector_store(self):
        """ä»å‘é‡æ•°æ®åº“åŠ è½½è¡¨æ ¼æ–‡æ¡£"""
        try:
            if hasattr(self.vector_store, 'get_table_documents'):
                # ä½¿ç”¨ä¸“é—¨çš„è¡¨æ ¼æ–‡æ¡£è·å–æ–¹æ³•
                return self.vector_store.get_table_documents()
            elif hasattr(self.vector_store, 'docstore') and hasattr(self.vector_store.docstore, '_dict'):
                # ä»docstoreä¸­ç­›é€‰è¡¨æ ¼æ–‡æ¡£
                table_docs = []
                docstore_dict = self.vector_store.docstore._dict
                
                logger.info(f"å¼€å§‹ä»docstoreç­›é€‰è¡¨æ ¼æ–‡æ¡£ï¼Œæ€»æ–‡æ¡£æ•°: {len(docstore_dict)}")
                
                for doc_id, doc in docstore_dict.items():
                    # ä¸¥æ ¼æ£€æŸ¥æ–‡æ¡£ç±»å‹
                    if not hasattr(doc, 'metadata'):
                        logger.debug(f"è·³è¿‡æ–‡æ¡£ {doc_id}: æ²¡æœ‰metadataå±æ€§")
                        continue
                    
                    chunk_type = doc.metadata.get('chunk_type', '')
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºè¡¨æ ¼æ–‡æ¡£
                    if chunk_type == 'table':
                        # éªŒè¯æ–‡æ¡£ç»“æ„
                        if hasattr(doc, 'page_content') and hasattr(doc, 'metadata'):
                            table_docs.append(doc)
                            if len(table_docs) <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                                logger.debug(f"âœ… åŠ è½½è¡¨æ ¼æ–‡æ¡£: {doc_id}, chunk_type: {chunk_type}")
                                logger.debug(f"  å†…å®¹é•¿åº¦: {len(doc.page_content)}")
                                logger.debug(f"  å…ƒæ•°æ®: {doc.metadata}")
                        else:
                            logger.warning(f"è·³è¿‡æ–‡æ¡£ {doc_id}: ç¼ºå°‘å¿…è¦å±æ€§ (page_content: {hasattr(doc, 'page_content')}, metadata: {hasattr(doc, 'metadata')})")
                    else:
                        logger.debug(f"è·³è¿‡æ–‡æ¡£ {doc_id}: chunk_type={chunk_type} (ä¸æ˜¯è¡¨æ ¼)")
                
                logger.info(f"ä»docstoreç­›é€‰å‡º {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¡¨æ ¼æ–‡æ¡£ï¼Œå°è¯•å…¶ä»–ç±»å‹
                if not table_docs:
                    logger.warning("æœªæ‰¾åˆ°chunk_type='table'çš„æ–‡æ¡£ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«è¡¨æ ¼å†…å®¹çš„æ–‡æ¡£...")
                    for doc_id, doc in docstore_dict.items():
                        if hasattr(doc, 'metadata') and hasattr(doc, 'page_content'):
                            content = doc.page_content.lower()
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«è¡¨æ ¼ç‰¹å¾
                            if any(keyword in content for keyword in ['è¡¨æ ¼', 'è¡¨', 'è¡Œ', 'åˆ—', 'æ•°æ®', 'ç»Ÿè®¡']):
                                table_docs.append(doc)
                                logger.debug(f"âœ… é€šè¿‡å†…å®¹è¯†åˆ«è¡¨æ ¼æ–‡æ¡£: {doc_id}")
                
                logger.info(f"æœ€ç»ˆåŠ è½½ {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
                return table_docs
            else:
                logger.warning("å‘é‡æ•°æ®åº“ä¸æ”¯æŒè¡¨æ ¼æ–‡æ¡£è·å–")
                return []
                
        except Exception as e:
            logger.error(f"ä»å‘é‡æ•°æ®åº“åŠ è½½è¡¨æ ¼æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _ensure_docs_loaded(self):
        """ç¡®ä¿æ–‡æ¡£å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if not self._docs_loaded:
            if self.document_loader:
                logger.info("å»¶è¿ŸåŠ è½½ï¼šä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨")
                self._load_from_document_loader()
            else:
                logger.info("å»¶è¿ŸåŠ è½½ï¼šä½¿ç”¨å‘é‡æ•°æ®åº“")
                self.table_docs = self._load_from_vector_store()
                self._docs_loaded = True
            
            # éªŒè¯åŠ è½½çš„æ–‡æ¡£
            self._validate_loaded_documents()
    
    def _validate_loaded_documents(self):
        """éªŒè¯å·²åŠ è½½çš„æ–‡æ¡£"""
        try:
            logger.info(f"å¼€å§‹éªŒè¯å·²åŠ è½½çš„æ–‡æ¡£ï¼Œæ€»æ•°: {len(self.table_docs)}")
            
            if not self.table_docs:
                logger.warning("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è¡¨æ ¼æ–‡æ¡£")
                return
            
            valid_docs = []
            invalid_docs = []
            
            for i, doc in enumerate(self.table_docs):
                # æ£€æŸ¥æ–‡æ¡£ç»“æ„
                if not hasattr(doc, 'metadata'):
                    logger.warning(f"æ–‡æ¡£ {i}: ç¼ºå°‘metadataå±æ€§")
                    invalid_docs.append(i)
                    continue
                
                if not hasattr(doc, 'page_content'):
                    logger.warning(f"æ–‡æ¡£ {i}: ç¼ºå°‘page_contentå±æ€§")
                    invalid_docs.append(i)
                    continue
                
                # æ£€æŸ¥å…ƒæ•°æ®å®Œæ•´æ€§
                metadata = doc.metadata
                if not isinstance(metadata, dict):
                    logger.warning(f"æ–‡æ¡£ {i}: metadataä¸æ˜¯å­—å…¸ç±»å‹ï¼Œå®é™…ç±»å‹: {type(metadata)}")
                    invalid_docs.append(i)
                    continue
                
                # æ£€æŸ¥å†…å®¹
                content = doc.page_content
                if not isinstance(content, str):
                    logger.warning(f"æ–‡æ¡£ {i}: page_contentä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå®é™…ç±»å‹: {type(content)}")
                    invalid_docs.append(i)
                    continue
                
                if len(content.strip()) == 0:
                    logger.warning(f"æ–‡æ¡£ {i}: page_contentä¸ºç©º")
                    invalid_docs.append(i)
                    continue
                
                valid_docs.append(doc)
                logger.debug(f"âœ… æ–‡æ¡£ {i} éªŒè¯é€šè¿‡")
            
            # æ›´æ–°æ–‡æ¡£åˆ—è¡¨
            if invalid_docs:
                logger.warning(f"å‘ç° {len(invalid_docs)} ä¸ªæ— æ•ˆæ–‡æ¡£ï¼Œæ­£åœ¨ç§»é™¤...")
                self.table_docs = valid_docs
                logger.info(f"ç§»é™¤æ— æ•ˆæ–‡æ¡£åï¼Œå‰©ä½™ {len(self.table_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
            else:
                logger.info("æ‰€æœ‰æ–‡æ¡£éªŒè¯é€šè¿‡")
            
            logger.info(f"æ–‡æ¡£éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆæ–‡æ¡£: {len(valid_docs)}, æ— æ•ˆæ–‡æ¡£: {len(invalid_docs)}")
                
        except Exception as e:
            logger.error(f"æ–‡æ¡£éªŒè¯å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
    
    def _load_from_document_loader(self):
        """ä»ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨è·å–è¡¨æ ¼æ–‡æ¡£"""
        if self.document_loader:
            try:
                self.table_docs = self.document_loader.get_documents_by_type('table')
                self._docs_loaded = True
                logger.info(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–è¡¨æ ¼æ–‡æ¡£: {len(self.table_docs)} ä¸ª")
            except Exception as e:
                logger.error(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–è¡¨æ ¼æ–‡æ¡£å¤±è´¥: {e}")
                # é™çº§åˆ°å‘é‡æ•°æ®åº“åŠ è½½æ–¹å¼
                self.table_docs = self._load_from_vector_store()
                self._docs_loaded = True
        else:
            logger.warning("æ–‡æ¡£åŠ è½½å™¨æœªæä¾›ï¼Œä½¿ç”¨å‘é‡æ•°æ®åº“åŠ è½½æ–¹å¼")
            self.table_docs = self._load_from_vector_store()
            self._docs_loaded = True
    
    def _validate_config(self):
        """éªŒè¯è¡¨æ ¼å¼•æ“é…ç½® - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒTableä¸“ç”¨é…ç½®éªŒè¯"""
        # é…ç½®ç±»å‹æ£€æŸ¥
        from ..config.v2_config import TableEngineConfigV2
        
        if not isinstance(self.config, TableEngineConfigV2):
            raise ValueError("é…ç½®å¿…é¡»æ˜¯TableEngineConfigV2ç±»å‹")
        
        # è·å–ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œæ”¯æŒä¸¤ç§é…ç½®ç±»å‹
        threshold = getattr(self.config, 'table_similarity_threshold', 0.7)
        if not isinstance(threshold, (int, float)) or threshold < 0 or threshold > 1:
            raise ValueError("è¡¨æ ¼ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
        
        # éªŒè¯Tableä¸“ç”¨é…ç½®å‚æ•°
        self._validate_table_specific_config()
        
        # éªŒè¯äº”å±‚å¬å›ç­–ç•¥é…ç½®
        self._validate_recall_strategy_config()
        
        # éªŒè¯é‡æ’åºé…ç½®
        self._validate_reranking_config()
        
        logger.info("âœ… è¡¨æ ¼å¼•æ“é…ç½®éªŒè¯å®Œæˆ")
    
    def _validate_table_specific_config(self):
        """éªŒè¯Tableä¸“ç”¨é…ç½®å‚æ•°"""
        try:
            # éªŒè¯è¡¨æ ¼å¤„ç†ç›¸å…³é…ç½®
            table_configs = [
                ('max_table_rows', int, 'è¡¨æ ¼æœ€å¤§è¡Œæ•°'),
                ('header_weight', float, 'è¡¨å¤´æƒé‡'),
                ('content_weight', float, 'å†…å®¹æƒé‡'),
                ('structure_weight', float, 'ç»“æ„æƒé‡'),
                ('enable_structure_search', bool, 'å¯ç”¨ç»“æ„æœç´¢'),
                ('enable_content_search', bool, 'å¯ç”¨å†…å®¹æœç´¢')
            ]
            
            for config_name, expected_type, description in table_configs:
                if hasattr(self.config, config_name):
                    value = getattr(self.config, config_name)
                    if not isinstance(value, expected_type):
                        logger.warning(f"âš ï¸ {description}é…ç½®ç±»å‹é”™è¯¯: æœŸæœ›{expected_type.__name__}, å®é™…{type(value).__name__}")
                    else:
                        logger.debug(f"âœ… {description}é…ç½®éªŒè¯é€šè¿‡: {value}")
                else:
                    logger.debug(f"â„¹ï¸ {description}é…ç½®æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            
            # éªŒè¯æƒé‡é…ç½®çš„åˆç†æ€§
            if hasattr(self.config, 'header_weight') and hasattr(self.config, 'content_weight') and hasattr(self.config, 'structure_weight'):
                total_weight = self.config.header_weight + self.config.content_weight + self.config.structure_weight
                if abs(total_weight - 1.0) > 0.01:
                    logger.warning(f"âš ï¸ æƒé‡é…ç½®æ€»å’Œä¸ä¸º1.0: {total_weight}")
            
        except Exception as e:
            logger.error(f"éªŒè¯Tableä¸“ç”¨é…ç½®å¤±è´¥: {e}")
    
    def _validate_recall_strategy_config(self):
        """éªŒè¯å…­å±‚å¬å›ç­–ç•¥é…ç½®"""
        try:
            if not hasattr(self.config, 'recall_strategy'):
                logger.warning("âš ï¸ æœªé…ç½®å¬å›ç­–ç•¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return
            
            strategy = self.config.recall_strategy
            required_layers = [
                'layer1_structure_search',    # æ–°å¢ï¼šè¡¨æ ¼ç»“æ„æœç´¢
                'layer2_vector_search',       # åŸç¬¬ä¸€å±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢
                'layer3_keyword_search',      # åŸç¬¬äºŒå±‚ï¼šè¯­ä¹‰å…³é”®è¯æœç´¢
                'layer4_hybrid_search',       # åŸç¬¬ä¸‰å±‚ï¼šæ··åˆæœç´¢ç­–ç•¥
                'layer5_fuzzy_search',        # åŸç¬¬å››å±‚ï¼šæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…
                'layer6_expansion_search'     # åŸç¬¬äº”å±‚ï¼šæ™ºèƒ½æ‰©å±•å¬å›
            ]
            
            for layer in required_layers:
                if layer not in strategy:
                    logger.warning(f"âš ï¸ ç¼ºå°‘å¬å›ç­–ç•¥é…ç½®: {layer}")
                else:
                    layer_config = strategy[layer]
                    if not isinstance(layer_config, dict):
                        logger.warning(f"âš ï¸ å¬å›ç­–ç•¥é…ç½®æ ¼å¼é”™è¯¯: {layer}")
                    else:
                        enabled = layer_config.get('enabled', True)
                        top_k = layer_config.get('top_k', 50)
                        logger.info(f"âœ… {layer}: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}, top_k: {top_k}")
            
        except Exception as e:
            logger.error(f"éªŒè¯å¬å›ç­–ç•¥é…ç½®å¤±è´¥: {e}")
    
    def _validate_reranking_config(self):
        """éªŒè¯é‡æ’åºé…ç½®"""
        try:
            if not hasattr(self.config, 'reranking'):
                logger.warning("âš ï¸ æœªé…ç½®é‡æ’åºï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return
            
            reranking = self.config.reranking
            reranking_configs = [
                ('target_count', int, 'ç›®æ ‡ç»“æœæ•°é‡'),
                ('use_llm_enhancement', bool, 'ä½¿ç”¨LLMå¢å¼º'),
                ('model_name', str, 'æ¨¡å‹åç§°'),
                ('similarity_threshold', float, 'ç›¸ä¼¼åº¦é˜ˆå€¼')
            ]
            
            for config_name, expected_type, description in reranking_configs:
                if config_name in reranking:
                    value = reranking[config_name]
                    if not isinstance(value, expected_type):
                        logger.warning(f"âš ï¸ é‡æ’åº{description}é…ç½®ç±»å‹é”™è¯¯: æœŸæœ›{expected_type.__name__}, å®é™…{type(value).__name__}")
                    else:
                        logger.debug(f"âœ… é‡æ’åº{description}é…ç½®éªŒè¯é€šè¿‡: {value}")
                else:
                    logger.debug(f"â„¹ï¸ é‡æ’åº{description}é…ç½®æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            
        except Exception as e:
            logger.error(f"éªŒè¯é‡æ’åºé…ç½®å¤±è´¥: {e}")
    
    def _initialize_table_reranking_service(self):
        """åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡"""
        try:
            if not hasattr(self.config, 'reranking'):
                logger.warning("âš ï¸ æœªé…ç½®é‡æ’åºï¼Œè·³è¿‡è¡¨æ ¼é‡æ’åºæœåŠ¡åˆå§‹åŒ–")
                return
            
            reranking_config = self.config.reranking
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨LLMå¢å¼º
            if not reranking_config.get('use_llm_enhancement', False):
                logger.info("â„¹ï¸ LLMå¢å¼ºæœªå¯ç”¨ï¼Œè·³è¿‡è¡¨æ ¼é‡æ’åºæœåŠ¡åˆå§‹åŒ–")
                return
            
            # åˆ›å»ºè¡¨æ ¼é‡æ’åºæœåŠ¡å®ä¾‹
            self.table_reranking_service = TableRerankingService(reranking_config)
            logger.info(f"âœ… è¡¨æ ¼é‡æ’åºæœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {reranking_config.get('model_name', 'unknown')}")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.table_reranking_service = None
    
    def _rerank_table_results(self, query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨è¡¨æ ¼é‡æ’åºæœåŠ¡å¯¹ç»“æœè¿›è¡Œé‡æ’åº
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param candidates: å€™é€‰ç»“æœåˆ—è¡¨
        :return: é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            if not self.table_reranking_service:
                logger.info("â„¹ï¸ è¡¨æ ¼é‡æ’åºæœåŠ¡æœªåˆå§‹åŒ–ï¼Œè·³è¿‡é‡æ’åº")
                return candidates
            
            if not candidates:
                logger.info("â„¹ï¸ å€™é€‰ç»“æœä¸ºç©ºï¼Œè·³è¿‡é‡æ’åº")
                return candidates
            
            logger.info(f"ğŸ” å¼€å§‹è¡¨æ ¼é‡æ’åºï¼Œè¾“å…¥ {len(candidates)} ä¸ªå€™é€‰ç»“æœ")
            start_time = time.time()
            
            # å‡†å¤‡é‡æ’åºæ•°æ®æ ¼å¼
            rerank_candidates = []
            for candidate in candidates:
                # ä»docå¯¹è±¡ä¸­æå–å†…å®¹
                doc = candidate.get('doc')
                if doc and hasattr(doc, 'page_content') and hasattr(doc, 'metadata'):
                    rerank_candidate = {
                        'content': getattr(doc, 'page_content', ''),
                        'metadata': getattr(doc, 'metadata', {})
                    }
                    rerank_candidates.append(rerank_candidate)
                else:
                    logger.warning(f"å€™é€‰æ–‡æ¡£ç¼ºå°‘å¿…è¦å±æ€§ï¼Œè·³è¿‡é‡æ’åº")
            
            if not rerank_candidates:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„é‡æ’åºå€™é€‰æ–‡æ¡£ï¼Œè¿”å›åŸå§‹ç»“æœ")
                return candidates
            
            # è°ƒç”¨è¡¨æ ¼é‡æ’åºæœåŠ¡
            reranked_results = self.table_reranking_service.rerank(query, rerank_candidates)
            
            # ä¿®å¤ï¼šç¡®ä¿è¿”å›ç»“æœæ ¼å¼ä¸€è‡´
            final_results = []
            for i, reranked_result in enumerate(reranked_results):
                if isinstance(reranked_result, dict):
                    # å¦‚æœé‡æ’åºç»“æœåŒ…å«åŸå§‹å€™é€‰ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
                    if 'doc' in reranked_result:
                        final_results.append({
                            'doc': reranked_result['doc'],
                            'score': reranked_result.get('score', 0.5),
                            'source': reranked_result.get('source', 'rerank'),
                            'layer': reranked_result.get('layer', 1)
                        })
                else:
                        # å¦åˆ™ï¼Œæ„é€ æ ‡å‡†æ ¼å¼
                        original_candidate = candidates[i] if i < len(candidates) else candidates[0]
                        final_results.append({
                            'doc': original_candidate['doc'],
                            'score': reranked_result.get('score', original_candidate.get('score', 0.5)),
                            'source': reranked_result.get('source', 'rerank'),
                            'layer': reranked_result.get('layer', original_candidate.get('layer', 1))
                        })
                else:
                    # éå­—å…¸ç±»å‹ï¼Œä½¿ç”¨åŸå§‹å€™é€‰ç»“æœ
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„é‡æ’åºç»“æœç±»å‹: {type(reranked_result)}")
                    if i < len(candidates):
                        final_results.append({
                            'doc': candidates[i]['doc'],
                            'score': candidates[i].get('score', 0.5),
                            'source': candidates[i].get('source', 'unknown'),
                            'layer': candidates[i].get('layer', 1)
                        })
            
            rerank_time = time.time() - start_time
            logger.info(f"âœ… è¡¨æ ¼é‡æ’åºå®Œæˆï¼Œå¤„ç† {len(candidates)} ä¸ªç»“æœï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœï¼Œè€—æ—¶: {rerank_time:.2f}ç§’")
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ è¡¨æ ¼é‡æ’åºå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            # è¿”å›åŸå§‹ç»“æœ
            return candidates
    
    def _setup_components(self):
        """è®¾ç½®å¼•æ“ç»„ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•ï¼Œä½¿ç”¨æ–°çš„æ–‡æ¡£åŠ è½½æœºåˆ¶"""
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²åŠ è½½ï¼Œå¦‚æœæ²¡æœ‰åˆ™åŠ è½½
        if not self._docs_loaded:
            try:
                logger.info("è¡¨æ ¼å¼•æ“åœ¨_setup_componentsä¸­å¼€å§‹åŠ è½½æ–‡æ¡£")
                self._ensure_docs_loaded()
                logger.info(f"âœ… è¡¨æ ¼å¼•æ“åœ¨_setup_componentsä¸­æˆåŠŸåŠ è½½ {len(self.table_docs)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.error(f"âŒ è¡¨æ ¼å¼•æ“åœ¨_setup_componentsä¸­åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise
    
    def _analyze_query_intent(self, query: str) -> Dict[str, Any]:
        """
        åˆ†ææŸ¥è¯¢æ„å›¾
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :return: æŸ¥è¯¢æ„å›¾åˆ†æç»“æœ
        """
        try:
            intent_analysis = {
                'primary_intent': 'search',
                'target_type': 'unknown',
                'target_domain': 'unknown',
                'target_purpose': 'unknown',
                'specific_keywords': [],
                'requires_full_table': False
            }
            
            query_lower = query.lower()
            # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†è¯å’Œå…³é”®è¯æå–
            query_keywords = self._extract_keywords(query, top_k=20)
            query_tokens = self._tokenize_text(query_lower)
            
            # è¯†åˆ«ä¸»è¦æ„å›¾
            detail_keywords = ['è¯¦ç»†', 'å®Œæ•´', 'å…¨éƒ¨', 'å…·ä½“', 'è¯¦æƒ…', 'æ˜ç»†']
            if any(kw in query_lower for kw in detail_keywords):
                intent_analysis['primary_intent'] = 'detail_view'
                intent_analysis['requires_full_table'] = True
            
            summary_keywords = ['æ€»ç»“', 'æ±‡æ€»', 'æ€»è®¡', 'æ¦‚è¿°', 'æ¦‚è§ˆ', 'æ€»ä½“']
            if any(kw in query_lower for kw in summary_keywords):
                intent_analysis['primary_intent'] = 'summary'
            
            comparison_keywords = ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'å·®å¼‚', 'å˜åŒ–', 'å¢é•¿', 'ä¸‹é™']
            if any(kw in query_lower for kw in comparison_keywords):
                intent_analysis['primary_intent'] = 'comparison'
            
            # è¯†åˆ«ç›®æ ‡è¡¨æ ¼ç±»å‹
            financial_keywords = ['æ”¶å…¥', 'æ”¯å‡º', 'åˆ©æ¶¦', 'æˆæœ¬', 'è´¹ç”¨', 'æ¯›åˆ©', 'å‡€åˆ©', 'èµ„äº§', 'è´Ÿå€º', 'æƒç›Š', 'ç°é‡‘æµ', 'é¢„ç®—', 'å®é™…', 'å·®å¼‚', 'é‡‘é¢', 'æ€»é¢', 'å°è®¡', 'åˆè®¡']
            if any(kw in query_keywords for kw in financial_keywords):
                intent_analysis['target_type'] = 'financial'
                intent_analysis['specific_keywords'].extend([keyword for keyword in financial_keywords if keyword in query_keywords])
            
            hr_keywords = ['å§“å', 'å‘˜å·¥', 'éƒ¨é—¨', 'èŒä½', 'è–ªèµ„', 'å·¥èµ„', 'å¥–é‡‘', 'å…¥èŒ', 'ç¦»èŒ', 'è€ƒå‹¤', 'ç»©æ•ˆ', 'å·¥å·', 'æ€§åˆ«', 'å¹´é¾„']
            if any(kw in query_keywords for kw in hr_keywords):
                intent_analysis['target_type'] = 'hr'
                intent_analysis['specific_keywords'].extend([keyword for keyword in hr_keywords if keyword in query_keywords])
            
            statistical_keywords = ['æ•°é‡', 'æ¬¡æ•°', 'é¢‘ç‡', 'æ¯”ä¾‹', 'ç™¾åˆ†æ¯”', 'å¢é•¿', 'ä¸‹é™', 'è¶‹åŠ¿', 'ç»Ÿè®¡', 'æ±‡æ€»', 'æ€»æ•°', 'å¹³å‡', 'æœ€å¤§', 'æœ€å°', 'æ ‡å‡†å·®']
            if any(kw in query_keywords for kw in statistical_keywords):
                intent_analysis['target_type'] = 'statistical'
                intent_analysis['specific_keywords'].extend([keyword for keyword in statistical_keywords if keyword in query_keywords])
            
            configuration_keywords = ['é…ç½®', 'è®¾ç½®', 'å‚æ•°', 'é€‰é¡¹', 'å€¼', 'é»˜è®¤', 'èŒƒå›´', 'é™åˆ¶', 'æ¡ä»¶', 'è§„åˆ™']
            if any(kw in query_keywords for kw in configuration_keywords):
                intent_analysis['target_type'] = 'configuration'
                intent_analysis['specific_keywords'].extend([keyword for keyword in configuration_keywords if keyword in query_keywords])
            
            inventory_keywords = ['äº§å“', 'å•†å“', 'åº“å­˜', 'æ•°é‡', 'è¿›è´§', 'å‡ºè´§', 'åº“å­˜é‡', 'åº“å­˜å€¼', 'è´§å·', 'å‹å·', 'è§„æ ¼', 'å•ä»·', 'æ€»ä»·']
            if any(kw in query_keywords for kw in inventory_keywords):
                intent_analysis['target_type'] = 'inventory'
                intent_analysis['specific_keywords'].extend([keyword for keyword in inventory_keywords if keyword in query_keywords])
            
            # è¯†åˆ«ç›®æ ‡ä¸šåŠ¡é¢†åŸŸ
            finance_keywords = ['æ”¶å…¥', 'æ”¯å‡º', 'åˆ©æ¶¦', 'æˆæœ¬', 'è´¹ç”¨', 'èµ„äº§', 'è´Ÿå€º', 'æƒç›Š', 'ç°é‡‘æµ', 'é¢„ç®—', 'å®é™…', 'å·®å¼‚', 'é‡‘é¢', 'è´¦æˆ·', 'äº¤æ˜“', 'æŠ•èµ„', 'è´·æ¬¾', 'åˆ©ç‡']
            if any(kw in query_keywords for kw in finance_keywords):
                intent_analysis['target_domain'] = 'finance'
                intent_analysis['specific_keywords'].extend([keyword for keyword in finance_keywords if keyword in query_keywords and keyword not in intent_analysis['specific_keywords']])
            
            manufacturing_keywords = ['äº§å“', 'ç”Ÿäº§', 'åˆ¶é€ ', 'å·¥å‚', 'è®¾å¤‡', 'é›¶ä»¶', 'ç»„ä»¶', 'åº“å­˜', 'äº§é‡', 'è´¨é‡', 'ç¼ºé™·', 'ç»´ä¿®', 'ç»´æŠ¤', 'å·¥è‰º', 'æµç¨‹']
            if any(kw in query_keywords for kw in manufacturing_keywords):
                intent_analysis['target_domain'] = 'manufacturing'
                intent_analysis['specific_keywords'].extend([keyword for keyword in manufacturing_keywords if keyword in query_keywords and keyword not in intent_analysis['specific_keywords']])
            
            retail_keywords = ['é”€å”®', 'é”€å”®é¢', 'å•†å“', 'å®¢æˆ·', 'è®¢å•', 'é€€è´§', 'æŠ˜æ‰£', 'ä¿ƒé”€', 'åº“å­˜', 'ä»·æ ¼', 'æ¯›åˆ©', 'å‡€åˆ©', 'æ¸ é“', 'é—¨åº—', 'ç”µå•†']
            if any(kw in query_keywords for kw in retail_keywords):
                intent_analysis['target_domain'] = 'retail'
                intent_analysis['specific_keywords'].extend([keyword for keyword in retail_keywords if keyword in query_keywords and keyword not in intent_analysis['specific_keywords']])
            
            education_keywords = ['å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è¯¾ç¨‹', 'æˆç»©', 'è€ƒè¯•', 'å­¦å¹´', 'å­¦æœŸ', 'ç­çº§', 'å­¦ç§‘', 'å­¦è´¹', 'å¥–å­¦é‡‘', 'å‡ºå‹¤', 'æ¯•ä¸š', 'å…¥å­¦']
            if any(kw in query_keywords for kw in education_keywords):
                intent_analysis['target_domain'] = 'education'
                intent_analysis['specific_keywords'].extend([keyword for keyword in education_keywords if keyword in query_keywords and keyword not in intent_analysis['specific_keywords']])
            
            medical_keywords = ['æ‚£è€…', 'åŒ»ç”Ÿ', 'åŒ»é™¢', 'è¯Šæ‰€', 'è¯Šæ–­', 'æ²»ç–—', 'è¯ç‰©', 'å¤„æ–¹', 'æ‰‹æœ¯', 'ç—…å†', 'æ£€æŸ¥', 'è´¹ç”¨', 'ä¿é™©', 'ä½é™¢', 'é—¨è¯Š']
            if any(kw in query_keywords for kw in medical_keywords):
                intent_analysis['target_domain'] = 'medical'
                intent_analysis['specific_keywords'].extend([keyword for keyword in medical_keywords if keyword in query_keywords and keyword not in intent_analysis['specific_keywords']])
            
            # è¯†åˆ«ç›®æ ‡ç”¨é€”
            reporting_keywords = ['æŠ¥å‘Š', 'æ€»ç»“', 'æ±‡æ€»', 'ç»Ÿè®¡', 'åˆ†æ', 'ç»“æœ', 'æ•°æ®', 'æŒ‡æ ‡', 'ç»©æ•ˆ', 'çŠ¶æ€', 'è¿›å±•', 'è¶‹åŠ¿']
            if any(kw in query_keywords for kw in reporting_keywords):
                intent_analysis['target_purpose'] = 'reporting'
            
            planning_keywords = ['è®¡åˆ’', 'è§„åˆ’', 'é¢„ç®—', 'ç›®æ ‡', 'é¢„æµ‹', 'å®‰æ’', 'æ—¶é—´è¡¨', 'æ—¥ç¨‹', 'æœªæ¥', 'é¢„æœŸ', 'åˆ†é…']
            if any(kw in query_keywords for kw in planning_keywords):
                intent_analysis['target_purpose'] = 'planning'
            
            monitoring_keywords = ['ç›‘æ§', 'ç›‘æµ‹', 'è·Ÿè¸ª', 'çŠ¶æ€', 'è¿›å±•', 'å®Œæˆ', 'è¾¾æˆ', 'æŒ‡æ ‡', 'KPI', 'å¼‚å¸¸', 'é¢„è­¦', 'æŠ¥è­¦']
            if any(kw in query_keywords for kw in monitoring_keywords):
                intent_analysis['target_purpose'] = 'monitoring'
            
            comparison_keywords = ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'å·®å¼‚', 'å˜åŒ–', 'å¢é•¿', 'ä¸‹é™', 'ä¹‹å‰', 'ä¹‹å', 'å»å¹´', 'ä»Šå¹´', 'ä¸Šæœˆ', 'æœ¬æœˆ', 'å­£åº¦']
            if any(kw in query_keywords for kw in comparison_keywords):
                intent_analysis['target_purpose'] = 'comparison'
            
            inventory_keywords = ['åº“å­˜', 'å­˜è´§', 'æ•°é‡', 'è¿›è´§', 'å‡ºè´§', 'ç»“ä½™', 'ç›˜ç‚¹', 'åº“å­˜é‡', 'åº“å­˜å€¼']
            if any(kw in query_keywords for kw in inventory_keywords):
                intent_analysis['target_purpose'] = 'inventory'
            
            scheduling_keywords = ['å®‰æ’', 'æ—¥ç¨‹', 'æ—¶é—´è¡¨', 'æ’ç­', 'é¢„çº¦', 'ä¼šè®®', 'æ´»åŠ¨', 'æ—¶é—´', 'æ—¥æœŸ', 'åœ°ç‚¹']
            if any(kw in query_keywords for kw in scheduling_keywords):
                intent_analysis['target_purpose'] = 'scheduling'
            
            logger.debug(f"æŸ¥è¯¢æ„å›¾åˆ†æç»“æœ: {intent_analysis}")
            return intent_analysis
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ„å›¾åˆ†æå¤±è´¥: {e}")
            return {
                'primary_intent': 'search',
                'target_type': 'unknown',
                'target_domain': 'unknown',
                'target_purpose': 'unknown',
                'specific_keywords': [],
                'requires_full_table': False
            }
    
    def process_query(self, query: str, **kwargs) -> QueryResult:
        """
        å¤„ç†è¡¨æ ¼æŸ¥è¯¢è¯·æ±‚ - ä¿®å¤ç‰ˆæœ¬ï¼Œä¸text_engine.pyä¿æŒä¸€è‡´
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param kwargs: é¢å¤–å‚æ•°ï¼ˆåŒ…æ‹¬query_typeç­‰ï¼‰
        :return: QueryResultå¯¹è±¡
        """
        if not self.is_enabled():
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.TABLE,
                results=[],
                total_count=0,
                processing_time=0.0,
                engine_name=self.name,
                metadata={},
                error_message="è¡¨æ ¼å¼•æ“æœªå¯ç”¨"
            )
        
        start_time = time.time()
        
        try:
            # ç¡®ä¿æ–‡æ¡£å·²åŠ è½½
            self._ensure_docs_loaded()
            
            # æ·»åŠ æ–‡æ¡£çŠ¶æ€è¯Šæ–­
            logger.info(f"ğŸ” è¡¨æ ¼æŸ¥è¯¢è¯Šæ–­ä¿¡æ¯:")
            logger.info(f"  - æŸ¥è¯¢æ–‡æœ¬: {query}")
            logger.info(f"  - æ–‡æ¡£åŠ è½½çŠ¶æ€: {self._docs_loaded}")
            logger.info(f"  - è¡¨æ ¼æ–‡æ¡£æ•°é‡: {len(self.table_docs)}")
            logger.info(f"  - å‘é‡æ•°æ®åº“çŠ¶æ€: {self.vector_store is not None}")
            logger.info(f"  - æ–‡æ¡£åŠ è½½å™¨çŠ¶æ€: {self.document_loader is not None}")
            
            # å¦‚æœæ–‡æ¡£æ•°é‡ä¸º0ï¼Œå°è¯•é‡æ–°åŠ è½½
            if len(self.table_docs) == 0:
                logger.warning("âš ï¸ è¡¨æ ¼æ–‡æ¡£æ•°é‡ä¸º0ï¼Œå°è¯•é‡æ–°åŠ è½½...")
                self._docs_loaded = False
                self._ensure_docs_loaded()
                logger.info(f"é‡æ–°åŠ è½½åè¡¨æ ¼æ–‡æ¡£æ•°é‡: {len(self.table_docs)}")
            
            # åˆ†ææŸ¥è¯¢æ„å›¾
            intent_analysis = self._analyze_query_intent(query)
            logger.info(f"æŸ¥è¯¢æ„å›¾åˆ†æ: {intent_analysis['primary_intent']}, ç›®æ ‡ç±»å‹: {intent_analysis['target_type']}")
            
            # æ‰§è¡Œæœç´¢
            search_results = self._search_tables(query)
            
            # æ ¹æ®æ„å›¾è°ƒæ•´ç»“æœ
            if intent_analysis['primary_intent'] == 'detail_view' and intent_analysis['requires_full_table']:
                # å¦‚æœç”¨æˆ·æ„å›¾æ˜¯æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼Œå°è¯•è·å–å®Œæ•´è¡¨æ ¼
                if search_results and len(search_results) > 0:
                    top_result = search_results[0]
                    table_id = top_result['doc'].metadata.get('table_id', 'unknown')
                    full_table_result = self.get_full_table(table_id)
                    if full_table_result['status'] == 'success':
                        search_results[0]['full_content'] = full_table_result['content']
                        search_results[0]['full_metadata'] = full_table_result['metadata']
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for result in search_results:
                # ä¿®å¤ï¼šå¤„ç†é‡æ’åºåå¯èƒ½æ²¡æœ‰'doc'é”®çš„æƒ…å†µ
                if 'doc' not in result:
                    logger.warning(f"è·³è¿‡æ— æ•ˆç»“æœï¼Œç¼ºå°‘'doc'é”®: {result}")
                    # å°è¯•ä¿®å¤ç»“æœæ ¼å¼
                    if isinstance(result, dict) and 'content' in result and 'metadata' in result:
                        # æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„docå¯¹è±¡
                        class MockDoc:
                            def __init__(self, content, metadata):
                                self.page_content = content
                                self.metadata = metadata
                        
                        mock_doc = MockDoc(result['content'], result['metadata'])
                        result['doc'] = mock_doc
                        result['score'] = result.get('score', 0.5)
                        result['source'] = result.get('source', 'unknown')
                        result['layer'] = result.get('layer', 1)
                        logger.info(f"å·²ä¿®å¤ç»“æœæ ¼å¼: {result}")
                    else:
                        continue
                
                doc = result['doc']
                metadata = getattr(doc, 'metadata', {})
                structure_analysis = result.get('structure_analysis', {})
                
                formatted_result = {
                    'id': metadata.get('table_id', 'unknown'),
                    'content': getattr(doc, 'page_content', ''),
                    'score': result['score'],
                    'source': result.get('source', 'unknown'),  # ä¿®å¤ï¼šä½¿ç”¨getæ–¹æ³•æä¾›é»˜è®¤å€¼
                    'layer': result.get('layer', 1),  # ä¿®å¤ï¼šä½¿ç”¨getæ–¹æ³•æä¾›é»˜è®¤å€¼
                    'metadata': {
                        'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                        'page_number': metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                        'table_type': structure_analysis.get('table_type', 'unknown'),
                        'business_domain': structure_analysis.get('business_domain', 'unknown'),
                        'quality_score': structure_analysis.get('quality_score', 0.0),
                        'is_truncated': structure_analysis.get('is_truncated', False),
                        'truncation_type': structure_analysis.get('truncation_type', 'none'),
                        'truncated_rows': structure_analysis.get('truncated_rows', 0),
                        'current_rows': structure_analysis.get('row_count', 0),
                        'original_rows': structure_analysis.get('original_row_count', 0)
                    }
                }
                
                # å¦‚æœæœ‰å®Œæ•´è¡¨æ ¼å†…å®¹ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
                if 'full_content' in result:
                    formatted_result['full_content'] = result['full_content']
                    formatted_result['full_metadata'] = result['full_metadata']
                
                formatted_results.append(formatted_result)
            
            processing_time = time.time() - start_time
            
            # è¿”å›QueryResultå¯¹è±¡ï¼Œä¸text_engine.pyä¿æŒä¸€è‡´
            return QueryResult(
                success=True,
                query=query,
                query_type=QueryType.TABLE,
                results=formatted_results,
                total_count=len(formatted_results),
                processing_time=processing_time,
                engine_name=self.name,
                metadata={
                    'total_tables': len(self.table_docs),
                    'pipeline': 'table_engine',
                    'intent_analysis': intent_analysis,
                    'search_strategy': 'six_layer_recall',
                    'docs_loaded': self._docs_loaded,
                    'vector_store_available': self.vector_store is not None,
                    'document_loader_available': self.document_loader is not None
                }
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"å¤„ç†è¡¨æ ¼æŸ¥è¯¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.TABLE,
                results=[],
                total_count=0,
                processing_time=processing_time,
                engine_name=self.name,
                metadata={},
                error_message=str(e)
            )
    
    def _search_tables(self, query: str) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œè¡¨æ ¼æœç´¢ - é‡æ„çš„äº”å±‚å¬å›ç­–ç•¥
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        # è·å–é…ç½®å‚æ•°
        threshold = getattr(self.config, 'table_similarity_threshold', 0.65)
        max_results = getattr(self.config, 'max_results', 10)
        max_recall_results = getattr(self.config, 'max_recall_results', 150)
        
        try:
            # ç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„æœç´¢ï¼ˆæ–°å¢ï¼‰
            if hasattr(self.config, 'recall_strategy') and self.config.recall_strategy.get('layer1_structure_search', {}).get('enabled', True):
                structure_results = self._table_structure_search(query, max_recall_results)
                results.extend(structure_results)
                logger.info(f"ç¬¬ä¸€å±‚è¡¨æ ¼ç»“æ„æœç´¢è¿”å› {len(structure_results)} ä¸ªç»“æœ")
            
            # ç¬¬äºŒå±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼ˆåŸç¬¬ä¸€å±‚ï¼‰
            if hasattr(self.config, 'recall_strategy') and self.config.recall_strategy.get('layer2_vector_search', {}).get('enabled', True):
                vector_results = self._vector_search(query, max_recall_results)
                results.extend(vector_results)
                logger.info(f"ç¬¬äºŒå±‚å‘é‡æœç´¢è¿”å› {len(vector_results)} ä¸ªç»“æœ")
            
            # ç¬¬ä¸‰å±‚ï¼šè¯­ä¹‰å…³é”®è¯æœç´¢ï¼ˆåŸç¬¬äºŒå±‚ï¼‰
            if hasattr(self.config, 'recall_strategy') and self.config.recall_strategy.get('layer3_keyword_search', {}).get('enabled', True):
                keyword_results = self._keyword_search(query, max_recall_results)
                results.extend(keyword_results)
                logger.info(f"ç¬¬ä¸‰å±‚å…³é”®è¯æœç´¢è¿”å› {len(keyword_results)} ä¸ªç»“æœ")
            
            # ç¬¬å››å±‚ï¼šæ··åˆæœç´¢ç­–ç•¥ï¼ˆåŸç¬¬ä¸‰å±‚ï¼‰
            if hasattr(self.config, 'recall_strategy') and self.config.recall_strategy.get('layer4_hybrid_search', {}).get('enabled', True):
                hybrid_results = self._hybrid_search(query, max_recall_results)
                results.extend(hybrid_results)
                logger.info(f"ç¬¬å››å±‚æ··åˆæœç´¢è¿”å› {len(hybrid_results)} ä¸ªç»“æœ")
            
            # ç¬¬äº”å±‚ï¼šæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…ï¼ˆåŸç¬¬å››å±‚ï¼‰
            if hasattr(self.config, 'recall_strategy') and self.config.recall_strategy.get('layer5_fuzzy_search', {}).get('enabled', True):
                fuzzy_results = self._fuzzy_search(query, max_recall_results)
                results.extend(fuzzy_results)
                logger.info(f"ç¬¬äº”å±‚æ¨¡ç³Šæœç´¢è¿”å› {len(fuzzy_results)} ä¸ªç»“æœ")
            
            # ç¬¬å…­å±‚ï¼šæŸ¥è¯¢æ‰©å±•å¬å›ï¼ˆåŸç¬¬äº”å±‚ï¼‰
            if hasattr(self.config, 'recall_strategy') and self.config.recall_strategy.get('layer6_expansion_search', {}).get('enabled', True):
                expansion_results = self._expansion_search(query, max_recall_results)
                results.extend(expansion_results)
                logger.info(f"ç¬¬å…­å±‚æ‰©å±•æœç´¢è¿”å› {len(expansion_results)} ä¸ªç»“æœ")
            
            # å¦‚æœæ²¡æœ‰ç»“æœï¼Œé™ä½é˜ˆå€¼é‡è¯•
            if not results and threshold > 0.3:
                logger.info(f"æœªæ‰¾åˆ°ç»“æœï¼Œé™ä½é˜ˆå€¼ä» {threshold} åˆ° 0.3")
                return self._search_tables_with_lower_threshold(query, 0.3)
            
            # å»é‡å’Œæ’åº
            results = self._deduplicate_and_sort_results(results)
            
            # åº”ç”¨è¡¨æ ¼é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(self.config, 'enable_enhanced_reranking') and self.config.enable_enhanced_reranking:
                if self.table_reranking_service:
                    logger.info("ğŸ” å¯ç”¨è¡¨æ ¼é‡æ’åºæœåŠ¡")
                    results = self._rerank_table_results(query, results)
                else:
                    logger.info("â„¹ï¸ è¡¨æ ¼é‡æ’åºæœåŠ¡æœªåˆå§‹åŒ–ï¼Œè·³è¿‡é‡æ’åº")
            else:
                logger.info("â„¹ï¸ æœªå¯ç”¨å¢å¼ºé‡æ’åºï¼Œè·³è¿‡é‡æ’åº")
            
            # é™åˆ¶ç»“æœæ•°é‡
            return results[:max_results]
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼æœç´¢å¤±è´¥: {e}")
            return []
    
    def _table_structure_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„æœç´¢ï¼ˆæ–°å¢ï¼‰
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.table_docs:
            return []
        
        try:
            # è·å–é…ç½®å‚æ•°
            layer_config = self.config.recall_strategy.get('layer1_structure_search', {})
            top_k = layer_config.get('top_k', 50)
            structure_threshold = layer_config.get('structure_threshold', 0.1)
            
            results = []
            query_lower = query.lower()
            
            for doc in self.table_docs:
                # ä¸¥æ ¼æ£€æŸ¥æ–‡æ¡£ç±»å‹
                if not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: ç¼ºå°‘å¿…è¦å±æ€§")
                    continue
                
                if not isinstance(doc.metadata, dict):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: metadataä¸æ˜¯å­—å…¸ç±»å‹")
                    continue
                
                if not isinstance(doc.page_content, str):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: page_contentä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
                    continue
                
                score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                try:
                    # åˆ†æè¡¨æ ¼ç»“æ„
                    structure_analysis = self._analyze_table_structure(doc)
                    
                    # è¡¨æ ¼ç±»å‹åŒ¹é…
                    if structure_analysis['table_type'] != 'unknown':
                        table_type_lower = structure_analysis['table_type'].lower()
                        if query_lower in table_type_lower:
                            score += 0.4
                        elif any(word in table_type_lower for word in query_lower.split()):
                            score += 0.4
                    
                    # ä¸šåŠ¡é¢†åŸŸåŒ¹é…
                    if structure_analysis['business_domain'] != 'unknown':
                        domain_lower = structure_analysis['business_domain'].lower()
                        if query_lower in domain_lower:
                            score += 0.5
                        elif any(word in domain_lower for word in query_lower.split()):
                            score += 0.5
                    
                    # ä¸»è¦ç”¨é€”åŒ¹é…
                    if structure_analysis['primary_purpose'] != 'unknown':
                        purpose_lower = structure_analysis['primary_purpose'].lower()
                        if query_lower in purpose_lower:
                            score += 0.4
                        elif any(word in purpose_lower for word in query_lower.split()):
                            score += 0.4
                    
                    # åˆ—åç²¾ç¡®åŒ¹é…
                    columns = structure_analysis['columns']
                    if isinstance(columns, list):
                        for col in columns:
                            if isinstance(col, str):
                                col_lower = col.lower()
                                if query_lower in col_lower:
                                    score += 0.8  # åˆ—ååŒ¹é…æƒé‡æœ€é«˜
                                elif any(word in col_lower for word in query_lower.split()):
                                    score += 0.5
                    
                    # è¡¨æ ¼è´¨é‡åŠ åˆ†
                    quality_score = structure_analysis['quality_score']
                    score += quality_score * 0.3  # æé«˜è´¨é‡åˆ†æ•°æƒé‡  # è´¨é‡åˆ†æ•°ä½œä¸ºé¢å¤–åŠ åˆ†
                    
                    # æˆªæ–­æƒ©ç½š
                    if structure_analysis['is_truncated']:
                        score -= 0.1  # æˆªæ–­çš„è¡¨æ ¼ç•¥å¾®é™ä½åˆ†æ•°
                        logger.debug(f"è¡¨æ ¼ {doc.metadata.get('table_id', 'unknown')} å› æˆªæ–­è¢«æ‰£åˆ†: -0.1")
                        
                except Exception as e:
                    logger.debug(f"è®¡ç®—è¡¨æ ¼ç»“æ„æœç´¢åˆ†æ•°å¤±è´¥: {e}")
                    score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                if score >= structure_threshold:
                    results.append({
                        'doc': doc,
                        'score': score,
                        'source': 'structure_search',
                        'layer': 1,
                        'structure_analysis': structure_analysis
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            logger.info(f"è¡¨æ ¼ç»“æ„æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç¬¦åˆé˜ˆå€¼çš„ç»“æœ")
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼ç»“æ„æœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _vector_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬äºŒå±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.vector_store:
            logger.warning("âš ï¸ å‘é‡æ•°æ®åº“æœªè¿æ¥ï¼Œè·³è¿‡å‘é‡æœç´¢")
            return []
        
        try:
            # è·å–é…ç½®å‚æ•° - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„é…ç½®é”®å
            layer_config = self.config.recall_strategy.get('layer2_vector_search', {})
            top_k = layer_config.get('top_k', 50)
            similarity_threshold = layer_config.get('similarity_threshold', 0.65)
            
            logger.info(f"ğŸ” ç¬¬äºŒå±‚å‘é‡æœç´¢é…ç½®: top_k={top_k}, similarity_threshold={similarity_threshold}")
            logger.info(f"ğŸ” å‘é‡æ•°æ®åº“çŠ¶æ€: {self.vector_store is not None}")
            
            # æ‰§è¡Œå‘é‡æœç´¢
            logger.info(f"ğŸ” å¼€å§‹æ‰§è¡Œå‘é‡æœç´¢ï¼ŒæŸ¥è¯¢: {query}")
            
            # ä¿®å¤ï¼šæ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦æ”¯æŒç›¸ä¼¼åº¦æœç´¢
            if not hasattr(self.vector_store, 'similarity_search'):
                logger.warning("âš ï¸ å‘é‡æ•°æ®åº“ä¸æ”¯æŒsimilarity_searchæ–¹æ³•")
                return []
            
            vector_results = self.vector_store.similarity_search(
                query, 
                k=top_k,
                filter={'chunk_type': 'table'}  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
            )
            
            logger.info(f"ğŸ” å‘é‡æœç´¢åŸå§‹ç»“æœæ•°é‡: {len(vector_results)}")
            
            results = []
            logger.info(f"ğŸ” å¼€å§‹å¤„ç†å‘é‡æœç´¢ç»“æœï¼Œç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
            
            for i, doc in enumerate(vector_results):
                # ä¿®å¤ï¼šå¤„ç†å¯èƒ½æ²¡æœ‰scoreå±æ€§çš„æƒ…å†µ
                if hasattr(doc, 'score'):
                    score = doc.score
                elif hasattr(doc, 'metadata') and 'score' in doc.metadata:
                    score = doc.metadata['score']
                else:
                    # å¦‚æœæ²¡æœ‰åˆ†æ•°ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°
                    score = 0.5
                    logger.debug(f"æ–‡æ¡£ {i+1} æ²¡æœ‰åˆ†æ•°ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°: {score}")
                
                logger.info(f"ğŸ” æ–‡æ¡£ {i+1}: score={score}, é˜ˆå€¼={similarity_threshold}, é€šè¿‡={score >= similarity_threshold}")
                
                if score >= similarity_threshold:
                    results.append({
                        'doc': doc,
                        'score': score,
                        'source': 'vector_search',
                        'layer': 2  # ä¿®å¤ï¼šç¬¬äºŒå±‚å‘é‡æœç´¢
                    })
            
            logger.info(f"ğŸ” å‘é‡æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç¬¦åˆé˜ˆå€¼çš„ç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _keyword_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸‰å±‚ï¼šå…³é”®è¯æœç´¢
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.table_docs:
            return []
        
        try:
            # è·å–é…ç½®å‚æ•°
            layer_config = self.config.recall_strategy.get('layer3_keyword_search', {})
            top_k = layer_config.get('top_k', 50)
            keyword_threshold = layer_config.get('keyword_threshold', 0.3)
            
            results = []
            # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†è¯å’Œå…³é”®è¯æå–
            query_keywords = self._extract_keywords(query, top_k=20)
            query_tokens = self._tokenize_text(query.lower())
            
            for doc in self.table_docs:
                # ä¸¥æ ¼æ£€æŸ¥æ–‡æ¡£ç±»å‹
                if not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: ç¼ºå°‘å¿…è¦å±æ€§")
                    continue
                
                if not isinstance(doc.metadata, dict):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: metadataä¸æ˜¯å­—å…¸ç±»å‹")
                    continue
                
                if not isinstance(doc.page_content, str):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: page_contentä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
                    continue
                
                score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                try:
                    content = doc.page_content.lower()
                    metadata = doc.metadata
                    
                    # å†…å®¹å…³é”®è¯åŒ¹é…
                    content_keywords = self._extract_keywords(content, top_k=20)
                    content_tokens = self._tokenize_text(content)
                    
                    # å…³é”®è¯åŒ¹é…ï¼ˆæƒé‡è¾ƒé«˜ï¼‰
                    common_keywords = set(query_keywords) & set(content_keywords)
                    if common_keywords:
                        score += len(common_keywords) * 0.4
                    
                    # åˆ†è¯åŒ¹é…
                    common_tokens = set(query_tokens) & set(content_tokens)
                    if common_tokens:
                        score += len(common_tokens) * 0.2
                    
                    # åˆ—åå…³é”®è¯åŒ¹é…
                    columns = metadata.get('columns', [])
                    if isinstance(columns, list):
                        for col in columns:
                            if isinstance(col, str):
                                col_lower = col.lower()
                                col_keywords = self._extract_keywords(col_lower, top_k=10)
                                col_tokens = self._tokenize_text(col_lower)
                                
                                # åˆ—åå…³é”®è¯åŒ¹é…ï¼ˆæƒé‡æœ€é«˜ï¼‰
                                if any(kw in col_keywords for kw in query_keywords):
                                    score += 0.4
                                # åˆ—ååˆ†è¯åŒ¹é…
                                elif any(token in col_tokens for token in query_tokens):
                                    score += 0.4
                                    
                except Exception as e:
                    logger.debug(f"è®¡ç®—å…³é”®è¯æœç´¢åˆ†æ•°å¤±è´¥: {e}")
                    score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                if score >= keyword_threshold:
                    results.append({
                        'doc': doc,
                        'score': score,
                        'source': 'keyword_search',
                        'layer': 3
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            logger.info(f"å…³é”®è¯æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç¬¦åˆé˜ˆå€¼çš„ç»“æœ")
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _hybrid_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸‰å±‚ï¼šæ··åˆæœç´¢ç­–ç•¥
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.table_docs:
            return []
        
        try:
            # è·å–é…ç½®å‚æ•°
            layer_config = self.config.recall_strategy.get('layer4_hybrid_search', {})
            top_k = layer_config.get('top_k', 50)
            vector_weight = layer_config.get('vector_weight', 0.7)
            keyword_weight = layer_config.get('keyword_weight', 0.3)
            
            results = []
            query_lower = query.lower()
            
            for doc in self.table_docs:
                # ä¸¥æ ¼æ£€æŸ¥æ–‡æ¡£ç±»å‹
                if not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: ç¼ºå°‘å¿…è¦å±æ€§")
                    continue
                
                if not isinstance(doc.metadata, dict):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: metadataä¸æ˜¯å­—å…¸ç±»å‹")
                    continue
                
                if not isinstance(doc.page_content, str):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: page_contentä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
                    continue
                
                # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
                vector_score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                try:
                    content = doc.page_content.lower()
                    query_words = query_lower.split()
                    matched_words = sum(1 for word in query_words if word in content)
                    if matched_words > 0:
                        vector_score = min(1.0, matched_words / len(query_words))
                except Exception as e:
                    logger.debug(f"è®¡ç®—å‘é‡åˆ†æ•°å¤±è´¥: {e}")
                    vector_score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
                keyword_score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                try:
                    title = doc.metadata.get('title', '').lower()
                    if query_lower in title:
                        keyword_score += 0.4
                    
                    columns = doc.metadata.get('columns', [])
                    if isinstance(columns, list):
                        for col in columns:
                            if isinstance(col, str) and query_lower in col.lower():
                                keyword_score += 0.4
                except Exception as e:
                    logger.debug(f"è®¡ç®—å…³é”®è¯åˆ†æ•°å¤±è´¥: {e}")
                    keyword_score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                # æ··åˆåˆ†æ•°è®¡ç®—
                hybrid_score = (vector_score * vector_weight) + (keyword_score * keyword_weight)
                
                if hybrid_score > 0:
                    results.append({
                        'doc': doc,
                        'score': hybrid_score,
                        'source': 'hybrid_search',
                        'layer': 4,
                        'vector_score': vector_score,
                        'keyword_score': keyword_score
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            logger.info(f"æ··åˆæœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"æ··åˆæœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _fuzzy_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬äº”å±‚ï¼šæ¨¡ç³Šæœç´¢
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.table_docs:
            return []
        
        try:
            # è·å–é…ç½®å‚æ•°
            layer_config = self.config.recall_strategy.get('layer5_fuzzy_search', {})
            top_k = layer_config.get('top_k', 25)
            fuzzy_threshold = layer_config.get('fuzzy_threshold', 0.2)
            
            results = []
            query_keywords = self._extract_keywords(query, top_k=20)
            query_tokens = self._tokenize_text(query.lower())
            
            for doc in self.table_docs:
                # ä¸¥æ ¼æ£€æŸ¥æ–‡æ¡£ç±»å‹
                if not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: ç¼ºå°‘å¿…è¦å±æ€§")
                    continue
                
                if not isinstance(doc.metadata, dict):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: metadataä¸æ˜¯å­—å…¸ç±»å‹")
                    continue
                
                if not isinstance(doc.page_content, str):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: page_contentä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
                    continue
                
                score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                try:
                    content = doc.page_content.lower()
                    metadata = doc.metadata
                    
                    # å†…å®¹æ¨¡ç³ŠåŒ¹é…
                    content_keywords = self._extract_keywords(content, top_k=20)
                    content_tokens = self._tokenize_text(content)
                    
                    # å…³é”®è¯æ¨¡ç³ŠåŒ¹é…
                    for q_kw in query_keywords:
                        for c_kw in content_keywords:
                            if q_kw in c_kw or c_kw in q_kw:
                                score += 0.15
                    
                    # åˆ†è¯æ¨¡ç³ŠåŒ¹é…
                    for q_token in query_tokens:
                        for c_token in content_tokens:
                            if q_token in c_token or c_token in q_token:
                                score += 0.08
                    
                    # åˆ—åæ¨¡ç³ŠåŒ¹é…
                    columns = metadata.get('columns', [])
                    if isinstance(columns, list):
                        for col in columns:
                            if isinstance(col, str):
                                col_lower = col.lower()
                                col_keywords = self._extract_keywords(col_lower, top_k=10)
                                col_tokens = self._tokenize_text(col_lower)
                                
                                # åˆ—åå…³é”®è¯æ¨¡ç³ŠåŒ¹é…ï¼ˆæƒé‡è¾ƒé«˜ï¼‰
                                for q_kw in query_keywords:
                                    for c_kw in col_keywords:
                                        if q_kw in c_kw or c_kw in q_kw:
                                            score += 0.25
                                
                                # åˆ—ååˆ†è¯æ¨¡ç³ŠåŒ¹é…
                                for q_token in query_tokens:
                                    for c_token in col_tokens:
                                        if q_token in c_token or c_token in q_token:
                                            score += 0.15
                                            
                except Exception as e:
                    logger.debug(f"è®¡ç®—æ¨¡ç³Šæœç´¢åˆ†æ•°å¤±è´¥: {e}")
                    score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                if score >= fuzzy_threshold:
                    results.append({
                        'doc': doc,
                        'score': score,
                        'source': 'fuzzy_search',
                        'layer': 5
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            logger.info(f"æ¨¡ç³Šæœç´¢æ‰¾åˆ° {len(results)} ä¸ªç¬¦åˆé˜ˆå€¼çš„ç»“æœ")
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"æ¨¡ç³Šæœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _expansion_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        ç¬¬å…­å±‚ï¼šæŸ¥è¯¢æ‰©å±•å¬å›
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.table_docs:
            return []
        
        try:
            # è·å–é…ç½®å‚æ•°
            layer_config = self.config.recall_strategy.get('layer6_expansion_search', {})
            top_k = layer_config.get('top_k', 25)
            
            results = []
            query_lower = query.lower()
            
            # ç®€å•çš„æŸ¥è¯¢æ‰©å±•ç­–ç•¥
            expanded_terms = self._expand_query_terms(query_lower)
            
            for doc in self.table_docs:
                # ä¸¥æ ¼æ£€æŸ¥æ–‡æ¡£ç±»å‹
                if not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: ç¼ºå°‘å¿…è¦å±æ€§")
                    continue
                
                if not isinstance(doc.metadata, dict):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: metadataä¸æ˜¯å­—å…¸ç±»å‹")
                    continue
                
                if not isinstance(doc.page_content, str):
                    logger.debug(f"è·³è¿‡æ–‡æ¡£: page_contentä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
                    continue
                
                score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                try:
                    # åŸºäºæ‰©å±•æœ¯è¯­çš„åŒ¹é…
                    title = doc.metadata.get('title', '').lower()
                    content = doc.page_content.lower()
                    columns = doc.metadata.get('columns', [])
                    table_type = doc.metadata.get('table_type', '').lower()
                    
                    for term in expanded_terms:
                        # æ ‡é¢˜åŒ¹é…
                        if term in title:
                            score += 0.4
                        
                        # åˆ—ååŒ¹é…
                        if isinstance(columns, list):
                            for col in columns:
                                if isinstance(col, str) and term in col.lower():
                                    score += 0.3
                        
                        # å†…å®¹åŒ¹é…
                        if term in content:
                            score += 0.2
                        
                        # è¡¨æ ¼ç±»å‹åŒ¹é…
                        if term in table_type:
                            score += 0.3
                            
                except Exception as e:
                    logger.debug(f"è®¡ç®—æ‰©å±•æœç´¢åˆ†æ•°å¤±è´¥: {e}")
                    score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
                
                if score > 0:
                    results.append({
                        'doc': doc,
                        'score': score,
                        'source': 'expansion_search',
                        'layer': 6,
                        'expanded_terms': expanded_terms
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            logger.info(f"æ‰©å±•æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœï¼Œæ‰©å±•æœ¯è¯­: {expanded_terms}")
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"æ‰©å±•æœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _expand_query_terms(self, query: str) -> List[str]:
        """
        æ‰©å±•æŸ¥è¯¢æœ¯è¯­
        
        :param query: åŸå§‹æŸ¥è¯¢
        :return: æ‰©å±•åçš„æœ¯è¯­åˆ—è¡¨
        """
        # ç®€å•çš„åŒä¹‰è¯æ‰©å±•
        synonyms = {
            'è´¢åŠ¡': ['è´¢åŠ¡', 'ä¼šè®¡', 'èµ„é‡‘', 'é¢„ç®—', 'æˆæœ¬'],
            'æ•°æ®': ['æ•°æ®', 'ç»Ÿè®¡', 'æ•°å­—', 'æŒ‡æ ‡', 'æŠ¥è¡¨'],
            'è¡¨æ ¼': ['è¡¨æ ¼', 'è¡¨', 'æ¸…å•', 'ç›®å½•', 'ç´¢å¼•'],
            'æŠ¥å‘Š': ['æŠ¥å‘Š', 'æŠ¥è¡¨', 'æ€»ç»“', 'åˆ†æ', 'è¯„ä¼°'],
            'æ”¶å…¥': ['æ”¶å…¥', 'è¥æ”¶', 'é”€å”®é¢', 'è¥ä¸šé¢', 'æ”¶ç›Š'],
            'æ”¯å‡º': ['æ”¯å‡º', 'è´¹ç”¨', 'æˆæœ¬', 'å¼€é”€', 'èŠ±è´¹'],
            'åˆ©æ¶¦': ['åˆ©æ¶¦', 'ç›ˆåˆ©', 'æ”¶ç›Š', 'å‡€åˆ©', 'æ¯›åˆ©']
        }
        
        expanded_terms = [query]
        
        # æŸ¥æ‰¾åŒä¹‰è¯
        for key, values in synonyms.items():
            if key in query:
                expanded_terms.extend(values)
        
        # å»é‡å¹¶è¿”å›
        return list(set(expanded_terms))
    
    def _search_tables_with_lower_threshold(self, query: str, threshold: float) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨è¾ƒä½é˜ˆå€¼é‡æ–°æœç´¢
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        # ä¸´æ—¶é™ä½é˜ˆå€¼
        original_threshold = getattr(self.config, 'table_similarity_threshold', 0.65)
        setattr(self.config, 'table_similarity_threshold', threshold)
        
        try:
            results = self._search_tables(query)
            return results
        finally:
            # æ¢å¤åŸå§‹é˜ˆå€¼
            setattr(self.config, 'table_similarity_threshold', original_threshold)
    
    def _deduplicate_and_sort_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é‡å’Œæ’åºç»“æœ - æ”¯æŒäº”å±‚å¬å›ç­–ç•¥
        
        :param results: åŸå§‹ç»“æœåˆ—è¡¨
        :return: å»é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        if not results:
            return []
        
        try:
            # å»é‡ï¼ˆåŸºäºæ–‡æ¡£IDï¼‰
            seen_docs = set()
            unique_results = []
            
            for result in results:
                doc_id = result['doc'].metadata.get('id', id(result['doc']))
                if doc_id not in seen_docs:
                    seen_docs.add(doc_id)
                    unique_results.append(result)
                else:
                    # å¦‚æœæ–‡æ¡£å·²å­˜åœ¨ï¼Œé€‰æ‹©åˆ†æ•°æ›´é«˜çš„ç»“æœ
                    existing_result = next(r for r in unique_results if r['doc'].metadata.get('id', id(r['doc'])) == doc_id)
                    if result['score'] > existing_result['score']:
                        # æ›¿æ¢ä¸ºåˆ†æ•°æ›´é«˜çš„ç»“æœ
                        unique_results.remove(existing_result)
                        unique_results.append(result)
            
            # å±‚é—´æƒé‡è°ƒæ•´
            for result in unique_results:
                layer = result.get('layer', 1)
                # æ ¹æ®å±‚çº§è°ƒæ•´åˆ†æ•°ï¼Œå±‚çº§è¶Šä½åˆ†æ•°è¶Šé«˜
                layer_weight = 1.0 / layer
                result['adjusted_score'] = result['score'] * layer_weight
            
            # æŒ‰è°ƒæ•´åçš„åˆ†æ•°æ’åº
            unique_results.sort(key=lambda x: x.get('adjusted_score', x['score']), reverse=True)
            
            logger.info(f"å»é‡åå‰©ä½™ {len(unique_results)} ä¸ªå”¯ä¸€ç»“æœ")
            return unique_results
            
        except Exception as e:
            logger.error(f"å»é‡å’Œæ’åºå¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šç®€å•æ’åº
            return sorted(results, key=lambda x: x['score'], reverse=True)
    
    def _validate_recall_strategy(self):
        """éªŒè¯å…­å±‚å¬å›ç­–ç•¥é…ç½®"""
        try:
            if not hasattr(self.config, 'recall_strategy'):
                logger.warning("âš ï¸ æœªé…ç½®å¬å›ç­–ç•¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return
            
            strategy = self.config.recall_strategy
            required_layers = [
                'layer1_structure_search',    # æ–°å¢ï¼šè¡¨æ ¼ç»“æ„æœç´¢
                'layer2_vector_search',       # åŸç¬¬ä¸€å±‚ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢
                'layer3_keyword_search',      # åŸç¬¬äºŒå±‚ï¼šè¯­ä¹‰å…³é”®è¯æœç´¢
                'layer4_hybrid_search',       # åŸç¬¬ä¸‰å±‚ï¼šæ··åˆæœç´¢ç­–ç•¥
                'layer5_fuzzy_search',        # åŸç¬¬å››å±‚ï¼šæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…
                'layer6_expansion_search'     # åŸç¬¬äº”å±‚ï¼šæ™ºèƒ½æ‰©å±•å¬å›
            ]
            
            for layer in required_layers:
                if layer not in strategy:
                    logger.warning(f"âš ï¸ ç¼ºå°‘å¬å›ç­–ç•¥é…ç½®: {layer}")
                else:
                    layer_config = strategy[layer]
                    if not isinstance(layer_config, dict):
                        logger.warning(f"âš ï¸ å¬å›ç­–ç•¥é…ç½®æ ¼å¼é”™è¯¯: {layer}")
                    else:
                        enabled = layer_config.get('enabled', True)
                        top_k = layer_config.get('top_k', 50)
                        logger.info(f"âœ… {layer}: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}, top_k: {top_k}")
            
        except Exception as e:
            logger.error(f"éªŒè¯å¬å›ç­–ç•¥é…ç½®å¤±è´¥: {e}")
    
    def _initialize_recall_strategy(self):
        """åˆå§‹åŒ–å…­å±‚å¬å›ç­–ç•¥"""
        try:
            self._validate_recall_strategy()
            
            # æ£€æŸ¥å¿…è¦çš„é…ç½®é¡¹
            if not hasattr(self.config, 'use_new_pipeline'):
                logger.warning("æœªé…ç½®use_new_pipelineï¼Œé»˜è®¤å¯ç”¨")
            
            if not hasattr(self.config, 'enable_enhanced_reranking'):
                logger.warning("æœªé…ç½®enable_enhanced_rerankingï¼Œé»˜è®¤å¯ç”¨")
            
            logger.info("å…­å±‚å¬å›ç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¬å›ç­–ç•¥å¤±è´¥: {e}")
    
    def clear_cache(self):
        """æ¸…ç†è¡¨æ ¼å¼•æ“ç¼“å­˜"""
        self.table_docs = []
        self._docs_loaded = False
        logger.info("è¡¨æ ¼å¼•æ“ç¼“å­˜å·²æ¸…ç†")

    def _analyze_table_structure(self, doc):
        """
        åˆ†æè¡¨æ ¼ç»“æ„ï¼Œæå–æ·±å±‚ç‰¹å¾
        
        :param doc: è¡¨æ ¼æ–‡æ¡£
        :return: è¡¨æ ¼ç»“æ„åˆ†æç»“æœ
        """
        try:
            analysis = {
                'table_type': 'unknown',
                'columns': [],
                'row_count': 0,
                'column_count': 0,
                'data_completeness': 0.0,
                'quality_score': 0.0,
                'business_domain': 'unknown',
                'primary_purpose': 'unknown',
                'is_truncated': False,
                'truncation_type': 'none',
                'truncated_rows': 0,
                'original_row_count': 0
            }
            
            # ä»å…ƒæ•°æ®ä¸­æå–åŸºæœ¬ä¿¡æ¯
            metadata = getattr(doc, 'metadata', {})
            if metadata:
                analysis['columns'] = metadata.get('columns', [])
                analysis['row_count'] = metadata.get('table_row_count', 0)
                analysis['column_count'] = metadata.get('table_column_count', 0)
                analysis['table_type'] = metadata.get('table_type', 'unknown')
                analysis['original_row_count'] = metadata.get('original_row_count', analysis['row_count'])
            
            # åˆ†æè¡¨æ ¼å†…å®¹
            content = getattr(doc, 'page_content', '')
            if content:
                # è®¡ç®—æ•°æ®å®Œæ•´æ€§
                analysis['data_completeness'] = self._calculate_data_completeness(content)
                
                # æ£€æµ‹æˆªæ–­çŠ¶æ€
                truncation_info = self._detect_truncation(content, analysis['row_count'], analysis['original_row_count'])
                analysis['is_truncated'] = truncation_info['is_truncated']
                analysis['truncation_type'] = truncation_info['truncation_type']
                analysis['truncated_rows'] = truncation_info['truncated_rows']
                
                # è¯†åˆ«è¡¨æ ¼ç±»å‹
                analysis['table_type'] = self._identify_table_type(content, analysis['columns'])
                
                # è¯†åˆ«ä¸šåŠ¡é¢†åŸŸ
                analysis['business_domain'] = self._identify_business_domain(content, analysis['columns'])
                
                # è¯†åˆ«ä¸»è¦ç”¨é€”
                analysis['primary_purpose'] = self._identify_primary_purpose(content, analysis['columns'])
            
            # è®¡ç®—è´¨é‡è¯„åˆ†
            analysis['quality_score'] = self._calculate_quality_score(analysis)
            
            logger.debug(f"è¡¨æ ¼ç»“æ„åˆ†æå®Œæˆ: {analysis}")
            return analysis
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {
                'table_type': 'unknown',
                'columns': [],
                'row_count': 0,
                'column_count': 0,
                'data_completeness': 0.0,
                'quality_score': 0.0,
                'business_domain': 'unknown',
                'primary_purpose': 'unknown',
                'is_truncated': False,
                'truncation_type': 'none',
                'truncated_rows': 0,
                'original_row_count': 0
            }
    
    def _detect_truncation(self, content, current_rows, original_rows):
        """
        æ£€æµ‹è¡¨æ ¼æ˜¯å¦è¢«æˆªæ–­ä»¥åŠæˆªæ–­ç±»å‹
        
        :param content: è¡¨æ ¼å†…å®¹
        :param current_rows: å½“å‰è¡Œæ•°
        :param original_rows: åŸå§‹è¡Œæ•°
        :return: æˆªæ–­ä¿¡æ¯å­—å…¸
        """
        try:
            truncation_info = {
                'is_truncated': False,
                'truncation_type': 'none',
                'truncated_rows': 0
            }
            
            # æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«æˆªæ–­æ ‡è®°
            content_lower = content.lower()
            if '[è¡¨æ ¼æ•°æ®è¡Œå·²æˆªæ–­å¤„ç†]' in content_lower:
                truncation_info['is_truncated'] = True
                truncation_info['truncation_type'] = 'row_truncation'
            elif '[è¡¨æ ¼å†…å®¹å·²æˆªæ–­å¤„ç†]' in content_lower:
                truncation_info['is_truncated'] = True
                truncation_info['truncation_type'] = 'content_truncation'
            elif '[è¡¨æ ¼æ ¼å¼å·²ä¼˜åŒ–]' in content_lower:
                truncation_info['is_truncated'] = True
                truncation_info['truncation_type'] = 'format_optimization'
            
            # æ£€æŸ¥è¡Œæ•°å·®å¼‚
            if original_rows > current_rows:
                truncation_info['is_truncated'] = True
                truncation_info['truncated_rows'] = original_rows - current_rows
                if truncation_info['truncation_type'] == 'none':
                    truncation_info['truncation_type'] = 'row_truncation'
            
            return truncation_info
            
        except Exception as e:
            logger.error(f"æ£€æµ‹æˆªæ–­çŠ¶æ€å¤±è´¥: {e}")
            return {
                'is_truncated': False,
                'truncation_type': 'none',
                'truncated_rows': 0
            }
    
    def _calculate_data_completeness(self, content):
        """è®¡ç®—æ•°æ®å®Œæ•´æ€§"""
        try:
            if not content:
                return 0.0
            
            # ç®€å•çš„å®Œæ•´æ€§è®¡ç®—ï¼šåŸºäºéç©ºè¡Œå’Œæœ‰æ•ˆæ•°æ®
            lines = content.split('\n')
            if not lines:
                return 0.0
            
            valid_lines = 0
            total_lines = len(lines)
            
            for line in lines:
                line = line.strip()
                if line and not line.startswith('[') and len(line) > 5:  # æ’é™¤æˆªæ–­æ ‡è®°å’Œç©ºè¡Œ
                    valid_lines += 1
            
            return valid_lines / total_lines if total_lines > 0 else 0.0
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ•°æ®å®Œæ•´æ€§å¤±è´¥: {e}")
            return 0.0
    
    def _identify_table_type(self, content: str, columns: List[str]) -> str:
        """
        è¯†åˆ«è¡¨æ ¼ç±»å‹
        
        :param content: è¡¨æ ¼å†…å®¹
        :param columns: è¡¨æ ¼åˆ—ååˆ—è¡¨
        :return: è¡¨æ ¼ç±»å‹
        """
        try:
            content_lower = content.lower()
            columns_lower = [col.lower() for col in columns]
            
            # è´¢åŠ¡è¡¨æ ¼
            financial_keywords = ['æ”¶å…¥', 'æ”¯å‡º', 'åˆ©æ¶¦', 'æˆæœ¬', 'è´¹ç”¨', 'æ¯›åˆ©', 'å‡€åˆ©', 'èµ„äº§', 'è´Ÿå€º', 'æƒç›Š', 'ç°é‡‘æµ', 'é¢„ç®—', 'å®é™…', 'å·®å¼‚', 'é‡‘é¢', 'æ€»é¢', 'å°è®¡', 'åˆè®¡']
            if any(kw in content_lower for kw in financial_keywords) or any(any(kw in col for kw in financial_keywords) for col in columns_lower):
                return 'financial'
            
            # äººäº‹è¡¨æ ¼
            hr_keywords = ['å§“å', 'å‘˜å·¥', 'éƒ¨é—¨', 'èŒä½', 'è–ªèµ„', 'å·¥èµ„', 'å¥–é‡‘', 'å…¥èŒ', 'ç¦»èŒ', 'è€ƒå‹¤', 'ç»©æ•ˆ', 'å·¥å·', 'æ€§åˆ«', 'å¹´é¾„']
            if any(kw in content_lower for kw in hr_keywords) or any(any(kw in col for kw in hr_keywords) for col in columns_lower):
                return 'hr'
            
            # ç»Ÿè®¡è¡¨æ ¼
            statistical_keywords = ['æ•°é‡', 'æ¬¡æ•°', 'é¢‘ç‡', 'æ¯”ä¾‹', 'ç™¾åˆ†æ¯”', 'å¢é•¿', 'ä¸‹é™', 'è¶‹åŠ¿', 'ç»Ÿè®¡', 'æ±‡æ€»', 'æ€»æ•°', 'å¹³å‡', 'æœ€å¤§', 'æœ€å°', 'æ ‡å‡†å·®']
            if any(kw in content_lower for kw in statistical_keywords) or any(any(kw in col for kw in statistical_keywords) for col in columns_lower):
                return 'statistical'
            
            # é…ç½®è¡¨æ ¼
            configuration_keywords = ['é…ç½®', 'è®¾ç½®', 'å‚æ•°', 'é€‰é¡¹', 'å€¼', 'é»˜è®¤', 'èŒƒå›´', 'é™åˆ¶', 'æ¡ä»¶', 'è§„åˆ™']
            if any(kw in content_lower for kw in configuration_keywords) or any(any(kw in col for kw in configuration_keywords) for col in columns_lower):
                return 'configuration'
            
            # åº“å­˜è¡¨æ ¼
            inventory_keywords = ['äº§å“', 'å•†å“', 'åº“å­˜', 'æ•°é‡', 'è¿›è´§', 'å‡ºè´§', 'åº“å­˜é‡', 'åº“å­˜å€¼', 'è´§å·', 'å‹å·', 'è§„æ ¼', 'å•ä»·', 'æ€»ä»·']
            if any(kw in content_lower for kw in inventory_keywords) or any(any(kw in col for kw in inventory_keywords) for col in columns_lower):
                return 'inventory'
            
            return 'general'  # é»˜è®¤ç±»å‹
            
        except Exception as e:
            logger.error(f"è¯†åˆ«è¡¨æ ¼ç±»å‹å¤±è´¥: {e}")
            return 'unknown'
    
    def _identify_business_domain(self, content: str, columns: List[str]) -> str:
        """
        è¯†åˆ«è¡¨æ ¼æ‰€å±ä¸šåŠ¡é¢†åŸŸ
        
        :param content: è¡¨æ ¼å†…å®¹
        :param columns: è¡¨æ ¼åˆ—ååˆ—è¡¨
        :return: ä¸šåŠ¡é¢†åŸŸ
        """
        try:
            content_lower = content.lower()
            columns_lower = [col.lower() for col in columns]
            
            # é‡‘èé¢†åŸŸ
            finance_keywords = ['æ”¶å…¥', 'æ”¯å‡º', 'åˆ©æ¶¦', 'æˆæœ¬', 'è´¹ç”¨', 'èµ„äº§', 'è´Ÿå€º', 'æƒç›Š', 'ç°é‡‘æµ', 'é¢„ç®—', 'å®é™…', 'å·®å¼‚', 'é‡‘é¢', 'è´¦æˆ·', 'äº¤æ˜“', 'æŠ•èµ„', 'è´·æ¬¾', 'åˆ©ç‡']
            if any(kw in content_lower for kw in finance_keywords) or any(any(kw in col for kw in finance_keywords) for col in columns_lower):
                return 'finance'
            
            # åˆ¶é€ ä¸š
            manufacturing_keywords = ['äº§å“', 'ç”Ÿäº§', 'åˆ¶é€ ', 'å·¥å‚', 'è®¾å¤‡', 'é›¶ä»¶', 'ç»„ä»¶', 'åº“å­˜', 'äº§é‡', 'è´¨é‡', 'ç¼ºé™·', 'ç»´ä¿®', 'ç»´æŠ¤', 'å·¥è‰º', 'æµç¨‹']
            if any(kw in content_lower for kw in manufacturing_keywords) or any(any(kw in col for kw in manufacturing_keywords) for col in columns_lower):
                return 'manufacturing'
            
            # é›¶å”®ä¸š
            retail_keywords = ['é”€å”®', 'é”€å”®é¢', 'å•†å“', 'å®¢æˆ·', 'è®¢å•', 'é€€è´§', 'æŠ˜æ‰£', 'ä¿ƒé”€', 'åº“å­˜', 'ä»·æ ¼', 'æ¯›åˆ©', 'å‡€åˆ©', 'æ¸ é“', 'é—¨åº—', 'ç”µå•†']
            if any(kw in content_lower for kw in retail_keywords) or any(any(kw in col for kw in retail_keywords) for col in columns_lower):
                return 'retail'
            
            # æ•™è‚²é¢†åŸŸ
            education_keywords = ['å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è¯¾ç¨‹', 'æˆç»©', 'è€ƒè¯•', 'å­¦å¹´', 'å­¦æœŸ', 'ç­çº§', 'å­¦ç§‘', 'å­¦è´¹', 'å¥–å­¦é‡‘', 'å‡ºå‹¤', 'æ¯•ä¸š', 'å…¥å­¦']
            if any(kw in content_lower for kw in education_keywords) or any(any(kw in col for kw in education_keywords) for col in columns_lower):
                return 'education'
            
            # åŒ»ç–—é¢†åŸŸ
            medical_keywords = ['æ‚£è€…', 'åŒ»ç”Ÿ', 'åŒ»é™¢', 'è¯Šæ‰€', 'è¯Šæ–­', 'æ²»ç–—', 'è¯ç‰©', 'å¤„æ–¹', 'æ‰‹æœ¯', 'ç—…å†', 'æ£€æŸ¥', 'è´¹ç”¨', 'ä¿é™©', 'ä½é™¢', 'é—¨è¯Š']
            if any(kw in content_lower for kw in medical_keywords) or any(any(kw in col for kw in medical_keywords) for col in columns_lower):
                return 'medical'
            
            return 'general'  # é»˜è®¤é¢†åŸŸ
            
        except Exception as e:
            logger.error(f"è¯†åˆ«ä¸šåŠ¡é¢†åŸŸå¤±è´¥: {e}")
            return 'unknown'
    
    def _identify_primary_purpose(self, content: str, columns: List[str]) -> str:
        """
        è¯†åˆ«è¡¨æ ¼ä¸»è¦ç”¨é€”
        
        :param content: è¡¨æ ¼å†…å®¹
        :param columns: è¡¨æ ¼åˆ—ååˆ—è¡¨
        :return: ä¸»è¦ç”¨é€”
        """
        try:
            content_lower = content.lower()
            columns_lower = [col.lower() for col in columns]
            
            # æŠ¥å‘Šç”¨é€”
            reporting_keywords = ['æŠ¥å‘Š', 'æ€»ç»“', 'æ±‡æ€»', 'ç»Ÿè®¡', 'åˆ†æ', 'ç»“æœ', 'æ•°æ®', 'æŒ‡æ ‡', 'ç»©æ•ˆ', 'çŠ¶æ€', 'è¿›å±•', 'è¶‹åŠ¿']
            if any(kw in content_lower for kw in reporting_keywords) or any(any(kw in col for kw in reporting_keywords) for col in columns_lower):
                return 'reporting'
            
            # è®¡åˆ’ç”¨é€”
            planning_keywords = ['è®¡åˆ’', 'è§„åˆ’', 'é¢„ç®—', 'ç›®æ ‡', 'é¢„æµ‹', 'å®‰æ’', 'æ—¶é—´è¡¨', 'æ—¥ç¨‹', 'æœªæ¥', 'é¢„æœŸ', 'åˆ†é…']
            if any(kw in content_lower for kw in planning_keywords) or any(any(kw in col for kw in planning_keywords) for col in columns_lower):
                return 'planning'
            
            # ç›‘æ§ç”¨é€”
            monitoring_keywords = ['ç›‘æ§', 'ç›‘æµ‹', 'è·Ÿè¸ª', 'çŠ¶æ€', 'è¿›å±•', 'å®Œæˆ', 'è¾¾æˆ', 'æŒ‡æ ‡', 'KPI', 'å¼‚å¸¸', 'é¢„è­¦', 'æŠ¥è­¦']
            if any(kw in content_lower for kw in monitoring_keywords) or any(any(kw in col for kw in monitoring_keywords) for col in columns_lower):
                return 'monitoring'
            
            # å¯¹æ¯”ç”¨é€”
            comparison_keywords = ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'å·®å¼‚', 'å˜åŒ–', 'å¢é•¿', 'ä¸‹é™', 'ä¹‹å‰', 'ä¹‹å', 'å»å¹´', 'ä»Šå¹´', 'ä¸Šæœˆ', 'æœ¬æœˆ', 'å­£åº¦']
            if any(kw in content_lower for kw in comparison_keywords) or any(any(kw in col for kw in comparison_keywords) for col in columns_lower):
                return 'comparison'
            
            # åº“å­˜ç”¨é€”
            inventory_keywords = ['åº“å­˜', 'å­˜è´§', 'æ•°é‡', 'è¿›è´§', 'å‡ºè´§', 'ç»“ä½™', 'ç›˜ç‚¹', 'åº“å­˜é‡', 'åº“å­˜å€¼']
            if any(kw in content_lower for kw in inventory_keywords) or any(any(kw in col for kw in inventory_keywords) for col in columns_lower):
                return 'inventory'
            
            # å®‰æ’ç”¨é€”
            scheduling_keywords = ['å®‰æ’', 'æ—¥ç¨‹', 'æ—¶é—´è¡¨', 'æ’ç­', 'é¢„çº¦', 'ä¼šè®®', 'æ´»åŠ¨', 'æ—¶é—´', 'æ—¥æœŸ', 'åœ°ç‚¹']
            if any(kw in content_lower for kw in scheduling_keywords) or any(any(kw in col for kw in scheduling_keywords) for col in columns_lower):
                return 'scheduling'
            
            return 'general'  # é»˜è®¤ç”¨é€”
            
        except Exception as e:
            logger.error(f"è¯†åˆ«ä¸»è¦ç”¨é€”å¤±è´¥: {e}")
            return 'unknown'
    
    def _calculate_quality_score(self, analysis):
        """è®¡ç®—è¡¨æ ¼è´¨é‡è¯„åˆ†"""
        try:
            score = 0.3  # åŸºç¡€åˆ†æ•°ï¼Œæé«˜å¬å›ç‡
            
            # åŸºç¡€åˆ†æ•°ï¼šæ•°æ®å®Œæ•´æ€§ (40%)
            score += analysis['data_completeness'] * 0.4
            
            # ç»“æ„åˆ†æ•°ï¼šåˆ—æ•°å’Œè¡Œæ•°åˆç†æ€§ (30%)
            if analysis['column_count'] > 0 and analysis['row_count'] > 0:
                # åˆ—æ•°åˆç†æ€§ï¼š2-20åˆ—ä¸ºä½³
                if 2 <= analysis['column_count'] <= 20:
                    score += 0.3
                elif analysis['column_count'] > 20:
                    score += 0.15  # åˆ—æ•°è¿‡å¤šï¼Œå‡åˆ†
                else:
                    score += 0.1   # åˆ—æ•°è¿‡å°‘ï¼Œå‡åˆ†
            
            # ç±»å‹è¯†åˆ«åˆ†æ•°ï¼šèƒ½è¯†åˆ«å‡ºå…·ä½“ç±»å‹ (20%)
            if analysis['table_type'] not in ['unknown', 'general']:
                score += 0.2
            
            # ä¸šåŠ¡é¢†åŸŸè¯†åˆ«åˆ†æ•°ï¼šèƒ½è¯†åˆ«å‡ºå…·ä½“é¢†åŸŸ (10%)
            if analysis['business_domain'] not in ['unknown', 'general']:
                score += 0.1
            
            # æˆªæ–­æƒ©ç½šï¼šå¦‚æœè¡¨æ ¼è¢«æˆªæ–­ï¼Œè´¨é‡åˆ†æ•°é™ä½ (10%)
            if analysis['is_truncated']:
                score -= 0.1
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"è®¡ç®—è´¨é‡è¯„åˆ†å¤±è´¥: {e}")
            return 0.0

    def get_full_table(self, table_id: str) -> Dict[str, Any]:
        """
        è·å–å®Œæ•´è¡¨æ ¼å†…å®¹ï¼ˆæœªæˆªæ–­ç‰ˆæœ¬ï¼‰
        
        :param table_id: è¡¨æ ¼ID
        :return: å®Œæ•´è¡¨æ ¼å†…å®¹å­—å…¸
        """
        try:
            # é¦–å…ˆå°è¯•ä»æ–‡æ¡£åŠ è½½å™¨è·å–å®Œæ•´è¡¨æ ¼
            if self.document_loader:
                full_doc = self.document_loader.get_full_document_by_id(table_id, chunk_type='table')
                if full_doc:
                    logger.info(f"ä»æ–‡æ¡£åŠ è½½å™¨è·å–å®Œæ•´è¡¨æ ¼ {table_id} æˆåŠŸ")
                    return {
                        'status': 'success',
                        'table_id': table_id,
                        'content': getattr(full_doc, 'page_content', ''),
                        'metadata': getattr(full_doc, 'metadata', {}),
                        'message': 'å®Œæ•´è¡¨æ ¼å†…å®¹ä»æ–‡æ¡£åŠ è½½å™¨è·å–æˆåŠŸ'
                    }
            
            # å¦‚æœæ²¡æœ‰æ–‡æ¡£åŠ è½½å™¨æˆ–è·å–å¤±è´¥ï¼Œå°è¯•ä»å‘é‡å­˜å‚¨è·å–
            if self.vector_store:
                full_doc = self.vector_store.get_full_document_by_id(table_id)
                if full_doc:
                    logger.info(f"ä»å‘é‡å­˜å‚¨è·å–å®Œæ•´è¡¨æ ¼ {table_id} æˆåŠŸ")
                    return {
                        'status': 'success',
                        'table_id': table_id,
                        'content': getattr(full_doc, 'page_content', ''),
                        'metadata': getattr(full_doc, 'metadata', {}),
                        'message': 'å®Œæ•´è¡¨æ ¼å†…å®¹ä»å‘é‡å­˜å‚¨è·å–æˆåŠŸ'
                    }
            
            # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            logger.warning(f"æ— æ³•è·å–å®Œæ•´è¡¨æ ¼ {table_id}")
            return {
                'status': 'error',
                'table_id': table_id,
                'content': '',
                'metadata': {},
                'message': f'æ— æ³•è·å–å®Œæ•´è¡¨æ ¼ {table_id}ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰å®Œæ•´æ•°æ®æˆ–æ²¡æœ‰é…ç½®æ–‡æ¡£åŠ è½½å™¨/å‘é‡å­˜å‚¨'
            }
            
        except Exception as e:
            logger.error(f"è·å–å®Œæ•´è¡¨æ ¼ {table_id} å¤±è´¥: {e}")
            return {
                'status': 'error',
                'table_id': table_id,
                'content': '',
                'metadata': {},
                'message': f'è·å–å®Œæ•´è¡¨æ ¼å¤±è´¥: {str(e)}'
            }

    def _extract_keywords(self, text: str, top_k: int = 10) -> List[str]:
        """
        æå–æ–‡æœ¬å…³é”®è¯
        
        :param text: è¾“å…¥æ–‡æœ¬
        :param top_k: è¿”å›å…³é”®è¯æ•°é‡
        :return: å…³é”®è¯åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨jieba.analyseæå–å…³é”®è¯
            keywords = jieba.analyse.extract_tags(text, topK=top_k, withWeight=False)
            
            # è¿‡æ»¤åœç”¨è¯
            filtered_keywords = [kw for kw in keywords if kw not in stop_words and len(kw) > 1]
            
            return filtered_keywords[:top_k]
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    def _tokenize_text(self, text: str) -> List[str]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        
        :param text: è¾“å…¥æ–‡æœ¬
        :return: åˆ†è¯ç»“æœåˆ—è¡¨
        """
        try:
            # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
            tokens = list(jieba.cut(text))
            
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]
            
            return filtered_tokens
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†è¯å¤±è´¥: {e}")
            return []
