#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨‹åºè¯´æ˜ï¼š

## 1. è¡¨æ ¼å¼•æ“æ ¸å¿ƒå®ç°
## 2. æ”¯æŒè¡¨æ ¼æŸ¥è¯¢çš„ä¸“ç”¨å¼•æ“
## 3. é›†æˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢
## 4. æ”¯æŒè¡¨æ ¼ç»“æ„è¯†åˆ«å’ŒæŸ¥è¯¢ä¼˜åŒ–
"""

import logging
import time
from typing import List, Dict, Any, Optional
from ..core.base_engine import BaseEngine
from ..core.base_engine import EngineConfig
from ..core.base_engine import QueryResult, QueryType
try:
    from .reranking_services import TableRerankingService
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from v2.core.reranking_services import TableRerankingService
    except ImportError:
        TableRerankingService = None
import jieba
import jieba.analyse
import re

# åˆå§‹åŒ–jiebaåˆ†è¯å™¨
jieba.initialize()

# æ·»åŠ è‡ªå®šä¹‰è¯å…¸
custom_words = [
    'è´¢åŠ¡æŠ¥è¡¨', 'æ”¶å…¥æƒ…å†µ', 'å‘˜å·¥è–ªèµ„', 'éƒ¨é—¨åˆ†å¸ƒ', 'äº§å“åº“å­˜', 'æ•°é‡ç»Ÿè®¡',
    'è¯¦ç»†æ˜ç»†', 'æ±‡æ€»ç»Ÿè®¡', 'å¯¹æ¯”åˆ†æ', 'å¢é•¿è¶‹åŠ¿', 'æˆæœ¬æ§åˆ¶', 'åˆ©æ¶¦åˆ†æ',
    'åº“å­˜ç®¡ç†', 'é”€å”®ä¸šç»©', 'è´¢åŠ¡æŒ‡æ ‡', 'ä¸šåŠ¡æ•°æ®', 'è¿è¥æŠ¥è¡¨', 'ç»©æ•ˆè¯„ä¼°'
]

for word in custom_words:
    jieba.add_word(word)

# å®šä¹‰åœç”¨è¯
stop_words = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
}

logger = logging.getLogger(__name__)


class TableEngine(BaseEngine):
    """
    è¡¨æ ¼å¼•æ“
    
    ä¸“é—¨å¤„ç†è¡¨æ ¼æŸ¥è¯¢ï¼Œæ”¯æŒå¤šç§æœç´¢ç­–ç•¥
    """
    
    def __init__(self, config, vector_store=None, document_loader=None, skip_initial_load=False, 
                 llm_engine=None, source_filter_engine=None):
        """
        åˆå§‹åŒ–è¡¨æ ¼å¼•æ“ - é‡æ„ç‰ˆæœ¬ï¼Œæ”¯æŒæ›´å¥½çš„é…ç½®éªŒè¯å’Œæ–‡æ¡£åŠ è½½
        
        :param config: è¡¨æ ¼å¼•æ“é…ç½®
        :param vector_store: å‘é‡æ•°æ®åº“
        :param document_loader: æ–‡æ¡£åŠ è½½å™¨
        :param skip_initial_load: æ˜¯å¦è·³è¿‡åˆå§‹æ–‡æ¡£åŠ è½½
        :param llm_engine: LLMå¼•æ“ï¼ˆç”¨äºæ–°Pipelineï¼‰
        :param source_filter_engine: æºè¿‡æ»¤å¼•æ“ï¼ˆç”¨äºæ–°Pipelineï¼‰
        """
        super().__init__(config)
        
        logger.info("ğŸ” å¼€å§‹åˆå§‹åŒ–TableEngine")
        logger.info(f"é…ç½®ç±»å‹: {type(config)}")
        logger.info(f"å‘é‡æ•°æ®åº“: {vector_store}")
        logger.info(f"æ–‡æ¡£åŠ è½½å™¨: {document_loader}")
        logger.info(f"è·³è¿‡åˆå§‹åŠ è½½: {skip_initial_load}")
        logger.info(f"LLMå¼•æ“: {llm_engine}")
        logger.info(f"æºè¿‡æ»¤å¼•æ“: {source_filter_engine}")
        
        self.vector_store = vector_store
        self.document_loader = document_loader
        self.table_docs = []  # è¡¨æ ¼æ–‡æ¡£ç¼“å­˜
        self._docs_loaded = False
        
        # æ–°Pipelineç›¸å…³å¼•æ“
        self.llm_engine = llm_engine
        self.source_filter_engine = source_filter_engine
        
        # åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡
        self.table_reranking_service = None
        
        logger.info("âœ… åŸºç¡€å±æ€§è®¾ç½®å®Œæˆ")
        
        # éªŒè¯é…ç½®
        logger.info("å¼€å§‹éªŒè¯é…ç½®...")
        self._validate_config()
        logger.info("âœ… é…ç½®éªŒè¯å®Œæˆ")
        
        # åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡
        logger.info("å¼€å§‹åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡...")
        self._initialize_table_reranking_service()
        logger.info("âœ… è¡¨æ ¼é‡æ’åºæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–äº”å±‚å¬å›ç­–ç•¥
        logger.info("å¼€å§‹åˆå§‹åŒ–äº”å±‚å¬å›ç­–ç•¥...")
        self._initialize_recall_strategy()
        logger.info("âœ… äº”å±‚å¬å›ç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åŠ è½½æ–‡æ¡£
        if not skip_initial_load:
            logger.info("å¼€å§‹åˆå§‹æ–‡æ¡£åŠ è½½...")
            self._load_documents()
        else:
            logger.info("è·³è¿‡åˆå§‹æ–‡æ¡£åŠ è½½")
        
        logger.info(f"âœ… TableEngineåˆå§‹åŒ–å®Œæˆï¼Œè¡¨æ ¼æ–‡æ¡£æ•°é‡: {len(self.table_docs)}")
    
    def _load_documents(self):
        """åŠ è½½è¡¨æ ¼æ–‡æ¡£ - é‡æ„ç‰ˆæœ¬ï¼Œæ”¯æŒé‡è¯•å’Œé™çº§ç­–ç•¥"""
        if self._docs_loaded:
            return
            
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                logger.info(f"ğŸ”„ ç¬¬{retry_count + 1}æ¬¡å°è¯•åŠ è½½è¡¨æ ¼æ–‡æ¡£")
                
                # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨
                if self.document_loader:
                    logger.info("ä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨åŠ è½½è¡¨æ ¼æ–‡æ¡£")
                    self.table_docs = self.document_loader.get_documents_by_type('table')
                    if self.table_docs:
                        logger.info(f"âœ… ä»ç»Ÿä¸€åŠ è½½å™¨æˆåŠŸåŠ è½½ {len(self.table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
                        self._docs_loaded = True
                        return
                    else:
                        logger.warning("ç»Ÿä¸€åŠ è½½å™¨æœªè¿”å›è¡¨æ ¼æ–‡æ¡£ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ")
                
                # å¤‡é€‰æ–¹æ¡ˆï¼šä»å‘é‡æ•°æ®åº“åŠ è½½
                if self.vector_store:
                    logger.info("ä»å‘é‡æ•°æ®åº“åŠ è½½è¡¨æ ¼æ–‡æ¡£")
                    self.table_docs = self._load_from_vector_store()
                    if self.table_docs:
                        logger.info(f"âœ… ä»å‘é‡æ•°æ®åº“æˆåŠŸåŠ è½½ {len(self.table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
                        self._docs_loaded = True
                        return
                    else:
                        logger.warning("å‘é‡æ•°æ®åº“æœªè¿”å›è¡¨æ ¼æ–‡æ¡£")
                
                # å¦‚æœä¸¤ç§æ–¹å¼éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise ValueError("æ— æ³•é€šè¿‡ä»»ä½•æ–¹å¼åŠ è½½è¡¨æ ¼æ–‡æ¡£")
                    
            except Exception as e:
                retry_count += 1
                logger.warning(f"âš ï¸ è¡¨æ ¼æ–‡æ¡£åŠ è½½å¤±è´¥ï¼Œç¬¬{retry_count}æ¬¡å°è¯•: {e}")
                logger.warning(f"é”™è¯¯ç±»å‹: {type(e)}")
                
                if retry_count >= max_retries:
                    # æœ€ç»ˆå¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶æ¸…ç©ºç¼“å­˜
                    logger.error(f"âŒ è¡¨æ ¼æ–‡æ¡£åŠ è½½æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    self.table_docs = []
                    self._docs_loaded = False
                    return
                else:
                    # ç­‰å¾…åé‡è¯•
                    import time
                    time.sleep(1)
                    logger.info(f"â³ ç­‰å¾…1ç§’åè¿›è¡Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
    
    def _load_from_vector_store(self):
        """ä»å‘é‡æ•°æ®åº“åŠ è½½è¡¨æ ¼æ–‡æ¡£"""
        try:
            if hasattr(self.vector_store, 'get_table_documents'):
                # ä½¿ç”¨ä¸“é—¨çš„è¡¨æ ¼æ–‡æ¡£è·å–æ–¹æ³•
                return self.vector_store.get_table_documents()
            elif hasattr(self.vector_store, 'docstore') and hasattr(self.vector_store.docstore, '_dict'):
                # ä»docstoreä¸­ç­›é€‰è¡¨æ ¼æ–‡æ¡£
                table_docs = []
                docstore_dict = self.vector_store.docstore._dict
                
                logger.info(f"å¼€å§‹ä»docstoreç­›é€‰è¡¨æ ¼æ–‡æ¡£ï¼Œæ€»æ–‡æ¡£æ•°: {len(docstore_dict)}")
                
                for doc_id, doc in docstore_dict.items():
                    # ä¸¥æ ¼æ£€æŸ¥æ–‡æ¡£ç±»å‹
                    if not hasattr(doc, 'metadata'):
                        logger.debug(f"è·³è¿‡æ–‡æ¡£ {doc_id}: æ²¡æœ‰metadataå±æ€§")
                        continue
                    
                    chunk_type = doc.metadata.get('chunk_type', '')
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºè¡¨æ ¼æ–‡æ¡£
                    if chunk_type == 'table':
                        # éªŒè¯æ–‡æ¡£ç»“æ„
                        if hasattr(doc, 'page_content') and hasattr(doc, 'metadata'):
                            table_docs.append(doc)
                            if len(table_docs) <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                                logger.debug(f"âœ… åŠ è½½è¡¨æ ¼æ–‡æ¡£: {doc_id}, chunk_type: {chunk_type}")
                                logger.debug(f"  å†…å®¹é•¿åº¦: {len(doc.page_content)}")
                                logger.debug(f"  å…ƒæ•°æ®: {doc.metadata}")
                        else:
                            logger.warning(f"è·³è¿‡æ–‡æ¡£ {doc_id}: ç¼ºå°‘å¿…è¦å±æ€§ (page_content: {hasattr(doc, 'page_content')}, metadata: {hasattr(doc, 'metadata')})")
                    else:
                        logger.debug(f"è·³è¿‡æ–‡æ¡£ {doc_id}: chunk_type={chunk_type} (ä¸æ˜¯è¡¨æ ¼)")
                
                logger.info(f"ä»docstoreç­›é€‰å‡º {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¡¨æ ¼æ–‡æ¡£ï¼Œå°è¯•å…¶ä»–ç±»å‹
                if not table_docs:
                    logger.warning("æœªæ‰¾åˆ°chunk_type='table'çš„æ–‡æ¡£ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«è¡¨æ ¼å†…å®¹çš„æ–‡æ¡£...")
                    for doc_id, doc in docstore_dict.items():
                        if hasattr(doc, 'metadata') and hasattr(doc, 'page_content'):
                            content = doc.page_content.lower()
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«è¡¨æ ¼ç‰¹å¾
                            if any(keyword in content for keyword in ['è¡¨æ ¼', 'è¡¨', 'è¡Œ', 'åˆ—', 'æ•°æ®', 'ç»Ÿè®¡']):
                                table_docs.append(doc)
                                logger.debug(f"âœ… é€šè¿‡å†…å®¹è¯†åˆ«è¡¨æ ¼æ–‡æ¡£: {doc_id}")
                
                logger.info(f"æœ€ç»ˆåŠ è½½ {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
                return table_docs
            else:
                logger.warning("å‘é‡æ•°æ®åº“ä¸æ”¯æŒè¡¨æ ¼æ–‡æ¡£è·å–")
                return []
                
        except Exception as e:
            logger.error(f"ä»å‘é‡æ•°æ®åº“åŠ è½½è¡¨æ ¼æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _ensure_docs_loaded(self):
        """ç¡®ä¿æ–‡æ¡£å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if not self._docs_loaded:
            if self.document_loader:
                logger.info("å»¶è¿ŸåŠ è½½ï¼šä½¿ç”¨ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨")
                self._load_from_document_loader()
            else:
                logger.info("å»¶è¿ŸåŠ è½½ï¼šä½¿ç”¨å‘é‡æ•°æ®åº“")
                self.table_docs = self._load_from_vector_store()
                self._docs_loaded = True
            
            # éªŒè¯åŠ è½½çš„æ–‡æ¡£
            self._validate_loaded_documents()
    
    def _validate_loaded_documents(self):
        """éªŒè¯å·²åŠ è½½çš„æ–‡æ¡£"""
        try:
            logger.info(f"å¼€å§‹éªŒè¯å·²åŠ è½½çš„æ–‡æ¡£ï¼Œæ€»æ•°: {len(self.table_docs)}")
            
            if not self.table_docs:
                logger.warning("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è¡¨æ ¼æ–‡æ¡£")
                return
            
            valid_docs = []
            invalid_docs = []
            
            for i, doc in enumerate(self.table_docs):
                # æ£€æŸ¥æ–‡æ¡£ç»“æ„
                if not hasattr(doc, 'metadata'):
                    logger.warning(f"æ–‡æ¡£ {i}: ç¼ºå°‘metadataå±æ€§")
                    invalid_docs.append(i)
                    continue
                
                if not hasattr(doc, 'page_content'):
                    logger.warning(f"æ–‡æ¡£ {i}: ç¼ºå°‘page_contentå±æ€§")
                    invalid_docs.append(i)
                    continue
                
                # æ£€æŸ¥å…ƒæ•°æ®å®Œæ•´æ€§
                metadata = doc.metadata
                if not isinstance(metadata, dict):
                    logger.warning(f"æ–‡æ¡£ {i}: metadataä¸æ˜¯å­—å…¸ç±»å‹ï¼Œå®é™…ç±»å‹: {type(metadata)}")
                    invalid_docs.append(i)
                    continue
                
                # æ£€æŸ¥å†…å®¹
                content = doc.page_content
                if not isinstance(content, str):
                    logger.warning(f"æ–‡æ¡£ {i}: page_contentä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå®é™…ç±»å‹: {type(content)}")
                    invalid_docs.append(i)
                    continue
                
                if len(content.strip()) == 0:
                    logger.warning(f"æ–‡æ¡£ {i}: page_contentä¸ºç©º")
                    invalid_docs.append(i)
                    continue
                
                valid_docs.append(doc)
                logger.debug(f"âœ… æ–‡æ¡£ {i} éªŒè¯é€šè¿‡")
            
            # æ›´æ–°æ–‡æ¡£åˆ—è¡¨
            if invalid_docs:
                logger.warning(f"å‘ç° {len(invalid_docs)} ä¸ªæ— æ•ˆæ–‡æ¡£ï¼Œæ­£åœ¨ç§»é™¤...")
                self.table_docs = valid_docs
                logger.info(f"ç§»é™¤æ— æ•ˆæ–‡æ¡£åï¼Œå‰©ä½™ {len(self.table_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
            else:
                logger.info("æ‰€æœ‰æ–‡æ¡£éªŒè¯é€šè¿‡")
            
            logger.info(f"æ–‡æ¡£éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆæ–‡æ¡£: {len(valid_docs)}, æ— æ•ˆæ–‡æ¡£: {len(invalid_docs)}")
                
        except Exception as e:
            logger.error(f"æ–‡æ¡£éªŒè¯å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
    
    def _load_from_document_loader(self):
        """ä»ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨è·å–è¡¨æ ¼æ–‡æ¡£"""
        if self.document_loader:
            try:
                self.table_docs = self.document_loader.get_documents_by_type('table')
                self._docs_loaded = True
                logger.info(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–è¡¨æ ¼æ–‡æ¡£: {len(self.table_docs)} ä¸ª")
            except Exception as e:
                logger.error(f"ä»ç»Ÿä¸€åŠ è½½å™¨è·å–è¡¨æ ¼æ–‡æ¡£å¤±è´¥: {e}")
                # é™çº§åˆ°å‘é‡æ•°æ®åº“åŠ è½½æ–¹å¼
                self.table_docs = self._load_from_vector_store()
                self._docs_loaded = True
        else:
            logger.warning("æ–‡æ¡£åŠ è½½å™¨æœªæä¾›ï¼Œä½¿ç”¨å‘é‡æ•°æ®åº“åŠ è½½æ–¹å¼")
            self.table_docs = self._load_from_vector_store()
            self._docs_loaded = True
    
    def _validate_config(self):
        """éªŒè¯è¡¨æ ¼å¼•æ“é…ç½® - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒTableä¸“ç”¨é…ç½®éªŒè¯"""
        # é…ç½®ç±»å‹æ£€æŸ¥
        from ..config.v2_config import TableEngineConfigV2
        
        if not isinstance(self.config, TableEngineConfigV2):
            raise ValueError("é…ç½®å¿…é¡»æ˜¯TableEngineConfigV2ç±»å‹")
        
        # è·å–ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œæ”¯æŒä¸¤ç§é…ç½®ç±»å‹
        threshold = getattr(self.config, 'table_similarity_threshold', 0.7)
        if not isinstance(threshold, (int, float)) or threshold < 0 or threshold > 1:
            raise ValueError("è¡¨æ ¼ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
        
        # éªŒè¯Tableä¸“ç”¨é…ç½®å‚æ•°
        self._validate_table_specific_config()
        
        # éªŒè¯äº”å±‚å¬å›ç­–ç•¥é…ç½®
        self._validate_recall_strategy_config()
        
        # éªŒè¯é‡æ’åºé…ç½®
        self._validate_reranking_config()
        
        logger.info("âœ… è¡¨æ ¼å¼•æ“é…ç½®éªŒè¯å®Œæˆ")
    
    def _validate_table_specific_config(self):
        """éªŒè¯Tableä¸“ç”¨é…ç½®å‚æ•°"""
        try:
            # éªŒè¯è¡¨æ ¼å¤„ç†ç›¸å…³é…ç½®
            table_configs = [
                ('max_table_rows', int, 'è¡¨æ ¼æœ€å¤§è¡Œæ•°'),
                ('header_weight', float, 'è¡¨å¤´æƒé‡'),
                ('content_weight', float, 'å†…å®¹æƒé‡'),
                ('structure_weight', float, 'ç»“æ„æƒé‡'),
                ('enable_structure_search', bool, 'å¯ç”¨ç»“æ„æœç´¢'),
                ('enable_content_search', bool, 'å¯ç”¨å†…å®¹æœç´¢')
            ]
            
            for config_name, expected_type, description in table_configs:
                if hasattr(self.config, config_name):
                    value = getattr(self.config, config_name)
                    if not isinstance(value, expected_type):
                        logger.warning(f"âš ï¸ {description}é…ç½®ç±»å‹é”™è¯¯: æœŸæœ›{expected_type.__name__}, å®é™…{type(value).__name__}")
                    else:
                        logger.debug(f"âœ… {description}é…ç½®éªŒè¯é€šè¿‡: {value}")
                else:
                    logger.debug(f"â„¹ï¸ {description}é…ç½®æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            
            # éªŒè¯æƒé‡é…ç½®çš„åˆç†æ€§
            if hasattr(self.config, 'header_weight') and hasattr(self.config, 'content_weight') and hasattr(self.config, 'structure_weight'):
                total_weight = self.config.header_weight + self.config.content_weight + self.config.structure_weight
                if abs(total_weight - 1.0) > 0.01:
                    logger.warning(f"âš ï¸ æƒé‡é…ç½®æ€»å’Œä¸ä¸º1.0: {total_weight}")
            
        except Exception as e:
            logger.error(f"éªŒè¯Tableä¸“ç”¨é…ç½®å¤±è´¥: {e}")
    
    def _validate_recall_strategy_config(self):
        """éªŒè¯äº”å±‚å¬å›ç­–ç•¥é…ç½®"""
        try:
            if not hasattr(self.config, 'recall_strategy'):
                logger.warning("âš ï¸ æœªé…ç½®å¬å›ç­–ç•¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return
            
            strategy = self.config.recall_strategy
            required_layers = [
                'layer1_structure_search',    # ç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„æœç´¢
                'layer2_vector_search',       # ç¬¬äºŒå±‚ï¼šå‘é‡è¯­ä¹‰æœç´¢
                'layer3_keyword_search',      # ç¬¬ä¸‰å±‚ï¼šå…³é”®è¯åŒ¹é…
                'layer4_hybrid_search',       # ç¬¬å››å±‚ï¼šæ··åˆæ™ºèƒ½æœç´¢
                'layer5_expansion_search'     # ç¬¬äº”å±‚ï¼šå®¹é”™æ‰©å±•æœç´¢
            ]
            
            for layer in required_layers:
                if layer not in strategy:
                    logger.warning(f"âš ï¸ ç¼ºå°‘å¬å›ç­–ç•¥é…ç½®: {layer}")
                else:
                    layer_config = strategy[layer]
                    # ä¿®å¤ï¼šæ”¯æŒå¯¹è±¡å’Œå­—å…¸ä¸¤ç§æ ¼å¼
                    if hasattr(layer_config, 'enabled'):
                        # å¯¹è±¡æ ¼å¼ï¼ˆé€šè¿‡_convert_recall_strategy_to_objectsè½¬æ¢åï¼‰
                        enabled = layer_config.enabled
                        top_k = getattr(layer_config, 'top_k', 50)
                        logger.info(f"âœ… {layer}: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}, top_k: {top_k}")
                    elif isinstance(layer_config, dict):
                        # å­—å…¸æ ¼å¼ï¼ˆåŸå§‹é…ç½®ï¼‰
                        enabled = layer_config.get('enabled', True)
                        top_k = layer_config.get('top_k', 50)
                        logger.info(f"âœ… {layer}: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}, top_k: {top_k}")
                    else:
                        logger.warning(f"âš ï¸ å¬å›ç­–ç•¥é…ç½®æ ¼å¼é”™è¯¯: {layer}ï¼Œç±»å‹: {type(layer_config)}")
            
        except Exception as e:
            logger.error(f"éªŒè¯å¬å›ç­–ç•¥é…ç½®å¤±è´¥: {e}")
    
    def _validate_reranking_config(self):
        """éªŒè¯é‡æ’åºé…ç½®"""
        try:
            if not hasattr(self.config, 'reranking'):
                logger.warning("âš ï¸ æœªé…ç½®é‡æ’åºï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return
            
            reranking = self.config.reranking
            reranking_configs = [
                ('target_count', int, 'ç›®æ ‡ç»“æœæ•°é‡'),
                ('use_llm_enhancement', bool, 'ä½¿ç”¨LLMå¢å¼º'),
                ('model_name', str, 'æ¨¡å‹åç§°'),
                ('similarity_threshold', float, 'ç›¸ä¼¼åº¦é˜ˆå€¼')
            ]
            
            for config_name, expected_type, description in reranking_configs:
                if config_name in reranking:
                    value = reranking[config_name]
                    if not isinstance(value, expected_type):
                        logger.warning(f"âš ï¸ é‡æ’åº{description}é…ç½®ç±»å‹é”™è¯¯: æœŸæœ›{expected_type.__name__}, å®é™…{type(value).__name__}")
                    else:
                        logger.debug(f"âœ… é‡æ’åº{description}é…ç½®éªŒè¯é€šè¿‡: {value}")
                else:
                    logger.debug(f"â„¹ï¸ é‡æ’åº{description}é…ç½®æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            
        except Exception as e:
            logger.error(f"éªŒè¯é‡æ’åºé…ç½®å¤±è´¥: {e}")
    
    def _initialize_table_reranking_service(self):
        """åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡"""
        try:
            if not hasattr(self.config, 'reranking'):
                logger.warning("âš ï¸ æœªé…ç½®é‡æ’åºï¼Œè·³è¿‡è¡¨æ ¼é‡æ’åºæœåŠ¡åˆå§‹åŒ–")
                return
            
            reranking_config = self.config.reranking
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨LLMå¢å¼º
            if not reranking_config.get('use_llm_enhancement', False):
                logger.info("â„¹ï¸ LLMå¢å¼ºæœªå¯ç”¨ï¼Œè·³è¿‡è¡¨æ ¼é‡æ’åºæœåŠ¡åˆå§‹åŒ–")
                return
            
            # åˆ›å»ºè¡¨æ ¼é‡æ’åºæœåŠ¡å®ä¾‹
            self.table_reranking_service = TableRerankingService(reranking_config)
            logger.info(f"âœ… è¡¨æ ¼é‡æ’åºæœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {reranking_config.get('model_name', 'unknown')}")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–è¡¨æ ¼é‡æ’åºæœåŠ¡å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.table_reranking_service = None
    
    def _rerank_table_results(self, query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨è¡¨æ ¼é‡æ’åºæœåŠ¡å¯¹ç»“æœè¿›è¡Œé‡æ’åº
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param candidates: å€™é€‰ç»“æœåˆ—è¡¨
        :return: é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            if not self.table_reranking_service:
                logger.info("â„¹ï¸ è¡¨æ ¼é‡æ’åºæœåŠ¡æœªåˆå§‹åŒ–ï¼Œè·³è¿‡é‡æ’åº")
                return candidates
            
            if not candidates:
                logger.info("â„¹ï¸ å€™é€‰ç»“æœä¸ºç©ºï¼Œè·³è¿‡é‡æ’åº")
                return candidates
            
            logger.info(f"ğŸ” å¼€å§‹è¡¨æ ¼é‡æ’åºï¼Œè¾“å…¥ {len(candidates)} ä¸ªå€™é€‰ç»“æœ")
            start_time = time.time()
            
            # å‡†å¤‡é‡æ’åºæ•°æ®æ ¼å¼
            rerank_candidates = []
            for candidate in candidates:
                # ä»docå¯¹è±¡ä¸­æå–å†…å®¹
                doc = candidate.get('doc')
                if doc and hasattr(doc, 'page_content') and hasattr(doc, 'metadata'):
                    rerank_candidate = {
                        'content': getattr(doc, 'page_content', ''),
                        'metadata': getattr(doc, 'metadata', {}),
                        'original_candidate': candidate  # ä¿å­˜åŸå§‹å€™é€‰æ–‡æ¡£çš„å¼•ç”¨
                    }
                    rerank_candidates.append(rerank_candidate)
                else:
                    logger.warning(f"å€™é€‰æ–‡æ¡£ç¼ºå°‘å¿…è¦å±æ€§ï¼Œè·³è¿‡é‡æ’åº")
            
            if not rerank_candidates:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„é‡æ’åºå€™é€‰æ–‡æ¡£ï¼Œè¿”å›åŸå§‹ç»“æœ")
                return candidates
            
            # è°ƒç”¨è¡¨æ ¼é‡æ’åºæœåŠ¡
            reranked_results = self.table_reranking_service.rerank(query, rerank_candidates)
            
            # è°ƒè¯•ï¼šæŸ¥çœ‹é‡æ’åºç»“æœçš„æ ¼å¼
            logger.info(f"é‡æ’åºæœåŠ¡è¿”å› {len(reranked_results)} ä¸ªç»“æœ")
            for i, result in enumerate(reranked_results):
                if isinstance(result, dict):
                    logger.info(f"é‡æ’åºç»“æœ {i}: é”®={list(result.keys())}")
                    if 'doc' in result:
                        doc_data = result['doc']
                        if isinstance(doc_data, dict):
                            logger.info(f"é‡æ’åºç»“æœ {i}: docå­—æ®µåŒ…å«é”®={list(doc_data.keys())}")
                            if 'original_candidate' in doc_data:
                                logger.info(f"é‡æ’åºç»“æœ {i}: æ‰¾åˆ°original_candidateå¼•ç”¨")
                        else:
                            logger.info(f"é‡æ’åºç»“æœ {i}: docå­—æ®µç±»å‹={type(doc_data)}")
                else:
                    logger.info(f"é‡æ’åºç»“æœ {i}: ç±»å‹={type(result)}")
            
            # ä¿®å¤ï¼šç¡®ä¿è¿”å›ç»“æœæ ¼å¼ä¸€è‡´
            final_results = []
            for i, reranked_result in enumerate(reranked_results):
                if isinstance(reranked_result, dict):
                    # å¦‚æœé‡æ’åºç»“æœåŒ…å«docå­—æ®µ
                    if 'doc' in reranked_result:
                        # æ£€æŸ¥docå­—æ®µæ˜¯å¦åŒ…å«original_candidateå¼•ç”¨
                        doc_data = reranked_result['doc']
                        if isinstance(doc_data, dict) and 'original_candidate' in doc_data:
                            # ä½¿ç”¨åŸå§‹å€™é€‰æ–‡æ¡£å¼•ç”¨
                            original_candidate = doc_data['original_candidate']
                            logger.info(f"é‡æ’åºç»“æœ {i}: ä½¿ç”¨åŸå§‹å€™é€‰æ–‡æ¡£å¼•ç”¨ï¼Œç±»å‹={type(original_candidate)}")
                        else:
                            # ç›´æ¥ä½¿ç”¨docå­—æ®µ
                            original_candidate = doc_data
                            logger.info(f"é‡æ’åºç»“æœ {i}: ç›´æ¥ä½¿ç”¨docå­—æ®µï¼Œç±»å‹={type(original_candidate)}")
                        
                        # éªŒè¯åŸå§‹å€™é€‰æ–‡æ¡£çš„å†…å®¹
                        if 'doc' in original_candidate and original_candidate['doc']:
                            doc = original_candidate['doc']
                            content = getattr(doc, 'page_content', '')
                            logger.info(f"é‡æ’åºç»“æœ {i}: åŸå§‹å€™é€‰æ–‡æ¡£ä¸­doc.page_contenté•¿åº¦: {len(content)}")
                        
                        final_results.append({
                            'doc': original_candidate,
                            'score': reranked_result.get('score', 0.5),
                            'source': reranked_result.get('source', 'rerank'),
                            'layer': reranked_result.get('layer', 1)
                        })
                    else:
                        # å¦åˆ™ï¼Œæ„é€ æ ‡å‡†æ ¼å¼
                        original_candidate = candidates[i] if i < len(candidates) else candidates[0]
                        final_results.append({
                            'doc': original_candidate['doc'],
                            'score': reranked_result.get('score', original_candidate.get('score', 0.5)),
                            'source': reranked_result.get('source', 'rerank'),
                            'layer': reranked_result.get('layer', original_candidate.get('layer', 1))
                        })
                else:
                    # éå­—å…¸ç±»å‹ï¼Œä½¿ç”¨åŸå§‹å€™é€‰ç»“æœ
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„é‡æ’åºç»“æœç±»å‹: {type(reranked_result)}")
                    if i < len(candidates):
                        final_results.append({
                            'doc': candidates[i]['doc'],
                            'score': candidates[i].get('score', 0.5),
                            'source': candidates[i].get('source', 'unknown'),
                            'layer': candidates[i].get('layer', 1)
                        })
            
            rerank_time = time.time() - start_time
            logger.info(f"âœ… è¡¨æ ¼é‡æ’åºå®Œæˆï¼Œå¤„ç† {len(candidates)} ä¸ªç»“æœï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœï¼Œè€—æ—¶: {rerank_time:.2f}ç§’")
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ è¡¨æ ¼é‡æ’åºå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            # è¿”å›åŸå§‹ç»“æœ
            return candidates
    
    def _setup_components(self):
        """è®¾ç½®å¼•æ“ç»„ä»¶ - å®ç°æŠ½è±¡æ–¹æ³•ï¼Œä½¿ç”¨æ–°çš„æ–‡æ¡£åŠ è½½æœºåˆ¶"""
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²åŠ è½½ï¼Œå¦‚æœæ²¡æœ‰åˆ™åŠ è½½
        if not self._docs_loaded:
            try:
                logger.info("è¡¨æ ¼å¼•æ“åœ¨_setup_componentsä¸­å¼€å§‹åŠ è½½æ–‡æ¡£")
                self._ensure_docs_loaded()
                logger.info(f"âœ… è¡¨æ ¼å¼•æ“åœ¨_setup_componentsä¸­æˆåŠŸåŠ è½½ {len(self.table_docs)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.error(f"âŒ è¡¨æ ¼å¼•æ“åœ¨_setup_componentsä¸­åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise
    
    def _analyze_query_intent(self, query: str) -> Dict[str, Any]:
        """
        åˆ†ææŸ¥è¯¢æ„å›¾
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :return: æŸ¥è¯¢æ„å›¾åˆ†æç»“æœ
        """
        try:
            intent = {
                'query_type': 'unknown',
                'business_domain': 'unknown',
                'data_requirement': 'unknown',
                'time_range': 'unknown',
                'comparison_type': 'unknown'
            }
            
            query_lower = query.lower()
            
            # åˆ†ææŸ¥è¯¢ç±»å‹
            if any(word in query_lower for word in ['è¶‹åŠ¿', 'å˜åŒ–', 'å¢é•¿', 'ä¸‹é™', 'æ³¢åŠ¨']):
                intent['query_type'] = 'trend_analysis'
            elif any(word in query_lower for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚', 'é«˜ä½']):
                intent['query_type'] = 'comparison'
            elif any(word in query_lower for word in ['æ’å', 'æ’åº', 'å‰å‡ ', 'åå‡ ']):
                intent['query_type'] = 'ranking'
            elif any(word in query_lower for word in ['ç»Ÿè®¡', 'æ±‡æ€»', 'æ€»è®¡', 'å¹³å‡']):
                intent['query_type'] = 'statistics'
            
            # åˆ†æä¸šåŠ¡é¢†åŸŸ
            if any(word in query_lower for word in ['è´¢åŠ¡', 'æ”¶å…¥', 'åˆ©æ¶¦', 'æˆæœ¬', 'èµ„äº§']):
                intent['business_domain'] = 'finance'
            elif any(word in query_lower for word in ['é”€å”®', 'å¸‚åœº', 'å®¢æˆ·', 'äº§å“']):
                intent['business_domain'] = 'sales'
            elif any(word in query_lower for word in ['æŠ€æœ¯', 'ç ”å‘', 'åˆ›æ–°', 'ä¸“åˆ©']):
                intent['business_domain'] = 'technology'
            
            # åˆ†ææ•°æ®è¦æ±‚
            if any(word in query_lower for word in ['è¯¦ç»†', 'å…·ä½“', 'å®Œæ•´']):
                intent['data_requirement'] = 'detailed'
            elif any(word in query_lower for word in ['æ¦‚è§ˆ', 'æ€»ç»“', 'ç®€è¦']):
                intent['data_requirement'] = 'overview'
            
            # åˆ†ææ—¶é—´èŒƒå›´
            if any(word in query_lower for word in ['å¹´', 'å­£åº¦', 'æœˆ', 'æ—¥']):
                intent['time_range'] = 'time_series'
            
            return intent
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ„å›¾åˆ†æå¤±è´¥: {e}")
            return {'query_type': 'unknown', 'business_domain': 'unknown', 'data_requirement': 'unknown', 'time_range': 'unknown', 'comparison_type': 'unknown'}
    
    def _analyze_structure_requirements(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢å¯¹è¡¨æ ¼ç»“æ„çš„è¦æ±‚"""
        try:
            requirements = {
                'min_rows': 1,
                'max_rows': 1000,
                'min_columns': 1,
                'max_columns': 20,
                'preferred_structure': 'unknown'
            }
            
            query_lower = query.lower()
            
            # åˆ†æè¡Œæ•°è¦æ±‚
            if any(word in query_lower for word in ['è¯¦ç»†', 'å®Œæ•´', 'æ‰€æœ‰']):
                requirements['min_rows'] = 10
                requirements['max_rows'] = 1000
            elif any(word in query_lower for word in ['æ¦‚è§ˆ', 'æ€»ç»“', 'ä¸»è¦']):
                requirements['min_rows'] = 1
                requirements['max_rows'] = 20
            
            # åˆ†æåˆ—æ•°è¦æ±‚
            if any(word in query_lower for word in ['å¤šç»´åº¦', 'å…¨é¢', 'ç»¼åˆ']):
                requirements['min_columns'] = 3
                requirements['max_columns'] = 20
            elif any(word in query_lower for word in ['ç®€å•', 'åŸºç¡€']):
                requirements['min_columns'] = 1
                requirements['max_columns'] = 5
            
            return requirements
            
        except Exception as e:
            logger.error(f"ç»“æ„è¦æ±‚åˆ†æå¤±è´¥: {e}")
            return {'min_rows': 1, 'max_rows': 1000, 'min_columns': 1, 'max_columns': 20, 'preferred_structure': 'unknown'}
    
    def process_query(self, query: str, **kwargs) -> QueryResult:
        """
        å¤„ç†è¡¨æ ¼æŸ¥è¯¢è¯·æ±‚ - ä¿®å¤ç‰ˆæœ¬ï¼Œä¸text_engine.pyä¿æŒä¸€è‡´
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param kwargs: é¢å¤–å‚æ•°ï¼ˆåŒ…æ‹¬query_typeç­‰ï¼‰
        :return: QueryResultå¯¹è±¡
        """
        if not self.is_enabled():
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.TABLE,
                results=[],
                total_count=0,
                processing_time=0.0,
                engine_name=self.name,
                metadata={},
                error_message="è¡¨æ ¼å¼•æ“æœªå¯ç”¨"
            )
        
        start_time = time.time()
        
        try:
            # ç¡®ä¿æ–‡æ¡£å·²åŠ è½½
            self._ensure_docs_loaded()
            
            # æ·»åŠ æ–‡æ¡£çŠ¶æ€è¯Šæ–­
            logger.info(f"ğŸ” è¡¨æ ¼æŸ¥è¯¢è¯Šæ–­ä¿¡æ¯:")
            logger.info(f"  - æŸ¥è¯¢æ–‡æœ¬: {query}")
            logger.info(f"  - æ–‡æ¡£åŠ è½½çŠ¶æ€: {self._docs_loaded}")
            logger.info(f"  - è¡¨æ ¼æ–‡æ¡£æ•°é‡: {len(self.table_docs)}")
            logger.info(f"  - å‘é‡æ•°æ®åº“çŠ¶æ€: {self.vector_store is not None}")
            logger.info(f"  - æ–‡æ¡£åŠ è½½å™¨çŠ¶æ€: {self.document_loader is not None}")
            
            # å¦‚æœæ–‡æ¡£æ•°é‡ä¸º0ï¼Œå°è¯•é‡æ–°åŠ è½½
            if len(self.table_docs) == 0:
                logger.warning("âš ï¸ è¡¨æ ¼æ–‡æ¡£æ•°é‡ä¸º0ï¼Œå°è¯•é‡æ–°åŠ è½½...")
                self._docs_loaded = False
                self._ensure_docs_loaded()
                logger.info(f"é‡æ–°åŠ è½½åè¡¨æ ¼æ–‡æ¡£æ•°é‡: {len(self.table_docs)}")
            
            # åˆ†ææŸ¥è¯¢æ„å›¾
            intent_analysis = self._analyze_query_intent(query)
            logger.info(f"æŸ¥è¯¢æ„å›¾åˆ†æ: {intent_analysis['query_type']}, ä¸šåŠ¡é¢†åŸŸ: {intent_analysis['business_domain']}")
            
            # æ‰§è¡Œæœç´¢
            search_results = self._search_tables(query)
            
            # æ ¹æ®æ„å›¾è°ƒæ•´ç»“æœ
            if intent_analysis['query_type'] == 'detail_view' and intent_analysis['data_requirement'] == 'detailed':
                # å¦‚æœç”¨æˆ·æ„å›¾æ˜¯æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼Œå°è¯•è·å–å®Œæ•´è¡¨æ ¼
                if search_results and len(search_results) > 0:
                    top_result = search_results[0]
                    table_id = top_result['doc'].metadata.get('table_id', 'unknown')
                    full_table_result = self.get_full_table(table_id)
                    if full_table_result['status'] == 'success':
                        search_results[0]['full_content'] = full_table_result['content']
                        search_results[0]['full_metadata'] = full_table_result['metadata']
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°Pipeline
            use_new_pipeline = getattr(self.config, 'use_new_pipeline', True)
            if use_new_pipeline:
                logger.info("ä½¿ç”¨æ–°çš„ç»Ÿä¸€Pipelineå¤„ç†é‡æ’åºç»“æœ")
                # ä½¿ç”¨æ–°Pipelineå¤„ç†ç»“æœ
                formatted_results = self._process_with_new_pipeline(query, search_results)
            else:
                logger.info("ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ ¼å¼åŒ–ç»“æœ")
                # ä¼ ç»Ÿæ ¼å¼åŒ–æ–¹å¼
                formatted_results = []
            for result in search_results:
                # ä¿®å¤ï¼šå¤„ç†é‡æ’åºåå¯èƒ½æ²¡æœ‰'doc'é”®çš„æƒ…å†µ
                if 'doc' not in result:
                    logger.warning(f"è·³è¿‡æ— æ•ˆç»“æœï¼Œç¼ºå°‘'doc'é”®: {result}")
                    # å°è¯•ä¿®å¤ç»“æœæ ¼å¼
                    if isinstance(result, dict) and 'content' in result and 'metadata' in result:
                        # æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„docå¯¹è±¡
                        class MockDoc:
                            def __init__(self, content, metadata):
                                self.page_content = content
                                self.metadata = metadata
                        
                        mock_doc = MockDoc(result['content'], result['metadata'])
                        result['doc'] = mock_doc
                        result['score'] = result.get('score', 0.5)
                        result['source'] = result.get('source', 'unknown')
                        result['layer'] = result.get('layer', 1)
                        logger.info(f"å·²ä¿®å¤ç»“æœæ ¼å¼: {result}")
                    else:
                        continue
                
                doc = result['doc']
                metadata = getattr(doc, 'metadata', {})
                structure_analysis = result.get('structure_analysis', {})
                
                # æ–¹æ¡ˆAï¼šä¿ç•™ç°æœ‰å­—æ®µï¼ŒåŒæ—¶è¡¥å……é¡¶å±‚é”®ï¼Œç¡®ä¿Webç«¯å…¼å®¹æ€§
                formatted_result = {
                    'id': metadata.get('table_id', 'unknown'),
                    'content': getattr(doc, 'page_content', ''),
                    'score': result['score'],
                    'source': result.get('source', 'unknown'),  # ä¿®å¤ï¼šä½¿ç”¨getæ–¹æ³•æä¾›é»˜è®¤å€¼
                    'layer': result.get('layer', 1),  # ä¿®å¤ï¼šä½¿ç”¨getæ–¹æ³•æä¾›é»˜è®¤å€¼
                    
                    # æ–°å¢ï¼šé¡¶å±‚å­—æ®µæ˜ å°„ï¼Œç¡®ä¿Webç«¯èƒ½æ­£ç¡®è·å–table_content
                    'page_content': getattr(doc, 'page_content', ''),  # åŸcontentçš„åˆ«å
                    'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                    'page_number': metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                    'chunk_type': 'table',  # å›ºå®šä¸ºtableç±»å‹
                    'table_type': structure_analysis.get('table_type', 'unknown'),
                    'doc_id': metadata.get('table_id') or metadata.get('doc_id') or metadata.get('id', 'unknown'),
                    
                    'metadata': {
                        'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                        'page_number': metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                        'table_type': structure_analysis.get('table_type', 'unknown'),
                        'business_domain': structure_analysis.get('business_domain', 'unknown'),
                        'quality_score': structure_analysis.get('quality_score', 0.0),
                        'is_truncated': structure_analysis.get('is_truncated', False),
                        'truncation_type': structure_analysis.get('truncation_type', 'none'),
                        'truncated_rows': structure_analysis.get('truncated_rows', 0),
                        'current_rows': structure_analysis.get('row_count', 0),
                        'original_rows': structure_analysis.get('original_row_count', 0)
                    }
                }
                
                # å¦‚æœæœ‰å®Œæ•´è¡¨æ ¼å†…å®¹ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
                if 'full_content' in result:
                    formatted_result['full_content'] = result['full_content']
                    formatted_result['full_metadata'] = result['full_metadata']
                
                formatted_results.append(formatted_result)
            
            processing_time = time.time() - start_time
            
            # è¿”å›QueryResultå¯¹è±¡ï¼Œä¸text_engine.pyä¿æŒä¸€è‡´
            return QueryResult(
                success=True,
                query=query,
                query_type=QueryType.TABLE,
                results=formatted_results,
                total_count=len(formatted_results),
                processing_time=processing_time,
                engine_name=self.name,
                metadata={
                    'total_tables': len(self.table_docs),
                    'pipeline': 'unified_pipeline',  # æ ‡è®°ä½¿ç”¨æ–°Pipeline
                    'intent_analysis': intent_analysis,
                    'search_strategy': 'five_layer_recall',
                    'docs_loaded': self._docs_loaded,
                    'vector_store_available': self.vector_store is not None,
                    'document_loader_available': self.document_loader is not None,
                    'llm_answer': getattr(self, '_last_pipeline_result', {}).get('llm_answer', 'åŸºäºæ–°Pipelineç”Ÿæˆçš„ç­”æ¡ˆ'),  # ä½¿ç”¨Pipelineç”Ÿæˆçš„ç­”æ¡ˆ
                    'recall_count': len(search_results),  # å¬å›æ•°é‡
                    'final_count': len(formatted_results),  # æœ€ç»ˆç»“æœæ•°é‡
                    'pipeline_metrics': getattr(self, '_last_pipeline_result', {}).get('pipeline_metrics', {})  # PipelineæŒ‡æ ‡
                }
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"å¤„ç†è¡¨æ ¼æŸ¥è¯¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            return QueryResult(
                success=False,
                query=query,
                query_type=QueryType.TABLE,
                results=[],
                total_count=0,
                processing_time=processing_time,
                engine_name=self.name,
                metadata={},
                error_message=str(e)
            )
    
    def _process_with_new_pipeline(self, query: str, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨æ–°çš„ç»Ÿä¸€Pipelineå¤„ç†æœç´¢ç»“æœ
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param search_results: æœç´¢ç»“æœ
        :return: å¤„ç†åçš„ç»“æœ
        """
        try:
            logger.info("å¼€å§‹ä½¿ç”¨æ–°Pipelineå¤„ç†è¡¨æ ¼æœç´¢ç»“æœ")
            
            # 1. é¦–å…ˆè¿›è¡Œé‡æ’åº
            reranked_results = self._rerank_table_results(query, search_results)
            logger.info(f"é‡æ’åºå®Œæˆï¼Œç»“æœæ•°é‡: {len(reranked_results)}")
            
            # è°ƒè¯•ï¼šéªŒè¯é‡æ’åºç»“æœçš„å†…å®¹
            for i, result in enumerate(reranked_results):
                if 'doc' in result and result['doc']:
                    doc = result['doc']
                    if 'doc' in doc and doc['doc']:
                        content = getattr(doc['doc'], 'page_content', '')
                        logger.info(f"é‡æ’åºç»“æœ {i}: åœ¨_process_with_new_pipelineä¸­doc.page_contenté•¿åº¦: {len(content)}")
                    else:
                        logger.warning(f"é‡æ’åºç»“æœ {i}: docå­—æ®µç»“æ„å¼‚å¸¸: {list(doc.keys()) if isinstance(doc, dict) else type(doc)}")
                else:
                    logger.warning(f"é‡æ’åºç»“æœ {i}: ç¼ºå°‘docå­—æ®µ")
            
            # 2. ä½¿ç”¨ç»Ÿä¸€Pipelineå¤„ç†
            from v2.core.unified_pipeline import UnifiedPipeline
            
            # è·å–Pipelineé…ç½®
            pipeline_config = {
                'enable_llm_generation': True,
                'enable_source_filtering': True,
                'max_context_results': 10,
                'max_content_length': 1000
            }
            
            # åˆ›å»ºç»Ÿä¸€Pipelineå®ä¾‹
            # ä½¿ç”¨æ³¨å…¥çš„å¼•æ“
            if not self.llm_engine:
                logger.warning("LLMå¼•æ“æœªæ³¨å…¥ï¼Œä½¿ç”¨Mockå¼•æ“")
                # åˆ›å»ºMock LLMå¼•æ“
                class MockLLMEngine:
                    def generate_answer(self, query, context):
                        return f"åŸºäºæŸ¥è¯¢'{query}'ç”Ÿæˆçš„Mockç­”æ¡ˆï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)}"
                
                llm_engine = MockLLMEngine()
            else:
                llm_engine = self.llm_engine
            
            if not self.source_filter_engine:
                logger.warning("æºè¿‡æ»¤å¼•æ“æœªæ³¨å…¥ï¼Œä½¿ç”¨Mockå¼•æ“")
                # åˆ›å»ºMockæºè¿‡æ»¤å¼•æ“
                class MockSourceFilterEngine:
                    def filter_sources(self, llm_answer, sources, query):
                        return sources[:5]  # ç®€å•è¿”å›å‰5ä¸ªæº
                
                source_filter_engine = MockSourceFilterEngine()
            else:
                source_filter_engine = self.source_filter_engine
            
            unified_pipeline = UnifiedPipeline(
                config=pipeline_config,
                llm_engine=llm_engine,
                source_filter_engine=source_filter_engine
            )
            
            # è½¬æ¢ç»“æœæ ¼å¼ä¸ºPipelineæœŸæœ›çš„æ ¼å¼
            # å¢åŠ è¾“å…¥æ•°é‡é™åˆ¶ï¼Œè®©LLMèƒ½çœ‹åˆ°æ›´å¤šä¸Šä¸‹æ–‡
            max_pipeline_inputs = min(8, len(reranked_results))  # æœ€å¤š8ä¸ªè¾“å…¥
            pipeline_input = []
            logger.info(f"å¼€å§‹è½¬æ¢ {len(reranked_results)} ä¸ªé‡æ’åºç»“æœä¸ºPipelineè¾“å…¥æ ¼å¼ï¼Œé™åˆ¶ä¸º {max_pipeline_inputs} ä¸ª")
            
            for i, result in enumerate(reranked_results[:max_pipeline_inputs]):
                logger.info(f"å¤„ç†ç»“æœ {i}: ç±»å‹={type(result)}, é”®={list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                
                # å¤„ç†ä¸åŒçš„ç»“æœæ ¼å¼
                if 'doc' in result and result['doc']:
                    doc = result['doc']
                    # æ£€æŸ¥docæ˜¯å¦æ˜¯åŒ…å«docå­—æ®µçš„å­—å…¸ï¼ˆé‡æ’åºç»“æœæ ¼å¼ï¼‰
                    if isinstance(doc, dict) and 'doc' in doc and doc['doc']:
                        # é‡æ’åºç»“æœæ ¼å¼ï¼š{'doc': {'doc': doc_object, ...}, ...}
                        actual_doc = doc['doc']
                        content = getattr(actual_doc, 'page_content', '')
                        metadata = getattr(actual_doc, 'metadata', {})
                        logger.info(f"ç»“æœ {i}: ä»é‡æ’åºç»“æœdoc.docå¯¹è±¡æå–å†…å®¹ï¼Œé•¿åº¦: {len(content)}, å†…å®¹é¢„è§ˆ: {content[:100] if content else 'ç©º'}")
                    else:
                        # ç›´æ¥åŒ…å«docå¯¹è±¡çš„æƒ…å†µ
                        content = getattr(doc, 'page_content', '')
                        metadata = getattr(doc, 'metadata', {})
                        logger.info(f"ç»“æœ {i}: ä»docå¯¹è±¡æå–å†…å®¹ï¼Œé•¿åº¦: {len(content)}, å†…å®¹é¢„è§ˆ: {content[:100] if content else 'ç©º'}")
                elif 'content' in result:
                    # ç›´æ¥åŒ…å«contentçš„æƒ…å†µ
                    content = result['content']
                    metadata = result.get('metadata', {})
                    logger.info(f"ç»“æœ {i}: ç›´æ¥ä½¿ç”¨contentï¼Œé•¿åº¦: {len(content)}, å†…å®¹é¢„è§ˆ: {content[:100] if content else 'ç©º'}")
                else:
                    logger.warning(f"ç»“æœ {i} æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡: {result}")
                    continue
                
                # æ„é€ Pipelineè¾“å…¥
                pipeline_item = {
                    'content': content,
                    'metadata': metadata,
                    'score': result.get('score', 0.5),
                    'source': result.get('source', 'unknown'),
                    'layer': result.get('layer', 1)
                }
                pipeline_input.append(pipeline_item)
                logger.debug(f"ç»“æœ {i} è½¬æ¢å®Œæˆ: score={pipeline_item['score']}, layer={pipeline_item['layer']}")
            
            logger.info(f"Pipelineè¾“å…¥è½¬æ¢å®Œæˆï¼Œå…± {len(pipeline_input)} ä¸ªæœ‰æ•ˆè¾“å…¥")
            
            # æ‰§è¡ŒPipelineå¤„ç†
            pipeline_result = unified_pipeline.process(query, pipeline_input)
            
            if pipeline_result.success:
                logger.info("æ–°Pipelineå¤„ç†æˆåŠŸ")
                logger.info(f"Pipelineè¿”å›ç»“æœ: llm_answeré•¿åº¦={len(pipeline_result.llm_answer) if pipeline_result.llm_answer else 0}, filtered_sourcesæ•°é‡={len(pipeline_result.filtered_sources)}")
                
                # å°†Pipelineç»“æœè½¬æ¢ä¸ºTableEngineæœŸæœ›çš„æ ¼å¼
                formatted_results = []
                for i, source in enumerate(pipeline_result.filtered_sources):
                    logger.debug(f"å¤„ç†Pipelineæº {i}: {type(source)}")
                    
                    # æ„é€ æ ‡å‡†æ ¼å¼
                    formatted_result = {
                        'id': source.get('metadata', {}).get('table_id', f'table_{i}'),
                        'content': source.get('content', ''),
                        'score': source.get('score', 0.5),
                        'source': source.get('source', 'pipeline'),
                        'layer': source.get('layer', 1),
                        
                        # é¡¶å±‚å­—æ®µæ˜ å°„
                        'page_content': source.get('content', ''),
                        'document_name': source.get('metadata', {}).get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                        'page_number': source.get('metadata', {}).get('page_number', 'æœªçŸ¥é¡µ'),
                        'chunk_type': 'table',
                        'table_type': source.get('metadata', {}).get('table_type', 'unknown'),
                        'doc_id': source.get('metadata', {}).get('table_id', f'table_{i}'),
                        
                        'metadata': source.get('metadata', {})
                    }
                    formatted_results.append(formatted_result)
                    logger.debug(f"Pipelineæº {i} è½¬æ¢å®Œæˆ: id={formatted_result['id']}, contenté•¿åº¦={len(formatted_result['content'])}")
                
                # ä¿å­˜Pipelineç»“æœåˆ°å®ä¾‹å˜é‡ï¼Œä¾›åç»­ä½¿ç”¨
                self._last_pipeline_result = {
                    'llm_answer': pipeline_result.llm_answer,
                    'filtered_sources': pipeline_result.filtered_sources,
                    'pipeline_metrics': pipeline_result.pipeline_metrics
                }
                
                logger.info(f"æ–°Pipelineå¤„ç†å®Œæˆï¼Œè¿”å› {len(formatted_results)} ä¸ªç»“æœ")
                return formatted_results
            else:
                logger.warning(f"æ–°Pipelineå¤„ç†å¤±è´¥: {pipeline_result.error_message}")
                # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
                return self._format_results_traditional(search_results)
                
        except Exception as e:
            logger.error(f"æ–°Pipelineå¤„ç†å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
            return self._format_results_traditional(search_results)
    
    def _format_results_traditional(self, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä¼ ç»Ÿæ–¹å¼æ ¼å¼åŒ–ç»“æœï¼ˆä½œä¸ºæ–°Pipelineçš„å›é€€æ–¹æ¡ˆï¼‰
        
        :param search_results: æœç´¢ç»“æœ
        :return: æ ¼å¼åŒ–åçš„ç»“æœ
        """
        logger.info("ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ ¼å¼åŒ–ç»“æœ")
        formatted_results = []
        
        for result in search_results:
            # ä¿®å¤ï¼šå¤„ç†é‡æ’åºåå¯èƒ½æ²¡æœ‰'doc'é”®çš„æƒ…å†µ
            if 'doc' not in result:
                logger.warning(f"è·³è¿‡æ— æ•ˆç»“æœï¼Œç¼ºå°‘'doc'é”®: {result}")
                # å°è¯•ä¿®å¤ç»“æœæ ¼å¼
                if isinstance(result, dict) and 'content' in result and 'metadata' in result:
                    # æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„docå¯¹è±¡
                    class MockDoc:
                        def __init__(self, content, metadata):
                            self.page_content = content
                            self.metadata = metadata
                    
                    mock_doc = MockDoc(result['content'], result['metadata'])
                    result['doc'] = mock_doc
                    result['score'] = result.get('score', 0.5)
                    result['source'] = result.get('source', 'unknown')
                    result['layer'] = result.get('layer', 1)
                    logger.info(f"å·²ä¿®å¤ç»“æœæ ¼å¼: {result}")
                else:
                    continue
            
            doc = result['doc']
            metadata = getattr(doc, 'metadata', {})
            structure_analysis = result.get('structure_analysis', {})
            
            # æ–¹æ¡ˆAï¼šä¿ç•™ç°æœ‰å­—æ®µï¼ŒåŒæ—¶è¡¥å……é¡¶å±‚é”®ï¼Œç¡®ä¿Webç«¯å…¼å®¹æ€§
            formatted_result = {
                'id': metadata.get('table_id', 'unknown'),
                'content': getattr(doc, 'page_content', ''),
                'score': result['score'],
                'source': result.get('source', 'unknown'),
                'layer': result.get('layer', 1),
                
                # æ–°å¢ï¼šé¡¶å±‚å­—æ®µæ˜ å°„ï¼Œç¡®ä¿Webç«¯èƒ½æ­£ç¡®è·å–table_content
                'page_content': getattr(doc, 'page_content', ''),
                'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                'page_number': metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                'chunk_type': 'table',
                'table_type': structure_analysis.get('table_type', 'unknown'),
                'doc_id': metadata.get('table_id') or metadata.get('doc_id') or metadata.get('id', 'unknown'),
                
                'metadata': {
                    'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                    'page_number': metadata.get('page_number', 'æœªçŸ¥é¡µ'),
                    'table_type': structure_analysis.get('table_type', 'unknown'),
                    'business_domain': structure_analysis.get('business_domain', 'unknown'),
                    'quality_score': structure_analysis.get('quality_score', 0.0),
                    'is_truncated': structure_analysis.get('is_truncated', False),
                    'truncation_type': structure_analysis.get('truncation_type', 'none'),
                    'truncated_rows': structure_analysis.get('truncated_rows', 0),
                    'current_rows': structure_analysis.get('row_count', 0),
                    'original_rows': structure_analysis.get('original_row_count', 0)
                }
            }
            
            # å¦‚æœæœ‰å®Œæ•´è¡¨æ ¼å†…å®¹ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if 'full_content' in result:
                formatted_result['full_content'] = result['full_content']
                formatted_result['full_metadata'] = result['full_metadata']
            
            formatted_results.append(formatted_result)
        
        return formatted_results
    
    def _search_tables(self, query: str) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œè¡¨æ ¼æœç´¢ - æ–°çš„äº”å±‚å¬å›ç­–ç•¥ï¼ˆä¸Text/Image Engineä¿æŒä¸€è‡´ï¼‰
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        # ğŸ” è¯Šæ–­ä¿¡æ¯ï¼šæ£€æŸ¥ç³»ç»ŸçŠ¶æ€
        logger.info("=" * 50)
        logger.info("ğŸ” å¼€å§‹è¯Šæ–­äº”å±‚å¬å›ç­–ç•¥")
        logger.info(f"æŸ¥è¯¢æ–‡æœ¬: {query}")
        logger.info(f"å‘é‡æ•°æ®åº“çŠ¶æ€: {self.vector_store}")
        logger.info(f"è¡¨æ ¼æ–‡æ¡£ç¼“å­˜æ•°é‡: {len(self.table_docs)}")
        logger.info(f"æ–‡æ¡£åŠ è½½çŠ¶æ€: {self._docs_loaded}")
        
        # æ£€æŸ¥å‘é‡æ•°æ®åº“è¯¦ç»†ä¿¡æ¯
        if self.vector_store:
            logger.info(f"å‘é‡æ•°æ®åº“å±æ€§: {dir(self.vector_store)}")
            if hasattr(self.vector_store, 'docstore'):
                logger.info(f"docstoreç±»å‹: {type(self.vector_store.docstore)}")
                if hasattr(self.vector_store.docstore, '_dict'):
                    logger.info(f"docstore._dicté•¿åº¦: {len(self.vector_store.docstore._dict)}")
                    # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡æ¡£çš„å…ƒæ•°æ®
                    doc_count = 0
                    for doc_id, doc in list(self.vector_store.docstore._dict.items())[:3]:
                        logger.info(f"æ–‡æ¡£ {doc_count}: ID={doc_id}, ç±»å‹={type(doc)}")
                        if hasattr(doc, 'metadata'):
                            logger.info(f"  å…ƒæ•°æ®: {doc.metadata}")
                        if hasattr(doc, 'page_content'):
                            logger.info(f"  å†…å®¹é•¿åº¦: {len(doc.page_content)}")
                        doc_count += 1
                else:
                    logger.warning("âŒ docstoreæ²¡æœ‰_dictå±æ€§")
            else:
                logger.warning("âŒ å‘é‡æ•°æ®åº“æ²¡æœ‰docstoreå±æ€§")
        else:
            logger.error("âŒ å‘é‡æ•°æ®åº“ä¸ºç©ºï¼")
        
        logger.info("=" * 50)
        
        all_results = []
        min_required = getattr(self.config, 'min_required_results', 20)
        max_recall_results = getattr(self.config, 'max_recall_results', 150)
        
        logger.info(f"å¼€å§‹æ‰§è¡Œäº”å±‚å¬å›ç­–ç•¥ï¼ŒæŸ¥è¯¢: {query}")
        
        # ç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„ç²¾ç¡®åŒ¹é…ï¼ˆé«˜ç²¾åº¦ï¼Œä½å¬å›ï¼‰
        logger.info("æ‰§è¡Œç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„ç²¾ç¡®åŒ¹é…")
        layer1_results = self._table_structure_precise_search(query, top_k=30)
        all_results.extend(layer1_results)
        logger.info(f"âœ… ç¬¬ä¸€å±‚ç»“æ„æœç´¢æˆåŠŸï¼Œå¬å› {len(layer1_results)} ä¸ªç»“æœ")
        
        # ç¬¬äºŒå±‚ï¼šå‘é‡è¯­ä¹‰æœç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œä¸­ç­‰å¬å›ï¼‰
        logger.info("æ‰§è¡Œç¬¬äºŒå±‚ï¼šå‘é‡è¯­ä¹‰æœç´¢")
        layer2_results = self._enhanced_vector_search(query, top_k=40)
        all_results.extend(layer2_results)
        logger.info(f"âœ… ç¬¬äºŒå±‚å‘é‡æœç´¢æˆåŠŸï¼Œå¬å› {len(layer2_results)} ä¸ªç»“æœ")
        
        # ç¬¬ä¸‰å±‚ï¼šè¡¨æ ¼å†…å®¹å…³é”®è¯åŒ¹é…ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œé«˜å¬å›ï¼‰
        logger.info("æ‰§è¡Œç¬¬ä¸‰å±‚ï¼šè¡¨æ ¼å†…å®¹å…³é”®è¯åŒ¹é…")
        layer3_results = self._enhanced_content_keyword_search(query, top_k=35)
        all_results.extend(layer3_results)
        logger.info(f"âœ… ç¬¬ä¸‰å±‚å…³é”®è¯æœç´¢æˆåŠŸï¼Œå¬å› {len(layer3_results)} ä¸ªç»“æœ")
        
        # ç¬¬å››å±‚ï¼šæ··åˆæ™ºèƒ½æœç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œé«˜å¬å›ï¼‰
        logger.info("æ‰§è¡Œç¬¬å››å±‚ï¼šæ··åˆæ™ºèƒ½æœç´¢")
        layer4_results = self._enhanced_hybrid_search(query, top_k=30)
        all_results.extend(layer4_results)
        logger.info(f"âœ… ç¬¬å››å±‚æ··åˆæœç´¢æˆåŠŸï¼Œå¬å› {len(layer4_results)} ä¸ªç»“æœ")
        
        # æ£€æŸ¥å‰å››å±‚ç»“æœæ•°é‡ï¼Œå†³å®šæ˜¯å¦æ¿€æ´»ç¬¬äº”å±‚
        total_results = len(all_results)
        logger.info(f"å‰å››å±‚æ€»ç»“æœæ•°é‡: {total_results}")
        
        if total_results >= min_required:
            logger.info(f"å‰å››å±‚å¬å›æ•°é‡å……è¶³({total_results} >= {min_required})ï¼Œè·³è¿‡ç¬¬äº”å±‚")
        else:
            # ç¬¬äº”å±‚ï¼šå®¹é”™æ‰©å±•æœç´¢ï¼ˆå…œåº•ç­–ç•¥ï¼‰
            logger.warning(f"å‰å››å±‚å¬å›æ•°é‡ä¸è¶³({total_results} < {min_required})ï¼Œæ¿€æ´»ç¬¬äº”å±‚")
            layer5_results = self._fault_tolerant_expansion_search(query, top_k=25)
            all_results.extend(layer5_results)
            logger.info(f"ç¬¬äº”å±‚è¿”å› {len(layer5_results)} ä¸ªç»“æœ")
        
        # ç»“æœèåˆä¸å»é‡
        logger.info("å¼€å§‹ç»“æœèåˆä¸å»é‡")
        final_results = self._merge_and_deduplicate_results(all_results)
        
        # æœ€ç»ˆæ’åº
        final_results = self._final_ranking(query, final_results)
        
        # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        max_results = getattr(self.config, 'max_results', 10)
        final_results = final_results[:max_results]
        
        logger.info(f"äº”å±‚å¬å›ç­–ç•¥å®Œæˆï¼Œæœ€ç»ˆç»“æœæ•°é‡: {len(final_results)}")
        return final_results
    
    def _table_structure_precise_search(self, query: str, top_k: int = 30) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸€å±‚ï¼šè¡¨æ ¼ç»“æ„ç²¾ç¡®åŒ¹é…ï¼ˆé«˜ç²¾åº¦ï¼Œä½å¬å›ï¼‰
        
        åŸºäºè¡¨æ ¼çš„ç»“æ„ç‰¹å¾è¿›è¡Œç²¾ç¡®åŒ¹é…ï¼š
        1. è¡¨æ ¼æ ‡é¢˜åŒ¹é…
        2. åˆ—åç²¾ç¡®åŒ¹é…
        3. è¡¨æ ¼ç±»å‹åŒ¹é…
        4. è¡¨æ ¼å†…å®¹ç»“æ„åˆ†æ
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ç¬¬ä¸€å±‚ç»“æ„æœç´¢ - æŸ¥è¯¢: {query}, ç›®æ ‡æ•°é‡: {top_k}")
            
            # 1. è¡¨æ ¼æ ‡é¢˜ç²¾ç¡®åŒ¹é…
            title_matches = self._search_by_table_title(query, top_k // 3)
            results.extend(title_matches)
            logger.info(f"æ ‡é¢˜åŒ¹é…ç»“æœ: {len(title_matches)} ä¸ª")
            
            # 2. åˆ—åç²¾ç¡®åŒ¹é…
            column_matches = self._search_by_column_names(query, top_k // 3)
            results.extend(column_matches)
            logger.info(f"åˆ—ååŒ¹é…ç»“æœ: {len(column_matches)} ä¸ª")
            
            # 3. è¡¨æ ¼ç±»å‹åŒ¹é…
            type_matches = self._search_by_table_type(query, top_k // 3)
            results.extend(type_matches)
            logger.info(f"ç±»å‹åŒ¹é…ç»“æœ: {len(type_matches)} ä¸ª")
            
            # 4. è¡¨æ ¼å†…å®¹ç»“æ„åˆ†æ
            structure_matches = self._search_by_table_structure(query, top_k // 3)
            results.extend(structure_matches)
            logger.info(f"ç»“æ„åŒ¹é…ç»“æœ: {len(structure_matches)} ä¸ª")
            
            # å»é‡å’Œæ’åº
            unique_results = self._deduplicate_by_doc_id(results)
            sorted_results = sorted(unique_results, key=lambda x: x.get('structure_score', 0), reverse=True)
            
            # é™åˆ¶ç»“æœæ•°é‡
            final_results = sorted_results[:top_k]
            
            logger.info(f"âœ… ç¬¬ä¸€å±‚ç»“æ„æœç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬ä¸€å±‚ç»“æ„æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_by_table_title(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """åŸºäºè¡¨æ ¼æ ‡é¢˜æœç´¢"""
        results = []
        
        try:
            # æå–æŸ¥è¯¢ä¸­çš„å…³é”®æ¦‚å¿µ
            query_keywords = self._extract_keywords(query)
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                table_title = metadata.get('table_title', '')
                
                if not table_title:
                    continue
                
                # è®¡ç®—æ ‡é¢˜åŒ¹é…åˆ†æ•°
                title_score = self._calculate_title_similarity(query_keywords, table_title)
                
                if title_score > 0.6:  # æ ‡é¢˜åŒ¹é…é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'score': title_score,
                        'source': 'structure_search',
                        'layer': 1,
                        'search_method': 'title_match',
                        'structure_score': title_score,
                        'match_details': f"æ ‡é¢˜åŒ¹é…: {table_title}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            return sorted_results[:max_results]
            
        except Exception as e:
            logger.error(f"æ ‡é¢˜æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_by_column_names(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """åŸºäºåˆ—åæœç´¢"""
        results = []
        
        try:
            # æå–æŸ¥è¯¢ä¸­çš„å…³é”®æ¦‚å¿µ
            query_keywords = self._extract_keywords(query)
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                table_headers = metadata.get('table_headers', [])
                
                if not table_headers:
                    continue
                
                # è®¡ç®—åˆ—ååŒ¹é…åˆ†æ•°
                column_score = self._calculate_column_similarity(query_keywords, table_headers)
                
                if column_score > 0.5:  # åˆ—ååŒ¹é…é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'score': column_score,
                        'source': 'structure_search',
                        'layer': 1,
                        'search_method': 'column_match',
                        'structure_score': column_score,
                        'match_details': f"åˆ—ååŒ¹é…: {', '.join(table_headers[:3])}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            return sorted_results[:max_results]
            
        except Exception as e:
            logger.error(f"åˆ—åæœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_by_table_type(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """åŸºäºè¡¨æ ¼ç±»å‹æœç´¢"""
        results = []
        
        try:
            # åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œåˆ¤æ–­è¡¨æ ¼ç±»å‹
            query_intent = self._analyze_query_intent(query)
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                table_type = metadata.get('table_type', '')
                
                if not table_type:
                    continue
                
                # è®¡ç®—ç±»å‹åŒ¹é…åˆ†æ•°
                type_score = self._calculate_type_similarity(query_intent, table_type)
                
                if type_score > 0.4:  # ç±»å‹åŒ¹é…é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'score': type_score,
                        'source': 'structure_search',
                        'layer': 1,
                        'search_method': 'type_match',
                        'structure_score': type_score,
                        'match_details': f"ç±»å‹åŒ¹é…: {table_type}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            return sorted_results[:max_results]
            
        except Exception as e:
            logger.error(f"ç±»å‹æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_by_table_structure(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """åŸºäºè¡¨æ ¼ç»“æ„æœç´¢"""
        results = []
        
        try:
            # åˆ†ææŸ¥è¯¢å¯¹è¡¨æ ¼ç»“æ„çš„è¦æ±‚
            structure_requirements = self._analyze_structure_requirements(query)
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                row_count = metadata.get('table_row_count', 0)
                column_count = metadata.get('table_column_count', 0)
                
                # è®¡ç®—ç»“æ„åŒ¹é…åˆ†æ•°
                structure_score = self._calculate_structure_similarity(structure_requirements, row_count, column_count)
                
                if structure_score > 0.3:  # ç»“æ„åŒ¹é…é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'score': structure_score,
                        'source': 'structure_search',
                        'layer': 1,
                        'search_method': 'structure_match',
                        'structure_score': structure_score,
                        'match_details': f"ç»“æ„åŒ¹é…: {row_count}è¡ŒÃ—{column_count}åˆ—"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            return sorted_results[:max_results]
            
        except Exception as e:
            logger.error(f"ç»“æ„æœç´¢å¤±è´¥: {e}")
            return []
    
    def _calculate_search_k(self, target_k: int, layer_config) -> int:
        """
        æ™ºèƒ½è®¡ç®—æœç´¢èŒƒå›´ï¼Œç”¨äºpost-filterç­–ç•¥
        
        :param target_k: ç›®æ ‡ç»“æœæ•°é‡
        :param layer_config: å±‚çº§é…ç½®å¯¹è±¡
        :return: æœç´¢èŒƒå›´
        """
        try:
            if hasattr(layer_config, 'top_k'):
                base_top_k = layer_config.top_k
            else:
                base_top_k = 40
            if hasattr(layer_config, 'similarity_threshold'):
                similarity_threshold = layer_config.similarity_threshold
            else:
                similarity_threshold = 0.65
            
            # æ ¹æ®é˜ˆå€¼åŠ¨æ€è°ƒæ•´æœç´¢èŒƒå›´
            if similarity_threshold < 0.3:
                # ä½é˜ˆå€¼ï¼Œéœ€è¦æœç´¢æ›´å¤šå€™é€‰ç»“æœ
                search_k = max(target_k * 4, base_top_k * 2)
            elif similarity_threshold < 0.6:
                # ä¸­ç­‰é˜ˆå€¼
                search_k = max(target_k * 3, base_top_k * 1.5)
            else:
                # é«˜é˜ˆå€¼ï¼Œå¯ä»¥æœç´¢è¾ƒå°‘å€™é€‰ç»“æœ
                search_k = max(target_k * 2, base_top_k)
            
            # è®¾ç½®ä¸Šé™é¿å…è¿‡åº¦æœç´¢
            search_k = min(search_k, 150)
            
            logger.info(f"æ™ºèƒ½è®¡ç®—search_k: ç›®æ ‡{target_k}, åŸºç¡€top_k{base_top_k}, é˜ˆå€¼{similarity_threshold}, æœ€ç»ˆsearch_k{search_k}")
            return search_k
            
        except Exception as e:
            logger.error(f"è®¡ç®—search_kå¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            return max(target_k * 3, 80)

    def _enhanced_vector_search(self, query: str, top_k: int = 40) -> List[Dict[str, Any]]:
        """
        ç¬¬äºŒå±‚ï¼šå¢å¼ºçš„å‘é‡è¯­ä¹‰æœç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œä¸­ç­‰å¬å›ï¼‰ï¼Œæ”¯æŒpost-filterç­–ç•¥
        
        åˆ©ç”¨å¤šç§å‘é‡åŒ–ç­–ç•¥è¿›è¡Œè¡¨æ ¼å¬å›ï¼š
        1. ç­–ç•¥1ï¼šå°è¯•ä½¿ç”¨FAISS filterç›´æ¥æœç´¢
        2. ç­–ç•¥2ï¼šä½¿ç”¨post-filterç­–ç•¥ï¼ˆå…ˆæœç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤ï¼‰
        3. ç­–ç•¥3ï¼šå¤‡é€‰æ–¹æ¡ˆï¼ˆå¦‚æœå‰ä¸¤ç§éƒ½å¤±è´¥ï¼‰
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        if not self.vector_store or not getattr(self.config, 'enable_vector_search', True):
            logger.info("å‘é‡æœç´¢æœªå¯ç”¨æˆ–å‘é‡æ•°æ®åº“ä¸å¯ç”¨")
            return results
        
        try:
            # è·å–ç¬¬äºŒå±‚é…ç½®
            layer2_config = getattr(self.config, 'recall_strategy', {}).get('layer2_vector_search', {})
            if hasattr(layer2_config, 'similarity_threshold'):
                threshold = layer2_config.similarity_threshold
            else:
                threshold = 0.65
            if hasattr(layer2_config, 'top_k'):
                base_top_k = layer2_config.top_k
            else:
                base_top_k = 40
            
            # æ™ºèƒ½è®¡ç®—æœç´¢èŒƒå›´
            search_k = self._calculate_search_k(top_k, layer2_config)
            
            logger.info(f"ç¬¬äºŒå±‚å‘é‡æœç´¢ - æŸ¥è¯¢: {query}, é˜ˆå€¼: {threshold}, ç›®æ ‡æ•°é‡: {top_k}, æœç´¢èŒƒå›´: {search_k}")
            
            # ç­–ç•¥1ï¼šå°è¯•ä½¿ç”¨FAISS filterç›´æ¥æœç´¢tableç±»å‹æ–‡æ¡£
            logger.info("ç­–ç•¥1ï¼šå°è¯•ä½¿ç”¨FAISS filterç›´æ¥æœç´¢tableç±»å‹æ–‡æ¡£")
            try:
                content_results = self.vector_store.similarity_search(
                    query, 
                    k=top_k,
                    filter={'chunk_type': 'table'}  # å°è¯•ä½¿ç”¨filter
                )
                
                logger.info(f"âœ… ç­–ç•¥1 filteræœç´¢æˆåŠŸï¼Œè¿”å› {len(content_results)} ä¸ªç»“æœ")
                
                # å¤„ç†filteræœç´¢ç»“æœ
                for doc in content_results:
                    if not hasattr(doc, 'metadata'):
                        continue
                    
                    # è·å–ç›¸ä¼¼åº¦åˆ†æ•°
                    score = getattr(doc, 'score', 0.5)
                    
                    # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                    if score >= threshold:
                        results.append({
                            'doc': doc,
                            'score': score,
                            'source': 'vector_search',
                            'layer': 2,
                            'search_method': 'content_semantic_similarity_filter',
                            'vector_score': score,
                            'match_details': 'processed_table_contentè¯­ä¹‰åŒ¹é…(filter)'
                        })
                
                logger.info(f"ç­–ç•¥1é€šè¿‡é˜ˆå€¼æ£€æŸ¥çš„ç»“æœæ•°é‡: {len(results)}")
                
                # å¦‚æœfilteræœç´¢è¿”å›è¶³å¤Ÿçš„ç»“æœï¼Œç›´æ¥è¿”å›
                if len(results) >= top_k * 0.8:  # 80%çš„ç›®æ ‡æ•°é‡
                    return results[:top_k]
                    
            except Exception as e:
                logger.warning(f"ç­–ç•¥1 filteræœç´¢å¤±è´¥: {e}")
                logger.info("é™çº§åˆ°post-filterç­–ç•¥")
            
            # ç­–ç•¥2ï¼šä½¿ç”¨post-filterç­–ç•¥ï¼ˆå…ˆæœç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤ï¼‰
            logger.info("ç­–ç•¥2ï¼šä½¿ç”¨post-filterç­–ç•¥ï¼ˆå…ˆæœç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤ï¼‰")
            
            # æœç´¢æ›´å¤šå€™é€‰ç»“æœç”¨äºåè¿‡æ»¤
            all_candidates = self.vector_store.similarity_search(
                query, 
                k=search_k
            )
            
            logger.info(f"ç­–ç•¥2æœç´¢è¿”å› {len(all_candidates)} ä¸ªå€™é€‰ç»“æœ")
            
            # åè¿‡æ»¤ï¼šç­›é€‰å‡ºtableç±»å‹çš„æ–‡æ¡£
            table_candidates = []
            for doc in all_candidates:
                if (hasattr(doc, 'metadata') and doc.metadata and 
                    doc.metadata.get('chunk_type') == 'table'):
                    table_candidates.append(doc)
            
            logger.info(f"åè¿‡æ»¤åæ‰¾åˆ° {len(table_candidates)} ä¸ªtableæ–‡æ¡£")
            
            # å¤„ç†tableæœç´¢ç»“æœï¼Œåº”ç”¨é˜ˆå€¼è¿‡æ»¤
            for doc in table_candidates:
                # è·å–ç›¸ä¼¼åº¦åˆ†æ•°
                score = getattr(doc, 'score', 0.5)
                
                # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                if score >= threshold:
                    results.append({
                        'doc': doc,
                        'score': score,
                        'source': 'vector_search',
                        'layer': 2,
                        'search_method': 'content_semantic_similarity_post_filter',
                        'vector_score': score,
                        'match_details': 'processed_table_contentè¯­ä¹‰åŒ¹é…(post-filter)'
                    })
            
            logger.info(f"ç­–ç•¥2é€šè¿‡é˜ˆå€¼æ£€æŸ¥çš„ç»“æœæ•°é‡: {len(results)}")
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            final_results = results[:top_k]
            
            logger.info(f"âœ… ç­–ç•¥2 post-filteræˆåŠŸï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬äºŒå±‚å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def _enhanced_content_keyword_search(self, query: str, top_k: int = 35) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸‰å±‚ï¼šè¡¨æ ¼å†…å®¹å…³é”®è¯åŒ¹é…ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œé«˜å¬å›ï¼‰
        
        åŸºäºè¡¨æ ¼å†…å®¹çš„å…³é”®è¯åŒ¹é…ç­–ç•¥ï¼š
        1. è¡¨æ ¼æ ‡é¢˜å…³é”®è¯åŒ¹é…
        2. åˆ—åå…³é”®è¯åŒ¹é…
        3. è¡¨æ ¼å†…å®¹å…³é”®è¯åŒ¹é…
        4. è¡¨æ ¼æ‘˜è¦å…³é”®è¯åŒ¹é…
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ç¬¬ä¸‰å±‚å…³é”®è¯æœç´¢ - æŸ¥è¯¢: {query}, ç›®æ ‡æ•°é‡: {top_k}")
            
            # æå–æŸ¥è¯¢å…³é”®è¯
            query_keywords = self._extract_keywords(query)
            logger.info(f"æå–çš„æŸ¥è¯¢å…³é”®è¯: {query_keywords}")
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                
                # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
                keyword_score = self._calculate_keyword_match_score(query_keywords, metadata)
                
                if keyword_score > 0.3:  # å…³é”®è¯åŒ¹é…é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'score': keyword_score,
                        'source': 'keyword_search',
                        'layer': 3,
                        'search_method': 'keyword_match',
                        'keyword_score': keyword_score,
                        'match_details': f"å…³é”®è¯åŒ¹é…åˆ†æ•°: {keyword_score:.2f}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            final_results = sorted_results[:top_k]
            
            logger.info(f"âœ… ç¬¬ä¸‰å±‚å…³é”®è¯æœç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬ä¸‰å±‚å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            return []
    
    def _enhanced_hybrid_search(self, query: str, top_k: int = 30) -> List[Dict[str, Any]]:
        """
        ç¬¬å››å±‚ï¼šæ··åˆæ™ºèƒ½æœç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œé«˜å¬å›ï¼‰
        
        ç»“åˆå¤šç§æœç´¢ç­–ç•¥çš„æ··åˆæ–¹æ³•ï¼š
        1. ç»“æ„ç‰¹å¾ + å†…å®¹ç‰¹å¾çš„ç»„åˆè¯„åˆ†
        2. è¡¨æ ¼è´¨é‡è¯„ä¼°
        3. æŸ¥è¯¢æ„å›¾åˆ†æ
        4. åŠ¨æ€æƒé‡è°ƒæ•´
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ç¬¬å››å±‚æ··åˆæœç´¢ - æŸ¥è¯¢: {query}, ç›®æ ‡æ•°é‡: {top_k}")
            
            # åˆ†ææŸ¥è¯¢æ„å›¾
            query_intent = self._analyze_query_intent(query)
            logger.info(f"æŸ¥è¯¢æ„å›¾åˆ†æ: {query_intent}")
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                
                # è®¡ç®—æ··åˆè¯„åˆ†
                hybrid_score = self._calculate_hybrid_score(query, query_intent, metadata)
                
                if hybrid_score > 0.2:  # æ··åˆæœç´¢é˜ˆå€¼
                    results.append({
                        'doc': table_doc,
                        'score': hybrid_score,
                        'source': 'hybrid_search',
                        'layer': 4,
                        'search_method': 'hybrid_intelligent',
                        'hybrid_score': hybrid_score,
                        'match_details': f"æ··åˆè¯„åˆ†: {hybrid_score:.2f}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            final_results = sorted_results[:top_k]
            
            logger.info(f"âœ… ç¬¬å››å±‚æ··åˆæœç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬å››å±‚æ··åˆæœç´¢å¤±è´¥: {e}")
            return []
    
    def _fault_tolerant_expansion_search(self, query: str, top_k: int = 25) -> List[Dict[str, Any]]:
        """
        ç¬¬äº”å±‚ï¼šå®¹é”™æ‰©å±•æœç´¢ï¼ˆå…œåº•ç­–ç•¥ï¼‰
        
        å½“å…¶ä»–å±‚å¬å›ä¸è¶³æ—¶çš„å…œåº•ç­–ç•¥ï¼š
        1. æ¨¡ç³ŠåŒ¹é…
        2. éƒ¨åˆ†å…³é”®è¯åŒ¹é…
        3. è¡¨æ ¼ç±»å‹æ³›åŒ–
        4. é™çº§é˜ˆå€¼åŒ¹é…
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            logger.info(f"ç¬¬äº”å±‚å®¹é”™æ‰©å±•æœç´¢ - æŸ¥è¯¢: {query}, ç›®æ ‡æ•°é‡: {top_k}")
            
            # æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆæ›´å®½æ¾çš„æå–ï¼‰
            query_keywords = self._extract_keywords_relaxed(query)
            logger.info(f"å®½æ¾æå–çš„æŸ¥è¯¢å…³é”®è¯: {query_keywords}")
            
            for table_doc in self.table_docs:
                if not hasattr(table_doc, 'metadata'):
                    continue
                
                metadata = table_doc.metadata
                
                # è®¡ç®—å®¹é”™è¯„åˆ†ï¼ˆæ›´å®½æ¾çš„é˜ˆå€¼ï¼‰
                fault_tolerant_score = self._calculate_fault_tolerant_score(query_keywords, metadata)
                
                if fault_tolerant_score > 0.1:  # å®¹é”™æœç´¢é˜ˆå€¼ï¼ˆå¾ˆä½ï¼‰
                    results.append({
                        'doc': table_doc,
                        'score': fault_tolerant_score,
                        'source': 'fault_tolerant_search',
                        'layer': 5,
                        'search_method': 'fault_tolerant_expansion',
                        'fault_tolerant_score': fault_tolerant_score,
                        'match_details': f"å®¹é”™è¯„åˆ†: {fault_tolerant_score:.2f}"
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            final_results = sorted_results[:top_k]
            
            logger.info(f"âœ… ç¬¬äº”å±‚å®¹é”™æ‰©å±•æœç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"ç¬¬äº”å±‚å®¹é”™æ‰©å±•æœç´¢å¤±è´¥: {e}")
            return []
    
    def _merge_and_deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ç»“æœèåˆä¸å»é‡
        
        :param results: åŸå§‹ç»“æœåˆ—è¡¨
        :return: å»é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        if not results:
            return []
        
        try:
            # å»é‡ï¼ˆåŸºäºæ–‡æ¡£IDï¼‰
            seen_docs = set()
            unique_results = []
            
            for result in results:
                doc_id = result['doc'].metadata.get('id', id(result['doc']))
                if doc_id not in seen_docs:
                    seen_docs.add(doc_id)
                    unique_results.append(result)
                else:
                    # å¦‚æœæ–‡æ¡£å·²å­˜åœ¨ï¼Œé€‰æ‹©åˆ†æ•°æ›´é«˜çš„ç»“æœ
                    existing_result = next(r for r in unique_results if r['doc'].metadata.get('id', id(r['doc'])) == doc_id)
                    if result['score'] > existing_result['score']:
                        # æ›¿æ¢ä¸ºåˆ†æ•°æ›´é«˜çš„ç»“æœ
                        unique_results.remove(existing_result)
                        unique_results.append(result)
            
            # å±‚é—´æƒé‡è°ƒæ•´
            for result in unique_results:
                layer = result.get('layer', 1)
                # æ ¹æ®å±‚çº§è°ƒæ•´åˆ†æ•°ï¼Œå±‚çº§è¶Šä½åˆ†æ•°è¶Šé«˜
                layer_weight = 1.0 / layer
                result['adjusted_score'] = result['score'] * layer_weight
            
            # æŒ‰è°ƒæ•´åçš„åˆ†æ•°æ’åº
            unique_results.sort(key=lambda x: x.get('adjusted_score', x['score']), reverse=True)
            
            logger.info(f"å»é‡åå‰©ä½™ {len(unique_results)} ä¸ªå”¯ä¸€ç»“æœ")
            return unique_results
            
        except Exception as e:
            logger.error(f"å»é‡å’Œæ’åºå¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šç®€å•æ’åº
            return sorted(results, key=lambda x: x['score'], reverse=True)
    
    def _final_ranking(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æœ€ç»ˆæ’åº
        
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param results: åŸå§‹ç»“æœåˆ—è¡¨
        :return: æœ€ç»ˆæ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            if not results:
                return []
            
            # ç®€å•çš„æ’åºï¼šæŒ‰åˆ†æ•°æ’åºå³å¯
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            
            # æ·»åŠ æ’åä¿¡æ¯
            for i, result in enumerate(sorted_results):
                result['final_rank'] = i + 1
                result['final_score'] = result.get('score', 0.0)
            
            logger.info(f"æœ€ç»ˆæ’åºå®Œæˆï¼Œç»“æœæ•°é‡: {len(sorted_results)}")
            return sorted_results
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆæ’åºå¤±è´¥: {e}")
            return results
    

    
    def _initialize_recall_strategy(self):
        """åˆå§‹åŒ–äº”å±‚å¬å›ç­–ç•¥"""
        try:
            # æ£€æŸ¥å¿…è¦çš„é…ç½®é¡¹
            if not hasattr(self.config, 'use_new_pipeline'):
                logger.warning("æœªé…ç½®use_new_pipelineï¼Œé»˜è®¤å¯ç”¨")
            
            if not hasattr(self.config, 'enable_enhanced_reranking'):
                logger.warning("æœªé…ç½®enable_enhanced_rerankingï¼Œé»˜è®¤å¯ç”¨")
            
            logger.info("äº”å±‚å¬å›ç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¬å›ç­–ç•¥å¤±è´¥: {e}")
    
    def clear_cache(self):
        """æ¸…ç†è¡¨æ ¼å¼•æ“ç¼“å­˜"""
        self.table_docs = []
        self._docs_loaded = False
        logger.info("è¡¨æ ¼å¼•æ“ç¼“å­˜å·²æ¸…ç†")

    def _analyze_table_structure(self, doc):
        """
        ç®€åŒ–ç‰ˆè¡¨æ ¼ç»“æ„åˆ†æ
        
        :param doc: è¡¨æ ¼æ–‡æ¡£
        :return: è¡¨æ ¼ç»“æ„åˆ†æç»“æœ
        """
        try:
            analysis = {
                'table_type': 'unknown',
                'columns': [],
                'row_count': 0,
                'column_count': 0,
                'quality_score': 0.0
            }
            
            # ä»å…ƒæ•°æ®ä¸­æå–åŸºæœ¬ä¿¡æ¯
            metadata = getattr(doc, 'metadata', {})
            if metadata:
                analysis['columns'] = metadata.get('table_headers', [])
                analysis['row_count'] = metadata.get('table_row_count', 0)
                analysis['column_count'] = metadata.get('table_column_count', 0)
                analysis['table_type'] = metadata.get('table_type', 'unknown')
            
            # è®¡ç®—ç®€å•çš„è´¨é‡è¯„åˆ†
            if analysis['row_count'] > 0 and analysis['column_count'] > 0:
                analysis['quality_score'] = min(1.0, (analysis['row_count'] + analysis['column_count']) / 20.0)
            
            return analysis
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {
                'table_type': 'unknown',
                'columns': [],
                'row_count': 0,
                'column_count': 0,
                'quality_score': 0.0
            }
    


    def get_full_table(self, table_id: str) -> Dict[str, Any]:
        """
        è·å–å®Œæ•´è¡¨æ ¼å†…å®¹ï¼ˆæœªæˆªæ–­ç‰ˆæœ¬ï¼‰
        
        :param table_id: è¡¨æ ¼ID
        :return: å®Œæ•´è¡¨æ ¼å†…å®¹å­—å…¸
        """
        try:
            # é¦–å…ˆå°è¯•ä»æ–‡æ¡£åŠ è½½å™¨è·å–å®Œæ•´è¡¨æ ¼
            if self.document_loader:
                full_doc = self.document_loader.get_full_document_by_id(table_id, chunk_type='table')
                if full_doc:
                    logger.info(f"ä»æ–‡æ¡£åŠ è½½å™¨è·å–å®Œæ•´è¡¨æ ¼ {table_id} æˆåŠŸ")
                    return {
                        'status': 'success',
                        'table_id': table_id,
                        'content': getattr(full_doc, 'page_content', ''),
                        'metadata': getattr(full_doc, 'metadata', {}),
                        'message': 'å®Œæ•´è¡¨æ ¼å†…å®¹ä»æ–‡æ¡£åŠ è½½å™¨è·å–æˆåŠŸ'
                    }
            
            # å¦‚æœæ²¡æœ‰æ–‡æ¡£åŠ è½½å™¨æˆ–è·å–å¤±è´¥ï¼Œå°è¯•ä»å‘é‡å­˜å‚¨è·å–
            if self.vector_store:
                full_doc = self.vector_store.get_full_document_by_id(table_id)
                if full_doc:
                    logger.info(f"ä»å‘é‡å­˜å‚¨è·å–å®Œæ•´è¡¨æ ¼ {table_id} æˆåŠŸ")
                    return {
                        'status': 'success',
                        'table_id': table_id,
                        'content': getattr(full_doc, 'page_content', ''),
                        'metadata': getattr(full_doc, 'metadata', {}),
                        'message': 'å®Œæ•´è¡¨æ ¼å†…å®¹ä»å‘é‡å­˜å‚¨è·å–æˆåŠŸ'
                    }
            
            # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            logger.warning(f"æ— æ³•è·å–å®Œæ•´è¡¨æ ¼ {table_id}")
            return {
                'status': 'error',
                'table_id': table_id,
                'content': '',
                'metadata': {},
                'message': f'æ— æ³•è·å–å®Œæ•´è¡¨æ ¼ {table_id}ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰å®Œæ•´æ•°æ®æˆ–æ²¡æœ‰é…ç½®æ–‡æ¡£åŠ è½½å™¨/å‘é‡å­˜å‚¨'
            }
            
        except Exception as e:
            logger.error(f"è·å–å®Œæ•´è¡¨æ ¼ {table_id} å¤±è´¥: {e}")
            return {
                'status': 'error',
                'table_id': table_id,
                'content': '',
                'metadata': {},
                'message': f'è·å–å®Œæ•´è¡¨æ ¼å¤±è´¥: {str(e)}'
            }

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬å…³é”®è¯"""
        try:
            if not text:
                return []
            
            # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥åç»­ä¼˜åŒ–ä¸ºæ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
            text_lower = text.lower()
            
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·
            text_clean = re.sub(r'[^\w\s]', ' ', text_lower)
            
            # åˆ†è¯
            words = text_clean.split()
            
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å¦‚æœ', 'é‚£ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'å“ªäº›', 'ä»€ä¹ˆ', 'å¤šå°‘', 'å‡ ', 'ä¸ª', 'å¹´', 'æœˆ', 'æ—¥', 'æ—¶', 'åˆ†', 'ç§’'}
            keywords = [word for word in words if len(word) > 1 and word not in stop_words]
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_keywords = list(set(keywords))[:20]
            
            return unique_keywords
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    def _extract_keywords_relaxed(self, text: str) -> List[str]:
        """å®½æ¾çš„å…³é”®è¯æå–ï¼ˆç”¨äºå®¹é”™æœç´¢ï¼‰"""
        try:
            if not text:
                return []
            
            # æ›´å®½æ¾çš„å…³é”®è¯æå–
            text_lower = text.lower()
            
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·
            text_clean = re.sub(r'[^\w\s]', ' ', text_lower)
            
            # åˆ†è¯
            words = text_clean.split()
            
            # åªè¿‡æ»¤éå¸¸çŸ­çš„è¯
            keywords = [word for word in words if len(word) > 0]
            
            # å»é‡
            unique_keywords = list(set(keywords))
            
            return unique_keywords
            
        except Exception as e:
            logger.error(f"å®½æ¾å…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    def _calculate_title_similarity(self, query_keywords: List[str], table_title: str) -> float:
        """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not table_title or not query_keywords:
                return 0.0
            
            title_lower = table_title.lower()
            title_words = set(title_lower.split())
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            matched_keywords = sum(1 for kw in query_keywords if kw.lower() in title_words)
            
            if matched_keywords == 0:
                return 0.0
            
            # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
            similarity = min(1.0, matched_keywords / len(query_keywords))
            
            return similarity
            
        except Exception as e:
            logger.error(f"æ ‡é¢˜ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_column_similarity(self, query_keywords: List[str], table_headers: List[str]) -> float:
        """è®¡ç®—åˆ—åç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not table_headers or not query_keywords:
                return 0.0
            
            # è®¡ç®—æ¯ä¸ªåˆ—åçš„åŒ¹é…åˆ†æ•°
            column_scores = []
            
            for header in table_headers:
                if not isinstance(header, str):
                    continue
                
                header_lower = header.lower()
                header_words = set(header_lower.split())
                
                # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                matched_keywords = sum(1 for kw in query_keywords if kw.lower() in header_words)
                
                if matched_keywords > 0:
                    similarity = min(1.0, matched_keywords / len(query_keywords))
                    column_scores.append(similarity)
            
            if not column_scores:
                return 0.0
            
            # è¿”å›æœ€é«˜åˆ†æ•°
            return max(column_scores)
            
        except Exception as e:
            logger.error(f"åˆ—åç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_type_similarity(self, query_intent: Dict[str, Any], table_type: str) -> float:
        """è®¡ç®—ç±»å‹ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not table_type or not query_intent:
                return 0.0
            
            table_type_lower = table_type.lower()
            query_type = query_intent.get('query_type', 'unknown')
            business_domain = query_intent.get('business_domain', 'unknown')
            
            score = 0.0
            
            # æŸ¥è¯¢ç±»å‹åŒ¹é…
            if query_type != 'unknown':
                if query_type in table_type_lower:
                    score += 0.5
                elif any(word in table_type_lower for word in query_type.split('_')):
                    score += 0.3
            
            # ä¸šåŠ¡é¢†åŸŸåŒ¹é…
            if business_domain != 'unknown':
                if business_domain in table_type_lower:
                    score += 0.3
                elif any(word in table_type_lower for word in business_domain.split('_')):
                    score += 0.2
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"ç±»å‹ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_structure_similarity(self, requirements: Dict[str, Any], row_count: int, column_count: int) -> float:
        """è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not requirements:
                return 0.0
            
            score = 0.0
            
            # è¡Œæ•°åŒ¹é…
            min_rows = requirements.get('min_rows', 1)
            max_rows = requirements.get('max_rows', 1000)
            
            if min_rows <= row_count <= max_rows:
                score += 0.5
            elif row_count > 0:
                # éƒ¨åˆ†åŒ¹é…
                if row_count >= min_rows * 0.5:
                    score += 0.3
                elif row_count <= max_rows * 1.5:
                    score += 0.2
            
            # åˆ—æ•°åŒ¹é…
            min_columns = requirements.get('min_columns', 1)
            max_columns = requirements.get('max_columns', 20)
            
            if min_columns <= column_count <= max_columns:
                score += 0.5
            elif column_count > 0:
                # éƒ¨åˆ†åŒ¹é…
                if column_count >= min_columns * 0.5:
                    score += 0.3
                elif column_count <= max_columns * 1.5:
                    score += 0.2
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"ç»“æ„ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_keyword_match_score(self, query_keywords: List[str], metadata: Dict[str, Any]) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        try:
            if not query_keywords or not metadata:
                return 0.0
            
            score = 0.0
            
            # è¡¨æ ¼æ ‡é¢˜å…³é”®è¯åŒ¹é…
            table_title = metadata.get('table_title', '')
            if table_title:
                title_score = self._calculate_title_similarity(query_keywords, table_title)
                score += title_score * 0.4  # æ ‡é¢˜æƒé‡40%
            
            # åˆ—åå…³é”®è¯åŒ¹é…
            table_headers = metadata.get('table_headers', [])
            if table_headers:
                column_score = self._calculate_column_similarity(query_keywords, table_headers)
                score += column_score * 0.4  # åˆ—åæƒé‡40%
            
            # è¡¨æ ¼æ‘˜è¦å…³é”®è¯åŒ¹é…
            table_summary = metadata.get('table_summary', '')
            if table_summary:
                summary_score = self._calculate_title_similarity(query_keywords, table_summary)
                score += summary_score * 0.2  # æ‘˜è¦æƒé‡20%
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"å…³é”®è¯åŒ¹é…åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_hybrid_score(self, query: str, query_intent: Dict[str, Any], metadata: Dict[str, Any]) -> float:
        """è®¡ç®—æ··åˆè¯„åˆ†"""
        try:
            if not metadata:
                return 0.0
            
            score = 0.0
            
            # ç»“æ„ç‰¹å¾è¯„åˆ†
            structure_score = 0.0
            
            # è¡¨æ ¼ç±»å‹åŒ¹é…
            table_type = metadata.get('table_type', '')
            if table_type:
                type_score = self._calculate_type_similarity(query_intent, table_type)
                structure_score += type_score * 0.3
            
            # è¡¨æ ¼ç»“æ„åŒ¹é…
            row_count = metadata.get('table_row_count', 0)
            column_count = metadata.get('table_column_count', 0)
            if row_count > 0 and column_count > 0:
                structure_requirements = self._analyze_structure_requirements(query)
                struct_score = self._calculate_structure_similarity(structure_requirements, row_count, column_count)
                structure_score += struct_score * 0.3
            
            # è¡¨æ ¼è´¨é‡è¯„åˆ†
            quality_score = 0.0
            if row_count > 5 and column_count > 2:
                quality_score = 0.4  # åŸºç¡€è´¨é‡åˆ†æ•°
            
            # æœ€ç»ˆæ··åˆåˆ†æ•°
            score = (structure_score * 0.6) + (quality_score * 0.4)
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"æ··åˆè¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_fault_tolerant_score(self, query_keywords: List[str], metadata: Dict[str, Any]) -> float:
        """è®¡ç®—å®¹é”™è¯„åˆ†ï¼ˆæ›´å®½æ¾çš„é˜ˆå€¼ï¼‰"""
        try:
            if not query_keywords or not metadata:
                return 0.0
            
            score = 0.0
            
            # éå¸¸å®½æ¾çš„æ ‡é¢˜åŒ¹é…
            table_title = metadata.get('table_title', '')
            if table_title:
                title_lower = table_title.lower()
                for kw in query_keywords:
                    if kw.lower() in title_lower:
                        score += 0.2
                        break
            
            # éå¸¸å®½æ¾çš„åˆ—ååŒ¹é…
            table_headers = metadata.get('table_headers', [])
            if table_headers:
                for header in table_headers:
                    if not isinstance(header, str):
                        continue
                    header_lower = header.lower()
                    for kw in query_keywords:
                        if kw.lower() in header_lower:
                            score += 0.2
                            break
                    if score > 0.2:
                        break
            
            # è¡¨æ ¼å­˜åœ¨æ€§åŠ åˆ†
            if metadata.get('table_id'):
                score += 0.1
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"å®¹é”™è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_text_similarity_simple(self, query: str, content: str) -> float:
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
        try:
            if not query or not content:
                return 0.0
            
            query_lower = query.lower()
            content_lower = content.lower()
            
            # è®¡ç®—è¯åŒ¹é…åº¦
            query_words = set(query_lower.split())
            content_words = set(content_lower.split())
            
            if not query_words:
                return 0.0
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            intersection = len(query_words & content_words)
            union = len(query_words | content_words)
            
            if union == 0:
                return 0.0
            
            similarity = intersection / union
            return similarity
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _deduplicate_by_doc_id(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ ¹æ®æ–‡æ¡£IDå»é‡"""
        try:
            seen_doc_ids = set()
            unique_results = []
            
            for result in results:
                doc = result.get('doc')
                if not doc:
                    continue
                
                # è·å–æ–‡æ¡£ID
                doc_id = None
                if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict):
                    doc_id = doc.metadata.get('id') or doc.metadata.get('doc_id') or doc.metadata.get('table_id')
                elif hasattr(doc, 'id'):
                    doc_id = doc.id
                
                if doc_id and doc_id not in seen_doc_ids:
                    seen_doc_ids.add(doc_id)
                    unique_results.append(result)
                elif not doc_id:
                    # å¦‚æœæ²¡æœ‰IDï¼Œç›´æ¥æ·»åŠ 
                    unique_results.append(result)
            
            logger.info(f"å»é‡å‰: {len(results)} ä¸ªç»“æœï¼Œå»é‡å: {len(unique_results)} ä¸ªç»“æœ")
            return unique_results
            
        except Exception as e:
            logger.error(f"ç»“æœå»é‡å¤±è´¥: {e}")
            return results
    
    def _merge_and_deduplicate_results(self, all_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶å’Œå»é‡æ‰€æœ‰ç»“æœ"""
        try:
            if not all_results:
                return []
            
            # å»é‡
            unique_results = self._deduplicate_by_doc_id(all_results)
            
            # ç®€å•çš„å±‚é—´æƒé‡è°ƒæ•´
            for result in unique_results:
                layer = result.get('layer', 1)
                # æ ¹æ®å±‚çº§è°ƒæ•´åˆ†æ•°ï¼Œå±‚çº§è¶Šä½åˆ†æ•°è¶Šé«˜
                layer_weight = 1.0 / layer
                result['adjusted_score'] = result['score'] * layer_weight
            
            # æŒ‰è°ƒæ•´åçš„åˆ†æ•°æ’åº
            unique_results.sort(key=lambda x: x.get('adjusted_score', x['score']), reverse=True)
            
            logger.info(f"ç»“æœåˆå¹¶å®Œæˆï¼Œæœ€ç»ˆç»“æœæ•°é‡: {len(unique_results)}")
            return unique_results
            
        except Exception as e:
            logger.error(f"ç»“æœåˆå¹¶å¤±è´¥: {e}")
            return all_results
    

    
    def _get_doc_id(self, doc) -> str:
        """è·å–æ–‡æ¡£ID"""
        try:
            if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict):
                return (doc.metadata.get('id') or 
                        doc.metadata.get('doc_id') or 
                        doc.metadata.get('table_id') or 
                        str(id(doc)))
            elif hasattr(doc, 'id'):
                return str(doc.id)
            else:
                return str(id(doc))
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£IDå¤±è´¥: {e}")
            return str(id(doc))
    
    def _final_ranking(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æœ€ç»ˆæ’åº"""
        try:
            if not results:
                return []
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
            
            # æ·»åŠ æ’åä¿¡æ¯
            for i, result in enumerate(sorted_results):
                result['final_rank'] = i + 1
                result['final_score'] = result.get('score', 0.0)
            
            logger.info(f"æœ€ç»ˆæ’åºå®Œæˆï¼Œç»“æœæ•°é‡: {len(sorted_results)}")
            return sorted_results
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆæ’åºå¤±è´¥: {e}")
            return results
