#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•FAISS filteråŠŸèƒ½çš„çœŸç›¸éªŒè¯

æµ‹è¯•ç›®æ ‡ï¼š
1. æ·±å…¥éªŒè¯FAISSæ˜¯å¦çœŸçš„æ”¯æŒfilteråŠŸèƒ½
2. æ£€æŸ¥filteræ˜¯å¦çœŸçš„åœ¨å·¥ä½œï¼Œè¿˜æ˜¯åªæ˜¯è¿”å›äº†æ‰€æœ‰ç»“æœ
3. å¯¹æ¯”æœ‰filterå’Œæ— filterçš„æœç´¢ç»“æœå·®å¼‚
4. éªŒè¯filteræ˜¯å¦çœŸçš„åœ¨è¿‡æ»¤ï¼Œè¿˜æ˜¯åªæ˜¯è£…é¥°æ€§çš„
"""

import os
import sys
import time
import logging
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_faiss_filter_truth():
    """æ·±å…¥éªŒè¯FAISS filteråŠŸèƒ½çš„çœŸç›¸"""
    print("="*80)
    print("æµ‹è¯•FAISS filteråŠŸèƒ½çš„çœŸç›¸éªŒè¯")
    print("="*80)
    
    try:
        # 1. å¯¼å…¥å¿…è¦çš„æ¨¡å—
        print("å¯¼å…¥å¿…è¦æ¨¡å—...")
        from langchain_community.vectorstores import FAISS
        from langchain_community.embeddings import DashScopeEmbeddings
        from config.api_key_manager import get_dashscope_api_key
        from config.settings import Settings
        
        print("æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # 2. è·å–é…ç½®
        print("è·å–é…ç½®...")
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        config_path = os.path.join(project_root, 'config.json')
        
        print(f"   é¡¹ç›®æ ¹ç›®å½•: {project_root}")
        print(f"   é…ç½®æ–‡ä»¶è·¯å¾„: {config_path}")
        
        if not os.path.exists(config_path):
            print(f"  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
        
        try:
            # ä¸´æ—¶åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œç¡®ä¿è·¯å¾„è§£ææ­£ç¡®
            old_cwd = os.getcwd()
            os.chdir(project_root)
            
            config = Settings.load_from_file('config.json')
            print(f"   é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
            
            # æ¢å¤åŸæ¥çš„å·¥ä½œç›®å½•
            os.chdir(old_cwd)
            
        except Exception as e:
            print(f"   é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            os.chdir(old_cwd)
            return False
        
        # 3. è·å–APIå¯†é’¥
        api_key = get_dashscope_api_key(config.dashscope_api_key)
        if not api_key:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return False
        
        print("é…ç½®è·å–æˆåŠŸ")
        
        # 4. åˆå§‹åŒ–embeddings
        print("åˆå§‹åŒ–embeddings...")
        text_embeddings = DashScopeEmbeddings(
            dashscope_api_key=api_key,
            model='text-embedding-v1'
        )
        print("text embeddingsåˆå§‹åŒ–æˆåŠŸ")
        
        # 5. åŠ è½½å‘é‡æ•°æ®åº“
        print("åŠ è½½å‘é‡æ•°æ®åº“...")
        vector_db_path = config.vector_db_dir
        
        if not os.path.exists(vector_db_path):
            print(f"å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {vector_db_path}")
            return False
        
        print(f"å‘é‡æ•°æ®åº“è·¯å¾„å­˜åœ¨: {vector_db_path}")
        
        try:
            vector_store = FAISS.load_local(
                vector_db_path, 
                text_embeddings,
                allow_dangerous_deserialization=True
            )
            print("å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
            return False
        
        # 6. åˆ†ææ•°æ®åº“ç»“æ„
        print("åˆ†ææ•°æ®åº“ç»“æ„...")
        if not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
            print("å‘é‡æ•°æ®åº“ç»“æ„ä¸ç¬¦åˆé¢„æœŸ")
            return False
        
        docstore_dict = vector_store.docstore._dict
        print(f"æ•°æ®åº“åŒ…å« {len(docstore_dict)} ä¸ªæ–‡æ¡£")
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„chunks
        chunk_type_stats = {}
        for doc_id, doc in docstore_dict.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            chunk_type = metadata.get('chunk_type', 'unknown')
            chunk_type_stats[chunk_type] = chunk_type_stats.get(chunk_type, 0) + 1
        
        print("æ•°æ®åº“chunkç±»å‹ç»Ÿè®¡:")
        for chunk_type, count in chunk_type_stats.items():
            print(f"   - {chunk_type}: {count} ä¸ª")
        
        # 7. å…³é”®æµ‹è¯•ï¼šéªŒè¯filteræ˜¯å¦çœŸçš„åœ¨å·¥ä½œ
        print("\n" + "="*80)
        print("å…³é”®æµ‹è¯•ï¼šéªŒè¯filteræ˜¯å¦çœŸçš„åœ¨å·¥ä½œ")
        print("="*80)
        
        test_query = "ä¸­èŠ¯å›½é™…è´¢åŠ¡å›¾è¡¨"
        print(f"æµ‹è¯•æŸ¥è¯¢: {test_query}")
        
        # æµ‹è¯•1ï¼šæ— filteræœç´¢
        print("\næµ‹è¯•1ï¼šæ— filteræœç´¢")
        try:
            no_filter_results = vector_store.similarity_search(test_query, k=50)
            print(f"   æ— filteræœç´¢ç»“æœæ•°é‡: {len(no_filter_results)}")
            
            # ç»Ÿè®¡ç»“æœä¸­çš„chunk_typeåˆ†å¸ƒ
            no_filter_stats = {}
            for doc in no_filter_results:
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                chunk_type = metadata.get('chunk_type', 'unknown')
                no_filter_stats[chunk_type] = no_filter_stats.get(chunk_type, 0) + 1
            
            print("   æ— filterç»“æœåˆ†å¸ƒ:")
            for chunk_type, count in no_filter_stats.items():
                print(f"     - {chunk_type}: {count} ä¸ª")
                
        except Exception as e:
            print(f"     æ— filteræœç´¢å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•2ï¼šä½¿ç”¨image filteræœç´¢
        print("\nï¿½ï¿½ æµ‹è¯•2ï¼šä½¿ç”¨image filteræœç´¢")
        try:
            image_filter_results = vector_store.similarity_search(
                test_query, 
                k=50,
                filter={'chunk_type': 'image'}
            )
            print(f"   image filteræœç´¢ç»“æœæ•°é‡: {len(image_filter_results)}")
            
            # ç»Ÿè®¡ç»“æœä¸­çš„chunk_typeåˆ†å¸ƒ
            image_filter_stats = {}
            for doc in image_filter_results:
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                chunk_type = metadata.get('chunk_type', 'unknown')
                image_filter_stats[chunk_type] = image_filter_stats.get(chunk_type, 0) + 1
            
            print("   image filterç»“æœåˆ†å¸ƒ:")
            for chunk_type, count in image_filter_stats.items():
                print(f"     - {chunk_type}: {count} ä¸ª")
                
            # å…³é”®éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦çœŸçš„åªè¿”å›äº†imageç±»å‹
            non_image_count = sum(count for chunk_type, count in image_filter_stats.items() if chunk_type != 'image')
            if non_image_count == 0:
                print("    image filterçœŸçš„åœ¨å·¥ä½œï¼åªè¿”å›äº†imageç±»å‹")
            else:
                print(f"     image filteræ²¡æœ‰å·¥ä½œï¼è¿”å›äº† {non_image_count} ä¸ªéimageç±»å‹")
                
        except Exception as e:
            print(f"     image filteræœç´¢å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•3ï¼šä½¿ç”¨text filteræœç´¢
        print("\nğŸ“‹ æµ‹è¯•3ï¼šä½¿ç”¨text filteræœç´¢")
        try:
            text_filter_results = vector_store.similarity_search(
                test_query, 
                k=50,
                filter={'chunk_type': 'text'}
            )
            print(f"   text filteræœç´¢ç»“æœæ•°é‡: {len(text_filter_results)}")
            
            # ç»Ÿè®¡ç»“æœä¸­çš„chunk_typeåˆ†å¸ƒ
            text_filter_stats = {}
            for doc in text_filter_results:
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                chunk_type = metadata.get('chunk_type', 'unknown')
                text_filter_stats[chunk_type] = text_filter_stats.get(chunk_type, 0) + 1
            
            print("   text filterç»“æœåˆ†å¸ƒ:")
            for chunk_type, count in text_filter_stats.items():
                print(f"     - {chunk_type}: {count} ä¸ª")
                
            # å…³é”®éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦çœŸçš„åªè¿”å›äº†textç±»å‹
            non_text_count = sum(count for chunk_type, count in text_filter_stats.items() if chunk_type != 'text')
            if non_text_count == 0:
                print("    text filterçœŸçš„åœ¨å·¥ä½œï¼åªè¿”å›äº†textç±»å‹")
            else:
                print(f"     text filteræ²¡æœ‰å·¥ä½œï¼è¿”å›äº† {non_text_count} ä¸ªétextç±»å‹")
                
        except Exception as e:
            print(f"     text filteræœç´¢å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•4ï¼šå¯¹æ¯”åˆ†æ
        print("\nï¿½ï¿½ æµ‹è¯•4ï¼šå¯¹æ¯”åˆ†æ")
        print("   å¯¹æ¯”æ— filter vs image filter vs text filter:")
        print(f"   æ— filter: {len(no_filter_results)} ä¸ªç»“æœ")
        print(f"   image filter: {len(image_filter_results)} ä¸ªç»“æœ")
        print(f"   text filter: {len(text_filter_results)} ä¸ªç»“æœ")
        
        # è®¡ç®—filteræ•ˆæœ
        if len(no_filter_results) > 0:
            image_filter_ratio = len(image_filter_results) / len(no_filter_results)
            text_filter_ratio = len(text_filter_results) / len(no_filter_results)
            
            print(f"   image filterè¿‡æ»¤æ•ˆæœ: {image_filter_ratio:.2%}")
            print(f"   text filterè¿‡æ»¤æ•ˆæœ: {text_filter_ratio:.2%}")
            
            # åˆ¤æ–­filteræ˜¯å¦çœŸçš„åœ¨å·¥ä½œ
            if image_filter_ratio < 0.5 and text_filter_ratio < 0.5:
                print("    filterçœŸçš„åœ¨å·¥ä½œï¼ç»“æœæ•°é‡æ˜æ˜¾å‡å°‘")
            elif image_filter_ratio > 0.8 and text_filter_ratio > 0.8:
                print("     filteræ²¡æœ‰å·¥ä½œï¼ç»“æœæ•°é‡å‡ ä¹æ²¡æœ‰å‡å°‘")
            else:
                print("     filteræ•ˆæœä¸æ˜ç¡®ï¼Œéœ€è¦è¿›ä¸€æ­¥éªŒè¯")
        
        # æµ‹è¯•5ï¼šæ£€æŸ¥FAISSç´¢å¼•ç±»å‹
        print("\nï¿½ï¿½ æµ‹è¯•5ï¼šæ£€æŸ¥FAISSç´¢å¼•ç±»å‹")
        try:
            if hasattr(vector_store, 'index'):
                faiss_index = vector_store.index
                print(f"   FAISSç´¢å¼•ç±»å‹: {type(faiss_index).__name__}")
                print(f"   ç´¢å¼•ç»´åº¦: {faiss_index.d}")
                print(f"   ç´¢å¼•å¤§å°: {faiss_index.ntotal}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰filterç›¸å…³å±æ€§
                filter_attrs = [attr for attr in dir(faiss_index) if 'filter' in attr.lower()]
                if filter_attrs:
                    print(f"   Filterç›¸å…³å±æ€§: {filter_attrs}")
                else:
                    print("     æ²¡æœ‰å‘ç°filterç›¸å…³å±æ€§")
            else:
                print("     æ— æ³•è·å–FAISSç´¢å¼•ä¿¡æ¯")
                
        except Exception as e:
            print(f"     æ£€æŸ¥FAISSç´¢å¼•å¤±è´¥: {e}")
        
        # 8. ç»“è®º
        print("\n" + "="*80)
        print("  æµ‹è¯•ç»“è®º")
        print("="*80)
        
        # åŸºäºæµ‹è¯•ç»“æœç»™å‡ºç»“è®º
        if (len(image_filter_results) < len(no_filter_results) * 0.5 and 
            len(text_filter_results) < len(no_filter_results) * 0.5):
            print(" FAISS filteråŠŸèƒ½çœŸçš„åœ¨å·¥ä½œï¼")
            print("   å»ºè®®ï¼šå¯ä»¥å®‰å…¨ä½¿ç”¨filteråŠŸèƒ½è¿›è¡Œchunk_typeè¿‡æ»¤")
        else:
            print(" FAISS filteråŠŸèƒ½å¯èƒ½æ²¡æœ‰å·¥ä½œï¼")
            print("   å»ºè®®ï¼šéœ€è¦æ£€æŸ¥FAISSé…ç½®æˆ–è€ƒè™‘å…¶ä»–è¿‡æ»¤æ–¹æ¡ˆ")
        
        print("\n" + "="*80)
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼šFAISS filteråŠŸèƒ½çœŸç›¸éªŒè¯å®Œæˆï¼")
        print("="*80)
        
        return True
        
    except Exception as e:
        print(f"  æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_faiss_filter_truth()
    if success:
        print("\n æµ‹è¯•é€šè¿‡ï¼šFAISS filteråŠŸèƒ½éªŒè¯å®Œæˆ")
    else:
        print("\n  æµ‹è¯•å¤±è´¥ï¼šéœ€è¦æ£€æŸ¥é…ç½®æˆ–æ•°æ®åº“ç»“æ„")