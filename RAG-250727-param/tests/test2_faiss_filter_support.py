#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•2ï¼šFAISS filteråŠŸèƒ½æ”¯æŒéªŒè¯

æµ‹è¯•ç›®æ ‡ï¼š
1. ç¡®è®¤FAISSæ˜¯å¦æ”¯æŒchunk_typeè¿‡æ»¤
2. éªŒè¯filteræœç´¢çš„å‡†ç¡®æ€§å’Œæ€§èƒ½
3. æµ‹è¯•è·¨æ¨¡æ€æœç´¢çš„ç¨³å®šæ€§
"""

import os
import sys
import time
import logging
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_faiss_filter_support():
    """æµ‹è¯•FAISSçš„filteråŠŸèƒ½"""
    print("="*60)
    print("ï¿½ï¿½ æµ‹è¯•2ï¼šFAISS filteråŠŸèƒ½æ”¯æŒéªŒè¯")
    print("="*60)
    
    try:
        # 1. å¯¼å…¥å¿…è¦çš„æ¨¡å—
        print("ğŸ“¦ å¯¼å…¥å¿…è¦æ¨¡å—...")
        from langchain_community.vectorstores import FAISS
        from langchain_community.embeddings import DashScopeEmbeddings
        from config.api_key_manager import get_dashscope_api_key
        from config.settings import Settings
        
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # 2. è·å–APIå¯†é’¥å’Œé…ç½®
        print("ğŸ”‘ è·å–é…ç½®...")
        
        # æ‰¾åˆ°æ­£ç¡®çš„é…ç½®æ–‡ä»¶è·¯å¾„
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        config_path = os.path.join(project_root, 'config.json')
        
        print(f"   é¡¹ç›®æ ¹ç›®å½•: {project_root}")
        print(f"   é…ç½®æ–‡ä»¶è·¯å¾„: {config_path}")
        
        if not os.path.exists(config_path):
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
        
        try:
            # ä¸´æ—¶åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œç¡®ä¿è·¯å¾„è§£ææ­£ç¡®
            old_cwd = os.getcwd()
            os.chdir(project_root)
            
            config = Settings.load_from_file('config.json')
            print(f"   âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
            
            # æ¢å¤åŸæ¥çš„å·¥ä½œç›®å½•
            os.chdir(old_cwd)
            
        except Exception as e:
            print(f"   âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            # æ¢å¤åŸæ¥çš„å·¥ä½œç›®å½•
            os.chdir(old_cwd)
            return False
        
        # è°ƒè¯•é…ç½®ä¿¡æ¯
        print(f"   é…ç½®ç±»å‹: {type(config)}")
        print(f"   vector_db_dir: {getattr(config, 'vector_db_dir', 'NOT_FOUND')}")
        
        api_key = get_dashscope_api_key(config.dashscope_api_key)
        
        if not api_key:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return False
        
        print("âœ… é…ç½®è·å–æˆåŠŸ")
        
        # 3. åˆå§‹åŒ–embeddings
        print("ğŸš€ åˆå§‹åŒ–embeddings...")
        # ä½¿ç”¨text embeddingsæ¥åŠ è½½FAISSæ•°æ®åº“
        text_embeddings = DashScopeEmbeddings(
            dashscope_api_key=api_key,
            model='text-embedding-v1'
        )
        print("âœ… text embeddingsåˆå§‹åŒ–æˆåŠŸ")
        
        # 4. åŠ è½½å‘é‡æ•°æ®åº“
        print("ğŸ“š åŠ è½½å‘é‡æ•°æ®åº“...")
        
        # ä½¿ç”¨é…ç½®ä¸­çš„å‘é‡æ•°æ®åº“è·¯å¾„
        vector_db_path = config.vector_db_dir
        
        # è°ƒè¯•è·¯å¾„ä¿¡æ¯
        print(f"   é…ç½®è·¯å¾„: {vector_db_path}")
        print(f"   å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        
        if not os.path.exists(vector_db_path):
            print(f"âŒ å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {vector_db_path}")
            return False
        
        print(f"âœ… å‘é‡æ•°æ®åº“è·¯å¾„å­˜åœ¨: {vector_db_path}")
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        index_file = os.path.join(vector_db_path, "index.faiss")
        pkl_file = os.path.join(vector_db_path, "index.pkl")
        
        if not os.path.exists(index_file) or not os.path.exists(pkl_file):
            print(f"âŒ å‘é‡å­˜å‚¨æ–‡ä»¶ä¸å®Œæ•´: {vector_db_path}")
            print(f"   index.faisså­˜åœ¨: {os.path.exists(index_file)}")
            print(f"   index.pklå­˜åœ¨: {os.path.exists(pkl_file)}")
            return False
        
        try:
            # ä½¿ç”¨æ­£ç¡®çš„åµŒå…¥æ¨¡å‹å’Œè®¾ç½®
            embedding_model = getattr(config, 'text_embedding_model', 'text-embedding-v1')
            allow_dangerous_deserialization = getattr(config, 'allow_dangerous_deserialization', True)
            
            print(f"   ä½¿ç”¨åµŒå…¥æ¨¡å‹: {embedding_model}")
            print(f"   å…è®¸ååºåˆ—åŒ–: {allow_dangerous_deserialization}")
            
            vector_store = FAISS.load_local(
                vector_db_path, 
                text_embeddings,
                allow_dangerous_deserialization=allow_dangerous_deserialization
            )
            print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        # 5. åˆ†ææ•°æ®åº“ç»“æ„
        print("ğŸ” åˆ†ææ•°æ®åº“ç»“æ„...")
        if not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
            print("âŒ å‘é‡æ•°æ®åº“ç»“æ„ä¸ç¬¦åˆé¢„æœŸ")
            return False
        
        docstore_dict = vector_store.docstore._dict
        print(f"âœ… æ•°æ®åº“åŒ…å« {len(docstore_dict)} ä¸ªæ–‡æ¡£")
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„chunks
        chunk_type_stats = {}
        for doc_id, doc in docstore_dict.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            chunk_type = metadata.get('chunk_type', 'unknown')
            chunk_type_stats[chunk_type] = chunk_type_stats.get(chunk_type, 0) + 1
        
        print("ğŸ“Š æ•°æ®åº“chunkç±»å‹ç»Ÿè®¡:")
        for chunk_type, count in chunk_type_stats.items():
            print(f"   - {chunk_type}: {count} ä¸ª")
        
        # 6. æµ‹è¯•filteråŠŸèƒ½
        print("ğŸ” æµ‹è¯•FAISS filteråŠŸèƒ½...")
        
        # æµ‹è¯•1ï¼šè¿‡æ»¤imageç±»å‹
        print("   æµ‹è¯•1ï¼šè¿‡æ»¤imageç±»å‹chunks...")
        try:
            image_filter_results = vector_store.similarity_search(
                "æµ‹è¯•æŸ¥è¯¢", 
                k=50,
                filter={'chunk_type': 'image'}
            )
            
            print(f"   âœ… image filteræœç´¢æˆåŠŸï¼Œè¿”å› {len(image_filter_results)} ä¸ªç»“æœ")
            
            # éªŒè¯ç»“æœæ˜¯å¦éƒ½æ˜¯imageç±»å‹
            image_count = 0
            for doc in image_filter_results:
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                if metadata.get('chunk_type') == 'image':
                    image_count += 1
            
            print(f"   ğŸ“Š ç»“æœéªŒè¯ï¼š{image_count}/{len(image_filter_results)} ä¸ªæ˜¯imageç±»å‹")
            
            if image_count == len(image_filter_results):
                print("   âœ… image filterè¿‡æ»¤å‡†ç¡®")
            else:
                print("   âš ï¸ image filterè¿‡æ»¤ä¸å‡†ç¡®")
                
        except Exception as e:
            print(f"   âŒ image filteræœç´¢å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•2ï¼šè¿‡æ»¤image_textç±»å‹
        print("   æµ‹è¯•2ï¼šè¿‡æ»¤image_textç±»å‹chunks...")
        try:
            image_text_filter_results = vector_store.similarity_search(
                "æµ‹è¯•æŸ¥è¯¢", 
                k=50,
                filter={'chunk_type': 'image_text'}
            )
            
            print(f"   âœ… image_text filteræœç´¢æˆåŠŸï¼Œè¿”å› {len(image_text_filter_results)} ä¸ªç»“æœ")
            
            # éªŒè¯ç»“æœæ˜¯å¦éƒ½æ˜¯image_textç±»å‹
            image_text_count = 0
            for doc in image_text_filter_results:
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                if metadata.get('chunk_type') == 'image_text':
                    image_text_count += 1
            
            print(f"   ğŸ“Š ç»“æœéªŒè¯ï¼š{image_text_count}/{len(image_text_filter_results)} ä¸ªæ˜¯image_textç±»å‹")
            
            if image_text_count == len(image_text_filter_results):
                print("   âœ… image_text filterè¿‡æ»¤å‡†ç¡®")
            else:
                print("   âš ï¸ image_text filterè¿‡æ»¤ä¸å‡†ç¡®")
                
        except Exception as e:
            print(f"   âŒ image_text filteræœç´¢å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•3ï¼šç»„åˆfilter
        print("   æµ‹è¯•3ï¼šç»„åˆfilteræµ‹è¯•...")
        try:
            # æµ‹è¯•å¤šä¸ªæ¡ä»¶çš„filter
            combined_filter_results = vector_store.similarity_search(
                "æµ‹è¯•æŸ¥è¯¢", 
                k=50,
                filter={'chunk_type': 'image', 'document_name': 'ä¸­èŠ¯å›½é™…'}
            )
            
            print(f"   âœ… ç»„åˆfilteræœç´¢æˆåŠŸï¼Œè¿”å› {len(combined_filter_results)} ä¸ªç»“æœ")
            
            # éªŒè¯ç»“æœ
            valid_count = 0
            for doc in combined_filter_results:
                metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
                if (metadata.get('chunk_type') == 'image' and 
                    'ä¸­èŠ¯å›½é™…' in metadata.get('document_name', '')):
                    valid_count += 1
            
            print(f"   ğŸ“Š ç»„åˆfilteréªŒè¯ï¼š{valid_count}/{len(combined_filter_results)} ä¸ªç¬¦åˆæ¡ä»¶")
            
        except Exception as e:
            print(f"   âš ï¸ ç»„åˆfilteræœç´¢å¤±è´¥: {e}")
            print("   è¿™ä¸ä¼šå½±å“åŸºæœ¬åŠŸèƒ½ï¼Œä½†å¯èƒ½å½±å“é«˜çº§æœç´¢")
        
        # 7. æµ‹è¯•è·¨æ¨¡æ€æœç´¢ç¨³å®šæ€§
        print("ğŸ”„ æµ‹è¯•è·¨æ¨¡æ€æœç´¢ç¨³å®šæ€§...")
        
        test_queries = [
            "ä¸­èŠ¯å›½é™…è´¢åŠ¡å›¾è¡¨",
            "åŠå¯¼ä½“åˆ¶é€ å·¥è‰º",
            "æ™¶åœ†ä»£å·¥äº§èƒ½"
        ]
        
        stability_results = []
        for i, query in enumerate(test_queries, 1):
            print(f"   ç¨³å®šæ€§æµ‹è¯• {i}: {query}")
            
            try:
                # ç”ŸæˆæŸ¥è¯¢å‘é‡
                query_embedding = multimodal_embeddings.embed_query(query)
                
                # ä½¿ç”¨filteræœç´¢
                start_time = time.time()
                filter_results = vector_store.similarity_search(
                    query, 
                    k=20,
                    filter={'chunk_type': 'image'}
                )
                search_time = time.time() - start_time
                
                stability_results.append({
                    'query': query,
                    'success': True,
                    'results_count': len(filter_results),
                    'search_time': search_time
                })
                
                print(f"   âœ… æŸ¥è¯¢ {i} æˆåŠŸï¼Œè¿”å› {len(filter_results)} ä¸ªç»“æœï¼Œè€—æ—¶ {search_time:.3f}ç§’")
                
            except Exception as e:
                stability_results.append({
                    'query': query,
                    'success': False,
                    'error': str(e)
                })
                print(f"   âŒ æŸ¥è¯¢ {i} å¤±è´¥: {e}")
        
        # ç»Ÿè®¡ç¨³å®šæ€§
        successful_searches = sum(1 for r in stability_results if r['success'])
        total_searches = len(stability_results)
        success_rate = successful_searches / total_searches if total_searches > 0 else 0
        
        print(f"ï¿½ï¿½ ç¨³å®šæ€§ç»Ÿè®¡ï¼šæˆåŠŸç‡ {success_rate:.1%} ({successful_searches}/{total_searches})")
        
        if success_rate >= 0.8:
            print("âœ… è·¨æ¨¡æ€æœç´¢ç¨³å®šæ€§è‰¯å¥½")
        elif success_rate >= 0.6:
            print("âš ï¸ è·¨æ¨¡æ€æœç´¢ç¨³å®šæ€§ä¸€èˆ¬")
        else:
            print("âŒ è·¨æ¨¡æ€æœç´¢ç¨³å®šæ€§è¾ƒå·®")
        
        # 8. æ€§èƒ½æµ‹è¯•
        print("âš¡ æ€§èƒ½æµ‹è¯•...")
        try:
            # æµ‹è¯•æœç´¢æ€§èƒ½
            performance_queries = ["æµ‹è¯•æŸ¥è¯¢"] * 5
            
            total_time = 0
            for i, query in enumerate(performance_queries, 1):
                start_time = time.time()
                vector_store.similarity_search(
                    query, 
                    k=10,
                    filter={'chunk_type': 'image'}
                )
                query_time = time.time() - start_time
                total_time += query_time
                
                print(f"   æŸ¥è¯¢ {i} è€—æ—¶: {query_time:.3f}ç§’")
            
            avg_time = total_time / len(performance_queries)
            print(f"ğŸ“Š å¹³å‡æŸ¥è¯¢è€—æ—¶: {avg_time:.3f}ç§’")
            
            if avg_time < 1.0:
                print("âœ… æœç´¢æ€§èƒ½ä¼˜ç§€")
            elif avg_time < 3.0:
                print("âœ… æœç´¢æ€§èƒ½è‰¯å¥½")
            else:
                print("âš ï¸ æœç´¢æ€§èƒ½ä¸€èˆ¬")
                
        except Exception as e:
            print(f"   âš ï¸ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        print("\n" + "="*60)
        print("ï¿½ï¿½ æµ‹è¯•2å®Œæˆï¼šFAISS filteråŠŸèƒ½æ”¯æŒéªŒè¯æˆåŠŸï¼")
        print("="*60)
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•2å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_faiss_filter_support()
    if success:
        print("\nâœ… æµ‹è¯•2é€šè¿‡ï¼šFAISS filteråŠŸèƒ½æ­£å¸¸å·¥ä½œï¼Œæ”¯æŒè·¨æ¨¡æ€æœç´¢")
    else:
        print("\nâŒ æµ‹è¯•2å¤±è´¥ï¼šéœ€è¦æ£€æŸ¥FAISSé…ç½®æˆ–æ•°æ®åº“ç»“æ„")