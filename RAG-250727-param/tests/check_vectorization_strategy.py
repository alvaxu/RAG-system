#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡åŒ–ç­–ç•¥æ£€æŸ¥ç¨‹åº

æ£€æŸ¥ç›®æ ‡ï¼š
1. ç¡®è®¤textå’Œimage_textä½¿ç”¨text-embedding-v1å‘é‡åŒ–
2. ç¡®è®¤imageä½¿ç”¨multimodal-embedding-v1å‘é‡åŒ–
3. éªŒè¯FAISSä¸­çš„å‘é‡å­˜å‚¨çŠ¶æ€
4. è¾“å‡ºè¯¦ç»†çš„å‘é‡åŒ–ç­–ç•¥ä¿¡æ¯
"""

import os
import sys
import logging
from typing import List, Dict, Any, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_vectorization_strategy():
    """æ£€æŸ¥å‘é‡åŒ–ç­–ç•¥"""
    print("="*100)
    print("å‘é‡åŒ–ç­–ç•¥æ£€æŸ¥ç¨‹åº")
    print("="*100)
    
    try:
        # 1. å¯¼å…¥å¿…è¦æ¨¡å—
        print("å¯¼å…¥å¿…è¦æ¨¡å—...")
        from langchain_community.vectorstores import FAISS
        from langchain_community.embeddings import DashScopeEmbeddings
        from config.api_key_manager import get_dashscope_api_key
        from config.settings import Settings
        
        print("æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # 2. è·å–é…ç½®
        print("è·å–é…ç½®...")
        try:
            old_cwd = os.getcwd()
            os.chdir(project_root)
            config = Settings.load_from_file('config.json')
            os.chdir(old_cwd)
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return False
        
        # 3. è¾“å‡ºé…ç½®ä¿¡æ¯
        print("\n" + "="*100)
        print("é…ç½®ä¿¡æ¯")
        print("="*100)
        print(f"æ–‡æœ¬åµŒå…¥æ¨¡å‹: {getattr(config, 'text_embedding_model', 'N/A')}")
        print(f"å›¾ç‰‡åµŒå…¥æ¨¡å‹: {getattr(config, 'image_embedding_model', 'N/A')}")
        print(f"å‘é‡ç»´åº¦: {getattr(config, 'vector_dimension', 'N/A')}")
        print(f"å‘é‡æ•°æ®åº“è·¯å¾„: {getattr(config, 'vector_db_dir', 'N/A')}")
        
        # 4. è·å–APIå¯†é’¥
        api_key = get_dashscope_api_key(config.dashscope_api_key)
        if not api_key:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return False
        
        print("é…ç½®è·å–æˆåŠŸ")
        
        # 5. åˆå§‹åŒ–embeddings
        text_embeddings = DashScopeEmbeddings(
            dashscope_api_key=api_key,
            model='text-embedding-v1'
        )
        print("text embeddingsåˆå§‹åŒ–æˆåŠŸ")
        
        # 6. åŠ è½½å‘é‡æ•°æ®åº“
        vector_db_path = config.vector_db_dir
        vector_store = FAISS.load_local(
            vector_db_path, 
            text_embeddings,
            allow_dangerous_deserialization=True
        )
        print("å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        
        # 7. åˆ†æFAISSç´¢å¼•ç»“æ„
        print("\n" + "="*100)
        print("FAISSç´¢å¼•ç»“æ„åˆ†æ")
        print("="*100)
        
        if hasattr(vector_store, 'index') and hasattr(vector_store.index, 'd'):
            print(f"FAISSç´¢å¼•ç»´åº¦: {vector_store.index.d}")
            print(f"FAISSç´¢å¼•å‘é‡æ•°é‡: {vector_store.index.ntotal}")
            print(f"FAISSç´¢å¼•ç±»å‹: {type(vector_store.index)}")
        else:
            print("æ— æ³•è·å–FAISSç´¢å¼•ä¿¡æ¯")
            return False
        
        # 8. åˆ†ææ–‡æ¡£å­˜å‚¨ç»“æ„
        print("\n" + "="*100)
        print("æ–‡æ¡£å­˜å‚¨ç»“æ„åˆ†æ")
        print("="*100)
        
        docstore_dict = vector_store.docstore._dict
        chunk_type_stats = {}
        total_docs = len(docstore_dict)
        
        # ç»Ÿè®¡å„ç±»å‹æ–‡æ¡£æ•°é‡
        for doc_id, doc in docstore_dict.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            chunk_type = metadata.get('chunk_type', 'unknown')
            chunk_type_stats[chunk_type] = chunk_type_stats.get(chunk_type, 0) + 1
        
        print(f"æ€»æ–‡æ¡£æ•°é‡: {total_docs}")
        print("å„ç±»å‹æ–‡æ¡£ç»Ÿè®¡:")
        for chunk_type, count in sorted(chunk_type_stats.items()):
            percentage = (count / total_docs) * 100
            print(f"  {chunk_type}: {count} ({percentage:.1f}%)")
        
        # 9. è¯¦ç»†åˆ†æå„ç±»å‹æ–‡æ¡£çš„å‘é‡åŒ–ç­–ç•¥
        print("\n" + "="*100)
        print("å‘é‡åŒ–ç­–ç•¥è¯¦ç»†åˆ†æ")
        print("="*100)
        
        # 9.1 åˆ†ætextç±»å‹æ–‡æ¡£
        print("\n--- textç±»å‹æ–‡æ¡£åˆ†æ ---")
        text_docs = [(doc_id, doc) for doc_id, doc in docstore_dict.items() 
                     if doc.metadata and doc.metadata.get('chunk_type') == 'text']
        
        if text_docs:
            print(f"æ‰¾åˆ° {len(text_docs)} ä¸ªtextæ–‡æ¡£")
            sample_text_doc = text_docs[0]
            print(f"æ ·æœ¬textæ–‡æ¡£ID: {sample_text_doc[0]}")
            print(f"æ ·æœ¬textæ–‡æ¡£metadata: {sample_text_doc[1].metadata}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡ç›¸å…³ä¿¡æ¯
            if 'semantic_features' in sample_text_doc[1].metadata:
                semantic_features = sample_text_doc[1].metadata['semantic_features']
                print(f"æ ·æœ¬textæ–‡æ¡£semantic_features: {semantic_features}")
            else:
                print("æ ·æœ¬textæ–‡æ¡£æ²¡æœ‰semantic_featuresä¿¡æ¯")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°textç±»å‹æ–‡æ¡£")
        
        # 9.2 åˆ†æimage_textç±»å‹æ–‡æ¡£
        print("\n--- image_textç±»å‹æ–‡æ¡£åˆ†æ ---")
        image_text_docs = [(doc_id, doc) for doc_id, doc in docstore_dict.items() 
                           if doc.metadata and doc.metadata.get('chunk_type') == 'image_text']
        
        if image_text_docs:
            print(f"æ‰¾åˆ° {len(image_text_docs)} ä¸ªimage_textæ–‡æ¡£")
            sample_image_text_doc = image_text_docs[0]
            print(f"æ ·æœ¬image_textæ–‡æ¡£ID: {sample_image_text_doc[0]}")
            print(f"æ ·æœ¬image_textæ–‡æ¡£metadata: {sample_image_text_doc[1].metadata}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡ç›¸å…³ä¿¡æ¯
            if 'semantic_features' in sample_image_text_doc[1].metadata:
                semantic_features = sample_image_text_doc[1].metadata['semantic_features']
                print(f"æ ·æœ¬image_textæ–‡æ¡£semantic_features: {semantic_features}")
            else:
                print("æ ·æœ¬image_textæ–‡æ¡£æ²¡æœ‰semantic_featuresä¿¡æ¯")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°image_textç±»å‹æ–‡æ¡£")
        
        # 9.3 åˆ†æimageç±»å‹æ–‡æ¡£
        print("\n--- imageç±»å‹æ–‡æ¡£åˆ†æ ---")
        image_docs = [(doc_id, doc) for doc_id, doc in docstore_dict.items() 
                      if doc.metadata and doc.metadata.get('chunk_type') == 'image']
        
        if image_docs:
            print(f"æ‰¾åˆ° {len(image_docs)} ä¸ªimageæ–‡æ¡£")
            sample_image_doc = image_docs[0]
            print(f"æ ·æœ¬imageæ–‡æ¡£ID: {sample_image_doc[0]}")
            print(f"æ ·æœ¬imageæ–‡æ¡£metadata: {sample_image_doc[1].metadata}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡ç›¸å…³ä¿¡æ¯
            if 'semantic_features' in sample_image_doc[1].metadata:
                semantic_features = sample_image_doc[1].metadata['semantic_features']
                print(f"æ ·æœ¬imageæ–‡æ¡£semantic_features: {semantic_features}")
            else:
                print("æ ·æœ¬imageæ–‡æ¡£æ²¡æœ‰semantic_featuresä¿¡æ¯")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°imageç±»å‹æ–‡æ¡£")
        
        # 10. éªŒè¯å‘é‡åŒ–ç­–ç•¥
        print("\n" + "="*100)
        print("å‘é‡åŒ–ç­–ç•¥éªŒè¯")
        print("="*100)
        
        # 10.1 éªŒè¯textå’Œimage_textä½¿ç”¨text-embedding-v1
        text_based_docs = text_docs + image_text_docs
        if text_based_docs:
            print(f"textå’Œimage_textç±»å‹æ–‡æ¡£æ€»æ•°: {len(text_based_docs)}")
            
            # æ£€æŸ¥è¿™äº›æ–‡æ¡£çš„å‘é‡æ˜¯å¦ä¸FAISSç´¢å¼•ç»´åº¦åŒ¹é…
            # text-embedding-v1åº”è¯¥ç”Ÿæˆ1536ç»´å‘é‡
            expected_dim = 1536
            if vector_store.index.d == expected_dim:
                print(f"âœ… textå’Œimage_textæ–‡æ¡£å‘é‡ç»´åº¦éªŒè¯é€šè¿‡: {expected_dim}ç»´")
                print("âœ… ç¡®è®¤ä½¿ç”¨text-embedding-v1è¿›è¡Œå‘é‡åŒ–")
            else:
                print(f"âŒ textå’Œimage_textæ–‡æ¡£å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{expected_dim}ï¼Œå®é™…{vector_store.index.d}")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°textå’Œimage_textç±»å‹æ–‡æ¡£")
        
        # 10.2 éªŒè¯imageä½¿ç”¨multimodal-embedding-v1
        if image_docs:
            print(f"imageç±»å‹æ–‡æ¡£æ€»æ•°: {len(image_docs)}")
            
            # æ£€æŸ¥imageæ–‡æ¡£çš„å‘é‡æ˜¯å¦ä¸FAISSç´¢å¼•ç»´åº¦åŒ¹é…
            # multimodal-embedding-v1åº”è¯¥ç”Ÿæˆ1536ç»´å‘é‡ï¼ˆæ ¹æ®æˆ‘ä»¬çš„éªŒè¯ï¼‰
            expected_dim = 1536
            if vector_store.index.d == expected_dim:
                print(f"âœ… imageæ–‡æ¡£å‘é‡ç»´åº¦éªŒè¯é€šè¿‡: {expected_dim}ç»´")
                print("âœ… ç¡®è®¤ä½¿ç”¨multimodal-embedding-v1è¿›è¡Œå‘é‡åŒ–")
            else:
                print(f"âŒ imageæ–‡æ¡£å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{expected_dim}ï¼Œå®é™…{vector_store.index.d}")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°imageç±»å‹æ–‡æ¡£")
        
        # 11. è¾“å‡ºæ€»ç»“æŠ¥å‘Š
        print("\n" + "="*100)
        print("å‘é‡åŒ–ç­–ç•¥æ€»ç»“æŠ¥å‘Š")
        print("="*100)
        
        print("ğŸ“Š æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
        for chunk_type, count in sorted(chunk_type_stats.items()):
            percentage = (count / total_docs) * 100
            print(f"  {chunk_type}: {count} ({percentage:.1f}%)")
        
        print(f"\nğŸ”§ å‘é‡åŒ–ç­–ç•¥:")
        print(f"  text + image_text â†’ text-embedding-v1 (1536ç»´)")
        print(f"  image â†’ multimodal-embedding-v1 (1536ç»´)")
        
        print(f"\nğŸ“ å‘é‡ç»´åº¦ä¿¡æ¯:")
        print(f"  FAISSç´¢å¼•ç»´åº¦: {vector_store.index.d}")
        print(f"  æ€»å‘é‡æ•°é‡: {vector_store.index.ntotal}")
        
        print(f"\nâœ… éªŒè¯ç»“æœ:")
        if vector_store.index.d == 1536:
            print("  æ‰€æœ‰æ–‡æ¡£ç±»å‹éƒ½ä½¿ç”¨1536ç»´å‘é‡ï¼Œå‘é‡åŒ–ç­–ç•¥ä¸€è‡´")
            print("  è·¨æ¨¡æ€æœç´¢å¯ä»¥å®ç°ï¼štextæŸ¥è¯¢ â†’ multimodalå‘é‡ â†’ imageå¬å›")
        else:
            print(f"  å‘é‡ç»´åº¦ä¸ä¸€è‡´ï¼Œéœ€è¦æ£€æŸ¥å‘é‡åŒ–ç­–ç•¥")
        
        print(f"\nğŸ’¡ æŠ€æœ¯è¦ç‚¹:")
        print("  1. textå’Œimage_textä½¿ç”¨ç›¸åŒçš„embeddingæ¨¡å‹ï¼Œè¯­ä¹‰ç©ºé—´ä¸€è‡´")
        print("  2. imageä½¿ç”¨multimodalæ¨¡å‹ï¼Œå¯ä»¥æ¥å—æ–‡æœ¬è¾“å…¥")
        print("  3. æ‰€æœ‰å‘é‡éƒ½å­˜å‚¨åœ¨åŒä¸€ä¸ªFAISSç´¢å¼•ä¸­")
        print("  4. ç­–ç•¥2éœ€è¦å°†æ–‡æœ¬æŸ¥è¯¢è½¬æ¢ä¸ºmultimodalå‘é‡ä»¥å®ç°è·¨æ¨¡æ€æœç´¢")
        
        print("\n" + "="*100)
        print("æ£€æŸ¥å®Œæˆ")
        print("="*100)
        
        return True
        
    except Exception as e:
        print(f"æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = check_vectorization_strategy()
    if success:
        print("\næ£€æŸ¥å®Œæˆ")
    else:
        print("\næ£€æŸ¥å¤±è´¥")
