#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•FAISS FilteråŠŸèƒ½æ”¯æŒæƒ…å†µ

ç›®æ ‡ï¼š
1. éªŒè¯FAISSæ˜¯å¦çœŸæ­£æ”¯æŒfilterå‚æ•°
2. æµ‹è¯•ä¸åŒfilteræ ¼å¼çš„æ•ˆæœ
3. ç¡®è®¤filterå¯¹æœç´¢ç»“æœçš„å½±å“
"""

import os
import sys
import json
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

def test_faiss_filter_support():
    """æµ‹è¯•FAISS FilteråŠŸèƒ½æ”¯æŒ"""
    print("="*80)
    print("æµ‹è¯•FAISS FilteråŠŸèƒ½æ”¯æŒæƒ…å†µ")
    print("="*80)
    
    try:
        # 1. å¯¼å…¥å¿…è¦æ¨¡å—
        print("å¯¼å…¥å¿…è¦æ¨¡å—...")
        from langchain_community.vectorstores import FAISS
        from langchain_community.embeddings import DashScopeEmbeddings
        from config.api_key_manager import get_dashscope_api_key
        from config.settings import Settings
        
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # 2. è·å–é…ç½®å’ŒAPIå¯†é’¥
        print("è·å–é…ç½®å’ŒAPIå¯†é’¥...")
        try:
            old_cwd = os.getcwd()
            os.chdir(project_root)
            config = Settings.load_from_file('config.json')
            os.chdir(old_cwd)
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return False
        
        api_key = get_dashscope_api_key(config.dashscope_api_key)
        if not api_key:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return False
        
        print("âœ… é…ç½®å’ŒAPIå¯†é’¥è·å–æˆåŠŸ")
        
        # 3. åŠ è½½å‘é‡æ•°æ®åº“
        print("åŠ è½½å‘é‡æ•°æ®åº“...")
        try:
            # åˆå§‹åŒ–embeddings
            text_embeddings = DashScopeEmbeddings(
                dashscope_api_key=api_key,
                model='text-embedding-v1'
            )
            
            # åŠ è½½å‘é‡æ•°æ®åº“
            vector_db_path = config.vector_db_dir
            vector_store = FAISS.load_local(
                vector_db_path, 
                text_embeddings,
                allow_dangerous_deserialization=True
            )
            print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(vector_store.docstore._dict)} ä¸ªæ–‡æ¡£")
            
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
            return False
        
        # 4. åˆ†ææ–‡æ¡£ç±»å‹åˆ†å¸ƒ
        print("\nåˆ†ææ–‡æ¡£ç±»å‹åˆ†å¸ƒ...")
        chunk_type_stats = {}
        for doc_id, doc in vector_store.docstore._dict.items():
            if hasattr(doc, 'metadata') and doc.metadata:
                chunk_type = doc.metadata.get('chunk_type', 'unknown')
                chunk_type_stats[chunk_type] = chunk_type_stats.get(chunk_type, 0) + 1
        
        print("æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
        for chunk_type, count in sorted(chunk_type_stats.items()):
            print(f"  {chunk_type}: {count} ä¸ª")
        
        # 5. æµ‹è¯•FilteråŠŸèƒ½
        print("\næµ‹è¯•FilteråŠŸèƒ½...")
        test_query = "ä¸­èŠ¯å›½é™…çš„ä¸»è¦ä¸šåŠ¡"
        
        # æµ‹è¯•1ï¼šæ— filter
        print("\næµ‹è¯•1ï¼šæ— filteræœç´¢")
        try:
            no_filter_results = vector_store.similarity_search(test_query, k=10)
            print(f"  æ— filterç»“æœæ•°é‡: {len(no_filter_results)}")
            
            # åˆ†æç»“æœç±»å‹
            result_types = {}
            for doc in no_filter_results:
                chunk_type = doc.metadata.get('chunk_type', 'unknown') if hasattr(doc, 'metadata') and doc.metadata else 'unknown'
                result_types[chunk_type] = result_types.get(chunk_type, 0) + 1
            
            print(f"  ç»“æœç±»å‹åˆ†å¸ƒ: {result_types}")
            
        except Exception as e:
            print(f"  æ— filteræœç´¢å¤±è´¥: {e}")
        
        # æµ‹è¯•2ï¼šä½¿ç”¨filteræœç´¢textç±»å‹
        print("\næµ‹è¯•2ï¼šä½¿ç”¨filteræœç´¢textç±»å‹")
        try:
            text_filter_results = vector_store.similarity_search(
                test_query, 
                k=10, 
                filter={'chunk_type': 'text'}
            )
            print(f"  text filterç»“æœæ•°é‡: {len(text_filter_results)}")
            
            # éªŒè¯ç»“æœæ˜¯å¦éƒ½æ˜¯textç±»å‹
            all_text = True
            for doc in text_filter_results:
                chunk_type = doc.metadata.get('chunk_type', 'unknown') if hasattr(doc, 'metadata') and doc.metadata else 'unknown'
                if chunk_type != 'text':
                    all_text = False
                    print(f"  å‘ç°étextç±»å‹æ–‡æ¡£: {chunk_type}")
                    break
            
            if all_text:
                print("  âœ… æ‰€æœ‰ç»“æœéƒ½æ˜¯textç±»å‹")
            else:
                print("  âŒ å‘ç°étextç±»å‹ç»“æœ")
                
        except Exception as e:
            print(f"  text filteræœç´¢å¤±è´¥: {e}")
            print(f"  é”™è¯¯ç±»å‹: {type(e)}")
        
        # æµ‹è¯•3ï¼šä½¿ç”¨filteræœç´¢image_textç±»å‹
        print("\næµ‹è¯•3ï¼šä½¿ç”¨filteræœç´¢image_textç±»å‹")
        try:
            image_text_filter_results = vector_store.similarity_search(
                test_query, 
                k=10, 
                filter={'chunk_type': 'image_text'}
            )
            print(f"  image_text filterç»“æœæ•°é‡: {len(image_text_filter_results)}")
            
            # éªŒè¯ç»“æœç±»å‹
            result_types = {}
            for doc in image_text_filter_results:
                chunk_type = doc.metadata.get('chunk_type', 'unknown') if hasattr(doc, 'metadata') and doc.metadata else 'unknown'
                result_types[chunk_type] = result_types.get(chunk_type, 0) + 1
            
            print(f"  ç»“æœç±»å‹åˆ†å¸ƒ: {result_types}")
            
        except Exception as e:
            print(f"  image_text filteræœç´¢å¤±è´¥: {e}")
        
        # æµ‹è¯•4ï¼šæµ‹è¯•filteræ˜¯å¦çœŸçš„ç”Ÿæ•ˆ
        print("\næµ‹è¯•4ï¼šéªŒè¯filteræ˜¯å¦çœŸçš„ç”Ÿæ•ˆ")
        try:
            # æ¯”è¾ƒæœ‰æ— filterçš„ç»“æœå·®å¼‚
            if 'no_filter_results' in locals() and 'text_filter_results' in locals():
                no_filter_count = len(no_filter_results)
                text_filter_count = len(text_filter_results)
                
                print(f"  æ— filterç»“æœæ•°é‡: {no_filter_count}")
                print(f"  text filterç»“æœæ•°é‡: {text_filter_count}")
                
                if text_filter_count < no_filter_count:
                    print("  âœ… Filterç”Ÿæ•ˆï¼šè¿‡æ»¤åç»“æœæ•°é‡å‡å°‘")
                elif text_filter_count == no_filter_count:
                    print("  âš ï¸ Filterå¯èƒ½æœªç”Ÿæ•ˆï¼šç»“æœæ•°é‡ç›¸åŒ")
                else:
                    print("  âŒ Filterå¼‚å¸¸ï¼šè¿‡æ»¤åç»“æœæ•°é‡å¢åŠ ")
                    
                # æ£€æŸ¥ç»“æœå†…å®¹å·®å¼‚
                if text_filter_count > 0:
                    print(f"  ç¬¬ä¸€ä¸ªtext filterç»“æœé¢„è§ˆ: {text_filter_results[0].page_content[:100]}...")
                    
        except Exception as e:
            print(f"  FilteréªŒè¯å¤±è´¥: {e}")
        
        # 6. æ€»ç»“åˆ†æ
        print("\n" + "="*80)
        print("FilteråŠŸèƒ½æ”¯æŒåˆ†ææ€»ç»“")
        print("="*80)
        
        print("ğŸ” åˆ†æç»“æœ:")
        print(f"1. å‘é‡æ•°æ®åº“æ€»æ–‡æ¡£æ•°: {len(vector_store.docstore._dict)}")
        print(f"2. æ–‡æ¡£ç±»å‹åˆ†å¸ƒ: {chunk_type_stats}")
        
        if 'text_filter_results' in locals():
            print("3. âœ… FilteråŠŸèƒ½æ”¯æŒ: æ˜¯")
            print(f"4. textç±»å‹æ–‡æ¡£æ•°é‡: {chunk_type_stats.get('text', 0)}")
            print(f"5. Filteråtextç»“æœæ•°é‡: {len(text_filter_results)}")
        else:
            print("3. âŒ FilteråŠŸèƒ½æ”¯æŒ: å¦")
        
        print("\nğŸ’¡ å»ºè®®:")
        if 'text_filter_results' in locals() and len(text_filter_results) > 0:
            print("  - FilteråŠŸèƒ½æ­£å¸¸å·¥ä½œï¼Œå¯ä»¥ç»§ç»­ä½¿ç”¨ç­–ç•¥1")
            print("  - é—®é¢˜å¯èƒ½åœ¨äºç›¸ä¼¼åº¦è®¡ç®—æˆ–é˜ˆå€¼è®¾ç½®")
        else:
            print("  - FilteråŠŸèƒ½ä¸æ”¯æŒï¼Œéœ€è¦ä¿®æ”¹ç­–ç•¥1")
            print("  - ç›´æ¥ä½¿ç”¨ç­–ç•¥2ï¼ˆpost-filterï¼‰")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_faiss_filter_support()
