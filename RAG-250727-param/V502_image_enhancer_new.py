#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾ç‰‡å¢å¼ºæ–°ç¨‹åº - æŸ¥è¯¢ã€ç¡®è®¤å’Œæ·±åº¦å¤„ç†

åŠŸèƒ½è¯´æ˜ï¼š
1. æŸ¥è¯¢æ•°æ®åº“ä¸­çš„å›¾ç‰‡æ˜¯å¦åšäº†æ·±åº¦å¤„ç†ï¼Œåˆ—å‡ºå“ªäº›æ²¡åš
2. æ ¹æ®ç”¨æˆ·ç¡®è®¤å†³å®šæ˜¯å¦è¡¥åšæ·±åº¦å¤„ç†
3. å¤„ç†æ–¹å¼è¦å’Œä¸»ç¨‹åº image_enhancer.py ä¸€è‡´
4. å­—æ®µå’Œé€»è¾‘ä»¥ç°æœ‰æ•°æ®åº“ç»“æ„å’Œ image_enhancer.py ä¸ºå‡†
5. å¯ä»¥è°ƒç”¨ image_enhancer.py ä¸­çš„ç›¸åº”æ¨¡å—
"""

import os
import sys
import json
import time
import logging
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

try:
    from langchain_community.vectorstores import FAISS
    from langchain_community.embeddings import DashScopeEmbeddings
    from config.settings import Settings
    from document_processing.image_enhancer import ImageEnhancer
    from document_processing.vector_generator import VectorGenerator
    from document_processing.image_processor import ImageProcessor
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®ä¾èµ–å·²æ­£ç¡®å®‰è£…")
    sys.exit(1)

# å¯¼å…¥ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—
from config.api_key_manager import get_dashscope_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ImageEnhancerNew:
    """
    å›¾ç‰‡å¢å¼ºæ–°ç¨‹åºï¼Œæ•´åˆå›¾ç‰‡å¤„ç†å’Œå‘é‡å­˜å‚¨åŠŸèƒ½
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç¨‹åº"""
        try:
            # åŠ è½½é…ç½®
            self.config = Settings.load_from_file('config.json')
            
            # ä½¿ç”¨ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—è·å–APIå¯†é’¥
            config_key = self.config.dashscope_api_key
            self.api_key = get_dashscope_api_key(config_key)
            
            if not self.api_key:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            
            # ä½¿ç”¨ä¸»ç¨‹åºä¸­çš„ImageProcessoræ¥ç®¡ç†é…ç½®å’Œåˆå§‹åŒ–
            self.image_processor = ImageProcessor(self.api_key, self.config.__dict__)
            
            # åˆå§‹åŒ–å›¾åƒå¢å¼ºå™¨ï¼ˆé€šè¿‡ImageProcessorï¼‰
            if self.image_processor.enhancement_enabled:
                self.image_enhancer = self.image_processor.image_enhancer
            else:
                # å¦‚æœImageProcessorä¸­æ²¡æœ‰å¯ç”¨ï¼Œåˆ™æ‰‹åŠ¨åˆå§‹åŒ–
                image_config = {
                    'enable_enhancement': self.config.enable_enhancement,
                    'enhancement_model': self.config.enhancement_model,
                    'enhancement_max_tokens': self.config.enhancement_max_tokens,
                    'enhancement_temperature': self.config.enhancement_temperature,
                    'enhancement_batch_size': self.config.enhancement_batch_size,
                    'enable_progress_logging': self.config.enable_progress_logging
                }
                self.image_enhancer = ImageEnhancer(
                    api_key=self.api_key,
                    config=image_config
                )
            
            # åˆå§‹åŒ–å‘é‡ç”Ÿæˆå™¨ï¼ˆç”¨äºä¿å­˜æ•°æ®åº“ï¼‰
            self.vector_generator = VectorGenerator(self.config.__dict__)
            
            # å‘é‡æ•°æ®åº“è·¯å¾„ - ä½¿ç”¨é…ç½®ç®¡ç†
            self.vector_db_path = self.config.vector_db_dir
            
            logger.info("å›¾ç‰‡å¢å¼ºæ–°ç¨‹åºåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def load_vector_store(self) -> Optional[FAISS]:
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        try:
            # ä½¿ç”¨é…ç½®ä¸­çš„åµŒå…¥æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
            embedding_model = getattr(self.config, 'text_embedding_model', 'text-embedding-v1')
            embeddings = DashScopeEmbeddings(
                dashscope_api_key=self.api_key, 
                model=embedding_model
            )
            # ä½¿ç”¨é…ç½®ä¸­çš„å®‰å…¨è®¾ç½®ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
            allow_dangerous_deserialization = getattr(self.config, 'allow_dangerous_deserialization', True)
            vector_store = FAISS.load_local(
                self.vector_db_path, 
                embeddings, 
                allow_dangerous_deserialization=allow_dangerous_deserialization
            )
            logger.info(f"å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(vector_store.docstore._dict)} ä¸ªæ–‡æ¡£")
            return vector_store
            
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return None
    
    def query_image_status(self) -> Dict[str, List[Dict[str, Any]]]:
        """æŸ¥è¯¢æ•°æ®åº“ä¸­å›¾ç‰‡çš„å¤„ç†çŠ¶æ€ï¼ŒåŒ…æ‹¬å‘é‡åŒ–çŠ¶æ€"""
        print("ğŸ” æ­£åœ¨æŸ¥è¯¢æ•°æ®åº“ä¸­çš„å›¾ç‰‡çŠ¶æ€...")
        
        vector_store = self.load_vector_store()
        if not vector_store:
            return {'processed': [], 'unprocessed': [], 'vectorized': [], 'unvectorized': []}
        
        processed_images = []
        unprocessed_images = []
        vectorized_images = []
        unvectorized_images = []
        
        # å…ˆæ”¶é›†æ‰€æœ‰image_text chunkçš„image_idï¼Œç”¨äºå¿«é€Ÿåˆ¤æ–­
        vectorized_image_ids = set()
        for doc_id, doc in vector_store.docstore._dict.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            if metadata.get('chunk_type') == 'image_text':
                related_image_id = metadata.get('related_image_id')
                if related_image_id:
                    vectorized_image_ids.add(related_image_id)
        
        for doc_id, doc in vector_store.docstore._dict.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡æ¡£
            if metadata.get('chunk_type') == 'image':
                image_path = metadata.get('image_path', '')
                image_type = 'unknown'
                image_id = metadata.get('image_id', 'unknown')
                
                # ä½¿ç”¨ä¸»ç¨‹åºä¸­çš„å‡½æ•°æ£€æµ‹å›¾ç‰‡ç±»å‹
                if image_path and os.path.exists(image_path):
                    try:
                        image_type = self._detect_image_type(image_path)
                    except Exception as e:
                        logger.warning(f"æ£€æµ‹å›¾ç‰‡ç±»å‹å¤±è´¥: {e}")
                
                # ä½¿ç”¨æ–°çš„ç®€å•é€»è¾‘åˆ¤æ–­æ˜¯å¦å·²å‘é‡åŒ–
                is_vectorized = image_id in vectorized_image_ids
                
                image_info = {
                    'doc_id': doc_id,
                    'image_path': image_path,
                    'document_name': metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£'),
                    'page_number': metadata.get('page_number', 1),
                    'image_id': image_id,
                    'image_type': image_type,
                    'enhanced_description': metadata.get('enhanced_description', ''),
                    'has_layered': 'layered_descriptions' in metadata,
                    'has_structured': 'structured_info' in metadata,
                    'has_timestamp': 'enhancement_timestamp' in metadata,
                    'has_enabled': 'enhancement_enabled' in metadata,
                    'is_vectorized': is_vectorized  # ä½¿ç”¨æ–°çš„åˆ¤æ–­é€»è¾‘
                }
                
                # åˆ¤æ–­æ˜¯å¦å·²æ·±åº¦å¤„ç†
                if self._is_deep_processed(metadata):
                    processed_images.append(image_info)
                else:
                    unprocessed_images.append(image_info)
                
                # åˆ¤æ–­æ˜¯å¦å·²å‘é‡åŒ–
                if is_vectorized:
                    vectorized_images.append(image_info)
                else:
                    unvectorized_images.append(image_info)
        
        print(f"ğŸ“Š æŸ¥è¯¢å®Œæˆ:")
        print(f"   âœ… å·²æ·±åº¦å¤„ç†: {len(processed_images)} å¼ ")
        print(f"   â³ æœªæ·±åº¦å¤„ç†: {len(unprocessed_images)} å¼ ")
        print(f"   ğŸ”¤ å·²å‘é‡åŒ–: {len(vectorized_images)} å¼ ")
        print(f"   ğŸ“ æœªå‘é‡åŒ–: {len(unvectorized_images)} å¼ ")
        
        return {
            'processed': processed_images,
            'unprocessed': unprocessed_images,
            'vectorized': vectorized_images,
            'unvectorized': unvectorized_images
        }
    
    def _is_deep_processed(self, metadata: Dict[str, Any]) -> bool:
        """åˆ¤æ–­å›¾ç‰‡æ˜¯å¦å·²æ·±åº¦å¤„ç†"""
        # ä¸»è¦æ£€æŸ¥enhanced_descriptionæ˜¯å¦åŒ…å«æ·±åº¦å¤„ç†æ ‡æ³¨
        enhanced_desc = metadata.get('enhanced_description', '')
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸»ç¨‹åºæ·»åŠ çš„æ·±åº¦å¤„ç†æ ‡æ³¨ - ä½¿ç”¨é…ç½®ç®¡ç†
        depth_markers = getattr(self.config, 'depth_processing_markers', [
            'åŸºç¡€è§†è§‰æè¿°:', 'å†…å®¹ç†è§£æè¿°:', 'æ•°æ®è¶‹åŠ¿æè¿°:', 'è¯­ä¹‰ç‰¹å¾æè¿°:',
            'chart_type:', 'data_points:', 'trends:', 'key_insights:',
            'enhancement_enabled'
        ])
        
        has_depth_markers = any(marker in enhanced_desc for marker in depth_markers)
        
        # å¦‚æœenhanced_descriptionåŒ…å«æ·±åº¦æ ‡è®°ï¼Œè¯´æ˜å·²æ·±åº¦å¤„ç†
        if has_depth_markers:
            return True
        
        # å¤‡ç”¨æ£€æŸ¥ï¼šå…ƒæ•°æ®å­—æ®µï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
        has_layered = 'layered_descriptions' in metadata
        has_structured = 'structured_info' in metadata
        
        if has_layered and has_structured:
            return True
        
        # å¦‚æœéƒ½ä¸æ»¡è¶³ï¼Œè¯´æ˜æœªæ·±åº¦å¤„ç†
        return False
    
    def _detect_image_type(self, image_path: str) -> str:
        """æ£€æµ‹å›¾ç‰‡ç±»å‹ - ä½¿ç”¨ä¸»ç¨‹åºä¸­çš„å‡½æ•°"""
        try:
            return self.image_processor._detect_image_type(image_path)
        except Exception as e:
            logger.warning(f"æ£€æµ‹å›¾ç‰‡ç±»å‹å¤±è´¥: {e}")
            return 'general'
    
    def _extract_semantic_features(self, image_path: str) -> Dict[str, Any]:
        """æå–å›¾ç‰‡è¯­ä¹‰ç‰¹å¾ - ä½¿ç”¨ä¸»ç¨‹åºä¸­çš„å‡½æ•°"""
        try:
            # è¿™é‡Œå¯ä»¥è°ƒç”¨ä¸»ç¨‹åºä¸­çš„è¯­ä¹‰ç‰¹å¾æå–å‡½æ•°
            # å¦‚æœéœ€è¦ç”Ÿæˆembeddingï¼Œå¯ä»¥ä½¿ç”¨ self.image_processor.generate_image_embedding(image_path)
            # ç„¶åè°ƒç”¨è¯­ä¹‰ç‰¹å¾æå–
            return {}
        except Exception as e:
            logger.warning(f"æå–è¯­ä¹‰ç‰¹å¾å¤±è´¥: {e}")
            return {}
    
    def display_image_status(self, image_status: Dict[str, List[Dict[str, Any]]]):
        """æ˜¾ç¤ºå›¾ç‰‡çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‘é‡åŒ–çŠ¶æ€"""
        print("\n" + "="*80)
        print("ğŸ“Š å›¾ç‰‡å¤„ç†çŠ¶æ€è¯¦æƒ…")
        print("="*80)
        
        # æ˜¾ç¤ºå·²å¤„ç†çš„å›¾ç‰‡
        processed = image_status['processed']
        if processed:
            print(f"\nâœ… å·²æ·±åº¦å¤„ç†çš„å›¾ç‰‡ ({len(processed)} å¼ ):")
            for i, img in enumerate(processed[:5], 1):
                image_type_info = f" [{img.get('image_type', 'unknown')}]" if img.get('image_type') != 'unknown' else ""
                vectorized_info = " ğŸ”¤" if img.get('is_vectorized', False) else " ğŸ“"
                print(f"   {i}. {img['document_name']} - ç¬¬{img['page_number']}é¡µ - {img['image_id']}{image_type_info}{vectorized_info}")
            if len(processed) > 5:
                print(f"   ... è¿˜æœ‰ {len(processed) - 5} å¼ å·²å¤„ç†å›¾ç‰‡")
        else:
            print("\nâœ… å·²æ·±åº¦å¤„ç†çš„å›¾ç‰‡: 0 å¼ ")
        
        # æ˜¾ç¤ºæœªå¤„ç†çš„å›¾ç‰‡
        unprocessed = image_status['unprocessed']
        if unprocessed:
            print(f"\nâ³ æœªæ·±åº¦å¤„ç†çš„å›¾ç‰‡ ({len(unprocessed)} å¼ ):")
            for i, img in enumerate(unprocessed[:10], 1):
                image_type_info = f" [{img.get('image_type', 'unknown')}]" if img.get('image_type') != 'unknown' else ""
                print(f"   {i}. {img['document_name']} - ç¬¬{img['page_number']}é¡µ - {img['image_id']}{image_type_info}")
            if len(unprocessed) > 10:
                print(f"   ... è¿˜æœ‰ {len(unprocessed) - 10} å¼ æœªå¤„ç†å›¾ç‰‡")
        else:
            print("\nâ³ æœªæ·±åº¦å¤„ç†çš„å›¾ç‰‡: 0 å¼ ")
        
        # æ˜¾ç¤ºå‘é‡åŒ–çŠ¶æ€
        vectorized = image_status['vectorized']
        unvectorized = image_status['unvectorized']
        
        if vectorized:
            print(f"\nğŸ”¤ å·²å‘é‡åŒ–çš„å›¾ç‰‡ ({len(vectorized)} å¼ ):")
            for i, img in enumerate(vectorized[:5], 1):
                print(f"   {i}. {img['document_name']} - ç¬¬{img['page_number']}é¡µ - {img['image_id']}")
            if len(vectorized) > 5:
                print(f"   ... è¿˜æœ‰ {len(vectorized) - 5} å¼ å·²å‘é‡åŒ–å›¾ç‰‡")
        else:
            print("\nğŸ”¤ å·²å‘é‡åŒ–çš„å›¾ç‰‡: 0 å¼ ")
        
        if unvectorized:
            print(f"\nğŸ“ æœªå‘é‡åŒ–çš„å›¾ç‰‡ ({len(unvectorized)} å¼ ):")
            for i, img in enumerate(unvectorized[:10], 1):
                print(f"   {i}. {img['document_name']} - ç¬¬{img['page_number']}é¡µ - {img['image_id']}")
            if len(unvectorized) > 10:
                print(f"   ... è¿˜æœ‰ {len(unvectorized) - 10} å¼ æœªå‘é‡åŒ–å›¾ç‰‡")
        else:
            print("\nğŸ“ æœªå‘é‡åŒ–çš„å›¾ç‰‡: 0 å¼ ")
        
        print("\n" + "="*80)
    
    def get_user_confirmation(self, image_status: Dict[str, List[Dict[str, Any]]]) -> Dict[str, bool]:
        """è·å–ç”¨æˆ·ç¡®è®¤æ˜¯å¦è¿›è¡Œæ·±åº¦å¤„ç†å’Œå‘é‡åŒ–"""
        unprocessed_count = len(image_status['unprocessed'])
        unvectorized_count = len(image_status['unvectorized'])
        
        if unprocessed_count == 0 and unvectorized_count == 0:
            print("ğŸ‰ æ‰€æœ‰å›¾ç‰‡éƒ½å·²å¤„ç†å®Œæˆï¼")
            return {'process': False, 'vectorize': False}
        
        print(f"\nâ“ å‘ç°å¤„ç†éœ€æ±‚:")
        if unprocessed_count > 0:
            print(f"   - {unprocessed_count} å¼ å›¾ç‰‡æœªæ·±åº¦å¤„ç†")
        if unvectorized_count > 0:
            print(f"   - {unvectorized_count} å¼ å›¾ç‰‡æœªå‘é‡åŒ–")
        
        # æ ¹æ®å®é™…éœ€æ±‚åŠ¨æ€æ˜¾ç¤ºé€‰é¡¹
        available_options = []
        
        if unprocessed_count > 0:
            available_options.append("1. è¿›è¡Œæ·±åº¦å¤„ç†")
        if unvectorized_count > 0:
            available_options.append("2. è¿›è¡Œå‘é‡åŒ–")
        if unprocessed_count > 0 and unvectorized_count > 0:
            available_options.append("3. åŒæ—¶è¿›è¡Œæ·±åº¦å¤„ç†å’Œå‘é‡åŒ–")
        
        available_options.append("4. é€€å‡ºç¨‹åº")
        
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        for option in available_options:
            print(f"   {option}")
        
        # æ„å»ºé€‰é¡¹æ˜ å°„
        option_map = {}
        option_num = 1
        
        if unprocessed_count > 0:
            option_map['1'] = {'process': True, 'vectorize': False}
            option_num += 1
        if unvectorized_count > 0:
            option_map['2'] = {'process': False, 'vectorize': True}
            option_num += 1
        if unprocessed_count > 0 and unvectorized_count > 0:
            option_map['3'] = {'process': True, 'vectorize': True}
            option_num += 1
        
        option_map['4'] = {'process': False, 'vectorize': False}
        
        while True:
            try:
                choice = input(f"\nè¯·è¾“å…¥é€‰æ‹© (1-{len(available_options)}): ").strip()
                if choice in option_map:
                    return option_map[choice]
                else:
                    print(f"âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-{len(available_options)} ä¹‹é—´çš„æ•°å­—")
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
                return {'process': False, 'vectorize': False}
    
    def process_unprocessed_images(self, unprocessed_images: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†æœªæ·±åº¦å¤„ç†çš„å›¾ç‰‡"""
        print(f"\nğŸ”„ å¼€å§‹æ·±åº¦å¤„ç† {len(unprocessed_images)} å¼ å›¾ç‰‡...")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        vector_store = self.load_vector_store()
        if not vector_store:
            print("âŒ æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“ï¼Œå¤„ç†ç»ˆæ­¢")
            return []
        
        # å‡†å¤‡æ‰¹é‡å¤„ç†çš„æ•°æ®æ ¼å¼
        image_batch = []
        for img_info in unprocessed_images:
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            image_path = img_info['image_path']
            if not os.path.exists(image_path):
                print(f"âš ï¸ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                continue
            
            # ä½¿ç”¨ä¸»ç¨‹åºä¸­çš„å‡½æ•°æ£€æµ‹å›¾ç‰‡ç±»å‹å’Œç”Ÿæˆå›¾ç‰‡ID
            image_type = self._detect_image_type(image_path)
            if not img_info.get('image_id') or img_info['image_id'] == 'unknown':
                try:
                    generated_image_id = self.image_processor._generate_image_id(image_path)
                    img_info['image_id'] = generated_image_id
                except Exception as e:
                    logger.warning(f"ç”Ÿæˆå›¾ç‰‡IDå¤±è´¥: {e}")
                
            image_batch.append({
                'image_path': image_path,
                'enhanced_description': img_info.get('enhanced_description', ''),
                'doc_id': img_info['doc_id'],
                'document_name': img_info['document_name'],
                'page_number': img_info['page_number'],
                'image_id': img_info['image_id'],
                'image_type': image_type
            })
        
        if not image_batch:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶å¯ä»¥å¤„ç†")
            return []
        
        # è°ƒç”¨ä¸»ç¨‹åºä¸­çš„æ‰¹é‡å¤„ç†å‡½æ•°
        print(f"ğŸ“· å‡†å¤‡å¤„ç† {len(image_batch)} å¼ æœ‰æ•ˆå›¾ç‰‡...")
        enhanced_results = self.image_enhancer.enhance_batch_images(image_batch)
        
        # å¤„ç†ç»“æœå¹¶æ›´æ–°æ•°æ®åº“
        results = []
        for i, (img_info, enhanced_info) in enumerate(zip(image_batch, enhanced_results), 1):
            try:
                print(f"\nğŸ“· å¤„ç†è¿›åº¦: {i}/{len(image_batch)}")
                print(f"   æ–‡æ¡£: {img_info['document_name']}")
                print(f"   é¡µç : {img_info['page_number']}")
                print(f"   å›¾ç‰‡ID: {img_info['image_id']}")
                
                # æ›´æ–°å‘é‡æ•°æ®åº“ä¸­çš„å…ƒæ•°æ®
                if enhanced_info and 'enhanced_description' in enhanced_info:
                    try:
                        # è·å–æ–‡æ¡£å¯¹è±¡
                        doc = vector_store.docstore._dict[img_info['doc_id']]
                        
                        # æ›´æ–°å…ƒæ•°æ®
                        updated_metadata = doc.metadata.copy()
                        updated_metadata['enhanced_description'] = enhanced_info['enhanced_description']
                        updated_metadata['enhancement_timestamp'] = int(time.time())
                        updated_metadata['enhancement_enabled'] = True
                        
                        # å¦‚æœæœ‰åˆ†å±‚æè¿°ï¼Œä¹Ÿä¿å­˜
                        if 'layered_descriptions' in enhanced_info:
                            updated_metadata['layered_descriptions'] = enhanced_info['layered_descriptions']
                        
                        # å¦‚æœæœ‰ç»“æ„åŒ–ä¿¡æ¯ï¼Œä¹Ÿä¿å­˜
                        if 'structured_info' in enhanced_info:
                            updated_metadata['structured_info'] = enhanced_info['structured_info']
                        
                        # æ·»åŠ å›¾ç‰‡ç±»å‹ä¿¡æ¯ï¼ˆä½¿ç”¨ä¸»ç¨‹åºä¸­çš„æ£€æµ‹ç»“æœï¼‰
                        if 'image_type' in img_info:
                            updated_metadata['image_type'] = img_info['image_type']
                        
                        # æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®
                        doc.metadata = updated_metadata
                        
                        print(f"   âœ… æ•°æ®åº“æ›´æ–°å®Œæˆ")
                        
                    except Exception as e:
                        print(f"   âš ï¸ æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}")
                        enhanced_info = {'enhanced_description': img_info['enhanced_description']}
                
                # è®°å½•å¤„ç†ç»“æœ
                results.append({
                    'doc_id': img_info['doc_id'],
                    'status': 'success',
                    'enhanced_info': enhanced_info,
                    'image_path': img_info['image_path']
                })
                
                print(f"   âœ… å¤„ç†å®Œæˆ")
                    
            except Exception as e:
                logger.error(f"å¤„ç†å›¾ç‰‡å¤±è´¥ {img_info['image_id']}: {e}")
                results.append({
                    'doc_id': img_info['doc_id'],
                    'status': 'failed',
                    'error': str(e),
                    'image_path': img_info['image_path']
                })
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
        
        # ä¿å­˜æ›´æ–°åçš„å‘é‡æ•°æ®åº“
        try:
            print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ›´æ–°åçš„å‘é‡æ•°æ®åº“...")
            # ä½¿ç”¨ä¸»ç¨‹åºä¸­çš„ä¿å­˜å‡½æ•°ï¼Œä¿æŒä¸€è‡´æ€§
            self.vector_generator._save_vector_store_with_metadata(vector_store, self.vector_db_path)
            print(f"âœ… å‘é‡æ•°æ®åº“ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ä¿å­˜å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            logger.error(f"ä¿å­˜å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
        
        print(f"\nâœ… æ·±åº¦å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(image_batch)} å¼ å›¾ç‰‡")
        return results
    
    def display_processing_results(self, results: List[Dict[str, Any]]):
        """æ˜¾ç¤ºå¤„ç†ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ“Š æ·±åº¦å¤„ç†ç»“æœç»Ÿè®¡")
        print("="*80)
        
        success_count = len([r for r in results if r['status'] == 'success'])
        failed_count = len([r for r in results if r['status'] == 'failed'])
        
        print(f"âœ… æˆåŠŸå¤„ç†: {success_count} å¼ ")
        print(f"âŒ å¤„ç†å¤±è´¥: {failed_count} å¼ ")
        print(f"ğŸ“Š æˆåŠŸç‡: {success_count/(success_count+failed_count)*100:.1f}%")
        
        if failed_count > 0:
            print(f"\nâŒ å¤±è´¥è¯¦æƒ…:")
            for result in results:
                if result['status'] == 'failed':
                    print(f"   - {result['image_path']}: {result['error']}")
        
        print("\n" + "="*80)
    
    def process_unvectorized_images(self, unvectorized_images: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†æœªå‘é‡åŒ–çš„å›¾ç‰‡ï¼Œè¿›è¡Œenhanced_descriptionå‘é‡åŒ–"""
        print(f"\nğŸ”¤ å¼€å§‹å‘é‡åŒ–å¤„ç† {len(unvectorized_images)} å¼ å›¾ç‰‡...")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        vector_store = self.load_vector_store()
        if not vector_store:
            print("âŒ æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“ï¼Œå¤„ç†ç»ˆæ­¢")
            return []
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å‘é‡åŒ–
        enable_vectorization = getattr(self.config, 'enable_enhanced_description_vectorization', False)
        if not enable_vectorization:
            print("âš ï¸ æœªå¯ç”¨enhanced_descriptionå‘é‡åŒ–åŠŸèƒ½ï¼Œè¯·åœ¨config.jsonä¸­è®¾ç½®enable_enhanced_description_vectorization: true")
            return []
        
        # å‡†å¤‡æ‰¹é‡å¤„ç†çš„æ•°æ®æ ¼å¼
        image_batch = []
        for img_info in unvectorized_images:
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            image_path = img_info['image_path']
            if not os.path.exists(image_path):
                print(f"âš ï¸ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰enhanced_description
            if not img_info.get('enhanced_description'):
                print(f"âš ï¸ å›¾ç‰‡ {img_info['image_id']} æ²¡æœ‰enhanced_descriptionï¼Œè·³è¿‡å‘é‡åŒ–")
                continue
            
            image_batch.append({
                'image_path': image_path,
                'enhanced_description': img_info['enhanced_description'],
                'doc_id': img_info['doc_id'],
                'document_name': img_info['document_name'],
                'page_number': img_info['page_number'],
                'image_id': img_info['image_id'],
                'image_type': img_info.get('image_type', 'general')
            })
        
        if not image_batch:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡å¯ä»¥å‘é‡åŒ–")
            return []
        
        # è°ƒç”¨vector_generatorçš„æ–¹æ³•è¿›è¡Œå‘é‡åŒ–
        print(f"ğŸ”¤ å‡†å¤‡å‘é‡åŒ– {len(image_batch)} å¼ æœ‰æ•ˆå›¾ç‰‡...")
        results = []
        
        # å‡†å¤‡æ–°çš„text documents
        new_text_documents = []
        
        for i, img_info in enumerate(image_batch, 1):
            try:
                print(f"\nğŸ”¤ å‘é‡åŒ–è¿›åº¦: {i}/{len(image_batch)}")
                print(f"   æ–‡æ¡£: {img_info['document_name']}")
                print(f"   é¡µç : {img_info['page_number']}")
                print(f"   å›¾ç‰‡ID: {img_info['image_id']}")
                
                # è°ƒç”¨image_enhancerè¿›è¡Œå‘é‡åŒ–
                enhanced_info = self.image_enhancer.enhance_image_description(
                    img_info['image_path'], 
                    img_info['enhanced_description']
                )
                
                if enhanced_info and 'enhanced_description' in enhanced_info:
                    try:
                        # è·å–æ–‡æ¡£å¯¹è±¡
                        doc = vector_store.docstore._dict[img_info['doc_id']]
                        
                        # æ³¨æ„ï¼šä¸å†éœ€è¦æ›´æ–°image chunkçš„metadataæ ‡è®°
                        # å› ä¸ºå‘é‡åŒ–çŠ¶æ€ç°åœ¨é€šè¿‡æ˜¯å¦å­˜åœ¨image_text chunkæ¥åˆ¤æ–­
                        # åªéœ€è¦åˆ›å»ºæ–°çš„image_text chunkå³å¯
                        
                        # åˆ›å»ºæ–°çš„image_text Documentå¯¹è±¡
                        from langchain.schema import Document
                        text_doc = Document(
                            page_content=img_info["enhanced_description"],
                            metadata={
                                "chunk_type": "image_text",  # ä¸“é—¨çš„image_textç±»å‹
                                "source_type": "image_description",
                                "image_id": img_info['image_id'],
                                "document_name": img_info['document_name'],
                                "page_number": img_info['page_number'],
                                "enhanced_description": img_info["enhanced_description"],
                                "related_image_id": img_info['image_id'],
                                "page_idx": img_info.get('page_idx', 0),
                                "img_caption": img_info.get('img_caption', []),
                                "img_footnote": img_info.get('img_footnote', [])
                            }
                        )
                        new_text_documents.append(text_doc)
                        
                        print(f"   âœ… å‘é‡åŒ–å®Œæˆï¼Œå·²åˆ›å»ºimage_text chunk")
                        
                        # è®°å½•å¤„ç†ç»“æœ
                        results.append({
                            'doc_id': img_info['doc_id'],
                            'status': 'success',
                            'enhanced_info': enhanced_info,
                            'image_path': img_info['image_path']
                        })
                        
                    except Exception as e:
                        print(f"   âš ï¸ æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}")
                        results.append({
                            'doc_id': img_info['doc_id'],
                            'status': 'failed',
                            'error': f"æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}",
                            'image_path': img_info['image_path']
                        })
                else:
                    print(f"   âŒ å‘é‡åŒ–å¤±è´¥ï¼Œæœªç”Ÿæˆenhanced_description")
                    results.append({
                        'doc_id': img_info['doc_id'],
                        'status': 'failed',
                        'error': 'å‘é‡åŒ–å¤±è´¥ï¼Œæœªç”Ÿæˆenhanced_description',
                        'image_path': img_info['image_path']
                    })
                    
            except Exception as e:
                logger.error(f"å‘é‡åŒ–å›¾ç‰‡å¤±è´¥ {img_info['image_id']}: {e}")
                results.append({
                    'doc_id': img_info['doc_id'],
                    'status': 'failed',
                    'error': str(e),
                    'image_path': img_info['image_path']
                })
                print(f"   âŒ å‘é‡åŒ–å¤±è´¥: {e}")
        
        # æ·»åŠ æ–°çš„text documentsåˆ°FAISSç´¢å¼•
        if new_text_documents:
            try:
                print(f"\nğŸ”¤ å¼€å§‹æ·»åŠ  {len(new_text_documents)} ä¸ªæ–°çš„æ–‡æœ¬å‘é‡åˆ°FAISSç´¢å¼•...")
                
                # ä½¿ç”¨text-embedding-v1ç”Ÿæˆæ–‡æœ¬å‘é‡
                texts = [doc.page_content for doc in new_text_documents]
                text_embeddings_list = self.vector_generator.embeddings.embed_documents(texts)
                
                # å‡†å¤‡æ–‡æœ¬å‘é‡å¯¹
                text_embedding_pairs = []
                text_metadatas = []
                
                for i, doc in enumerate(new_text_documents):
                    text_embedding_pairs.append((doc.page_content, text_embeddings_list[i]))
                    text_metadatas.append(doc.metadata)
                
                # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
                vector_store.add_embeddings(text_embedding_pairs, metadatas=text_metadatas)
                
                print(f"âœ… æˆåŠŸæ·»åŠ  {len(new_text_documents)} ä¸ªæ–°çš„æ–‡æœ¬å‘é‡åˆ°FAISSç´¢å¼•")
                
            except Exception as e:
                print(f"âŒ æ·»åŠ æ–°æ–‡æœ¬å‘é‡åˆ°FAISSç´¢å¼•å¤±è´¥: {e}")
                logger.error(f"æ·»åŠ æ–°æ–‡æœ¬å‘é‡åˆ°FAISSç´¢å¼•å¤±è´¥: {e}")
        
        # ä¿å­˜æ›´æ–°åçš„å‘é‡æ•°æ®åº“
        try:
            print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ›´æ–°åçš„å‘é‡æ•°æ®åº“...")
            self.vector_generator._save_vector_store_with_metadata(vector_store, self.vector_db_path)
            print(f"âœ… å‘é‡æ•°æ®åº“ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ä¿å­˜å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            logger.error(f"ä¿å­˜å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
        
        print(f"\nğŸ”¤ å‘é‡åŒ–å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(image_batch)} å¼ å›¾ç‰‡")
        if new_text_documents:
            print(f"ğŸ”¤ æ–°å¢ {len(new_text_documents)} ä¸ªæ–‡æœ¬å‘é‡åˆ°æ•°æ®åº“")
        return results
    
    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        try:
            print("ğŸš€ å›¾ç‰‡å¢å¼ºæ–°ç¨‹åºå¯åŠ¨")
            print("="*50)
            
            # 1. æŸ¥è¯¢å›¾ç‰‡çŠ¶æ€
            image_status = self.query_image_status()
            
            # 2. æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
            self.display_image_status(image_status)
            
            # 3. è·å–ç”¨æˆ·ç¡®è®¤
            user_choices = self.get_user_confirmation(image_status)
            
            # 4. è¿›è¡Œæ·±åº¦å¤„ç†
            if user_choices['process']:
                print("\nğŸ”„ å¼€å§‹æ·±åº¦å¤„ç†...")
                results = self.process_unprocessed_images(image_status['unprocessed'])
                self.display_processing_results(results)
            
            # 5. è¿›è¡Œå‘é‡åŒ–
            if user_choices['vectorize']:
                print("\nğŸ”¤ å¼€å§‹å‘é‡åŒ–å¤„ç†...")
                # é‡æ–°æŸ¥è¯¢çŠ¶æ€ï¼Œå› ä¸ºæ·±åº¦å¤„ç†åå¯èƒ½ä¼šæœ‰æ–°çš„æœªå‘é‡åŒ–å›¾ç‰‡
                image_status = self.query_image_status()
                self.display_image_status(image_status)
                
                unvectorized_images = image_status['unvectorized']
                if unvectorized_images:
                    results = self.process_unvectorized_images(unvectorized_images)
                    self.display_processing_results(results)
                else:
                    print("ğŸ‰ æ‰€æœ‰å›¾ç‰‡éƒ½å·²å‘é‡åŒ–å®Œæˆï¼")
            
            print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
            print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    try:
        enhancer = ImageEnhancerNew()
        enhancer.run()
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
