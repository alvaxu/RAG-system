#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
ç¨‹åºè¯´æ˜ï¼š

## 1. å…¨é¢åˆ†æå‘é‡æ•°æ®åº“ä¸­è¡¨æ ¼ç›¸å…³çš„æ‰€æœ‰å­—æ®µ
## 2. æ£€æŸ¥HTMLæ ¼å¼è¡¨æ ¼å†…å®¹çš„å­˜å‚¨ä½ç½®å’Œå®Œæ•´æ€§
## 3. éªŒè¯è¡¨æ ¼å¤„ç†ä¼˜åŒ–åçš„å­—æ®µå˜åŒ–
## 4. æä¾›è¯¦ç»†çš„å­—æ®µç»Ÿè®¡å’Œå†…å®¹åˆ†ææŠ¥å‘Š

## ä¸»è¦åŠŸèƒ½ï¼š
- åˆ†ææ‰€æœ‰è¡¨æ ¼æ–‡æ¡£çš„å…ƒæ•°æ®å­—æ®µ
- æ£€æŸ¥HTMLå†…å®¹çš„å­˜å‚¨ä½ç½®ï¼ˆpage_content vs metadataä¸­çš„page_contentï¼‰
- éªŒè¯è¯­ä¹‰åŒ–å¤„ç†å†…å®¹çš„å­˜åœ¨æ€§
- ç»Ÿè®¡å­—æ®µè¦†ç›–ç‡å’Œå†…å®¹è´¨é‡
- ç”Ÿæˆè¯¦ç»†çš„è¡¨æ ¼å­—æ®µåˆ†ææŠ¥å‘Š
'''

import sys
import os
import json
import pickle
import logging
import numpy as np
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Any, Optional, Tuple

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import Settings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings

# å¯¼å…¥ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—
from config.api_key_manager import get_dashscope_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveTableFieldAnalyzer:
    """å…¨é¢çš„è¡¨æ ¼å­—æ®µåˆ†æå™¨"""
    
    def __init__(self, vector_db_path: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        :param vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
        """
        self.vector_db_path = vector_db_path
        self.vector_store = None
        self.analysis_results = {
            'summary': {},
            'field_analysis': {},
            'content_analysis': {},
            'html_content_analysis': {},
            'semantic_content_analysis': {},
            'metadata_completeness': {},
            'sample_documents': []
        }
    
    def load_vector_store(self) -> bool:
        """åŠ è½½å‘é‡å­˜å‚¨"""
        try:
            config = Settings.load_from_file('config.json')
            
            # ä½¿ç”¨ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—è·å–APIå¯†é’¥
            config_key = config.dashscope_api_key
            api_key = get_dashscope_api_key(config_key)
            
            if not api_key:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
                return False
            
            # åˆå§‹åŒ–DashScope embeddings
            try:
                embedding_model = config.text_embedding_model
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®ï¼Œä½¿ç”¨é»˜è®¤embeddingæ¨¡å‹: {e}")
                embedding_model = 'text-embedding-v1'
            
            embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model=embedding_model)
            self.vector_store = FAISS.load_local(self.vector_db_path, embeddings, allow_dangerous_deserialization=True)
            logger.info(f"å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(self.vector_store.docstore._dict)} ä¸ªæ–‡æ¡£")
            return True
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            return False
    
    def analyze_table_documents(self) -> Dict[str, Any]:
        """åˆ†ææ‰€æœ‰è¡¨æ ¼æ–‡æ¡£"""
        if not self.vector_store or not hasattr(self.vector_store, 'docstore') or not hasattr(self.vector_store.docstore, '_dict'):
            print("âŒ å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸")
            return {}
        
        docstore = self.vector_store.docstore._dict
        table_docs = []
        
        # æ”¶é›†æ‰€æœ‰è¡¨æ ¼æ–‡æ¡£
        for doc_id, doc in docstore.items():
            try:
                if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'table':
                    table_docs.append((doc_id, doc))
            except Exception as e:
                logger.warning(f"å¤„ç†æ–‡æ¡£ {doc_id} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"ğŸ” æ‰¾åˆ° {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
        
        # åˆ†ææ¯ä¸ªè¡¨æ ¼æ–‡æ¡£
        field_counter = Counter()
        html_content_stats = {
            'in_metadata': 0,
            'in_page_content': 0,
            'both_locations': 0,
            'neither_location': 0,
            'html_tags_detected': 0
        }
        
        semantic_content_stats = {
            'processed_table_content': 0,
            'table_summary': 0,
            'table_title': 0,
            'table_headers': 0,
            'table_row_count': 0,
            'table_column_count': 0
        }
        
        content_lengths = []
        document_names = set()
        table_types = Counter()
        
        # è¯¦ç»†åˆ†æå‰5ä¸ªæ–‡æ¡£
        detailed_samples = min(5, len(table_docs))
        
        for i, (doc_id, doc) in enumerate(table_docs):
            # ç»Ÿè®¡å­—æ®µ
            if hasattr(doc, 'metadata') and doc.metadata:
                field_counter.update(doc.metadata.keys())
                
                # ç»Ÿè®¡ç‰¹å®šå­—æ®µ
                for field in semantic_content_stats.keys():
                    if doc.metadata.get(field):
                        semantic_content_stats[field] += 1
                
                # ç»Ÿè®¡æ–‡æ¡£åç§°å’Œè¡¨æ ¼ç±»å‹
                if doc.metadata.get('document_name'):
                    document_names.add(doc.metadata['document_name'])
                if doc.metadata.get('table_type'):
                    table_types[doc.metadata['table_type']] += 1
            
            # HTMLå†…å®¹åˆ†æ
            html_in_metadata = False
            html_in_page_content = False
            
            # æ£€æŸ¥å…ƒæ•°æ®ä¸­çš„HTMLå†…å®¹
            if 'page_content' in doc.metadata and doc.metadata['page_content']:
                content = str(doc.metadata['page_content'])
                if self._contains_html_tags(content):
                    html_in_metadata = True
                    html_content_stats['in_metadata'] += 1
                    html_content_stats['html_tags_detected'] += 1
                    content_lengths.append(len(content))
            
            # æ£€æŸ¥doc.page_contentå±æ€§
            if hasattr(doc, 'page_content') and doc.page_content:
                content = str(doc.page_content)
                if self._contains_html_tags(content):
                    html_in_page_content = True
                    html_content_stats['in_page_content'] += 1
                    html_content_stats['html_tags_detected'] += 1
                    if not html_in_metadata:
                        content_lengths.append(len(content))
            
            # ç»Ÿè®¡HTMLå†…å®¹ä½ç½®
            if html_in_metadata and html_in_page_content:
                html_content_stats['both_locations'] += 1
            elif not html_in_metadata and not html_in_page_content:
                html_content_stats['neither_location'] += 1
            
            # ğŸ”‘ ä¿®å¤ï¼šç»Ÿè®¡æ‰€æœ‰HTMLå†…å®¹çš„é•¿åº¦ï¼Œä¸é‡å¤è®¡ç®—
            if html_in_metadata:
                content_lengths.append(len(str(doc.metadata['page_content'])))
            if html_in_page_content:
                content_lengths.append(len(str(doc.page_content)))
            
            # ä¿å­˜è¯¦ç»†æ ·æœ¬
            if i < detailed_samples:
                sample = self._create_document_sample(doc_id, doc, html_in_metadata, html_in_page_content)
                self.analysis_results['sample_documents'].append(sample)
        
        # ğŸ”‘ æ–°å¢ï¼šåŠ¨æ€æ£€æµ‹æ‰€æœ‰å…ƒæ•°æ®å­—æ®µ
        all_metadata_fields = set()
        for doc_id, doc in table_docs:
            if hasattr(doc, 'metadata') and doc.metadata:
                all_metadata_fields.update(doc.metadata.keys())
        
        # ä¿å­˜åˆ†æç»“æœ
        self.analysis_results['summary'] = {
            'total_table_docs': len(table_docs),
            'unique_document_names': len(document_names),
            'document_names': list(document_names),
            'table_types': dict(table_types),
            'all_metadata_fields': list(all_metadata_fields)  # ğŸ”‘ æ–°å¢ï¼šæ‰€æœ‰å…ƒæ•°æ®å­—æ®µ
        }
        
        self.analysis_results['field_analysis'] = {
            'total_fields': len(field_counter),
            'field_frequency': dict(field_counter),
            'most_common_fields': field_counter.most_common(10)
        }
        
        self.analysis_results['html_content_analysis'] = html_content_stats
        self.analysis_results['semantic_content_analysis'] = semantic_content_stats
        self.analysis_results['content_analysis'] = {
            'total_content_lengths': len(content_lengths),
            'avg_content_length': np.mean(content_lengths) if content_lengths else 0,
            'min_content_length': min(content_lengths) if content_lengths else 0,
            'max_content_length': max(content_lengths) if content_lengths else 0
        }
        
        return self.analysis_results
    
    def _contains_html_tags(self, content: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«HTMLæ ‡ç­¾"""
        import re
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æµ‹æ‰€æœ‰HTMLæ ‡ç­¾ï¼Œæ›´å…¨é¢å‡†ç¡®
        html_pattern = r'<[^>]+>'
        return bool(re.search(html_pattern, content))
    
    def _create_document_sample(self, doc_id: str, doc: Any, html_in_metadata: bool, html_in_page_content: bool) -> Dict[str, Any]:
        """åˆ›å»ºæ–‡æ¡£æ ·æœ¬ä¿¡æ¯"""
        sample = {
            'doc_id': doc_id,
            'metadata_fields': list(doc.metadata.keys()) if hasattr(doc, 'metadata') and doc.metadata else [],
            'html_content_locations': {
                'in_metadata': html_in_metadata,
                'in_page_content': html_in_page_content
            }
        }
        
        # æ·»åŠ å…³é”®å…ƒæ•°æ®å­—æ®µ
        if hasattr(doc, 'metadata') and doc.metadata:
            for key in ['document_name', 'page_number', 'table_id', 'table_type', 'table_title', 'table_summary']:
                if key in doc.metadata:
                    sample[key] = doc.metadata[key]
        
        # æ·»åŠ å†…å®¹é¢„è§ˆ
        if hasattr(doc, 'page_content') and doc.page_content:
            sample['page_content_preview'] = str(doc.page_content)[:200] + '...' if len(str(doc.page_content)) > 200 else str(doc.page_content)
        
        if 'page_content' in doc.metadata and doc.metadata['page_content']:
            sample['metadata_page_content_preview'] = str(doc.metadata['page_content'])[:200] + '...' if len(str(doc.metadata['page_content'])) > 200 else str(doc.metadata['page_content'])
        
        return sample
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ“Š è¡¨æ ¼å­—æ®µå…¨é¢åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)
        
        # æ‘˜è¦ä¿¡æ¯
        summary = self.analysis_results['summary']
        report.append(f"\nğŸ“‹ æ‘˜è¦ä¿¡æ¯:")
        report.append(f"  æ€»è¡¨æ ¼æ–‡æ¡£æ•°: {summary['total_table_docs']}")
        report.append(f"  å”¯ä¸€æ–‡æ¡£åæ•°é‡: {summary['unique_document_names']}")
        report.append(f"  æ–‡æ¡£åç§°: {', '.join(summary['document_names'][:5])}")
        if len(summary['document_names']) > 5:
            report.append(f"  ... è¿˜æœ‰ {len(summary['document_names']) - 5} ä¸ª")
        
        # ğŸ”‘ æ–°å¢ï¼šæ˜¾ç¤ºæ‰€æœ‰å…ƒæ•°æ®å­—æ®µ
        if 'all_metadata_fields' in summary and summary['all_metadata_fields']:
            report.append(f"  æ‰€æœ‰å…ƒæ•°æ®å­—æ®µ ({len(summary['all_metadata_fields'])}ä¸ª):")
            fields_list = sorted(summary['all_metadata_fields'])
            for i in range(0, len(fields_list), 5):  # æ¯è¡Œæ˜¾ç¤º5ä¸ªå­—æ®µ
                row_fields = fields_list[i:i+5]
                report.append(f"    {', '.join(row_fields)}")
        
        # è¡¨æ ¼ç±»å‹åˆ†å¸ƒ
        if summary['table_types']:
            report.append(f"\nğŸ“Š è¡¨æ ¼ç±»å‹åˆ†å¸ƒ:")
            for table_type, count in sorted(summary['table_types'].items(), key=lambda x: x[1], reverse=True):
                report.append(f"  {table_type}: {count}")
        
        # å­—æ®µåˆ†æ
        field_analysis = self.analysis_results['field_analysis']
        report.append(f"\nğŸ” å­—æ®µåˆ†æ:")
        report.append(f"  æ€»å­—æ®µæ•°: {field_analysis['total_fields']}")
        report.append(f"  æœ€å¸¸è§å­—æ®µ (å‰10):")
        for field, count in field_analysis['most_common_fields']:
            report.append(f"    {field}: {count}")
        
        # HTMLå†…å®¹åˆ†æ
        html_analysis = self.analysis_results['html_content_analysis']
        report.append(f"\nğŸŒ HTMLå†…å®¹åˆ†æ:")
        report.append(f"  å…ƒæ•°æ®ä¸­åŒ…å«HTML: {html_analysis['in_metadata']}")
        report.append(f"  page_contentä¸­åŒ…å«HTML: {html_analysis['in_page_content']}")
        report.append(f"  ä¸¤å¤„éƒ½æœ‰HTML: {html_analysis['both_locations']}")
        report.append(f"  éƒ½æ²¡æœ‰HTML: {html_analysis['neither_location']}")
        report.append(f"  æ£€æµ‹åˆ°HTMLæ ‡ç­¾: {html_analysis['html_tags_detected']}")
        
        # è¯­ä¹‰åŒ–å†…å®¹åˆ†æ
        semantic_analysis = self.analysis_results['semantic_content_analysis']
        report.append(f"\nğŸ§  è¯­ä¹‰åŒ–å†…å®¹åˆ†æ:")
        for field, count in semantic_analysis.items():
            report.append(f"  {field}: {count}")
        
        # å†…å®¹é•¿åº¦åˆ†æ
        content_analysis = self.analysis_results['content_analysis']
        report.append(f"\nğŸ“ å†…å®¹é•¿åº¦åˆ†æ:")
        report.append(f"  æœ‰å†…å®¹é•¿åº¦çš„æ–‡æ¡£: {content_analysis['total_content_lengths']}")
        if content_analysis['total_content_lengths'] > 0:
            report.append(f"  å¹³å‡é•¿åº¦: {content_analysis['avg_content_length']:.1f}")
            report.append(f"  æœ€çŸ­é•¿åº¦: {content_analysis['min_content_length']}")
            report.append(f"  æœ€é•¿é•¿åº¦: {content_analysis['max_content_length']}")
        
        # è¯¦ç»†æ ·æœ¬
        if self.analysis_results['sample_documents']:
            report.append(f"\nğŸ“„ è¯¦ç»†æ ·æœ¬ (å‰{len(self.analysis_results['sample_documents'])}ä¸ª):")
            for i, sample in enumerate(self.analysis_results['sample_documents']):
                report.append(f"\n  ğŸ“‹ æ ·æœ¬ {i+1}:")
                report.append(f"    æ–‡æ¡£ID: {sample['doc_id']}")
                if 'document_name' in sample:
                    report.append(f"    æ–‡æ¡£å: {sample['document_name']}")
                if 'table_id' in sample:
                    report.append(f"    è¡¨æ ¼ID: {sample['table_id']}")
                if 'table_type' in sample:
                    report.append(f"    è¡¨æ ¼ç±»å‹: {sample['table_type']}")
                
                report.append(f"    HTMLå†…å®¹ä½ç½®:")
                report.append(f"      å…ƒæ•°æ®ä¸­: {'âœ…' if sample['html_content_locations']['in_metadata'] else 'âŒ'}")
                report.append(f"      page_contentä¸­: {'âœ…' if sample['html_content_locations']['in_page_content'] else 'âŒ'}")
                
                report.append(f"    å…ƒæ•°æ®å­—æ®µæ•°: {len(sample['metadata_fields'])}")
                report.append(f"    å…ƒæ•°æ®å­—æ®µ: {', '.join(sample['metadata_fields'][:10])}")
                if len(sample['metadata_fields']) > 10:
                    report.append(f"      ... è¿˜æœ‰ {len(sample['metadata_fields']) - 10} ä¸ª")
        
        # å…³é”®å‘ç°å’Œå»ºè®®
        report.append(f"\nğŸ’¡ å…³é”®å‘ç°å’Œå»ºè®®:")
        
        # æ£€æŸ¥HTMLå†…å®¹å­˜å‚¨æƒ…å†µ
        html_in_metadata = html_analysis['in_metadata']
        html_in_page_content = html_analysis['in_page_content']
        
        if html_in_metadata and html_in_page_content:
            report.append("  âœ… HTMLå†…å®¹åœ¨ä¸¤ä¸ªä½ç½®éƒ½æœ‰å­˜å‚¨ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ€§")
        elif html_in_metadata:
            report.append("  âœ… HTMLå†…å®¹å­˜å‚¨åœ¨å…ƒæ•°æ®ä¸­ï¼Œç¬¦åˆä¼˜åŒ–æ–¹æ¡ˆè®¾è®¡")
        elif html_in_page_content:
            report.append("  âš ï¸ HTMLå†…å®¹åªå­˜å‚¨åœ¨page_contentä¸­ï¼Œå»ºè®®æ£€æŸ¥å…ƒæ•°æ®å­˜å‚¨")
        else:
            report.append("  âŒ æœªæ£€æµ‹åˆ°HTMLå†…å®¹ï¼Œéœ€è¦æ£€æŸ¥è¡¨æ ¼å¤„ç†æµç¨‹")
        
        # æ£€æŸ¥è¯­ä¹‰åŒ–å†…å®¹
        processed_content_count = semantic_analysis['processed_table_content']
        if processed_content_count > 0:
            report.append(f"  âœ… æœ‰ {processed_content_count} ä¸ªæ–‡æ¡£åŒ…å«è¯­ä¹‰åŒ–å¤„ç†å†…å®¹")
        else:
            report.append("  âš ï¸ æœªæ£€æµ‹åˆ°è¯­ä¹‰åŒ–å¤„ç†å†…å®¹ï¼Œå»ºè®®æ£€æŸ¥è¡¨æ ¼å¤„ç†ä¼˜åŒ–")
        
        # ğŸ”‘ æ”¹è¿›ï¼šå­—æ®µå®Œæ•´æ€§æ£€æŸ¥ï¼ŒåŸºäºå®é™…å‘ç°çš„å­—æ®µ
        if 'all_metadata_fields' in summary:
            actual_fields = set(summary['all_metadata_fields'])
            expected_fields = ['table_id', 'table_type', 'table_title', 'table_summary', 'table_headers', 'table_row_count', 'table_column_count']
            missing_fields = [field for field in expected_fields if field not in actual_fields]
            extra_fields = [field for field in actual_fields if field not in expected_fields and not field.startswith('_')]
            
            if missing_fields:
                report.append(f"  âš ï¸ ç¼ºå°‘é¢„æœŸå­—æ®µ: {', '.join(missing_fields)}")
            else:
                report.append("  âœ… æ‰€æœ‰é¢„æœŸå­—æ®µéƒ½å­˜åœ¨")
            
            if extra_fields:
                report.append(f"  ğŸ” å‘ç°é¢å¤–å­—æ®µ: {', '.join(extra_fields[:10])}")
                if len(extra_fields) > 10:
                    report.append(f"      ... è¿˜æœ‰ {len(extra_fields) - 10} ä¸ªé¢å¤–å­—æ®µ")
        else:
            report.append("  âš ï¸ æ— æ³•æ£€æŸ¥å­—æ®µå®Œæ•´æ€§ï¼Œç¼ºå°‘å­—æ®µä¿¡æ¯")
        
        report.append("\n" + "=" * 80)
        return "\n".join(report)
    
    def save_analysis_results(self, output_file: str = "table_field_analysis_results.json"):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        try:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {key: convert_numpy_types(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj
            
            converted_results = convert_numpy_types(self.analysis_results)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(converted_results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” è¡¨æ ¼å­—æ®µå…¨é¢åˆ†æå™¨")
    print("=" * 50)
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“è·¯å¾„
    vector_db_path = "../central/vector_db"
    if not os.path.exists(vector_db_path):
        print(f"âŒ å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {vector_db_path}")
        return
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ComprehensiveTableFieldAnalyzer(vector_db_path)
    
    # åŠ è½½å‘é‡å­˜å‚¨
    print("ğŸ“‚ æ­£åœ¨åŠ è½½å‘é‡å­˜å‚¨...")
    if not analyzer.load_vector_store():
        print("âŒ æ— æ³•åŠ è½½å‘é‡å­˜å‚¨")
        return
    
    # åˆ†æè¡¨æ ¼æ–‡æ¡£
    print("ğŸ” æ­£åœ¨åˆ†æè¡¨æ ¼æ–‡æ¡£...")
    results = analyzer.analyze_table_documents()
    
    # ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“Š æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    report = analyzer.generate_report()
    print(report)
    
    # ä¿å­˜ç»“æœ
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ†æç»“æœ...")
    analyzer.save_analysis_results()
    
    print("\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
