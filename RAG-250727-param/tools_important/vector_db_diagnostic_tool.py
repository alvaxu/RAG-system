#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
ç¨‹åºè¯´æ˜ï¼š
## 1. ç»¼åˆè¯Šæ–­å·¥å…·ï¼Œç”¨äºåˆ†ævector_dbçš„ç»“æ„ã€indexã€å…ƒæ•°æ®åŠå†…å®¹
## 2. æ•´åˆäº†å¤šä¸ªç‹¬ç«‹è¯Šæ–­è„šæœ¬çš„åŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„è¯Šæ–­ç•Œé¢
'''

import sys
import os
import json
import pickle
import logging
import numpy as np
from pathlib import Path
from collections import defaultdict

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import Settings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings

# å¯¼å…¥ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—
from config.api_key_manager import get_dashscope_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_vector_store(vector_db_path):
    """åŠ è½½å‘é‡å­˜å‚¨"""
    try:
        config = Settings.load_from_file('config.json')
        
        # ä½¿ç”¨ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—è·å–APIå¯†é’¥
        config_key = config.dashscope_api_key
        api_key = get_dashscope_api_key(config_key)
        
        if not api_key:
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return None
        
        # åˆå§‹åŒ–DashScope embeddings
        try:
            embedding_model = config.text_embedding_model
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®ï¼Œä½¿ç”¨é»˜è®¤embeddingæ¨¡å‹: {e}")
            embedding_model = 'text-embedding-v1'
        
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model=embedding_model)
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        logger.info(f"å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(vector_store.docstore._dict)} ä¸ªæ–‡æ¡£")
        return vector_store
    except Exception as e:
        logger.error(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
        return None

def check_vector_db_files(vector_db_path):
    """æ£€æŸ¥vector_dbä¸­çš„æ–‡ä»¶"""
    print(f"ğŸ“ æ£€æŸ¥ç›®å½•: {vector_db_path}")
    print(f"ç›®å½•å­˜åœ¨: {os.path.exists(vector_db_path)}")
    
    if not os.path.exists(vector_db_path):
        return None
    
    vector_db_path = Path(vector_db_path)
    files = list(vector_db_path.glob('*'))
    print(f"æ–‡ä»¶æ•°é‡: {len(files)}")
    file_info = []
    
    for file_path in files:
        info = {
            'name': file_path.name,
            'size': file_path.stat().st_size
        }
        print(f"\nğŸ“„ æ–‡ä»¶: {file_path.name}")
        print(f"  å¤§å°: {file_path.stat().st_size} bytes")
        
        if file_path.suffix == '.pkl':
            try:
                with open(file_path, 'rb') as f:
                    data = pickle.load(f)
                
                info['type'] = str(type(data))
                info['length'] = len(data) if hasattr(data, '__len__') else 'No length'
                print(f"  ç±»å‹: {type(data)}")
                print(f"  é•¿åº¦: {len(data) if hasattr(data, '__len__') else 'No length'}")
                
                if hasattr(data, '__len__') and len(data) > 0:
                    if isinstance(data, list):
                        info['first_element_type'] = str(type(data[0]))
                        print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(data[0])}")
                        if isinstance(data[0], dict):
                            info['first_element_keys'] = list(data[0].keys())
                            print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ é”®: {list(data[0].keys())}")
                    elif isinstance(data, dict):
                        info['keys'] = list(data.keys())
                        print(f"  é”®: {list(data.keys())}")
                            
            except Exception as e:
                info['error'] = str(e)
                print(f"  è¯»å–å¤±è´¥: {e}")
        
        elif file_path.suffix == '.faiss':
            info['type'] = 'FAISSç´¢å¼•æ–‡ä»¶'
            print(f"  FAISSç´¢å¼•æ–‡ä»¶")
        
        file_info.append(info)
    
    return file_info

def check_index_structure(vector_db_path):
    """æ£€æŸ¥index.pklçš„è¯¦ç»†ç»“æ„"""
    index_path = Path(vector_db_path) / 'index.pkl'
    index_info = {
        'path': str(index_path),
        'exists': index_path.exists()
    }
    
    print(f"\nğŸ“Š index.pkl ç»“æ„åˆ†æ")
    print(f"æ–‡ä»¶è·¯å¾„: {index_path}")
    print(f"æ–‡ä»¶å­˜åœ¨: {index_path.exists()}")
    
    if not index_path.exists():
        print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
        return index_info
    
    try:
        with open(index_path, 'rb') as f:
            index_data = pickle.load(f)
        
        index_info['type'] = str(type(index_data))
        index_info['length'] = len(index_data)
        print(f"æ•°æ®ç±»å‹: {type(index_data)}")
        print(f"æ•°æ®é•¿åº¦: {len(index_data)}")
        print()
        
        index_info['elements'] = []
        for i, item in enumerate(index_data):
            element_info = {
                'index': i+1,
                'type': str(type(item)),
                'length': len(item) if hasattr(item, '__len__') else 'No length'
            }
            print(f"ğŸ“‹ ç¬¬{i+1}ä¸ªå…ƒç´ :")
            print(f"  ç±»å‹: {type(item)}")
            print(f"  é•¿åº¦: {len(item) if hasattr(item, '__len__') else 'No length'}")
            
            if isinstance(item, list):
                element_info['is_list'] = True
                element_info['list_length'] = len(item)
                print(f"  åˆ—è¡¨å…ƒç´ æ•°é‡: {len(item)}")
                if len(item) > 0:
                    element_info['first_element_type'] = str(type(item[0]))
                    print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(item[0])}")
                    if isinstance(item[0], dict):
                        element_info['first_element_keys'] = list(item[0].keys())
                        print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ é”®: {list(item[0].keys())}")
            
            elif isinstance(item, dict):
                element_info['is_dict'] = True
                element_info['keys'] = list(item.keys())
                print(f"  å­—å…¸é”®: {list(item.keys())}")
            
            index_info['elements'].append(element_info)
            print()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…ƒæ•°æ®
        print("ğŸ” æŸ¥æ‰¾å…ƒæ•°æ®:")
        index_info['metadata'] = []
        for i, item in enumerate(index_data):
            if isinstance(item, list) and len(item) > 0:
                # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦æœ‰document_nameå­—æ®µ
                if isinstance(item[0], dict) and 'document_name' in item[0]:
                    metadata_info = {
                        'element_index': i+1,
                        'doc_count': len(item)
                    }
                    print(f"  ç¬¬{i+1}ä¸ªå…ƒç´ åŒ…å«å…ƒæ•°æ®ï¼Œæ–‡æ¡£æ•°é‡: {len(item)}")
                    # ç»Ÿè®¡chunk_type
                    chunk_types = {}
                    for doc in item:
                        if isinstance(doc, dict):
                            chunk_type = doc.get('chunk_type', 'unknown')
                            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
                    metadata_info['chunk_types'] = chunk_types
                    print(f"  Chunkç±»å‹åˆ†å¸ƒ: {chunk_types}")
                    
                    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
                    print(f"  ç¤ºä¾‹æ–‡æ¡£:")
                    metadata_info['sample_docs'] = []
                    for j, doc in enumerate(item[:2]):
                        if isinstance(doc, dict):
                            doc_info = {
                                'index': j+1,
                                'chunk_type': doc.get('chunk_type', 'N/A'),
                                'document_name': doc.get('document_name', 'N/A'),
                                'page_number': doc.get('page_number', 'N/A')
                            }
                            print(f"    [{j+1}] {doc.get('chunk_type', 'N/A')}: {doc.get('document_name', 'N/A')} (p.{doc.get('page_number', 'N/A')})")
                            metadata_info['sample_docs'].append(doc_info)
                    index_info['metadata'].append(metadata_info)
                    break
        
        return index_info
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        index_info['error'] = str(e)
        return index_info

def analyze_metadata_structure(vector_db_path):
    """åˆ†æå…ƒæ•°æ®ç»“æ„"""
    metadata_path = Path(vector_db_path) / 'metadata.pkl'
    metadata_info = {
        'path': str(metadata_path),
        'exists': metadata_path.exists()
    }
    
    print(f"\nğŸ“‹ å…ƒæ•°æ®ç»“æ„åˆ†æ")
    print(f"æ–‡ä»¶è·¯å¾„: {metadata_path}")
    print(f"æ–‡ä»¶å­˜åœ¨: {metadata_path.exists()}")
    
    if not metadata_path.exists():
        print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
        return metadata_info
    
    try:
        with open(metadata_path, 'rb') as f:
            metadata = pickle.load(f)
        
        metadata_info['type'] = str(type(metadata))
        metadata_info['length'] = len(metadata)
        print(f"æ•°æ®ç±»å‹: {type(metadata)}")
        print(f"æ•°æ®é•¿åº¦: {len(metadata)}")
        print()
        
        # åˆ†æå­—æ®µç»“æ„
        field_types = defaultdict(set)
        field_examples = defaultdict(list)
        chunk_type_fields = defaultdict(set)
        
        for i, item in enumerate(metadata):
            if isinstance(item, dict):
                chunk_type = item.get('chunk_type', 'unknown')
                
                for field, value in item.items():
                    field_types[field].add(type(value).__name__)
                    chunk_type_fields[chunk_type].add(field)
                    
                    # ä¿å­˜å‰3ä¸ªç¤ºä¾‹
                    if len(field_examples[field]) < 3:
                        field_examples[field].append({
                            'chunk_type': chunk_type,
                            'value': str(value)[:100] + '...' if len(str(value)) > 100 else str(value)
                        })
        
        metadata_info['field_types'] = {k: list(v) for k, v in field_types.items()}
        metadata_info['chunk_type_fields'] = {k: list(v) for k, v in chunk_type_fields.items()}
        metadata_info['field_examples'] = {k: v for k, v in field_examples.items()}
        
        print("ğŸ” å­—æ®µç±»å‹åˆ†æ:")
        for field, types in sorted(field_types.items()):
            print(f"  {field}: {', '.join(sorted(types))}")
        
        print("\nğŸ“‹ æŒ‰chunk_typeåˆ†ç»„çš„å­—æ®µ:")
        for chunk_type, fields in sorted(chunk_type_fields.items()):
            print(f"  {chunk_type}: {', '.join(sorted(fields))}")
        
        print("\nğŸ“ å­—æ®µç¤ºä¾‹:")
        for field, examples in sorted(field_examples.items()):
            print(f"  {field}:")
            for example in examples:
                print(f"    [{example['chunk_type']}] {example['value']}")
        
        # åˆ†æç‰¹å®šå­—æ®µ
        print("\nğŸ¯ å…³é”®å­—æ®µåˆ†æ:")
        key_fields = ['document_name', 'page_number', 'chunk_type', 'source', 'title']
        metadata_info['key_fields'] = []
        for field in key_fields:
            if field in field_types:
                values = [item.get(field, 'N/A') for item in metadata if isinstance(item, dict)]
                unique_values = set(values)
                field_info = {
                    'field': field,
                    'unique_count': len(unique_values)
                }
                print(f"  {field}: {len(unique_values)} ä¸ªå”¯ä¸€å€¼")
                if len(unique_values) <= 5:
                    field_info['values'] = list(unique_values)
                    print(f"    å€¼: {list(unique_values)}")
                else:
                    field_info['sample_values'] = list(unique_values)[:5]
                    print(f"    å‰5ä¸ªå€¼: {list(unique_values)[:5]}")
                metadata_info['key_fields'].append(field_info)
        
        return metadata_info
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        metadata_info['error'] = str(e)
        return metadata_info

def check_faiss_structure(vector_db_path):
    """æ£€æŸ¥FAISSç´¢å¼•çš„å®é™…ç»“æ„"""
    faiss_path = Path(vector_db_path) / 'index.faiss'
    faiss_info = {
        'path': str(faiss_path),
        'exists': faiss_path.exists()
    }
    
    print(f"\nğŸ” æ£€æŸ¥FAISSç´¢å¼•ç»“æ„:")
    print("=" * 60)
    
    if not faiss_path.exists():
        print(f"âŒ FAISSæ–‡ä»¶ä¸å­˜åœ¨: {faiss_path}")
        return faiss_info
    
    try:
        # æ£€æŸ¥FAISSæ–‡ä»¶å¤§å°
        faiss_size = faiss_path.stat().st_size
        faiss_info['size'] = faiss_size
        print(f"ğŸ“ FAISSæ–‡ä»¶å¤§å°: {faiss_size} bytes")
        
        # å°è¯•åŠ è½½FAISSç´¢å¼•
        try:
            import faiss
            index = faiss.read_index(str(faiss_path))
            faiss_info['type'] = str(type(index))
            faiss_info['dimension'] = index.d
            faiss_info['vector_count'] = index.ntotal
            print(f"ğŸ”¢ FAISSç´¢å¼•ä¿¡æ¯:")
            print(f"  å‘é‡ç»´åº¦: {index.d}")
            print(f"  å‘é‡æ•°é‡: {index.ntotal}")
            print(f"  ç´¢å¼•ç±»å‹: {type(index)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡æ•°æ®
            if hasattr(index, 'get_xb'):
                try:
                    vectors = index.get_xb()
                    faiss_info['vectors_type'] = str(type(vectors))
                    print(f"  å‘é‡æ•°æ®ç±»å‹: {type(vectors)}")
                    if hasattr(vectors, 'shape'):
                        faiss_info['vectors_shape'] = vectors.shape
                        print(f"  å‘é‡æ•°æ®å½¢çŠ¶: {vectors.shape}")
                    
                    # æ£€æŸ¥å‰å‡ ä¸ªå‘é‡
                    if vectors.shape[0] > 0:
                        faiss_info['first_vector_sample'] = vectors[0][:5].tolist()
                        faiss_info['norm_range'] = {
                            'min': float(np.linalg.norm(vectors, axis=1).min()),
                            'max': float(np.linalg.norm(vectors, axis=1).max())
                        }
                        print(f"  ç¬¬ä¸€ä¸ªå‘é‡: {vectors[0][:5]}... (å‰5ç»´)")
                        print(f"  å‘é‡èŒƒæ•°èŒƒå›´: {np.linalg.norm(vectors, axis=1).min():.4f} - {np.linalg.norm(vectors, axis=1).max():.4f}")
                except Exception as e:
                    faiss_info['vectors_error'] = str(e)
                    print(f"  æ— æ³•è·å–å‘é‡æ•°æ®: {e}")
            
        except ImportError:
            faiss_info['error'] = "æ— æ³•å¯¼å…¥faissåº“"
            print("âŒ æ— æ³•å¯¼å…¥faissåº“")
        except Exception as e:
            faiss_info['error'] = str(e)
            print(f"âŒ åŠ è½½FAISSç´¢å¼•å¤±è´¥: {e}")
        
        return faiss_info
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        faiss_info['error'] = str(e)
        return faiss_info

def analyze_database_structure(vector_store):
    """åˆ†ææ•°æ®åº“ç»“æ„"""
    print("\nğŸ“Š å‘é‡æ•°æ®åº“ç»“æ„åˆ†æ")
    print("=" * 60)

    if not vector_store or not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
        print("âŒ å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸")
        return None

    docstore = vector_store.docstore._dict
    total_docs = len(docstore)
    print(f"ğŸ“š æ€»æ–‡æ¡£æ•°: {total_docs}")

    if total_docs == 0:
        print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
        return None

    # åˆ†ææ–‡æ¡£ç±»å‹åˆ†å¸ƒ
    chunk_types = {}
    document_names = set()
    all_fields = set()
    detailed_docs = []

    # æ”¶é›†æ‰€æœ‰å­—æ®µå’Œç»Ÿè®¡ä¿¡æ¯
    for doc_id, doc in docstore.items():
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}

        # ç»Ÿè®¡åˆ†å—ç±»å‹
        chunk_type = metadata.get('chunk_type', 'unknown')
        if chunk_type not in chunk_types:
            chunk_types[chunk_type] = 0
        chunk_types[chunk_type] += 1

        # ç»Ÿè®¡æ–‡æ¡£åç§°
        doc_name = metadata.get('document_name', 'unknown')
        document_names.add(doc_name)

        # æ”¶é›†æ‰€æœ‰å­—æ®µ
        all_fields.update(metadata.keys())

        # æ”¶é›†è¯¦ç»†æ–‡æ¡£ä¿¡æ¯
        doc_info = {
            'doc_id': doc_id,
            'chunk_type': chunk_type,
            'document_name': doc_name,
            'page_number': metadata.get('page_number', 'N/A'),
            'content_length': len(doc.page_content) if hasattr(doc, 'page_content') else 0,
            'metadata_fields_count': len(metadata),
            'metadata_keys': list(metadata.keys())
        }
        detailed_docs.append(doc_info)

    print(f"\nğŸ“Š åˆ†å—ç±»å‹åˆ†å¸ƒ:")
    for chunk_type, count in chunk_types.items():
        print(f"  {chunk_type}: {count}")

    print(f"\nğŸ“š æ–‡æ¡£ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æ¡£æ•°: {total_docs}")
    print(f"  å”¯ä¸€æ–‡æ¡£å: {len(document_names)}")
    print(f"  æ–‡æ¡£åç§°: {sorted(list(document_names))[:5]}...")
    if len(document_names) > 5:
        print(f"      ... è¿˜æœ‰ {len(document_names) - 5} ä¸ªæ–‡æ¡£")

    print(f"\nğŸ“‹ å­—æ®µç»Ÿè®¡:")
    print(f"  æ€»å­—æ®µæ•°: {len(all_fields)}")
    print(f"  æ‰€æœ‰å­—æ®µ: {sorted(list(all_fields))}")

    # æ˜¾ç¤ºè¯¦ç»†æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
    show_detailed = input("\næ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„æ–‡æ¡£åˆ—è¡¨? (y/n): ").strip().lower()
    if show_detailed == 'y':
        show_detailed_document_list(detailed_docs)

    return {
        'total_docs': total_docs,
        'chunk_types': chunk_types,
        'document_names': list(document_names),
        'all_fields': list(all_fields),
        'detailed_docs': detailed_docs
    }

def show_detailed_document_list(detailed_docs):
    """æ˜¾ç¤ºè¯¦ç»†çš„æ–‡æ¡£åˆ—è¡¨"""
    print("
ğŸ“‹ è¯¦ç»†æ–‡æ¡£åˆ—è¡¨"    print("=" * 80)

    # æŒ‰chunk_typeåˆ†ç»„æ˜¾ç¤º
    from collections import defaultdict
    docs_by_type = defaultdict(list)

    for doc in detailed_docs:
        docs_by_type[doc['chunk_type']].append(doc)

    for chunk_type, docs in docs_by_type.items():
        print(f"\nğŸ”¹ {chunk_type.upper()} æ–‡æ¡£ ({len(docs)} ä¸ª):")
        print("-" * 60)

        # æ˜¾ç¤ºå‰10ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        for i, doc in enumerate(docs[:10]):
            print("2d")
            print(f"      æ–‡æ¡£å: {doc['document_name']}")
            print(f"      é¡µç : {doc['page_number']}")
            print(f"      å†…å®¹é•¿åº¦: {doc['content_length']}")
            print(f"      å…ƒæ•°æ®å­—æ®µæ•°: {doc['metadata_fields_count']}")
            print(f"      å…ƒæ•°æ®å­—æ®µ: {', '.join(doc['metadata_keys'][:8])}...")
            if len(doc['metadata_keys']) > 8:
                print(f"                   ... è¿˜æœ‰ {len(doc['metadata_keys']) - 8} ä¸ªå­—æ®µ")

        if len(docs) > 10:
            print(f"      ... è¿˜æœ‰ {len(docs) - 10} ä¸ªæ–‡æ¡£")

        # æ˜¾ç¤ºè¯¥ç±»å‹çš„ç»Ÿè®¡ä¿¡æ¯
        total_content_length = sum(doc['content_length'] for doc in docs)
        avg_content_length = total_content_length / len(docs) if docs else 0
        print(f"\n  ğŸ“Š {chunk_type} ç»Ÿè®¡:")
        print(f"    æ€»å†…å®¹é•¿åº¦: {total_content_length}")
        print(f"    å¹³å‡å†…å®¹é•¿åº¦: {avg_content_length:.1f}")
        print(f"    å¹³å‡å…ƒæ•°æ®å­—æ®µæ•°: {sum(doc['metadata_fields_count'] for doc in docs) / len(docs):.1f}")

def check_image_docs(vector_store):
    """æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£çš„å­˜å‚¨ç»“æ„å’Œenhanced_descriptionå­—æ®µ"""
    print(f"\nğŸ” æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£")
    print("=" * 60)
    
    if not vector_store or not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
        print("âŒ å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸")
        return None
    
    docstore = vector_store.docstore._dict
    image_docs = []
    for doc_id, doc in docstore.items():
        if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'image':
            image_docs.append((doc_id, doc))
    
    image_info = {
        'total_image_docs': len(image_docs),
        'enhanced_description_stats': {
            'with_enhanced': 0,
            'without_enhanced': 0
        },
        'samples': []
    }
    
    print(f"ğŸ“· æ‰¾åˆ° {len(image_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
    
    # åˆ†æenhanced_descriptionå­—æ®µ
    enhanced_count = 0
    empty_count = 0
    for doc_id, doc in image_docs:
        enhanced_desc = doc.metadata.get('enhanced_description', '')
        if enhanced_desc:
            enhanced_count += 1
        else:
            empty_count += 1
    
    image_info['enhanced_description_stats']['with_enhanced'] = enhanced_count
    image_info['enhanced_description_stats']['without_enhanced'] = empty_count
    
    print(f"âœ… æœ‰enhanced_descriptionçš„å›¾ç‰‡: {enhanced_count}")
    print(f"âŒ æ— enhanced_descriptionçš„å›¾ç‰‡: {empty_count}")
    print(f"ğŸ“ˆ è¦†ç›–ç‡: {enhanced_count/(enhanced_count+empty_count)*100:.1f}%" if (enhanced_count+empty_count) > 0 else "ğŸ“ˆ è¦†ç›–ç‡: N/A")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªå›¾ç‰‡æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
    for i, (doc_id, doc) in enumerate(image_docs[:3]):
        sample_info = {
            'index': i+1,
            'doc_id': doc_id,
            'document_name': doc.metadata.get('document_name', 'N/A'),
            'page_number': doc.metadata.get('page_number', 'N/A'),
            'image_id': doc.metadata.get('image_id', 'N/A'),
            'image_type': doc.metadata.get('image_type', 'N/A'),
            'img_caption': doc.metadata.get('img_caption', 'N/A'),
            'enhanced_description': doc.metadata.get('enhanced_description', '')[:100] + '...' if len(doc.metadata.get('enhanced_description', '')) > 100 else doc.metadata.get('enhanced_description', ''),
            'page_content_length': len(doc.page_content) if hasattr(doc, 'page_content') else 'N/A'
        }
        image_info['samples'].append(sample_info)
        
        print(f"\nğŸ“· å›¾ç‰‡æ–‡æ¡£ {i+1}:")
        print(f"  ID: {doc_id}")
        print(f"  æ–‡æ¡£å: {doc.metadata.get('document_name', 'N/A')}")
        print(f"  é¡µç : {doc.metadata.get('page_number', 'N/A')}")
        print(f"  å›¾ç‰‡ID: {doc.metadata.get('image_id', 'N/A')}")
        print(f"  å›¾ç‰‡ç±»å‹: {doc.metadata.get('image_type', 'N/A')}")
        print(f"  å›¾ç‰‡æ ‡é¢˜: {doc.metadata.get('img_caption', 'N/A')}")
        print(f"  å¢å¼ºæè¿°: {doc.metadata.get('enhanced_description', '')[:100] + '...' if len(doc.metadata.get('enhanced_description', '')) > 100 else doc.metadata.get('enhanced_description', '')}")
        print(f"  page_contenté•¿åº¦: {len(doc.page_content) if hasattr(doc, 'page_content') else 'N/A'}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡ç›¸å…³çš„å±æ€§
        vector_attrs = []
        for attr in dir(doc):
            if 'vector' in attr.lower() or 'embedding' in attr.lower():
                vector_attrs.append(attr)
        
        if vector_attrs:
            print(f"  å‘é‡ç›¸å…³å±æ€§: {vector_attrs}")
        else:
            print("  å‘é‡ç›¸å…³å±æ€§: æ— ")
        
        # æ£€æŸ¥metadataä¸­çš„semantic_features
        if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
            semantic = doc.metadata['semantic_features']
            print(f"  semantic_features: {semantic}")
        else:
            print("  semantic_features: æ— ")
    
    return image_info

def analyze_table_docs(vector_store):
    """åˆ†æè¡¨æ ¼æ–‡æ¡£çš„å†…å®¹å’Œå…ƒæ•°æ®"""
    print(f"\nğŸ” åˆ†æè¡¨æ ¼æ–‡æ¡£")
    print("=" * 60)
    
    if not vector_store or not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
        print("âŒ å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸")
        return None
    
    docstore = vector_store.docstore._dict
    table_docs = []
    for doc_id, doc in docstore.items():
        if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'table':
            table_docs.append((doc_id, doc))
    
    table_info = {
        'total_table_docs': len(table_docs),
        'metadata_fields': set(),
        'table_types': defaultdict(int),
        'document_names': set(),
        'content_lengths': [],
        'has_columns': 0,
        'has_table_type': 0,
        'has_html_content': 0,
        'has_processed_content': 0,
        'samples': []
    }
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
    
    # åˆ†æå‰å‡ ä¸ªè¡¨æ ¼æ–‡æ¡£çš„è¯¦ç»†å†…å®¹
    for i, (doc_id, doc) in enumerate(table_docs[:3]):
        sample_info = {
            'index': i+1,
            'doc_id': doc_id,
            'document_name': doc.metadata.get('document_name', 'N/A'),
            'page_number': doc.metadata.get('page_number', 'N/A'),
            'table_id': doc.metadata.get('table_id', 'N/A'),
            'table_type': doc.metadata.get('table_type', 'N/A'),
            'content_preview': doc.page_content[:200] + '...' if hasattr(doc, 'page_content') and len(doc.page_content) > 200 else (doc.page_content if hasattr(doc, 'page_content') else 'N/A'),
            'metadata': doc.metadata
        }
        table_info['samples'].append(sample_info)
        
        print(f"\n{'='*50}")
        print(f"ğŸ“„ è¡¨æ ¼æ–‡æ¡£ {i+1}")
        print(f"{'='*50}")
        print(f"æ–‡æ¡£ID: {doc_id}")
        print(f"æ–‡æ¡£å: {doc.metadata.get('document_name', 'N/A')}")
        print(f"é¡µç : {doc.metadata.get('page_number', 'N/A')}")
        print(f"è¡¨æ ¼ID: {doc.metadata.get('table_id', 'N/A')}")
        print(f"è¡¨æ ¼ç±»å‹: {doc.metadata.get('table_type', 'N/A')}")
        
        # åˆ†æå…ƒæ•°æ® - æ˜¾ç¤ºæ‰€æœ‰å­—æ®µçš„å€¼
        print(f"\nğŸ“‹ å…ƒæ•°æ®åˆ†æ:")
        print(f"  å…ƒæ•°æ®å­—æ®µ: {list(doc.metadata.keys())}")
        table_info['metadata_fields'].update(doc.metadata.keys())
        
        # æ˜¾ç¤ºæ¯ä¸ªå…ƒæ•°æ®å­—æ®µçš„è¯¦ç»†å€¼
        print(f"\nğŸ” è¯¦ç»†å…ƒæ•°æ®å­—æ®µå€¼:")
        for field, value in doc.metadata.items():
            if isinstance(value, str) and len(value) > 100:
                print(f"  {field}: {value[:100]}... (é•¿åº¦: {len(value)})")
            elif isinstance(value, list):
                if len(value) > 5:
                    print(f"  {field}: {value[:5]}... (å…±{len(value)}é¡¹)")
                else:
                    print(f"  {field}: {value}")
            else:
                print(f"  {field}: {value}")
        
        # ç»Ÿè®¡ç‰¹å®šå­—æ®µ
        if doc.metadata.get('table_type'):
            table_info['table_types'][doc.metadata['table_type']] += 1
            table_info['has_table_type'] += 1
        if doc.metadata.get('document_name'):
            table_info['document_names'].add(doc.metadata['document_name'])
        headers = doc.metadata.get('table_headers')
        if isinstance(headers, list) and len(headers) > 0:
            table_info['has_columns'] += 1
        
        # åˆ†æå†…å®¹ - ä¼˜å…ˆæ£€æŸ¥å…ƒæ•°æ®ä¸­çš„HTMLå†…å®¹ï¼Œç„¶åæ£€æŸ¥doc.page_contentå±æ€§
        print(f"\nğŸ“ å†…å®¹åˆ†æ:")
        
        # ä¼˜å…ˆæ£€æŸ¥å…ƒæ•°æ®ä¸­çš„page_contentå­—æ®µï¼ˆHTMLå†…å®¹ï¼‰
        if 'page_content' in doc.metadata and doc.metadata['page_content']:
            html_content = doc.metadata['page_content']
            print(f"  HTMLå†…å®¹é•¿åº¦: {len(html_content)}")
            table_info['content_lengths'].append(len(html_content))
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«HTMLå†…å®¹
            if '<table' in str(html_content).lower() or '<tr' in str(html_content).lower() or '<td' in str(html_content).lower():
                print("  HTMLå†…å®¹ç±»å‹: åŒ…å«HTMLè¡¨æ ¼å†…å®¹")
                table_info['has_html_content'] += 1  # ç»Ÿè®¡HTMLå†…å®¹æ•°é‡
                # æ˜¾ç¤ºHTMLå†…å®¹é¢„è§ˆ
                html_preview = str(html_content)[:200] + "..." if len(str(html_content)) > 200 else str(html_content)
                print(f"  HTMLå†…å®¹é¢„è§ˆ: {html_preview}")
            else:
                print("  HTMLå†…å®¹ç±»å‹: ä¸åŒ…å«æ˜æ˜¾çš„HTMLè¡¨æ ¼å†…å®¹")
        
        # æ£€æŸ¥doc.page_contentå±æ€§ï¼ˆé€šå¸¸æ˜¯è¯­ä¹‰åŒ–å†…å®¹ï¼‰
        if hasattr(doc, 'page_content') and doc.page_content:
            semantic_content = doc.page_content
            print(f"  è¯­ä¹‰åŒ–å†…å®¹é•¿åº¦: {len(semantic_content)}")
            
            # æ£€æŸ¥è¯­ä¹‰åŒ–å†…å®¹æ˜¯å¦åŒ…å«HTMLæ ‡ç­¾
            if '<table' in semantic_content.lower() or '<tr' in semantic_content.lower() or '<td' in semantic_content.lower():
                print("  è¯­ä¹‰åŒ–å†…å®¹ç±»å‹: åŒ…å«HTMLè¡¨æ ¼å†…å®¹")
            else:
                print("  è¯­ä¹‰åŒ–å†…å®¹ç±»å‹: ä¸åŒ…å«æ˜æ˜¾çš„HTMLè¡¨æ ¼å†…å®¹")
            
            # æ˜¾ç¤ºè¯­ä¹‰åŒ–å†…å®¹é¢„è§ˆ
            semantic_preview = semantic_content[:200] + "..." if len(semantic_content) > 200 else semantic_content
            print(f"  è¯­ä¹‰åŒ–å†…å®¹é¢„è§ˆ: {semantic_preview}")
        else:
            print("  âŒ æ²¡æœ‰è¯­ä¹‰åŒ–å†…å®¹")
        
        # æ£€æŸ¥è¯­ä¹‰åŒ–å†…å®¹
        has_semantic_content = 0
        key = 'processed_table_content'
        if key in doc.metadata and doc.metadata[key] is not None and len(str(doc.metadata[key])) > 0:
            has_semantic_content += 1
            print(f"  è¯­ä¹‰åŒ–å†…å®¹: å­˜åœ¨ ({key})")
            print(f"  è¯­ä¹‰åŒ–å†…å®¹é¢„è§ˆ: {doc.metadata[key][:100] + '...' if len(doc.metadata[key]) > 100 else doc.metadata[key]}")
            # æ˜¾ç¤ºå®Œæ•´çš„processed_table_contentå†…å®¹ï¼ˆç”¨äºembeddingçš„æ–‡æœ¬ï¼‰
            print(f"  ğŸ” å®Œæ•´å†…å®¹ï¼ˆç”¨äºembeddingï¼‰:")
            print(f"    {doc.metadata[key]}")
        else:
            print(f"  è¯­ä¹‰åŒ–å†…å®¹: ä¸å­˜åœ¨")
            # è°ƒè¯•ï¼šè®°å½•ç¼ºå¤±çš„æ–‡æ¡£è¯¦æƒ…
            table_id = doc.metadata.get('table_id', 'Unknown')
            page_num = doc.metadata.get('page_number', 'Unknown')
            processed_content = doc.metadata.get('processed_table_content')
            print(f"    âš ï¸  è¯¦ç»†ä¿¡æ¯: {table_id} (é¡µç : {page_num})")
            print(f"    processed_table_contentå€¼: {repr(processed_content)}")
        table_info['has_processed_content'] += has_semantic_content
    
    # åˆ†æå‰©ä½™æ–‡æ¡£çš„å…ƒæ•°æ®
    if len(table_docs) > 3:
        print(f"\nğŸ” åˆ†æå‰©ä½™ {len(table_docs) - 3} ä¸ªæ–‡æ¡£çš„å…ƒæ•°æ®...")
        for i, (doc_id, doc) in enumerate(table_docs[3:]):
            if hasattr(doc, 'metadata') and doc.metadata:
                table_info['metadata_fields'].update(doc.metadata.keys())
                
                # ç»Ÿè®¡ç‰¹å®šå­—æ®µ
                if doc.metadata.get('table_type'):
                    table_info['table_types'][doc.metadata['table_type']] += 1
                    table_info['has_table_type'] += 1
                if doc.metadata.get('document_name'):
                    table_info['document_names'].add(doc.metadata['document_name'])
                headers = doc.metadata.get('table_headers')
                if isinstance(headers, list) and len(headers) > 0:
                    table_info['has_columns'] += 1
                # æ£€æŸ¥HTMLå†…å®¹ - ä¼˜å…ˆæ£€æŸ¥å…ƒæ•°æ®ä¸­çš„page_contentå­—æ®µï¼Œç„¶åæ£€æŸ¥doc.page_contentå±æ€§
                has_html_content = False
                if 'page_content' in doc.metadata and doc.metadata['page_content']:
                    # æ£€æŸ¥å…ƒæ•°æ®ä¸­çš„page_contentå­—æ®µ
                    if any(tag in str(doc.metadata['page_content']).lower() for tag in ['<table', '<tr', '<td']):
                        has_html_content = True
                elif hasattr(doc, 'page_content') and doc.page_content:
                    # æ£€æŸ¥doc.page_contentå±æ€§
                    if any(tag in doc.page_content.lower() for tag in ['<table', '<tr', '<td']):
                        has_html_content = True
                
                if has_html_content:
                    table_info['has_html_content'] += 1
                
                # æ£€æŸ¥è¯­ä¹‰åŒ–å†…å®¹ - åªç»Ÿè®¡processed_table_contentå­—æ®µ
                key = 'processed_table_content'
                processed_content = doc.metadata.get(key)
                if key in doc.metadata and doc.metadata[key] is not None and len(str(doc.metadata[key])) > 0:
                    table_info['has_processed_content'] += 1
                else:
                    # è°ƒè¯•ï¼šè®°å½•ç¼ºå¤±çš„æ–‡æ¡£
                    table_id = doc.metadata.get('table_id', 'Unknown')
                    page_num = doc.metadata.get('page_number', 'Unknown')
                    print(f"    âš ï¸  ç¼ºå°‘processed_table_content: {table_id} (é¡µç : {page_num})")
                    print(f"        processed_table_contentå€¼: {repr(processed_content)}")
        
        print(f"  å®Œæˆå‰©ä½™æ–‡æ¡£åˆ†æ")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š è¡¨æ ¼æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)
    print(f"æ€»æ–‡æ¡£æ•°: {table_info['total_table_docs']}")
    print(f"æœ‰è¡¨æ ¼ç±»å‹çš„æ–‡æ¡£: {table_info['has_table_type']}")
    print(f"æœ‰åˆ—ä¿¡æ¯çš„æ–‡æ¡£: {table_info['has_columns']}")
    print(f"åŒ…å«HTMLå†…å®¹çš„æ–‡æ¡£: {table_info['has_html_content']}")
    print(f"åŒ…å«è¯­ä¹‰åŒ–å¤„ç†å†…å®¹çš„æ–‡æ¡£: {table_info['has_processed_content']}")
    print(f"\nå…ƒæ•°æ®å­—æ®µæ€»æ•°: {len(table_info['metadata_fields'])}")
    print(f"å…ƒæ•°æ®å­—æ®µ: {sorted(list(table_info['metadata_fields']))}")
    
    if table_info['table_types']:
        print(f"\nè¡¨æ ¼ç±»å‹åˆ†å¸ƒ:")
        for table_type, count in sorted(table_info['table_types'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {table_type}: {count}")
    
    if table_info['document_names']:
        print(f"\næ–‡æ¡£åç§° (å…±{len(table_info['document_names'])}ä¸ª):")
        for doc_name in sorted(list(table_info['document_names']))[:5]:
            print(f"  {doc_name}")
        if len(table_info['document_names']) > 5:
            print(f"  ... è¿˜æœ‰ {len(table_info['document_names']) - 5} ä¸ª")
    
    if table_info['content_lengths']:
        avg_length = sum(table_info['content_lengths']) / len(table_info['content_lengths'])
        print(f"\nå†…å®¹é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {avg_length:.1f}")
        print(f"  æœ€çŸ­é•¿åº¦: {min(table_info['content_lengths'])}")
        print(f"  æœ€é•¿é•¿åº¦: {max(table_info['content_lengths'])}")
    
    return table_info

def analyze_vector_data(vector_store):
    """åˆ†æå‘é‡æ•°æ®çš„åˆ†å¸ƒå’Œæ ·æœ¬"""
    print("
ğŸ”¢ å‘é‡æ•°æ®åˆ†æ"    print("=" * 60)

    if not vector_store:
        print("âŒ å‘é‡å­˜å‚¨å¯¹è±¡æ— æ•ˆ")
        return None

    try:
        # è·å–å‘é‡æ•°æ®
        vectors = vector_store.index.reconstruct_n(0, vector_store.index.ntotal)
        vector_info = {
            'vector_count': vector_store.index.ntotal,
            'vector_dimension': vector_store.index.d,
            'vectors': vectors
        }

        print(f"ğŸ“Š å‘é‡ç»Ÿè®¡:")
        print(f"  å‘é‡æ•°é‡: {vector_info['vector_count']}")
        print(f"  å‘é‡ç»´åº¦: {vector_info['vector_dimension']}")

        if vector_info['vector_count'] > 0:
            # å‘é‡èŒƒæ•°åˆ†æ
            norms = np.linalg.norm(vectors, axis=1)
            vector_info['norm_stats'] = {
                'min': float(norms.min()),
                'max': float(norms.max()),
                'mean': float(norms.mean()),
                'std': float(norms.std())
            }

            print(f"\nğŸ“ å‘é‡èŒƒæ•°ç»Ÿè®¡:")
            print(f"  æœ€å°å€¼: {norms.min():.4f}")
            print(f"  æœ€å¤§å€¼: {norms.max():.4f}")
            print(f"  å¹³å‡å€¼: {norms.mean():.4f}")
            print(f"  æ ‡å‡†å·®: {norms.std():.4f}")

            # æ˜¾ç¤ºå‘é‡æ ·æœ¬
            show_samples = input("\næ˜¯å¦æ˜¾ç¤ºå‘é‡æ•°æ®æ ·æœ¬? (y/n): ").strip().lower()
            if show_samples == 'y':
                print(f"\nğŸ” å‘é‡æ ·æœ¬:")
                sample_count = min(5, vector_info['vector_count'])
                for i in range(sample_count):
                    print(f"  å‘é‡ {i+1}: [{vectors[i][0]:.4f}, {vectors[i][1]:.4f}, ..., {vectors[i][-1]:.4f}]")
                    print(f"    èŒƒæ•°: {norms[i]:.4f}")

                # æ˜¾ç¤ºå‘é‡åˆ†å¸ƒç›´æ–¹å›¾
                try:
                    import matplotlib.pyplot as plt
                    plt.figure(figsize=(10, 6))
                    plt.hist(norms, bins=50, alpha=0.7, color='blue', edgecolor='black')
                    plt.title('å‘é‡èŒƒæ•°åˆ†å¸ƒ')
                    plt.xlabel('èŒƒæ•°å€¼')
                    plt.ylabel('é¢‘æ¬¡')
                    plt.grid(True, alpha=0.3)
                    plt.savefig('vector_norm_distribution.png', dpi=150, bbox_inches='tight')
                    plt.close()
                    print("
ğŸ’¾ å‘é‡èŒƒæ•°åˆ†å¸ƒå›¾å·²ä¿å­˜ä¸º: vector_norm_distribution.png"                except ImportError:
                    print("
âš ï¸  matplotlibæœªå®‰è£…ï¼Œæ— æ³•ç”Ÿæˆåˆ†å¸ƒå›¾"                except Exception as e:
                    print(f"
âš ï¸  ç”Ÿæˆåˆ†å¸ƒå›¾å¤±è´¥: {e}"            # åˆ†æå‘é‡ç›¸ä¼¼åº¦
            if vector_info['vector_count'] > 1:
                analyze_vector_similarity(vectors, vector_info)

        return vector_info

    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åˆ†æå¤±è´¥: {e}")
        return None

def analyze_vector_similarity(vectors, vector_info):
    """åˆ†æå‘é‡ç›¸ä¼¼åº¦ - è¯„ä¼°å‘é‡åŒ–è´¨é‡å’Œå‘ç°æ½œåœ¨é—®é¢˜"""
    print("
ğŸ”— å‘é‡ç›¸ä¼¼åº¦åˆ†æ"    print("-" * 40)
    print("ğŸ¯ ä½œç”¨ï¼šè¯„ä¼°å‘é‡åŒ–è´¨é‡ï¼Œå‘ç°é‡å¤å†…å®¹ï¼Œä¼˜åŒ–æ£€ç´¢å‚æ•°")

    try:
        # è®¡ç®—å‰10ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦
        sample_size = min(10, vector_info['vector_count'])
        sample_vectors = vectors[:sample_size]

        # å½’ä¸€åŒ–å‘é‡ç”¨äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        norms = np.linalg.norm(sample_vectors, axis=1, keepdims=True)
        normalized_vectors = sample_vectors / norms

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = np.dot(normalized_vectors, normalized_vectors.T)

        print(f"ğŸ“ˆ å‰{sample_size}ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦çŸ©é˜µ:")
        print("     ", end="")
        for i in range(sample_size):
            print("8d")
        print()

        for i in range(sample_size):
            print("3d", end="")
            for j in range(sample_size):
                if i == j:
                    print("1.0000", end="")
                else:
                    sim = similarity_matrix[i, j]
                    # ç”¨ä¸åŒé¢œè‰²æ ‡è¯†ç›¸ä¼¼åº¦çº§åˆ«
                    if sim > 0.8:
                        print(".4f", end="")
                    elif sim > 0.6:
                        print(".4f", end="")
                    elif sim > 0.4:
                        print(".4f", end="")
                    else:
                        print(".4f", end="")
            print()

        # åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ
        similarities = []
        for i in range(sample_size):
            for j in range(i+1, sample_size):
                similarities.append(similarity_matrix[i, j])

        if similarities:
            similarities = np.array(similarities)
            print("
ğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡:"            print(".4f"            print(".4f"            print(".4f"            print(".4f"
            # è´¨é‡è¯„ä¼°
            print("
ğŸ” è´¨é‡è¯„ä¼°:"            if similarities.mean() > 0.7:
                print("  âš ï¸  ç›¸ä¼¼åº¦è¿‡é«˜ï¼šå¯èƒ½å­˜åœ¨å¤§é‡é‡å¤å†…å®¹")
            elif similarities.mean() > 0.5:
                print("  âœ… ç›¸ä¼¼åº¦é€‚ä¸­ï¼šå†…å®¹å¤šæ ·æ€§è‰¯å¥½")
            elif similarities.mean() > 0.3:
                print("  âœ… ç›¸ä¼¼åº¦æ­£å¸¸ï¼šå†…å®¹åŒºåˆ†åº¦è‰¯å¥½")
            else:
                print("  âœ… ç›¸ä¼¼åº¦è¾ƒä½ï¼šå†…å®¹é«˜åº¦å¤šæ ·åŒ–")

            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é«˜çš„ç›¸ä¼¼åº¦
            high_sim_pairs = [(i, j) for i in range(sample_size) for j in range(i+1, sample_size) if similarity_matrix[i, j] > 0.9]
            if high_sim_pairs:
                print(f"  ğŸš¨ å‘ç° {len(high_sim_pairs)} å¯¹é«˜åº¦ç›¸ä¼¼çš„å‘é‡ (>0.9)")
                print("     å¯èƒ½è¡¨ç¤ºé‡å¤å†…å®¹æˆ–å‘é‡åŒ–å¼‚å¸¸")

    except Exception as e:
        print(f"âŒ ç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {e}")

def generate_quality_report(vector_store, structure_info, image_info, table_info, vector_info):
    """ç”Ÿæˆæ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š"""
    print("
ğŸ“‹ æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š"    print("=" * 60)

    report = {
        'overall_score': 0,
        'issues': [],
        'recommendations': []
    }

    # æ£€æŸ¥1: æ–‡æ¡£æ•°é‡åˆç†æ€§
    if structure_info and structure_info['total_docs'] > 0:
        print("âœ… æ–‡æ¡£æ•°é‡æ£€æŸ¥é€šè¿‡")
        report['overall_score'] += 20
    else:
        print("âŒ æ–‡æ¡£æ•°é‡å¼‚å¸¸")
        report['issues'].append("æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
        report['recommendations'].append("æ£€æŸ¥æ–‡æ¡£å¤„ç†æµç¨‹æ˜¯å¦æ­£å¸¸")

    # æ£€æŸ¥2: å…ƒæ•°æ®å®Œæ•´æ€§
    if structure_info and len(structure_info['all_fields']) > 10:
        print("âœ… å…ƒæ•°æ®å­—æ®µä¸°å¯Œ")
        report['overall_score'] += 25
    else:
        print("âŒ å…ƒæ•°æ®å­—æ®µä¸è¶³")
        report['issues'].append("å…ƒæ•°æ®å­—æ®µæ•°é‡ä¸è¶³")
        report['recommendations'].append("æ£€æŸ¥metadataæå–é€»è¾‘")

    # æ£€æŸ¥3: å›¾ç‰‡æ–‡æ¡£è´¨é‡
    if image_info and image_info['total_image_docs'] > 0:
        enhanced_ratio = image_info['enhanced_description_stats']['with_enhanced'] / max(1, image_info['total_image_docs'])
        if enhanced_ratio > 0.8:
            print("âœ… å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡è‰¯å¥½")
            report['overall_score'] += 20
        elif enhanced_ratio > 0.5:
            print("âš ï¸  å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡ä¸€èˆ¬")
            report['overall_score'] += 15
            report['recommendations'].append("æå‡å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡")
        else:
            print("âŒ å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡ä¸è¶³")
            report['issues'].append("å›¾ç‰‡å¢å¼ºæè¿°è¦†ç›–ç‡ä½")
            report['recommendations'].append("æ£€æŸ¥å›¾ç‰‡å¢å¼ºå¤„ç†æµç¨‹")
    else:
        print("âš ï¸  æ²¡æœ‰å›¾ç‰‡æ–‡æ¡£")
        report['recommendations'].append("æ£€æŸ¥å›¾ç‰‡å¤„ç†æµç¨‹")

    # æ£€æŸ¥4: è¡¨æ ¼æ–‡æ¡£è´¨é‡
    if table_info and table_info['total_table_docs'] > 0:
        processed_ratio = table_info['has_processed_content'] / max(1, table_info['total_table_docs'])
        if processed_ratio > 0.8:
            print("âœ… è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†è¦†ç›–ç‡è‰¯å¥½")
            report['overall_score'] += 15
        else:
            print("âŒ è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†è¦†ç›–ç‡ä¸è¶³")
            report['issues'].append("è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†è¦†ç›–ç‡ä½")
            report['recommendations'].append("æ£€æŸ¥è¡¨æ ¼è¯­ä¹‰åŒ–å¤„ç†æµç¨‹")
    else:
        print("âš ï¸  æ²¡æœ‰è¡¨æ ¼æ–‡æ¡£")

    # æ£€æŸ¥5: å‘é‡æ•°æ®è´¨é‡
    if vector_info and vector_info['vector_count'] > 0:
        if 'norm_stats' in vector_info:
            norm_std = vector_info['norm_stats']['std']
            if norm_std < 1.0:
                print("âœ… å‘é‡èŒƒæ•°åˆ†å¸ƒå‡åŒ€")
                report['overall_score'] += 20
            else:
                print("âš ï¸  å‘é‡èŒƒæ•°åˆ†å¸ƒä¸å‡åŒ€")
                report['overall_score'] += 15
                report['recommendations'].append("æ£€æŸ¥å‘é‡æ ‡å‡†åŒ–å¤„ç†")

        # æ£€æŸ¥å‘é‡ç»´åº¦ä¸€è‡´æ€§
        if vector_info['vector_dimension'] == 1536:  # DashScopeé»˜è®¤ç»´åº¦
            print("âœ… å‘é‡ç»´åº¦æ­£ç¡®")
        else:
            print(f"âš ï¸  å‘é‡ç»´åº¦å¼‚å¸¸: {vector_info['vector_dimension']}")
            report['issues'].append(f"å‘é‡ç»´åº¦å¼‚å¸¸: {vector_info['vector_dimension']}")
            report['recommendations'].append("æ£€æŸ¥embeddingæ¨¡å‹é…ç½®")
    else:
        print("âŒ æ²¡æœ‰å‘é‡æ•°æ®")
        report['issues'].append("æ²¡æœ‰å‘é‡æ•°æ®")
        report['recommendations'].append("æ£€æŸ¥å‘é‡åŒ–æµç¨‹")

    # ç”Ÿæˆæ€»ä½“è¯„ä»·
    print("
ğŸ† æ€»ä½“è¯„ä»·:"    if report['overall_score'] >= 90:
        print("  ğŸ‰ ä¼˜ç§€ - æ•°æ®è´¨é‡éå¸¸è‰¯å¥½")
    elif report['overall_score'] >= 70:
        print("  âœ… è‰¯å¥½ - æ•°æ®è´¨é‡åŸºæœ¬æ»¡è¶³è¦æ±‚")
    elif report['overall_score'] >= 50:
        print("  âš ï¸  ä¸€èˆ¬ - æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›")
    else:
        print("  âŒ è¾ƒå·® - æ•°æ®è´¨é‡å­˜åœ¨ä¸¥é‡é—®é¢˜")

    print(f"  ğŸ“Š ç»¼åˆå¾—åˆ†: {report['overall_score']}/100")

    if report['issues']:
        print("
ğŸ”§ å‘ç°çš„é—®é¢˜:"        for issue in report['issues']:
            print(f"  â€¢ {issue}")

    if report['recommendations']:
        print("
ğŸ’¡ æ”¹è¿›å»ºè®®:"        for rec in report['recommendations']:
            print(f"  â€¢ {rec}")

    report['overall_score'] = report['overall_score']
    return report

def check_memory_content():
    """æ£€æŸ¥è®°å¿†æ–‡ä»¶å†…å®¹"""
    memory_info = {
        'conversation_contexts': {
            'exists': False,
            'users': []
        },
        'user_preferences': {
            'exists': False,
            'users': []
        }
    }
    
    print("\nğŸ“ æ£€æŸ¥è®°å¿†æ–‡ä»¶å†…å®¹")
    print("=" * 50)
    
    # æ£€æŸ¥conversation_contexts.json
    try:
        memory_db_path = Path('central/memory_db')
        conv_path = memory_db_path / 'conversation_contexts.json'
        if conv_path.exists():
            with open(conv_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            memory_info['conversation_contexts']['exists'] = True
            print("ğŸ“ conversation_contexts.json:")
            for user_id, context in data.items():
                history = context.get('conversation_history', [])
                user_info = {
                    'user_id': user_id,
                    'history_count': len(history)
                }
                print(f"  {user_id}: {len(history)} æ¡è®°å½•")
                if user_id == 'test_user':
                    print("    æœ€å3ä¸ªé—®é¢˜:")
                    user_info['last_questions'] = []
                    for i, item in enumerate(history[-3:]):
                        question = item['question']
                        print(f"      {i+1}. {question}")
                        user_info['last_questions'].append(question)
                    last_question = context.get('last_question', 'N/A')
                    print(f"    æœ€æ–°é—®é¢˜: {last_question}")
                    user_info['last_question'] = last_question
                memory_info['conversation_contexts']['users'].append(user_info)
        else:
            print(f"âŒ conversation_contexts.jsonæ–‡ä»¶ä¸å­˜åœ¨: {conv_path}")
    except Exception as e:
        print(f"âŒ è¯»å–conversation_contexts.jsonå¤±è´¥: {e}")
        memory_info['conversation_contexts']['error'] = str(e)
    
    # æ£€æŸ¥user_preferences.json
    try:
        pref_path = memory_db_path / 'user_preferences.json'
        if pref_path.exists():
            with open(pref_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            memory_info['user_preferences']['exists'] = True
            print("\nğŸ“Š user_preferences.json:")
            for user_id, prefs in data.items():
                interest_areas = prefs.get('interest_areas', [])
                frequent_queries = prefs.get('frequent_queries', [])
                user_info = {
                    'user_id': user_id,
                    'interest_areas_count': len(interest_areas),
                    'frequent_queries_count': len(frequent_queries)
                }
                print(f"  {user_id}: {len(interest_areas)} ä¸ªå…´è¶£é¢†åŸŸ, {len(frequent_queries)} ä¸ªå¸¸ç”¨æŸ¥è¯¢")
                memory_info['user_preferences']['users'].append(user_info)
        else:
            print(f"âŒ user_preferences.jsonæ–‡ä»¶ä¸å­˜åœ¨: {pref_path}")
    except Exception as e:
        print(f"âŒ è¯»å–user_preferences.jsonå¤±è´¥: {e}")
        memory_info['user_preferences']['error'] = str(e)
    
    return memory_info

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å‘é‡æ•°æ®åº“ç»¼åˆè¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    try:
        config = Settings.load_from_file('config.json')
        vector_db_path = config.vector_db_dir
        
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
        if not os.path.exists(vector_db_path):
            possible_paths = [
                "./central/vector_db",
                "./vector_db",
                "./central/vector_db_test",
                "./vector_db_test"
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    vector_db_path = path
                    print(f"âœ… æ‰¾åˆ°å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
                    break
            else:
                print(f"âŒ å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•è¿‡çš„è·¯å¾„:")
                for path in possible_paths:
                    print(f"   - {path}")
                return
        
        # æ£€æŸ¥æ–‡ä»¶ç»“æ„
        file_info = check_vector_db_files(vector_db_path)
        if not file_info:
            print("âŒ æ— æ³•æ£€æŸ¥æ–‡ä»¶ç»“æ„ï¼Œç›®å½•ä¸å­˜åœ¨")
            return
        
        # æ£€æŸ¥ç´¢å¼•ç»“æ„
        index_info = check_index_structure(vector_db_path)
        
        # åˆ†æå…ƒæ•°æ®ç»“æ„
        metadata_info = analyze_metadata_structure(vector_db_path)
        
        # æ£€æŸ¥FAISSç´¢å¼•ç»“æ„
        faiss_info = check_faiss_structure(vector_db_path)
        
        # åŠ è½½å‘é‡å­˜å‚¨
        vector_store = load_vector_store(vector_db_path)
        if not vector_store:
            print("âŒ æ— æ³•åŠ è½½å‘é‡å­˜å‚¨")
            return
        
        # åˆ†ææ•°æ®åº“ç»“æ„
        structure_info = analyze_database_structure(vector_store)
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£
        image_info = check_image_docs(vector_store)
        
        # åˆ†æè¡¨æ ¼æ–‡æ¡£
        table_info = analyze_table_docs(vector_store)

        # åˆ†æå‘é‡æ•°æ®
        vector_info = analyze_vector_data(vector_store)

        # ç”Ÿæˆæ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š
        quality_report = generate_quality_report(vector_store, structure_info, image_info, table_info, vector_info)

        # æ£€æŸ¥è®°å¿†æ–‡ä»¶å†…å®¹
        memory_info = check_memory_content()
        
        # TODO: æ•´åˆå…¶ä»–è¯Šæ–­åŠŸèƒ½
        print("ğŸ“Š å¼€å§‹ç»¼åˆè¯Šæ–­...")
        
        # ä¿å­˜åˆ†æç»“æœçš„é€‰æ‹©
        save_choice = input("\næ˜¯å¦ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶? (y/n): ").strip().lower()
        if save_choice == 'y':
            output_file = "vector_db_diagnostic_results.json"
            results = {
                'file_info': file_info,
                'index_info': index_info,
                'metadata_info': metadata_info,
                'faiss_info': faiss_info,
                'structure_info': structure_info,
                'image_info': image_info,
                'table_info': table_info,
                'vector_info': vector_info,
                'quality_report': quality_report,
                'memory_info': memory_info
            }
            
            # è½¬æ¢ç»“æœä¸­çš„setç±»å‹ä¸ºlistç±»å‹ä»¥ç¡®ä¿JSONå¯åºåˆ—åŒ–
            def convert_sets_to_lists(obj):
                if isinstance(obj, set):
                    return list(obj)
                elif isinstance(obj, dict):
                    return {k: convert_sets_to_lists(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_sets_to_lists(item) for item in obj]
                elif isinstance(obj, tuple):
                    return tuple(convert_sets_to_lists(item) for item in obj)
                else:
                    return obj
            
            serializable_results = convert_sets_to_lists(results)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        print("\nâœ… æ•°æ®åº“ç»¼åˆè¯Šæ–­å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
