#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
ç¨‹åºè¯´æ˜ï¼š
## 1. ç»¼åˆè¯Šæ–­å·¥å…·ï¼Œç”¨äºåˆ†ævector_dbçš„ç»“æ„ã€indexã€å…ƒæ•°æ®åŠå†…å®¹
## 2. æ•´åˆäº†å¤šä¸ªç‹¬ç«‹è¯Šæ–­è„šæœ¬çš„åŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„è¯Šæ–­ç•Œé¢
'''

import sys
import os
import json
import pickle
import logging
import numpy as np
from pathlib import Path
from collections import defaultdict

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import Settings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings

# å¯¼å…¥ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—
from config.api_key_manager import get_dashscope_api_key

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_vector_store(vector_db_path):
    """åŠ è½½å‘é‡å­˜å‚¨"""
    try:
        config = Settings.load_from_file('config.json')
        
        # ä½¿ç”¨ç»Ÿä¸€çš„APIå¯†é’¥ç®¡ç†æ¨¡å—è·å–APIå¯†é’¥
        config_key = config.dashscope_api_key
        api_key = get_dashscope_api_key(config_key)
        
        if not api_key:
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
            return None
        
        # åˆå§‹åŒ–DashScope embeddings
        try:
            embedding_model = config.text_embedding_model
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®ï¼Œä½¿ç”¨é»˜è®¤embeddingæ¨¡å‹: {e}")
            embedding_model = 'text-embedding-v1'
        
        embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model=embedding_model)
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        logger.info(f"å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(vector_store.docstore._dict)} ä¸ªæ–‡æ¡£")
        return vector_store
    except Exception as e:
        logger.error(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
        return None

def check_vector_db_files(vector_db_path):
    """æ£€æŸ¥vector_dbä¸­çš„æ–‡ä»¶"""
    print(f"ğŸ“ æ£€æŸ¥ç›®å½•: {vector_db_path}")
    print(f"ç›®å½•å­˜åœ¨: {os.path.exists(vector_db_path)}")
    
    if not os.path.exists(vector_db_path):
        return None
    
    vector_db_path = Path(vector_db_path)
    files = list(vector_db_path.glob('*'))
    print(f"æ–‡ä»¶æ•°é‡: {len(files)}")
    file_info = []
    
    for file_path in files:
        info = {
            'name': file_path.name,
            'size': file_path.stat().st_size
        }
        print(f"\nğŸ“„ æ–‡ä»¶: {file_path.name}")
        print(f"  å¤§å°: {file_path.stat().st_size} bytes")
        
        if file_path.suffix == '.pkl':
            try:
                with open(file_path, 'rb') as f:
                    data = pickle.load(f)
                
                info['type'] = str(type(data))
                info['length'] = len(data) if hasattr(data, '__len__') else 'No length'
                print(f"  ç±»å‹: {type(data)}")
                print(f"  é•¿åº¦: {len(data) if hasattr(data, '__len__') else 'No length'}")
                
                if hasattr(data, '__len__') and len(data) > 0:
                    if isinstance(data, list):
                        info['first_element_type'] = str(type(data[0]))
                        print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(data[0])}")
                        if isinstance(data[0], dict):
                            info['first_element_keys'] = list(data[0].keys())
                            print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ é”®: {list(data[0].keys())}")
                    elif isinstance(data, dict):
                        info['keys'] = list(data.keys())
                        print(f"  é”®: {list(data.keys())}")
                            
            except Exception as e:
                info['error'] = str(e)
                print(f"  è¯»å–å¤±è´¥: {e}")
        
        elif file_path.suffix == '.faiss':
            info['type'] = 'FAISSç´¢å¼•æ–‡ä»¶'
            print(f"  FAISSç´¢å¼•æ–‡ä»¶")
        
        file_info.append(info)
    
    return file_info

def check_index_structure(vector_db_path):
    """æ£€æŸ¥index.pklçš„è¯¦ç»†ç»“æ„"""
    index_path = Path(vector_db_path) / 'index.pkl'
    index_info = {
        'path': str(index_path),
        'exists': index_path.exists()
    }
    
    print(f"\nğŸ“Š index.pkl ç»“æ„åˆ†æ")
    print(f"æ–‡ä»¶è·¯å¾„: {index_path}")
    print(f"æ–‡ä»¶å­˜åœ¨: {index_path.exists()}")
    
    if not index_path.exists():
        print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
        return index_info
    
    try:
        with open(index_path, 'rb') as f:
            index_data = pickle.load(f)
        
        index_info['type'] = str(type(index_data))
        index_info['length'] = len(index_data)
        print(f"æ•°æ®ç±»å‹: {type(index_data)}")
        print(f"æ•°æ®é•¿åº¦: {len(index_data)}")
        print()
        
        index_info['elements'] = []
        for i, item in enumerate(index_data):
            element_info = {
                'index': i+1,
                'type': str(type(item)),
                'length': len(item) if hasattr(item, '__len__') else 'No length'
            }
            print(f"ğŸ“‹ ç¬¬{i+1}ä¸ªå…ƒç´ :")
            print(f"  ç±»å‹: {type(item)}")
            print(f"  é•¿åº¦: {len(item) if hasattr(item, '__len__') else 'No length'}")
            
            if isinstance(item, list):
                element_info['is_list'] = True
                element_info['list_length'] = len(item)
                print(f"  åˆ—è¡¨å…ƒç´ æ•°é‡: {len(item)}")
                if len(item) > 0:
                    element_info['first_element_type'] = str(type(item[0]))
                    print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(item[0])}")
                    if isinstance(item[0], dict):
                        element_info['first_element_keys'] = list(item[0].keys())
                        print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ é”®: {list(item[0].keys())}")
            
            elif isinstance(item, dict):
                element_info['is_dict'] = True
                element_info['keys'] = list(item.keys())
                print(f"  å­—å…¸é”®: {list(item.keys())}")
            
            index_info['elements'].append(element_info)
            print()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…ƒæ•°æ®
        print("ğŸ” æŸ¥æ‰¾å…ƒæ•°æ®:")
        index_info['metadata'] = []
        for i, item in enumerate(index_data):
            if isinstance(item, list) and len(item) > 0:
                # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦æœ‰document_nameå­—æ®µ
                if isinstance(item[0], dict) and 'document_name' in item[0]:
                    metadata_info = {
                        'element_index': i+1,
                        'doc_count': len(item)
                    }
                    print(f"  ç¬¬{i+1}ä¸ªå…ƒç´ åŒ…å«å…ƒæ•°æ®ï¼Œæ–‡æ¡£æ•°é‡: {len(item)}")
                    # ç»Ÿè®¡chunk_type
                    chunk_types = {}
                    for doc in item:
                        if isinstance(doc, dict):
                            chunk_type = doc.get('chunk_type', 'unknown')
                            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
                    metadata_info['chunk_types'] = chunk_types
                    print(f"  Chunkç±»å‹åˆ†å¸ƒ: {chunk_types}")
                    
                    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
                    print(f"  ç¤ºä¾‹æ–‡æ¡£:")
                    metadata_info['sample_docs'] = []
                    for j, doc in enumerate(item[:2]):
                        if isinstance(doc, dict):
                            doc_info = {
                                'index': j+1,
                                'chunk_type': doc.get('chunk_type', 'N/A'),
                                'document_name': doc.get('document_name', 'N/A'),
                                'page_number': doc.get('page_number', 'N/A')
                            }
                            print(f"    [{j+1}] {doc.get('chunk_type', 'N/A')}: {doc.get('document_name', 'N/A')} (p.{doc.get('page_number', 'N/A')})")
                            metadata_info['sample_docs'].append(doc_info)
                    index_info['metadata'].append(metadata_info)
                    break
        
        return index_info
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        index_info['error'] = str(e)
        return index_info

def analyze_metadata_structure(vector_db_path):
    """åˆ†æå…ƒæ•°æ®ç»“æ„"""
    metadata_path = Path(vector_db_path) / 'metadata.pkl'
    metadata_info = {
        'path': str(metadata_path),
        'exists': metadata_path.exists()
    }
    
    print(f"\nğŸ“‹ å…ƒæ•°æ®ç»“æ„åˆ†æ")
    print(f"æ–‡ä»¶è·¯å¾„: {metadata_path}")
    print(f"æ–‡ä»¶å­˜åœ¨: {metadata_path.exists()}")
    
    if not metadata_path.exists():
        print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
        return metadata_info
    
    try:
        with open(metadata_path, 'rb') as f:
            metadata = pickle.load(f)
        
        metadata_info['type'] = str(type(metadata))
        metadata_info['length'] = len(metadata)
        print(f"æ•°æ®ç±»å‹: {type(metadata)}")
        print(f"æ•°æ®é•¿åº¦: {len(metadata)}")
        print()
        
        # åˆ†æå­—æ®µç»“æ„
        field_types = defaultdict(set)
        field_examples = defaultdict(list)
        chunk_type_fields = defaultdict(set)
        
        for i, item in enumerate(metadata):
            if isinstance(item, dict):
                chunk_type = item.get('chunk_type', 'unknown')
                
                for field, value in item.items():
                    field_types[field].add(type(value).__name__)
                    chunk_type_fields[chunk_type].add(field)
                    
                    # ä¿å­˜å‰3ä¸ªç¤ºä¾‹
                    if len(field_examples[field]) < 3:
                        field_examples[field].append({
                            'chunk_type': chunk_type,
                            'value': str(value)[:100] + '...' if len(str(value)) > 100 else str(value)
                        })
        
        metadata_info['field_types'] = {k: list(v) for k, v in field_types.items()}
        metadata_info['chunk_type_fields'] = {k: list(v) for k, v in chunk_type_fields.items()}
        metadata_info['field_examples'] = {k: v for k, v in field_examples.items()}
        
        print("ğŸ” å­—æ®µç±»å‹åˆ†æ:")
        for field, types in sorted(field_types.items()):
            print(f"  {field}: {', '.join(sorted(types))}")
        
        print("\nğŸ“‹ æŒ‰chunk_typeåˆ†ç»„çš„å­—æ®µ:")
        for chunk_type, fields in sorted(chunk_type_fields.items()):
            print(f"  {chunk_type}: {', '.join(sorted(fields))}")
        
        print("\nğŸ“ å­—æ®µç¤ºä¾‹:")
        for field, examples in sorted(field_examples.items()):
            print(f"  {field}:")
            for example in examples:
                print(f"    [{example['chunk_type']}] {example['value']}")
        
        # åˆ†æç‰¹å®šå­—æ®µ
        print("\nğŸ¯ å…³é”®å­—æ®µåˆ†æ:")
        key_fields = ['document_name', 'page_number', 'chunk_type', 'source', 'title']
        metadata_info['key_fields'] = []
        for field in key_fields:
            if field in field_types:
                values = [item.get(field, 'N/A') for item in metadata if isinstance(item, dict)]
                unique_values = set(values)
                field_info = {
                    'field': field,
                    'unique_count': len(unique_values)
                }
                print(f"  {field}: {len(unique_values)} ä¸ªå”¯ä¸€å€¼")
                if len(unique_values) <= 5:
                    field_info['values'] = list(unique_values)
                    print(f"    å€¼: {list(unique_values)}")
                else:
                    field_info['sample_values'] = list(unique_values)[:5]
                    print(f"    å‰5ä¸ªå€¼: {list(unique_values)[:5]}")
                metadata_info['key_fields'].append(field_info)
        
        return metadata_info
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        metadata_info['error'] = str(e)
        return metadata_info

def check_faiss_structure(vector_db_path):
    """æ£€æŸ¥FAISSç´¢å¼•çš„å®é™…ç»“æ„"""
    faiss_path = Path(vector_db_path) / 'index.faiss'
    faiss_info = {
        'path': str(faiss_path),
        'exists': faiss_path.exists()
    }
    
    print(f"\nğŸ” æ£€æŸ¥FAISSç´¢å¼•ç»“æ„:")
    print("=" * 60)
    
    if not faiss_path.exists():
        print(f"âŒ FAISSæ–‡ä»¶ä¸å­˜åœ¨: {faiss_path}")
        return faiss_info
    
    try:
        # æ£€æŸ¥FAISSæ–‡ä»¶å¤§å°
        faiss_size = faiss_path.stat().st_size
        faiss_info['size'] = faiss_size
        print(f"ğŸ“ FAISSæ–‡ä»¶å¤§å°: {faiss_size} bytes")
        
        # å°è¯•åŠ è½½FAISSç´¢å¼•
        try:
            import faiss
            index = faiss.read_index(str(faiss_path))
            faiss_info['type'] = str(type(index))
            faiss_info['dimension'] = index.d
            faiss_info['vector_count'] = index.ntotal
            print(f"ğŸ”¢ FAISSç´¢å¼•ä¿¡æ¯:")
            print(f"  å‘é‡ç»´åº¦: {index.d}")
            print(f"  å‘é‡æ•°é‡: {index.ntotal}")
            print(f"  ç´¢å¼•ç±»å‹: {type(index)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡æ•°æ®
            if hasattr(index, 'get_xb'):
                try:
                    vectors = index.get_xb()
                    faiss_info['vectors_type'] = str(type(vectors))
                    print(f"  å‘é‡æ•°æ®ç±»å‹: {type(vectors)}")
                    if hasattr(vectors, 'shape'):
                        faiss_info['vectors_shape'] = vectors.shape
                        print(f"  å‘é‡æ•°æ®å½¢çŠ¶: {vectors.shape}")
                    
                    # æ£€æŸ¥å‰å‡ ä¸ªå‘é‡
                    if vectors.shape[0] > 0:
                        faiss_info['first_vector_sample'] = vectors[0][:5].tolist()
                        faiss_info['norm_range'] = {
                            'min': float(np.linalg.norm(vectors, axis=1).min()),
                            'max': float(np.linalg.norm(vectors, axis=1).max())
                        }
                        print(f"  ç¬¬ä¸€ä¸ªå‘é‡: {vectors[0][:5]}... (å‰5ç»´)")
                        print(f"  å‘é‡èŒƒæ•°èŒƒå›´: {np.linalg.norm(vectors, axis=1).min():.4f} - {np.linalg.norm(vectors, axis=1).max():.4f}")
                except Exception as e:
                    faiss_info['vectors_error'] = str(e)
                    print(f"  æ— æ³•è·å–å‘é‡æ•°æ®: {e}")
            
        except ImportError:
            faiss_info['error'] = "æ— æ³•å¯¼å…¥faissåº“"
            print("âŒ æ— æ³•å¯¼å…¥faissåº“")
        except Exception as e:
            faiss_info['error'] = str(e)
            print(f"âŒ åŠ è½½FAISSç´¢å¼•å¤±è´¥: {e}")
        
        return faiss_info
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        faiss_info['error'] = str(e)
        return faiss_info

def analyze_database_structure(vector_store):
    """åˆ†ææ•°æ®åº“ç»“æ„"""
    print("\nğŸ“Š å‘é‡æ•°æ®åº“ç»“æ„åˆ†æ")
    print("=" * 60)
    
    if not vector_store or not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
        print("âŒ å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸")
        return None
    
    docstore = vector_store.docstore._dict
    total_docs = len(docstore)
    print(f"ğŸ“š æ€»æ–‡æ¡£æ•°: {total_docs}")
    
    if total_docs == 0:
        print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
        return None
    
    # åˆ†ææ–‡æ¡£ç±»å‹åˆ†å¸ƒ
    chunk_types = {}
    document_names = set()
    all_fields = set()
    
    # æ”¶é›†æ‰€æœ‰å­—æ®µå’Œç»Ÿè®¡ä¿¡æ¯
    for doc_id, doc in docstore.items():
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        
        # ç»Ÿè®¡åˆ†å—ç±»å‹
        chunk_type = metadata.get('chunk_type', 'unknown')
        if chunk_type not in chunk_types:
            chunk_types[chunk_type] = 0
        chunk_types[chunk_type] += 1
        
        # ç»Ÿè®¡æ–‡æ¡£åç§°
        doc_name = metadata.get('document_name', 'unknown')
        document_names.add(doc_name)
        
        # æ”¶é›†æ‰€æœ‰å­—æ®µ
        all_fields.update(metadata.keys())
    
    print(f"\nğŸ“Š åˆ†å—ç±»å‹åˆ†å¸ƒ:")
    for chunk_type, count in chunk_types.items():
        print(f"  {chunk_type}: {count}")
    
    print(f"\nğŸ“š æ–‡æ¡£ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æ¡£æ•°: {total_docs}")
    print(f"  å”¯ä¸€æ–‡æ¡£å: {len(document_names)}")
    print(f"  æ–‡æ¡£åç§°: {sorted(list(document_names))[:5]}...")
    
    print(f"\nğŸ“‹ å­—æ®µç»Ÿè®¡:")
    print(f"  æ€»å­—æ®µæ•°: {len(all_fields)}")
    print(f"  æ‰€æœ‰å­—æ®µ: {sorted(list(all_fields))}")
    
    return {
        'total_docs': total_docs,
        'chunk_types': chunk_types,
        'document_names': list(document_names),
        'all_fields': list(all_fields)
    }

def check_image_docs(vector_store):
    """æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£çš„å­˜å‚¨ç»“æ„å’Œenhanced_descriptionå­—æ®µ"""
    print(f"\nğŸ” æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£")
    print("=" * 60)
    
    if not vector_store or not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
        print("âŒ å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸")
        return None
    
    docstore = vector_store.docstore._dict
    image_docs = []
    for doc_id, doc in docstore.items():
        if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'image':
            image_docs.append((doc_id, doc))
    
    image_info = {
        'total_image_docs': len(image_docs),
        'enhanced_description_stats': {
            'with_enhanced': 0,
            'without_enhanced': 0
        },
        'samples': []
    }
    
    print(f"ğŸ“· æ‰¾åˆ° {len(image_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
    
    # åˆ†æenhanced_descriptionå­—æ®µ
    enhanced_count = 0
    empty_count = 0
    for doc_id, doc in image_docs:
        enhanced_desc = doc.metadata.get('enhanced_description', '')
        if enhanced_desc:
            enhanced_count += 1
        else:
            empty_count += 1
    
    image_info['enhanced_description_stats']['with_enhanced'] = enhanced_count
    image_info['enhanced_description_stats']['without_enhanced'] = empty_count
    
    print(f"âœ… æœ‰enhanced_descriptionçš„å›¾ç‰‡: {enhanced_count}")
    print(f"âŒ æ— enhanced_descriptionçš„å›¾ç‰‡: {empty_count}")
    print(f"ğŸ“ˆ è¦†ç›–ç‡: {enhanced_count/(enhanced_count+empty_count)*100:.1f}%" if (enhanced_count+empty_count) > 0 else "ğŸ“ˆ è¦†ç›–ç‡: N/A")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªå›¾ç‰‡æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
    for i, (doc_id, doc) in enumerate(image_docs[:3]):
        sample_info = {
            'index': i+1,
            'doc_id': doc_id,
            'document_name': doc.metadata.get('document_name', 'N/A'),
            'page_number': doc.metadata.get('page_number', 'N/A'),
            'image_id': doc.metadata.get('image_id', 'N/A'),
            'image_type': doc.metadata.get('image_type', 'N/A'),
            'img_caption': doc.metadata.get('img_caption', 'N/A'),
            'enhanced_description': doc.metadata.get('enhanced_description', '')[:100] + '...' if len(doc.metadata.get('enhanced_description', '')) > 100 else doc.metadata.get('enhanced_description', ''),
            'page_content_length': len(doc.page_content) if hasattr(doc, 'page_content') else 'N/A'
        }
        image_info['samples'].append(sample_info)
        
        print(f"\nğŸ“· å›¾ç‰‡æ–‡æ¡£ {i+1}:")
        print(f"  ID: {doc_id}")
        print(f"  æ–‡æ¡£å: {doc.metadata.get('document_name', 'N/A')}")
        print(f"  é¡µç : {doc.metadata.get('page_number', 'N/A')}")
        print(f"  å›¾ç‰‡ID: {doc.metadata.get('image_id', 'N/A')}")
        print(f"  å›¾ç‰‡ç±»å‹: {doc.metadata.get('image_type', 'N/A')}")
        print(f"  å›¾ç‰‡æ ‡é¢˜: {doc.metadata.get('img_caption', 'N/A')}")
        print(f"  å¢å¼ºæè¿°: {doc.metadata.get('enhanced_description', '')[:100] + '...' if len(doc.metadata.get('enhanced_description', '')) > 100 else doc.metadata.get('enhanced_description', '')}")
        print(f"  page_contenté•¿åº¦: {len(doc.page_content) if hasattr(doc, 'page_content') else 'N/A'}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡ç›¸å…³çš„å±æ€§
        vector_attrs = []
        for attr in dir(doc):
            if 'vector' in attr.lower() or 'embedding' in attr.lower():
                vector_attrs.append(attr)
        
        if vector_attrs:
            print(f"  å‘é‡ç›¸å…³å±æ€§: {vector_attrs}")
        else:
            print("  å‘é‡ç›¸å…³å±æ€§: æ— ")
        
        # æ£€æŸ¥metadataä¸­çš„semantic_features
        if hasattr(doc, 'metadata') and 'semantic_features' in doc.metadata:
            semantic = doc.metadata['semantic_features']
            print(f"  semantic_features: {semantic}")
        else:
            print("  semantic_features: æ— ")
    
    return image_info

def analyze_table_docs(vector_store):
    """åˆ†æè¡¨æ ¼æ–‡æ¡£çš„å†…å®¹å’Œå…ƒæ•°æ®"""
    print(f"\nğŸ” åˆ†æè¡¨æ ¼æ–‡æ¡£")
    print("=" * 60)
    
    if not vector_store or not hasattr(vector_store, 'docstore') or not hasattr(vector_store.docstore, '_dict'):
        print("âŒ å‘é‡å­˜å‚¨ç»“æ„å¼‚å¸¸")
        return None
    
    docstore = vector_store.docstore._dict
    table_docs = []
    for doc_id, doc in docstore.items():
        if hasattr(doc, 'metadata') and doc.metadata and doc.metadata.get('chunk_type') == 'table':
            table_docs.append((doc_id, doc))
    
    table_info = {
        'total_table_docs': len(table_docs),
        'metadata_fields': set(),
        'table_types': defaultdict(int),
        'document_names': set(),
        'content_lengths': [],
        'has_columns': 0,
        'has_table_type': 0,
        'has_html_content': 0,
        'has_processed_content': 0,
        'samples': []
    }
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(table_docs)} ä¸ªè¡¨æ ¼æ–‡æ¡£")
    
    # åˆ†æå‰å‡ ä¸ªè¡¨æ ¼æ–‡æ¡£çš„è¯¦ç»†å†…å®¹
    for i, (doc_id, doc) in enumerate(table_docs[:3]):
        sample_info = {
            'index': i+1,
            'doc_id': doc_id,
            'document_name': doc.metadata.get('document_name', 'N/A'),
            'page_number': doc.metadata.get('page_number', 'N/A'),
            'table_id': doc.metadata.get('table_id', 'N/A'),
            'table_type': doc.metadata.get('table_type', 'N/A'),
            'content_preview': doc.page_content[:200] + '...' if hasattr(doc, 'page_content') and len(doc.page_content) > 200 else (doc.page_content if hasattr(doc, 'page_content') else 'N/A'),
            'metadata': doc.metadata
        }
        table_info['samples'].append(sample_info)
        
        print(f"\n{'='*50}")
        print(f"ğŸ“„ è¡¨æ ¼æ–‡æ¡£ {i+1}")
        print(f"{'='*50}")
        print(f"æ–‡æ¡£ID: {doc_id}")
        print(f"æ–‡æ¡£å: {doc.metadata.get('document_name', 'N/A')}")
        print(f"é¡µç : {doc.metadata.get('page_number', 'N/A')}")
        print(f"è¡¨æ ¼ID: {doc.metadata.get('table_id', 'N/A')}")
        print(f"è¡¨æ ¼ç±»å‹: {doc.metadata.get('table_type', 'N/A')}")
        
        # åˆ†æå…ƒæ•°æ® - æ˜¾ç¤ºæ‰€æœ‰å­—æ®µçš„å€¼
        print(f"\nğŸ“‹ å…ƒæ•°æ®åˆ†æ:")
        print(f"  å…ƒæ•°æ®å­—æ®µ: {list(doc.metadata.keys())}")
        table_info['metadata_fields'].update(doc.metadata.keys())
        
        # æ˜¾ç¤ºæ¯ä¸ªå…ƒæ•°æ®å­—æ®µçš„è¯¦ç»†å€¼
        print(f"\nğŸ” è¯¦ç»†å…ƒæ•°æ®å­—æ®µå€¼:")
        for field, value in doc.metadata.items():
            if isinstance(value, str) and len(value) > 100:
                print(f"  {field}: {value[:100]}... (é•¿åº¦: {len(value)})")
            elif isinstance(value, list):
                if len(value) > 5:
                    print(f"  {field}: {value[:5]}... (å…±{len(value)}é¡¹)")
                else:
                    print(f"  {field}: {value}")
            else:
                print(f"  {field}: {value}")
        
        # ç»Ÿè®¡ç‰¹å®šå­—æ®µ
        if doc.metadata.get('table_type'):
            table_info['table_types'][doc.metadata['table_type']] += 1
            table_info['has_table_type'] += 1
        if doc.metadata.get('document_name'):
            table_info['document_names'].add(doc.metadata['document_name'])
        if doc.metadata.get('columns'):
            table_info['has_columns'] += 1
        
        # åˆ†æå†…å®¹
        if hasattr(doc, 'page_content') and doc.page_content:
            print(f"\nğŸ“ å†…å®¹åˆ†æ:")
            print(f"  å†…å®¹é•¿åº¦: {len(doc.page_content)}")
            table_info['content_lengths'].append(len(doc.page_content))
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«HTMLå†…å®¹
            if '<table' in doc.page_content.lower() or '<tr' in doc.page_content.lower() or '<td' in doc.page_content.lower():
                table_info['has_html_content'] += 1
                print("  å†…å®¹ç±»å‹: åŒ…å«HTMLè¡¨æ ¼å†…å®¹")
            else:
                print("  å†…å®¹ç±»å‹: ä¸åŒ…å«æ˜æ˜¾çš„HTMLè¡¨æ ¼å†…å®¹")
            
            # æ˜¾ç¤ºå‰200å­—ç¬¦
            content_preview = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
            print(f"  å†…å®¹é¢„è§ˆ: {content_preview}")
        else:
            print(f"\nâŒ æ²¡æœ‰é¡µé¢å†…å®¹")
        
        # æ£€æŸ¥è¯­ä¹‰åŒ–å†…å®¹
        has_semantic_content = 0
        key = 'processed_table_content'
        if key in doc.metadata and doc.metadata[key] is not None:
            has_semantic_content += 1
            print(f"  è¯­ä¹‰åŒ–å†…å®¹: å­˜åœ¨ ({key})")
            if len(doc.metadata[key]) > 0:
                print(f"  è¯­ä¹‰åŒ–å†…å®¹é¢„è§ˆ: {doc.metadata[key][:100] + '...' if len(doc.metadata[key]) > 100 else doc.metadata[key]}")
            else:
                print("  è¯­ä¹‰åŒ–å†…å®¹é¢„è§ˆ: (ç©ºå†…å®¹)")
        else:
            print(f"  è¯­ä¹‰åŒ–å†…å®¹: ä¸å­˜åœ¨")
            for alt_key in ['table_summary', 'table_title']:
                if alt_key in doc.metadata and doc.metadata[alt_key] is not None and len(doc.metadata[alt_key]) > 0:
                    print(f"  è¯­ä¹‰åŒ–å†…å®¹: å­˜åœ¨ ({alt_key})")
                    print(f"  è¯­ä¹‰åŒ–å†…å®¹é¢„è§ˆ: {doc.metadata[alt_key][:100] + '...' if len(doc.metadata[alt_key]) > 100 else doc.metadata[alt_key]}")
                    has_semantic_content += 1
                    break
        table_info['has_processed_content'] = has_semantic_content
    
    # åˆ†æå‰©ä½™æ–‡æ¡£çš„å…ƒæ•°æ®
    if len(table_docs) > 3:
        print(f"\nğŸ” åˆ†æå‰©ä½™ {len(table_docs) - 3} ä¸ªæ–‡æ¡£çš„å…ƒæ•°æ®...")
        for i, (doc_id, doc) in enumerate(table_docs[3:]):
            if hasattr(doc, 'metadata') and doc.metadata:
                table_info['metadata_fields'].update(doc.metadata.keys())
                
                # ç»Ÿè®¡ç‰¹å®šå­—æ®µ
                if doc.metadata.get('table_type'):
                    table_info['table_types'][doc.metadata['table_type']] += 1
                    table_info['has_table_type'] += 1
                if doc.metadata.get('document_name'):
                    table_info['document_names'].add(doc.metadata['document_name'])
                if doc.metadata.get('columns'):
                    table_info['has_columns'] += 1
                if hasattr(doc, 'page_content') and doc.page_content and any(tag in doc.page_content.lower() for tag in ['<table', '<tr', '<td']):
                    table_info['has_html_content'] += 1
                # æ£€æŸ¥è¯­ä¹‰åŒ–å†…å®¹
                has_semantic_content = 0
                key = 'processed_table_content'
                if key in doc.metadata and doc.metadata[key] is not None:
                    has_semantic_content += 1
                else:
                    for alt_key in ['table_summary', 'table_title']:
                        if alt_key in doc.metadata and doc.metadata[alt_key] is not None and len(doc.metadata[alt_key]) > 0:
                            has_semantic_content += 1
                            break
                table_info['has_processed_content'] += has_semantic_content
        print(f"  å®Œæˆå‰©ä½™æ–‡æ¡£åˆ†æ")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š è¡¨æ ¼æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)
    print(f"æ€»æ–‡æ¡£æ•°: {table_info['total_table_docs']}")
    print(f"æœ‰è¡¨æ ¼ç±»å‹çš„æ–‡æ¡£: {table_info['has_table_type']}")
    print(f"æœ‰åˆ—ä¿¡æ¯çš„æ–‡æ¡£: {table_info['has_columns']}")
    print(f"åŒ…å«HTMLå†…å®¹çš„æ–‡æ¡£: {table_info['has_html_content']}")
    print(f"åŒ…å«è¯­ä¹‰åŒ–å¤„ç†å†…å®¹çš„æ–‡æ¡£: {table_info['has_processed_content']}")
    print(f"\nå…ƒæ•°æ®å­—æ®µæ€»æ•°: {len(table_info['metadata_fields'])}")
    print(f"å…ƒæ•°æ®å­—æ®µ: {sorted(list(table_info['metadata_fields']))}")
    
    if table_info['table_types']:
        print(f"\nè¡¨æ ¼ç±»å‹åˆ†å¸ƒ:")
        for table_type, count in sorted(table_info['table_types'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {table_type}: {count}")
    
    if table_info['document_names']:
        print(f"\næ–‡æ¡£åç§° (å…±{len(table_info['document_names'])}ä¸ª):")
        for doc_name in sorted(list(table_info['document_names']))[:5]:
            print(f"  {doc_name}")
        if len(table_info['document_names']) > 5:
            print(f"  ... è¿˜æœ‰ {len(table_info['document_names']) - 5} ä¸ª")
    
    if table_info['content_lengths']:
        avg_length = sum(table_info['content_lengths']) / len(table_info['content_lengths'])
        print(f"\nå†…å®¹é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {avg_length:.1f}")
        print(f"  æœ€çŸ­é•¿åº¦: {min(table_info['content_lengths'])}")
        print(f"  æœ€é•¿é•¿åº¦: {max(table_info['content_lengths'])}")
    
    return table_info

def check_memory_content():
    """æ£€æŸ¥è®°å¿†æ–‡ä»¶å†…å®¹"""
    memory_info = {
        'conversation_contexts': {
            'exists': False,
            'users': []
        },
        'user_preferences': {
            'exists': False,
            'users': []
        }
    }
    
    print("\nğŸ“ æ£€æŸ¥è®°å¿†æ–‡ä»¶å†…å®¹")
    print("=" * 50)
    
    # æ£€æŸ¥conversation_contexts.json
    try:
        memory_db_path = Path('central/memory_db')
        conv_path = memory_db_path / 'conversation_contexts.json'
        if conv_path.exists():
            with open(conv_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            memory_info['conversation_contexts']['exists'] = True
            print("ğŸ“ conversation_contexts.json:")
            for user_id, context in data.items():
                history = context.get('conversation_history', [])
                user_info = {
                    'user_id': user_id,
                    'history_count': len(history)
                }
                print(f"  {user_id}: {len(history)} æ¡è®°å½•")
                if user_id == 'test_user':
                    print("    æœ€å3ä¸ªé—®é¢˜:")
                    user_info['last_questions'] = []
                    for i, item in enumerate(history[-3:]):
                        question = item['question']
                        print(f"      {i+1}. {question}")
                        user_info['last_questions'].append(question)
                    last_question = context.get('last_question', 'N/A')
                    print(f"    æœ€æ–°é—®é¢˜: {last_question}")
                    user_info['last_question'] = last_question
                memory_info['conversation_contexts']['users'].append(user_info)
        else:
            print(f"âŒ conversation_contexts.jsonæ–‡ä»¶ä¸å­˜åœ¨: {conv_path}")
    except Exception as e:
        print(f"âŒ è¯»å–conversation_contexts.jsonå¤±è´¥: {e}")
        memory_info['conversation_contexts']['error'] = str(e)
    
    # æ£€æŸ¥user_preferences.json
    try:
        pref_path = memory_db_path / 'user_preferences.json'
        if pref_path.exists():
            with open(pref_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            memory_info['user_preferences']['exists'] = True
            print("\nğŸ“Š user_preferences.json:")
            for user_id, prefs in data.items():
                interest_areas = prefs.get('interest_areas', [])
                frequent_queries = prefs.get('frequent_queries', [])
                user_info = {
                    'user_id': user_id,
                    'interest_areas_count': len(interest_areas),
                    'frequent_queries_count': len(frequent_queries)
                }
                print(f"  {user_id}: {len(interest_areas)} ä¸ªå…´è¶£é¢†åŸŸ, {len(frequent_queries)} ä¸ªå¸¸ç”¨æŸ¥è¯¢")
                memory_info['user_preferences']['users'].append(user_info)
        else:
            print(f"âŒ user_preferences.jsonæ–‡ä»¶ä¸å­˜åœ¨: {pref_path}")
    except Exception as e:
        print(f"âŒ è¯»å–user_preferences.jsonå¤±è´¥: {e}")
        memory_info['user_preferences']['error'] = str(e)
    
    return memory_info

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å‘é‡æ•°æ®åº“ç»¼åˆè¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    try:
        config = Settings.load_from_file('config.json')
        vector_db_path = config.vector_db_dir
        
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
        if not os.path.exists(vector_db_path):
            possible_paths = [
                "./central/vector_db",
                "./vector_db",
                "./central/vector_db_test",
                "./vector_db_test"
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    vector_db_path = path
                    print(f"âœ… æ‰¾åˆ°å‘é‡æ•°æ®åº“è·¯å¾„: {vector_db_path}")
                    break
            else:
                print(f"âŒ å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•è¿‡çš„è·¯å¾„:")
                for path in possible_paths:
                    print(f"   - {path}")
                return
        
        # æ£€æŸ¥æ–‡ä»¶ç»“æ„
        file_info = check_vector_db_files(vector_db_path)
        if not file_info:
            print("âŒ æ— æ³•æ£€æŸ¥æ–‡ä»¶ç»“æ„ï¼Œç›®å½•ä¸å­˜åœ¨")
            return
        
        # æ£€æŸ¥ç´¢å¼•ç»“æ„
        index_info = check_index_structure(vector_db_path)
        
        # åˆ†æå…ƒæ•°æ®ç»“æ„
        metadata_info = analyze_metadata_structure(vector_db_path)
        
        # æ£€æŸ¥FAISSç´¢å¼•ç»“æ„
        faiss_info = check_faiss_structure(vector_db_path)
        
        # åŠ è½½å‘é‡å­˜å‚¨
        vector_store = load_vector_store(vector_db_path)
        if not vector_store:
            print("âŒ æ— æ³•åŠ è½½å‘é‡å­˜å‚¨")
            return
        
        # åˆ†ææ•°æ®åº“ç»“æ„
        structure_info = analyze_database_structure(vector_store)
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£
        image_info = check_image_docs(vector_store)
        
        # åˆ†æè¡¨æ ¼æ–‡æ¡£
        table_info = analyze_table_docs(vector_store)
        
        # æ£€æŸ¥è®°å¿†æ–‡ä»¶å†…å®¹
        memory_info = check_memory_content()
        
        # TODO: æ•´åˆå…¶ä»–è¯Šæ–­åŠŸèƒ½
        print("ğŸ“Š å¼€å§‹ç»¼åˆè¯Šæ–­...")
        
        # ä¿å­˜åˆ†æç»“æœçš„é€‰æ‹©
        save_choice = input("\næ˜¯å¦ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶? (y/n): ").strip().lower()
        if save_choice == 'y':
            output_file = "vector_db_diagnostic_results.json"
            results = {
                'file_info': file_info,
                'index_info': index_info,
                'metadata_info': metadata_info,
                'faiss_info': faiss_info,
                'structure_info': structure_info,
                'image_info': image_info,
                'table_info': table_info,
                'memory_info': memory_info
            }
            
            # è½¬æ¢ç»“æœä¸­çš„setç±»å‹ä¸ºlistç±»å‹ä»¥ç¡®ä¿JSONå¯åºåˆ—åŒ–
            def convert_sets_to_lists(obj):
                if isinstance(obj, set):
                    return list(obj)
                elif isinstance(obj, dict):
                    return {k: convert_sets_to_lists(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_sets_to_lists(item) for item in obj]
                elif isinstance(obj, tuple):
                    return tuple(convert_sets_to_lists(item) for item in obj)
                else:
                    return obj
            
            serializable_results = convert_sets_to_lists(results)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        print("\nâœ… æ•°æ®åº“ç»¼åˆè¯Šæ–­å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
