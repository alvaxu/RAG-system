{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3ad4f-9492-45c4-a693-570ecb300582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "属于 InternVL 2.5系列\n",
    "视频理解与生成：可以用于视频内容的分析、总结和生成相关的文本描述。\n",
    "视觉问答：能够回答与图像或视频内容相关的问题。\n",
    "多模态对话：支持与用户进行包含视觉信息的对话。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86cd027c-59e9-467e-b198-db28f1a11397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/autodl-tmp/models/OpenGVLab/InternVideo2_5_Chat_8B\n"
     ]
    }
   ],
   "source": [
    "#模型下载，需要下载3个大模型\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('OpenGVLab/InternVideo2_5_Chat_8B', cache_dir='/root/autodl-tmp/models')\n",
    "#model_dir = snapshot_download('internlm/internlm2_5-7b-chat', cache_dir='/root/autodl-tmp/models')\n",
    "#model_dir = snapshot_download('LLM-Research/Mistral-7B-Instruct-v0.3', cache_dir='/root/autodl-tmp/models')\n",
    "#model_dir = snapshot_download('AI-ModelScope/bert-base-uncased', cache_dir='/root/autodl-tmp/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0cfc4c-d3b1-47d0-ba82-8c4754ad5f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 08:49:09.167664: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 08:49:09.185359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747270149.210169  914572 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747270149.216043  914572 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747270149.232877  914572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747270149.232894  914572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747270149.232895  914572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747270149.232897  914572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 08:49:09.239558: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b0826d7220431ebb241282f1aafffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from modelscope import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# 模型配置\n",
    "model_path = '/root/autodl-tmp/models/OpenGVLab/InternVideo2_5_Chat_8B'\n",
    "\n",
    "# 初始化分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda().to(torch.bfloat16)\n",
    "\n",
    "# ImageNet 数据集的均值和标准差\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"\n",
    "    构建图像转换pipeline\n",
    "    \n",
    "    参数:\n",
    "        input_size: 输入图像大小\n",
    "    \n",
    "    返回:\n",
    "        transform: 转换pipeline\n",
    "    \"\"\"\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img), \n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC), \n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"\n",
    "    寻找最接近原始图像宽高比的目标比例\n",
    "    \n",
    "    参数:\n",
    "        aspect_ratio: 原始图像的宽高比\n",
    "        target_ratios: 目标比例列表\n",
    "        width: 原始图像宽度\n",
    "        height: 原始图像高度\n",
    "        image_size: 目标图像大小\n",
    "        \n",
    "    返回:\n",
    "        best_ratio: 最佳比例\n",
    "    \"\"\"\n",
    "    best_ratio_diff = float(\"inf\")\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"\n",
    "    动态预处理图像，根据宽高比将图像分割成多个块\n",
    "    \n",
    "    参数:\n",
    "        image: 原始图像\n",
    "        min_num: 最小块数\n",
    "        max_num: 最大块数\n",
    "        image_size: 目标图像大小\n",
    "        use_thumbnail: 是否使用缩略图\n",
    "        \n",
    "    返回:\n",
    "        processed_images: 处理后的图像列表\n",
    "    \"\"\"\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # 计算现有图像宽高比\n",
    "    target_ratios = set((i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # 寻找最接近目标的宽高比\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # 计算目标宽度和高度\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # 调整图像大小\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = ((i % (target_width // image_size)) * image_size, (i // (target_width // image_size)) * image_size, \n",
    "               ((i % (target_width // image_size)) + 1) * image_size, ((i // (target_width // image_size)) + 1) * image_size)\n",
    "        # 分割图像\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image, input_size=448, max_num=6):\n",
    "    \"\"\"\n",
    "    加载并处理图像\n",
    "    \n",
    "    参数:\n",
    "        image: 输入图像\n",
    "        input_size: 输入大小\n",
    "        max_num: 最大块数\n",
    "        \n",
    "    返回:\n",
    "        pixel_values: 处理后的图像张量\n",
    "    \"\"\"\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    \"\"\"\n",
    "    获取视频帧索引\n",
    "    \n",
    "    参数:\n",
    "        bound: 时间边界 [开始时间, 结束时间]\n",
    "        fps: 视频帧率\n",
    "        max_frame: 最大帧数\n",
    "        first_idx: 第一帧索引\n",
    "        num_segments: 分段数量\n",
    "        \n",
    "    返回:\n",
    "        frame_indices: 帧索引数组\n",
    "    \"\"\"\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([int(start_idx + (seg_size / 2) + np.round(seg_size * idx)) for idx in range(num_segments)])\n",
    "    return frame_indices\n",
    "\n",
    "def get_num_frames_by_duration(duration):\n",
    "    \"\"\"\n",
    "    根据视频时长计算帧数\n",
    "    \n",
    "    参数:\n",
    "        duration: 视频时长（秒）\n",
    "        \n",
    "    返回:\n",
    "        num_frames: 计算出的帧数\n",
    "    \"\"\"\n",
    "    local_num_frames = 4        \n",
    "    num_segments = int(duration // local_num_frames)\n",
    "    if num_segments == 0:\n",
    "        num_frames = local_num_frames\n",
    "    else:\n",
    "        num_frames = local_num_frames * num_segments\n",
    "    \n",
    "    num_frames = min(512, num_frames)\n",
    "    num_frames = max(128, num_frames)\n",
    "\n",
    "    return num_frames\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32, get_frame_by_duration = False):\n",
    "    \"\"\"\n",
    "    加载并处理视频\n",
    "    \n",
    "    参数:\n",
    "        video_path: 视频路径\n",
    "        bound: 时间边界\n",
    "        input_size: 输入大小\n",
    "        max_num: 最大块数\n",
    "        num_segments: 分段数量\n",
    "        get_frame_by_duration: 是否根据时长获取帧数\n",
    "        \n",
    "    返回:\n",
    "        pixel_values: 处理后的视频帧张量\n",
    "        num_patches_list: 每帧的块数列表\n",
    "    \"\"\"\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    if get_frame_by_duration:\n",
    "        duration = max_frame / fps\n",
    "        num_segments = get_num_frames_by_duration(duration)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert(\"RGB\")\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e992d5-6e66-469a-8f21-062c00967c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video shows a person inspecting the damage to a white car's fender after a minor collision. The car's paint is scratched and dented, and the person is discussing the extent of the damage and potential repair options. They mention that the dent is not too deep and that the paint can be buffed out. The person also notes that the car's bumper is damaged and will need to be replaced. The scene takes place in an outdoor parking area with a green wall in the background.\n",
      "Two people appear in the video.\n"
     ]
    }
   ],
   "source": [
    "# 评估设置\n",
    "max_num_frames = 512\n",
    "generation_config = dict(\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.1,\n",
    "    num_beams=1\n",
    ")\n",
    "video_path = \"car.mp4\"\n",
    "num_segments=128\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  # 加载视频并处理\n",
    "  pixel_values, num_patches_list = load_video(video_path, num_segments=num_segments, max_num=1, get_frame_by_duration=False)\n",
    "  pixel_values = pixel_values.to(torch.bfloat16).to(model.device)\n",
    "  video_prefix = \"\".join([f\"Frame{i+1}: <image>\\n\" for i in range(len(num_patches_list))])\n",
    "  \n",
    "  # 单轮对话：视频详细描述\n",
    "  question1 = \"Describe this video in detail.\"\n",
    "  question = video_prefix + question1\n",
    "  output1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "  print(output1)\n",
    "  \n",
    "  # 多轮对话：询问视频中的人数\n",
    "  question2 = \"How many people appear in the video?\"\n",
    "  output2, chat_history = model.chat(tokenizer, pixel_values, question2, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\n",
    "  \n",
    "  print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d691952-7126-4dce-8ded-b61af1ab9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4528ec18-0b11-4bd9-9c86-446bf345a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "车的门和轮胎都损伤了，门凹陷了，轮胎的漆面被刮掉了。\n",
      "车撞到了墙角。\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  # 单轮对话：询问车辆损伤部位（中文）\n",
    "  question1 = \"车的哪个部位损伤了？\"\n",
    "  question = video_prefix + question1\n",
    "  output1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "  print(output1)\n",
    "  \n",
    "  # 多轮对话：询问车辆碰撞位置（中文）\n",
    "  question2 = \"车撞到哪里了？\"\n",
    "  output2, chat_history = model.chat(tokenizer, pixel_values, question2, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\n",
    "  \n",
    "  print(output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
