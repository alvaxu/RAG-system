"""
ç¨‹åºè¯´æ˜ï¼š
## 1. æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­ä¸¤ä¸ªæ–‡æ¡£çš„åˆ†å¸ƒæƒ…å†µ
## 2. åˆ†æä¸ºä»€ä¹ˆç¬¬äºŒä¸ªæ–‡æ¡£æ²¡æœ‰è¢«ä½¿ç”¨
## 3. éªŒè¯æ–‡æ¡£æ£€ç´¢çš„å¹³è¡¡æ€§
"""

import os
import json
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings

def check_document_distribution():
    """
    æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­æ–‡æ¡£çš„åˆ†å¸ƒæƒ…å†µ
    """
    print("=== æ£€æŸ¥å‘é‡æ•°æ®åº“æ–‡æ¡£åˆ†å¸ƒ ===")
    
    # é…ç½®APIå¯†é’¥
    api_key = os.getenv('MY_DASHSCOPE_API_KEY', '')
    if not api_key or api_key == 'ä½ çš„APIKEY':
        print("âŒ æœªé…ç½®æœ‰æ•ˆçš„DashScope APIå¯†é’¥")
        return
    
    embeddings = DashScopeEmbeddings(dashscope_api_key=api_key, model="text-embedding-v1")
    
    # åŠ è½½å‘é‡æ•°æ®åº“
    vector_db_path = "central/vector_db"
    if not os.path.exists(vector_db_path):
        print(f"âŒ å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {vector_db_path}")
        return
    
    try:
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        
        # ç»Ÿè®¡æ–‡æ¡£åˆ†å¸ƒ
        doc_sources = {}
        chunk_types = {}
        
        for doc_id, doc in vector_store.docstore._dict.items():
            doc_name = doc.metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£')
            chunk_type = doc.metadata.get('chunk_type', 'text')
            
            if doc_name not in doc_sources:
                doc_sources[doc_name] = {'text': 0, 'image': 0, 'table': 0, 'total': 0}
            
            doc_sources[doc_name][chunk_type] += 1
            doc_sources[doc_name]['total'] += 1
            
            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
        
        print(f"\nğŸ“Š æ–‡æ¡£åˆ†å¸ƒç»Ÿè®¡:")
        print(f"æ€»æ–‡æ¡£æ•°: {len(vector_store.docstore._dict)}")
        print(f"æ–‡æ¡£ç±»å‹åˆ†å¸ƒ: {chunk_types}")
        
        print(f"\nğŸ“‹ å„æ–‡æ¡£è¯¦ç»†ç»Ÿè®¡:")
        for doc_name, stats in doc_sources.items():
            print(f"  {doc_name}:")
            print(f"    æ€»chunks: {stats['total']}")
            print(f"    æ–‡æœ¬chunks: {stats['text']}")
            print(f"    å›¾ç‰‡chunks: {stats['image']}")
            print(f"    è¡¨æ ¼chunks: {stats['table']}")
        
        # æµ‹è¯•æ£€ç´¢
        print(f"\nğŸ” æ£€ç´¢æµ‹è¯•:")
        test_queries = [
            "ä¸­èŠ¯å›½é™…çš„ä¸»è¦ä¸šåŠ¡",
            "ä¸­èŠ¯å›½é™…çš„äº§èƒ½åˆ©ç”¨ç‡",
            "ä¸­èŠ¯å›½é™…çš„æ¯›åˆ©ç‡",
            "ä¸­èŠ¯å›½é™…çš„å¸‚åœºåœ°ä½"
        ]
        
        for query in test_queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            docs = vector_store.similarity_search(query, k=5)
            
            # ç»Ÿè®¡æ£€ç´¢ç»“æœä¸­çš„æ–‡æ¡£æ¥æº
            result_sources = {}
            for doc in docs:
                doc_name = doc.metadata.get('document_name', 'æœªçŸ¥æ–‡æ¡£')
                result_sources[doc_name] = result_sources.get(doc_name, 0) + 1
            
            print(f"  æ£€ç´¢ç»“æœåˆ†å¸ƒ: {result_sources}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¬¬äºŒä¸ªæ–‡æ¡£
        second_doc_name = "ã€ä¸­åŸè¯åˆ¸ã€‘äº§èƒ½åˆ©ç”¨ç‡æ˜¾è‘—æå‡ï¼ŒæŒç»­æ¨è¿›å·¥è‰ºè¿­ä»£å‡çº§â€”â€”ä¸­èŠ¯å›½é™…(688981)å­£æŠ¥ç‚¹è¯„"
        if second_doc_name in doc_sources:
            print(f"\nâœ… ç¬¬äºŒä¸ªæ–‡æ¡£å­˜åœ¨äºå‘é‡æ•°æ®åº“ä¸­")
            print(f"  æ–‡æ¡£chunksæ•°: {doc_sources[second_doc_name]['total']}")
        else:
            print(f"\nâŒ ç¬¬äºŒä¸ªæ–‡æ¡£ä¸å­˜åœ¨äºå‘é‡æ•°æ®åº“ä¸­")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç±»ä¼¼çš„æ–‡æ¡£å
            similar_docs = [name for name in doc_sources.keys() if "ä¸­åŸè¯åˆ¸" in name]
            if similar_docs:
                print(f"  å‘ç°ç±»ä¼¼æ–‡æ¡£: {similar_docs}")
            else:
                print(f"  æ²¡æœ‰å‘ç°ä¸­åŸè¯åˆ¸ç›¸å…³çš„æ–‡æ¡£")
        
    except Exception as e:
        print(f"âŒ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")

if __name__ == "__main__":
    check_document_distribution() 