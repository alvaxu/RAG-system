'''
ç¨‹åºè¯´æ˜ï¼š
## 1. åˆ†æå½“å‰å›¾ç‰‡æ–‡æ¡£è®¾è®¡çš„ä¸¥å¯†æ€§
## 2. è¯†åˆ«æ½œåœ¨çš„è®¾è®¡é—®é¢˜å’Œæ”¹è¿›ç‚¹
## 3. æä¾›è®¾è®¡ä¼˜åŒ–å»ºè®®
'''

import pickle
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Set

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.settings import Settings

def analyze_design_rigor():
    """åˆ†æå½“å‰è®¾è®¡çš„ä¸¥å¯†æ€§"""
    
    print("ğŸ” åˆ†æå½“å‰å›¾ç‰‡æ–‡æ¡£è®¾è®¡çš„ä¸¥å¯†æ€§")
    print("=" * 80)
    
    try:
        # åŠ è½½é…ç½®
        config = Settings.load_from_file('config.json')
        embeddings = DashScopeEmbeddings(dashscope_api_key=config.dashscope_api_key, model="text-embedding-v1")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        vector_db_path = "./central/vector_db"
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(vector_store.docstore._dict)}")
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„æ–‡æ¡£
        image_docs = []
        text_docs = []
        table_docs = []
        
        for doc_id, doc in vector_store.docstore._dict.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            chunk_type = metadata.get('chunk_type', 'unknown')
            
            if chunk_type == 'image':
                image_docs.append((doc_id, doc))
            elif chunk_type == 'text':
                text_docs.append((doc_id, doc))
            elif chunk_type == 'table':
                table_docs.append((doc_id, doc))
        
        print(f"ğŸ–¼ï¸  å›¾ç‰‡æ–‡æ¡£æ•°: {len(image_docs)}")
        print(f"ğŸ“„ æ–‡æœ¬æ–‡æ¡£æ•°: {len(text_docs)}")
        print(f"ğŸ“Š è¡¨æ ¼æ–‡æ¡£æ•°: {len(table_docs)}")
        
        # åˆ†æè®¾è®¡é—®é¢˜
        print(f"\nğŸ” è®¾è®¡ä¸¥å¯†æ€§åˆ†æ:")
        print("=" * 80)
        
        # 1. æ•°æ®ä¸€è‡´æ€§é—®é¢˜
        print(f"\n1ï¸âƒ£ æ•°æ®ä¸€è‡´æ€§é—®é¢˜:")
        consistency_issues = analyze_data_consistency(image_docs)
        for issue in consistency_issues:
            print(f"   âš ï¸  {issue}")
        
        # 2. å­—æ®µå†—ä½™é—®é¢˜
        print(f"\n2ï¸âƒ£ å­—æ®µå†—ä½™é—®é¢˜:")
        redundancy_issues = analyze_field_redundancy(image_docs)
        for issue in redundancy_issues:
            print(f"   âš ï¸  {issue}")
        
        # 3. é€»è¾‘ä¸€è‡´æ€§é—®é¢˜
        print(f"\n3ï¸âƒ£ é€»è¾‘ä¸€è‡´æ€§é—®é¢˜:")
        logic_issues = analyze_logic_consistency(image_docs)
        for issue in logic_issues:
            print(f"   âš ï¸  {issue}")
        
        # 4. æ€§èƒ½é—®é¢˜
        print(f"\n4ï¸âƒ£ æ€§èƒ½é—®é¢˜:")
        performance_issues = analyze_performance_issues(image_docs)
        for issue in performance_issues:
            print(f"   âš ï¸  {issue}")
        
        # 5. ç»´æŠ¤æ€§é—®é¢˜
        print(f"\n5ï¸âƒ£ ç»´æŠ¤æ€§é—®é¢˜:")
        maintenance_issues = analyze_maintenance_issues(image_docs)
        for issue in maintenance_issues:
            print(f"   âš ï¸  {issue}")
        
        # 6. è®¾è®¡ä¼˜ç‚¹
        print(f"\nâœ… è®¾è®¡ä¼˜ç‚¹:")
        design_strengths = analyze_design_strengths(image_docs)
        for strength in design_strengths:
            print(f"   âœ… {strength}")
        
        # 7. æ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        improvement_suggestions = generate_improvement_suggestions(
            consistency_issues, redundancy_issues, logic_issues, 
            performance_issues, maintenance_issues
        )
        for suggestion in improvement_suggestions:
            print(f"   ğŸ’¡ {suggestion}")
        
        # 8. æ€»ä½“è¯„ä»·
        print(f"\nğŸ“Š æ€»ä½“è¯„ä»·:")
        overall_score = calculate_design_score(
            consistency_issues, redundancy_issues, logic_issues,
            performance_issues, maintenance_issues, design_strengths
        )
        print(f"   è®¾è®¡ä¸¥å¯†æ€§è¯„åˆ†: {overall_score}/10")
        
        if overall_score >= 8:
            print(f"   ğŸ‰ è®¾è®¡éå¸¸ä¸¥å¯†ï¼Œåªæœ‰å°‘é‡æ”¹è¿›ç©ºé—´")
        elif overall_score >= 6:
            print(f"   ğŸ‘ è®¾è®¡åŸºæœ¬ä¸¥å¯†ï¼Œå»ºè®®è¿›è¡Œä¼˜åŒ–")
        elif overall_score >= 4:
            print(f"   âš ï¸ è®¾è®¡å­˜åœ¨æ˜æ˜¾é—®é¢˜ï¼Œéœ€è¦é‡æ„")
        else:
            print(f"   âŒ è®¾è®¡å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œå»ºè®®é‡æ–°è®¾è®¡")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def analyze_data_consistency(image_docs: List[tuple]) -> List[str]:
    """åˆ†ææ•°æ®ä¸€è‡´æ€§é—®é¢˜"""
    issues = []
    
    if not image_docs:
        return ["æ²¡æœ‰å›¾ç‰‡æ–‡æ¡£å¯ä¾›åˆ†æ"]
    
    # æ£€æŸ¥page_contentä¸enhanced_descriptionçš„ä¸€è‡´æ€§
    inconsistent_count = 0
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        page_content = doc.page_content
        enhanced_desc = metadata.get('enhanced_description', '')
        
        if page_content != enhanced_desc:
            inconsistent_count += 1
    
    if inconsistent_count > 0:
        issues.append(f"æœ‰ {inconsistent_count} ä¸ªå›¾ç‰‡æ–‡æ¡£çš„page_contentä¸enhanced_descriptionä¸ä¸€è‡´")
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µçš„å®Œæ•´æ€§
    missing_fields = {}
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        
        required_fields = ['image_id', 'image_path', 'document_name', 'chunk_type']
        for field in required_fields:
            if field not in metadata or not metadata[field]:
                missing_fields[field] = missing_fields.get(field, 0) + 1
    
    for field, count in missing_fields.items():
        issues.append(f"æœ‰ {count} ä¸ªæ–‡æ¡£ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
    
    return issues

def analyze_field_redundancy(image_docs: List[tuple]) -> List[str]:
    """åˆ†æå­—æ®µå†—ä½™é—®é¢˜"""
    issues = []
    
    if not image_docs:
        return ["æ²¡æœ‰å›¾ç‰‡æ–‡æ¡£å¯ä¾›åˆ†æ"]
    
    # æ£€æŸ¥contentå­—æ®µçš„å†—ä½™
    redundant_content_count = 0
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        page_content = doc.page_content
        enhanced_desc = metadata.get('enhanced_description', '')
        
        if page_content == enhanced_desc:
            redundant_content_count += 1
    
    if redundant_content_count == len(image_docs):
        issues.append("æ‰€æœ‰å›¾ç‰‡æ–‡æ¡£çš„page_contentä¸enhanced_descriptionå®Œå…¨ç›¸åŒï¼Œå­˜åœ¨æ•°æ®å†—ä½™")
    
    # æ£€æŸ¥metadataä¸­çš„å†—ä½™å­—æ®µ
    sample_metadata = image_docs[0][1].metadata if image_docs else {}
    redundant_metadata_fields = []
    
    # æ£€æŸ¥å¯èƒ½çš„å†—ä½™å­—æ®µ
    potential_redundant = [
        ('page_number', 'page_idx'),
        ('image_filename', 'image_id'),
        ('content', 'enhanced_description')
    ]
    
    for field1, field2 in potential_redundant:
        if field1 in sample_metadata and field2 in sample_metadata:
            redundant_metadata_fields.append(f"{field1} å’Œ {field2}")
    
    if redundant_metadata_fields:
        issues.append(f"å‘ç°æ½œåœ¨çš„å†—ä½™å­—æ®µå¯¹: {', '.join(redundant_metadata_fields)}")
    
    return issues

def analyze_logic_consistency(image_docs: List[tuple]) -> List[str]:
    """åˆ†æé€»è¾‘ä¸€è‡´æ€§é—®é¢˜"""
    issues = []
    
    if not image_docs:
        return ["æ²¡æœ‰å›¾ç‰‡æ–‡æ¡£å¯ä¾›åˆ†æ"]
    
    # æ£€æŸ¥å›¾ç‰‡æ–‡æ¡£çš„å¤„ç†é€»è¾‘
    inconsistent_logic_count = 0
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        
        # æ£€æŸ¥chunk_typeä¸æ–‡æ¡£å†…å®¹çš„é€»è¾‘ä¸€è‡´æ€§
        if metadata.get('chunk_type') == 'image':
            page_content = doc.page_content
            # å¦‚æœpage_contentåŒ…å«å›¾ç‰‡æè¿°ï¼Œä½†chunk_typeæ˜¯imageï¼Œè¿™æ˜¯åˆç†çš„
            # ä½†å¦‚æœpage_contentæ˜¯ç©ºæˆ–ä¸æ˜¯å›¾ç‰‡ç›¸å…³æè¿°ï¼Œå¯èƒ½æœ‰é—®é¢˜
            if not page_content or len(page_content.strip()) == 0:
                inconsistent_logic_count += 1
    
    if inconsistent_logic_count > 0:
        issues.append(f"æœ‰ {inconsistent_logic_count} ä¸ªå›¾ç‰‡æ–‡æ¡£çš„page_contentä¸ºç©ºï¼Œé€»è¾‘ä¸ä¸€è‡´")
    
    # æ£€æŸ¥å›¾ç‰‡è·¯å¾„çš„æœ‰æ•ˆæ€§
    invalid_path_count = 0
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        image_path = metadata.get('image_path', '')
        
        if image_path and not os.path.exists(image_path):
            invalid_path_count += 1
    
    if invalid_path_count > 0:
        issues.append(f"æœ‰ {invalid_path_count} ä¸ªå›¾ç‰‡æ–‡æ¡£çš„å›¾ç‰‡è·¯å¾„æ— æ•ˆ")
    
    return issues

def analyze_performance_issues(image_docs: List[tuple]) -> List[str]:
    """åˆ†ææ€§èƒ½é—®é¢˜"""
    issues = []
    
    if not image_docs:
        return ["æ²¡æœ‰å›¾ç‰‡æ–‡æ¡£å¯ä¾›åˆ†æ"]
    
    # æ£€æŸ¥æ–‡æ¡£å¤§å°
    large_docs = 0
    total_size = 0
    
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        page_content = doc.page_content
        
        doc_size = len(str(page_content)) + len(str(metadata))
        total_size += doc_size
        
        if doc_size > 10000:  # 10KB
            large_docs += 1
    
    if large_docs > 0:
        issues.append(f"æœ‰ {large_docs} ä¸ªå›¾ç‰‡æ–‡æ¡£è¿‡å¤§ï¼ˆ>10KBï¼‰ï¼Œå¯èƒ½å½±å“æ£€ç´¢æ€§èƒ½")
    
    avg_size = total_size / len(image_docs) if image_docs else 0
    if avg_size > 5000:  # 5KB
        issues.append(f"å›¾ç‰‡æ–‡æ¡£å¹³å‡å¤§å°è¾ƒå¤§ï¼ˆ{avg_size:.0f}å­—èŠ‚ï¼‰ï¼Œå¯èƒ½å½±å“ç³»ç»Ÿæ€§èƒ½")
    
    # æ£€æŸ¥embeddingç»´åº¦ä¸€è‡´æ€§
    embedding_dims = set()
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        semantic_features = metadata.get('semantic_features', {})
        embedding_dim = semantic_features.get('embedding_dimension', 0)
        if embedding_dim > 0:
            embedding_dims.add(embedding_dim)
    
    if len(embedding_dims) > 1:
        issues.append(f"å‘ç°å¤šç§embeddingç»´åº¦: {embedding_dims}ï¼Œå¯èƒ½å½±å“æ£€ç´¢ä¸€è‡´æ€§")
    
    return issues

def analyze_maintenance_issues(image_docs: List[tuple]) -> List[str]:
    """åˆ†æç»´æŠ¤æ€§é—®é¢˜"""
    issues = []
    
    if not image_docs:
        return ["æ²¡æœ‰å›¾ç‰‡æ–‡æ¡£å¯ä¾›åˆ†æ"]
    
    # æ£€æŸ¥å­—æ®µå‘½åçš„ä¸€è‡´æ€§
    field_names = set()
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        field_names.update(metadata.keys())
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸ä¸€è‡´çš„å­—æ®µå‘½å
    inconsistent_naming = []
    for field in field_names:
        if '_' in field and field.replace('_', '') in field_names:
            inconsistent_naming.append(field)
    
    if inconsistent_naming:
        issues.append(f"å‘ç°ä¸ä¸€è‡´çš„å­—æ®µå‘½å: {inconsistent_naming}")
    
    # æ£€æŸ¥æ–‡æ¡£ç»“æ„çš„å¤æ‚æ€§
    complex_metadata_count = 0
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        
        # æ£€æŸ¥åµŒå¥—ç»“æ„çš„å¤æ‚æ€§
        nested_levels = 0
        for value in metadata.values():
            if isinstance(value, dict):
                nested_levels += 1
            elif isinstance(value, list) and value and isinstance(value[0], dict):
                nested_levels += 1
        
        if nested_levels > 2:
            complex_metadata_count += 1
    
    if complex_metadata_count > 0:
        issues.append(f"æœ‰ {complex_metadata_count} ä¸ªæ–‡æ¡£çš„å…ƒæ•°æ®ç»“æ„è¿‡äºå¤æ‚ï¼Œå¢åŠ ç»´æŠ¤éš¾åº¦")
    
    return issues

def analyze_design_strengths(image_docs: List[tuple]) -> List[str]:
    """åˆ†æè®¾è®¡ä¼˜ç‚¹"""
    strengths = []
    
    if not image_docs:
        return ["æ²¡æœ‰å›¾ç‰‡æ–‡æ¡£å¯ä¾›åˆ†æ"]
    
    # æ£€æŸ¥å­—æ®µçš„å®Œæ•´æ€§
    sample_metadata = image_docs[0][1].metadata if image_docs else {}
    important_fields = ['image_id', 'image_path', 'document_name', 'chunk_type', 
                       'img_caption', 'enhanced_description', 'semantic_features']
    
    present_fields = [field for field in important_fields if field in sample_metadata]
    if len(present_fields) >= 6:
        strengths.append(f"å›¾ç‰‡æ–‡æ¡£åŒ…å«å®Œæ•´çš„å…ƒæ•°æ®å­—æ®µ: {len(present_fields)}/{len(important_fields)}")
    
    # æ£€æŸ¥è¯­ä¹‰ç‰¹å¾çš„ä¸°å¯Œæ€§
    semantic_features_count = 0
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        semantic_features = metadata.get('semantic_features', {})
        if semantic_features and len(semantic_features) >= 3:
            semantic_features_count += 1
    
    if semantic_features_count > 0:
        strengths.append(f"æœ‰ {semantic_features_count} ä¸ªæ–‡æ¡£åŒ…å«ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾")
    
    # æ£€æŸ¥æ–‡æ¡£ç±»å‹çš„æ¸…æ™°æ€§
    clear_type_count = 0
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        if metadata.get('chunk_type') == 'image':
            clear_type_count += 1
    
    if clear_type_count == len(image_docs):
        strengths.append("æ‰€æœ‰å›¾ç‰‡æ–‡æ¡£éƒ½æœ‰æ¸…æ™°çš„ç±»å‹æ ‡è¯†")
    
    # æ£€æŸ¥å¢å¼ºæè¿°çš„è´¨é‡
    enhanced_desc_count = 0
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        enhanced_desc = metadata.get('enhanced_description', '')
        if enhanced_desc and len(enhanced_desc) > 20:
            enhanced_desc_count += 1
    
    if enhanced_desc_count > 0:
        strengths.append(f"æœ‰ {enhanced_desc_count} ä¸ªæ–‡æ¡£åŒ…å«é«˜è´¨é‡çš„å¢å¼ºæè¿°")
    
    return strengths

def generate_improvement_suggestions(consistency_issues: List[str], redundancy_issues: List[str], 
                                   logic_issues: List[str], performance_issues: List[str], 
                                   maintenance_issues: List[str]) -> List[str]:
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    suggestions = []
    
    # åŸºäºä¸€è‡´æ€§é—®é¢˜æå‡ºå»ºè®®
    if any("page_contentä¸enhanced_descriptionä¸ä¸€è‡´" in issue for issue in consistency_issues):
        suggestions.append("ç»Ÿä¸€page_contentå’Œenhanced_descriptionçš„ç”Ÿæˆé€»è¾‘ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§")
    
    if any("ç¼ºå°‘å¿…éœ€å­—æ®µ" in issue for issue in consistency_issues):
        suggestions.append("å»ºç«‹æ•°æ®éªŒè¯æœºåˆ¶ï¼Œç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µçš„å®Œæ•´æ€§")
    
    # åŸºäºå†—ä½™é—®é¢˜æå‡ºå»ºè®®
    if any("æ•°æ®å†—ä½™" in issue for issue in redundancy_issues):
        suggestions.append("è€ƒè™‘ç§»é™¤å†—ä½™å­—æ®µï¼Œæˆ–å»ºç«‹å­—æ®µé—´çš„è‡ªåŠ¨åŒæ­¥æœºåˆ¶")
    
    # åŸºäºé€»è¾‘é—®é¢˜æå‡ºå»ºè®®
    if any("é€»è¾‘ä¸ä¸€è‡´" in issue for issue in logic_issues):
        suggestions.append("å»ºç«‹æ–‡æ¡£ç±»å‹ä¸å†…å®¹çš„é€»è¾‘éªŒè¯è§„åˆ™")
    
    if any("å›¾ç‰‡è·¯å¾„æ— æ•ˆ" in issue for issue in logic_issues):
        suggestions.append("æ·»åŠ å›¾ç‰‡æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ï¼Œå»ºç«‹è·¯å¾„éªŒè¯æœºåˆ¶")
    
    # åŸºäºæ€§èƒ½é—®é¢˜æå‡ºå»ºè®®
    if any("æ–‡æ¡£è¿‡å¤§" in issue for issue in performance_issues):
        suggestions.append("ä¼˜åŒ–æ–‡æ¡£å¤§å°ï¼Œè€ƒè™‘å‹ç¼©æˆ–åˆ†å—å­˜å‚¨å¤§å‹å…ƒæ•°æ®")
    
    if any("embeddingç»´åº¦" in issue for issue in performance_issues):
        suggestions.append("ç»Ÿä¸€embeddingæ¨¡å‹å’Œç»´åº¦ï¼Œç¡®ä¿æ£€ç´¢ä¸€è‡´æ€§")
    
    # åŸºäºç»´æŠ¤æ€§é—®é¢˜æå‡ºå»ºè®®
    if any("å­—æ®µå‘½å" in issue for issue in maintenance_issues):
        suggestions.append("å»ºç«‹ç»Ÿä¸€çš„å­—æ®µå‘½åè§„èŒƒï¼Œé¿å…å‘½åä¸ä¸€è‡´")
    
    if any("ç»“æ„å¤æ‚" in issue for issue in maintenance_issues):
        suggestions.append("ç®€åŒ–å…ƒæ•°æ®ç»“æ„ï¼Œå‡å°‘åµŒå¥—å±‚çº§ï¼Œæé«˜å¯ç»´æŠ¤æ€§")
    
    # é€šç”¨å»ºè®®
    suggestions.append("å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æœºåˆ¶ï¼Œå®šæœŸæ£€æŸ¥æ•°æ®ä¸€è‡´æ€§")
    suggestions.append("å®Œå–„æ–‡æ¡£å¤„ç†æµç¨‹çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•")
    suggestions.append("è€ƒè™‘å»ºç«‹æ•°æ®ç‰ˆæœ¬æ§åˆ¶æœºåˆ¶ï¼Œæ”¯æŒæ•°æ®å›æ»š")
    
    return suggestions

def calculate_design_score(consistency_issues: List[str], redundancy_issues: List[str], 
                          logic_issues: List[str], performance_issues: List[str], 
                          maintenance_issues: List[str], strengths: List[str]) -> float:
    """è®¡ç®—è®¾è®¡è¯„åˆ†"""
    base_score = 10.0
    
    # æ ¹æ®é—®é¢˜æ•°é‡æ‰£åˆ†
    issue_weights = {
        'consistency': 2.0,  # ä¸€è‡´æ€§é—®é¢˜æƒé‡æœ€é«˜
        'logic': 1.5,        # é€»è¾‘é—®é¢˜æ¬¡ä¹‹
        'redundancy': 1.0,   # å†—ä½™é—®é¢˜
        'performance': 1.0,  # æ€§èƒ½é—®é¢˜
        'maintenance': 0.8   # ç»´æŠ¤æ€§é—®é¢˜
    }
    
    deductions = (
        len(consistency_issues) * issue_weights['consistency'] +
        len(logic_issues) * issue_weights['logic'] +
        len(redundancy_issues) * issue_weights['redundancy'] +
        len(performance_issues) * issue_weights['performance'] +
        len(maintenance_issues) * issue_weights['maintenance']
    )
    
    # æ ¹æ®ä¼˜ç‚¹åŠ åˆ†
    bonus = len(strengths) * 0.3
    
    final_score = max(0.0, min(10.0, base_score - deductions + bonus))
    return round(final_score, 1)

if __name__ == "__main__":
    analyze_design_rigor()
