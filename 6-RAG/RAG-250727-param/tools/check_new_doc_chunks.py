#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.settings import Settings
from document_processing.vector_generator import VectorGenerator
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_new_doc_chunks():
    """æ£€æŸ¥æ–°å¢æ–‡æ¡£çš„å…·ä½“åˆ†å—å†…å®¹"""
    
    print("=" * 60)
    print("æ–°å¢æ–‡æ¡£åˆ†å—å†…å®¹æ£€æŸ¥")
    print("=" * 60)
    
    try:
        # åŠ è½½é…ç½®
        settings = Settings.load_from_file("config.json")
        
        # åˆå§‹åŒ–å‘é‡ç”Ÿæˆå™¨
        vector_generator = VectorGenerator(settings)
        
        # åŠ è½½å‘é‡å­˜å‚¨
        vector_store = vector_generator.load_vector_store(settings.vector_db_dir)
        
        if vector_store is None:
            logger.error("æ— æ³•åŠ è½½å‘é‡å­˜å‚¨")
            return
        
        # è·å–æ‰€æœ‰æ–‡æ¡£çš„å…ƒæ•°æ®
        new_doc_chunks = []
        old_doc_chunks = []
        
        print(f"æ€»æ–‡æ¡£æ•°: {len(vector_store.docstore._dict)}")
        
        for doc_id, doc in vector_store.docstore._dict.items():
            if hasattr(doc, 'metadata') and doc.metadata:
                source = doc.metadata.get('source', '')
                print(f"æ–‡æ¡£ID: {doc_id}, æ¥æº: {source}")
                
                if 'ä¸Šæµ·è¯åˆ¸' in source:
                    new_doc_chunks.append({
                        'id': doc_id,
                        'content': doc.page_content,
                        'metadata': doc.metadata
                    })
                elif 'ä¸­åŸè¯åˆ¸' in source:
                    old_doc_chunks.append({
                        'id': doc_id,
                        'content': doc.page_content,
                        'metadata': doc.metadata
                    })
        
        print(f"\nğŸ“Š åˆ†å—ç»Ÿè®¡:")
        print(f"   - æ–°å¢æ–‡æ¡£å—æ•°: {len(new_doc_chunks)}")
        print(f"   - åŸå§‹æ–‡æ¡£å—æ•°: {len(old_doc_chunks)}")
        
        # æ£€æŸ¥æ–°å¢æ–‡æ¡£çš„å‰å‡ ä¸ªå—
        print(f"\nğŸ“‹ æ–°å¢æ–‡æ¡£å‰5ä¸ªå—çš„å†…å®¹:")
        for i, chunk in enumerate(new_doc_chunks[:5]):
            print(f"\n--- å— {i+1} ---")
            print(f"ID: {chunk['id']}")
            print(f"å†…å®¹é•¿åº¦: {len(chunk['content'])} å­—ç¬¦")
            print(f"å†…å®¹é¢„è§ˆ: {chunk['content'][:200]}...")
            print(f"å…ƒæ•°æ®: {chunk['metadata']}")
        
        # æ£€æŸ¥åŸå§‹æ–‡æ¡£çš„å‰å‡ ä¸ªå—ä½œä¸ºå¯¹æ¯”
        print(f"\nğŸ“‹ åŸå§‹æ–‡æ¡£å‰3ä¸ªå—çš„å†…å®¹:")
        for i, chunk in enumerate(old_doc_chunks[:3]):
            print(f"\n--- å— {i+1} ---")
            print(f"ID: {chunk['id']}")
            print(f"å†…å®¹é•¿åº¦: {len(chunk['content'])} å­—ç¬¦")
            print(f"å†…å®¹é¢„è§ˆ: {chunk['content'][:200]}...")
            print(f"å…ƒæ•°æ®: {chunk['metadata']}")
        
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        test_keywords = ['ä¸­èŠ¯å›½é™…', 'æ™¶åœ†åˆ¶é€ ', 'ä¸‰å¤§è¡Œä¸šç‰¹å¾', 'æ·±åº¦ç ”ç©¶']
        print(f"\nğŸ” å…³é”®è¯åŒ¹é…æ£€æŸ¥:")
        for keyword in test_keywords:
            new_matches = sum(1 for chunk in new_doc_chunks if keyword in chunk['content'])
            old_matches = sum(1 for chunk in old_doc_chunks if keyword in chunk['content'])
            print(f"   - '{keyword}': æ–°å¢æ–‡æ¡£ {new_matches} ä¸ªå—, åŸå§‹æ–‡æ¡£ {old_matches} ä¸ªå—")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    check_new_doc_chunks() 