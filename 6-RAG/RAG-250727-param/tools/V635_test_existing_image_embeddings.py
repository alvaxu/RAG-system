'''
ç¨‹åºè¯´æ˜ï¼š
## 1. æµ‹è¯•Qwençš„multimodal-embedding-v1æ¨¡å‹æ˜¯å¦æ”¯æŒå›¾ç‰‡å†…å®¹ç†è§£
## 2. å¯¹æ¯”ONE-PEACEå’ŒQwen multimodal embeddingçš„èƒ½åŠ›
## 3. æ¢ç´¢çœŸæ­£çš„å›¾åƒå†…å®¹ç†è§£åŠŸèƒ½
'''

import os
import sys
import numpy as np
import base64
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.settings import Settings
from dashscope import MultiModalEmbedding

def analyze_embedding_semantics(embedding: List[float]) -> Dict[str, Any]:
    """åˆ†æembeddingçš„è¯­ä¹‰ç‰¹å¾"""
    try:
        embedding_array = np.array(embedding)
        
        # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
        mean_val = np.mean(embedding_array)
        std_val = np.std(embedding_array)
        norm_val = np.linalg.norm(embedding_array)
        max_val = np.max(embedding_array)
        min_val = np.min(embedding_array)
        dynamic_range = max_val - min_val
        
        # åŸºäºç»Ÿè®¡ç‰¹å¾æ¨æ–­å›¾åƒå†…å®¹
        features = []
        
        # å¤æ‚åº¦åˆ†æ
        if norm_val > 1.8:
            complexity = "é«˜å¤æ‚åº¦è§†è§‰å†…å®¹"
        elif norm_val > 1.2:
            complexity = "ä¸­ç­‰å¤æ‚åº¦è§†è§‰å†…å®¹"
        else:
            complexity = "ç®€å•è§†è§‰å†…å®¹"
        
        # ç‰¹å¾ä¸°å¯Œåº¦åˆ†æ
        if std_val > 0.15:
            richness = "è§†è§‰ç‰¹å¾ä¸°å¯Œ"
        elif std_val > 0.08:
            richness = "è§†è§‰ç‰¹å¾ä¸­ç­‰"
        else:
            richness = "è§†è§‰ç‰¹å¾ç®€å•"
        
        # äº®åº¦åˆ†æ
        if mean_val > 0.12:
            brightness = "æ•´ä½“äº®åº¦è¾ƒé«˜"
        elif mean_val < -0.12:
            brightness = "æ•´ä½“äº®åº¦è¾ƒä½"
        else:
            brightness = "äº®åº¦é€‚ä¸­"
        
        # å¯¹æ¯”åº¦åˆ†æ
        if dynamic_range > 0.8:
            contrast = "é«˜å¯¹æ¯”åº¦"
        elif dynamic_range > 0.4:
            contrast = "ä¸­ç­‰å¯¹æ¯”åº¦"
        else:
            contrast = "ä½å¯¹æ¯”åº¦"
        
        return {
            'statistics': {
                'mean': mean_val,
                'std': std_val,
                'norm': norm_val,
                'max': max_val,
                'min': min_val,
                'dynamic_range': dynamic_range
            },
            'semantic_features': {
                'complexity': complexity,
                'richness': richness,
                'brightness': brightness,
                'contrast': contrast
            },
            'features_list': [complexity, richness, brightness, contrast]
        }
        
    except Exception as e:
        print(f"âŒ è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
        return None

def test_qwen_multimodal_embedding():
    """æµ‹è¯•Qwençš„multimodal-embedding-v1æ¨¡å‹æ˜¯å¦æ”¯æŒå›¾ç‰‡å†…å®¹ç†è§£"""
    print("ğŸ” æµ‹è¯•Qwençš„multimodal-embedding-v1æ¨¡å‹")
    print("=" * 80)
    
    try:
        # åŠ è½½é…ç½®
        config = Settings.load_from_file('config.json')
        api_key = config.dashscope_api_key
        
        print(f"ğŸ”‘ APIå¯†é’¥: {api_key[:10]}...")
        
        # 1. æµ‹è¯•ONE-PEACEæ¨¡å‹ï¼ˆå½“å‰ä½¿ç”¨çš„ï¼‰
        print(f"\nğŸ” 1. æµ‹è¯•ONE-PEACEæ¨¡å‹:")
        print("-" * 50)
        
        try:
            # è·å–ä¸€å¼ æµ‹è¯•å›¾ç‰‡
            test_image_path = "./central/images/0a61fa499a2c5c449f51eea53f8757ac5f4924e5ef90c68301b8cc75ee4c82b3.jpg"
            if not os.path.exists(test_image_path):
                # å°è¯•å…¶ä»–å›¾ç‰‡
                images_dir = "./central/images"
                if os.path.exists(images_dir):
                    image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]
                    if image_files:
                        test_image_path = os.path.join(images_dir, image_files[0])
            
            if os.path.exists(test_image_path):
                print(f"ğŸ“· ä½¿ç”¨æµ‹è¯•å›¾ç‰‡: {test_image_path}")
                
                # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64
                with open(test_image_path, 'rb') as f:
                    image_data = f.read()
                    image_base64 = base64.b64encode(image_data).decode('utf-8')
                
                # æ·»åŠ data URLå‰ç¼€
                image_data_url = f"data:image/jpeg;base64,{image_base64}"
                
                # æµ‹è¯•ONE-PEACEæ¨¡å‹
                print("ğŸ”„ è°ƒç”¨ONE-PEACEæ¨¡å‹...")
                result = MultiModalEmbedding.call(
                    model='multimodal_embedding_one_peace_v1',
                    input=[{'image': image_data_url}]
                )
                
                if result.status_code == 200:
                    print("âœ… ONE-PEACEæ¨¡å‹è°ƒç”¨æˆåŠŸ")
                    embedding = result.output['embedding']
                    print(f"ğŸ“Š Embeddingç»´åº¦: {len(embedding)}")
                    print(f"ğŸ“Š EmbeddingèŒƒæ•°: {np.linalg.norm(embedding):.4f}")
                    print(f"ğŸ“Š Embeddingå‡å€¼: {np.mean(embedding):.4f}")
                    print(f"ğŸ“Š Embeddingæ ‡å‡†å·®: {np.std(embedding):.4f}")
                    
                    # åˆ†æè¯­ä¹‰ç‰¹å¾
                    semantic_analysis = analyze_embedding_semantics(embedding)
                    if semantic_analysis:
                        print(f"ğŸ¯ è¯­ä¹‰åˆ†æ: {', '.join(semantic_analysis['features_list'])}")
                else:
                    print(f"âŒ ONE-PEACEæ¨¡å‹è°ƒç”¨å¤±è´¥: {result.message}")
                    
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•å›¾ç‰‡")
                
        except Exception as e:
            print(f"âŒ ONE-PEACEæµ‹è¯•å¤±è´¥: {e}")
        
        # 2. æµ‹è¯•Qwen multimodal-embedding-v1æ¨¡å‹
        print(f"\nğŸ” 2. æµ‹è¯•Qwen multimodal-embedding-v1æ¨¡å‹:")
        print("-" * 50)
        
        try:
            if os.path.exists(test_image_path):
                print("ğŸ”„ è°ƒç”¨Qwen multimodal-embedding-v1æ¨¡å‹...")
                
                # æµ‹è¯•Qwen multimodal-embedding-v1æ¨¡å‹
                result = MultiModalEmbedding.call(
                    model='multimodal-embedding-v1',
                    input=[{'image': image_data_url}]
                )
                
                if result.status_code == 200:
                    print("âœ… Qwen multimodal-embedding-v1æ¨¡å‹è°ƒç”¨æˆåŠŸ")
                    
                    # æ‰“å°å®Œæ•´çš„è¿”å›ç»“æœç»“æ„
                    print(f"ğŸ“‹ è¿”å›ç»“æœç»“æ„: {list(result.output.keys())}")
                    
                    # å°è¯•ä¸åŒçš„embeddingå­—æ®µå
                    embedding = None
                    if 'embedding' in result.output:
                        embedding = result.output['embedding']
                    elif 'embeddings' in result.output:
                        embeddings_list = result.output['embeddings']
                        if isinstance(embeddings_list, list) and len(embeddings_list) > 0:
                            if isinstance(embeddings_list[0], dict) and 'embedding' in embeddings_list[0]:
                                embedding = embeddings_list[0]['embedding']
                            else:
                                embedding = embeddings_list[0]
                    elif 'data' in result.output and 'embedding' in result.output['data']:
                        embedding = result.output['data']['embedding']
                    
                    if embedding:
                        print(f"ğŸ“Š Embeddingç»´åº¦: {len(embedding)}")
                        print(f"ğŸ“Š EmbeddingèŒƒæ•°: {np.linalg.norm(embedding):.4f}")
                        print(f"ğŸ“Š Embeddingå‡å€¼: {np.mean(embedding):.4f}")
                        print(f"ğŸ“Š Embeddingæ ‡å‡†å·®: {np.std(embedding):.4f}")
                        
                        # åˆ†æè¯­ä¹‰ç‰¹å¾
                        semantic_analysis = analyze_embedding_semantics(embedding)
                        if semantic_analysis:
                            print(f"ğŸ¯ è¯­ä¹‰åˆ†æ: {', '.join(semantic_analysis['features_list'])}")
                    else:
                        print("âš ï¸  æœªæ‰¾åˆ°embeddingå­—æ®µ")
                        print(f"ğŸ“‹ å®Œæ•´è¿”å›ç»“æœ: {result.output}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é¢å¤–çš„å†…å®¹ç†è§£ä¿¡æ¯
                    for key in ['text', 'description', 'content', 'caption', 'summary']:
                        if key in result.output:
                            print(f"ğŸ“ {key}: {result.output[key]}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¯èƒ½çš„å†…å®¹ç†è§£å­—æ®µ
                    for key, value in result.output.items():
                        if isinstance(value, str) and len(value) > 20:
                            print(f"ğŸ“ å¯èƒ½çš„æè¿°å­—æ®µ '{key}': {value[:100]}...")
                        
                else:
                    print(f"âŒ Qwen multimodal-embedding-v1æ¨¡å‹è°ƒç”¨å¤±è´¥: {result.message}")
                    
        except Exception as e:
            print(f"âŒ Qwen multimodal-embedding-v1æµ‹è¯•å¤±è´¥: {e}")
        
        # 3. å¯¹æ¯”åˆ†æ
        print(f"\nğŸ” 3. æ¨¡å‹å¯¹æ¯”åˆ†æ:")
        print("-" * 50)
        
        print("ğŸ“Š æ¨¡å‹èƒ½åŠ›å¯¹æ¯”:")
        print("- ONE-PEACE: ä¸»è¦ç”¨äºç”Ÿæˆå›¾åƒembeddingå‘é‡")
        print("- Qwen multimodal-embedding-v1: å¯èƒ½æ”¯æŒæ›´ä¸°å¯Œçš„å†…å®¹ç†è§£")
        print("- ä¸¤è€…éƒ½ä¸»è¦ç”¨äºå‘é‡åŒ–ï¼Œè€Œéç›´æ¥çš„å†…å®¹ç†è§£")
        
        print(f"\nğŸ’¡ ç»“è®º:")
        print("- ä¸¤ä¸ªæ¨¡å‹éƒ½ä¸»è¦ç”¨äºç”Ÿæˆå›¾åƒembedding")
        print("- çœŸæ­£çš„å›¾åƒå†…å®¹ç†è§£éœ€è¦ä¸“é—¨çš„è§†è§‰ç†è§£æ¨¡å‹")
        print("- å»ºè®®é›†æˆGPT-4Vã€Claude 3.5 Sonnetç­‰æ¨¡å‹è¿›è¡Œå†…å®¹ç†è§£")
        print("- å¯ä»¥å°†embeddingç”¨äºç›¸ä¼¼æ€§æ£€ç´¢ï¼Œé…åˆLLMè¿›è¡Œå†…å®¹åˆ†æ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_qwen_multimodal_embedding()
