'''
ç¨‹åºè¯´æ˜ï¼š
## 1. æµ‹è¯•æ›´ä¸°å¯Œçš„å›¾ç‰‡å†…å®¹å¯¹RAGç³»ç»Ÿçš„å½±å“
## 2. å¯¹æ¯”ä¸åŒå†…å®¹è´¨é‡ä¸‹çš„æ£€ç´¢æ•ˆæœ
## 3. åˆ†æå†…å®¹ä¸°å¯Œåº¦ä¸é—®ç­”è´¨é‡çš„å…³ç³»
'''

import pickle
import os
import sys
from collections import defaultdict
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from config.settings import Settings

def test_enhanced_image_content():
    """æµ‹è¯•æ›´ä¸°å¯Œçš„å›¾ç‰‡å†…å®¹å¯¹RAGç³»ç»Ÿçš„å½±å“"""
    
    print("ğŸ” æµ‹è¯•æ›´ä¸°å¯Œçš„å›¾ç‰‡å†…å®¹å¯¹RAGç³»ç»Ÿçš„å½±å“")
    print("=" * 80)
    
    try:
        # åŠ è½½é…ç½®
        config = Settings.load_from_file('config.json')
        embeddings = DashScopeEmbeddings(dashscope_api_key=config.dashscope_api_key, model="text-embedding-v1")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        vector_db_path = "./central/vector_db"
        vector_store = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)
        
        print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(vector_store.docstore._dict)}")
        
        # åˆ†æå›¾ç‰‡æ–‡æ¡£çš„å†…å®¹è´¨é‡
        print(f"\nğŸ” åˆ†æå›¾ç‰‡æ–‡æ¡£çš„å†…å®¹è´¨é‡:")
        print("=" * 80)
        
        image_docs = []
        for doc_id, doc in vector_store.docstore._dict.items():
            metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
            if metadata.get('chunk_type') == 'image':
                image_docs.append((doc_id, doc))
        
        print(f"ğŸ–¼ï¸  å›¾ç‰‡æ–‡æ¡£æ•°: {len(image_docs)}")
        
        # åˆ†æå†…å®¹è´¨é‡
        content_quality_analysis = analyze_content_quality(image_docs)
        
        # æ˜¾ç¤ºå†…å®¹è´¨é‡åˆ†æç»“æœ
        print(f"\nğŸ“Š å†…å®¹è´¨é‡åˆ†æ:")
        print("-" * 60)
        
        for quality_level, docs in content_quality_analysis.items():
            print(f"  {quality_level}: {len(docs)} ä¸ªæ–‡æ¡£")
            if docs:
                sample_doc = docs[0]
                metadata = sample_doc[1].metadata
                page_content = sample_doc[1].page_content
                print(f"    ç¤ºä¾‹å†…å®¹é•¿åº¦: {len(page_content)} å­—ç¬¦")
                print(f"    ç¤ºä¾‹å†…å®¹: {page_content[:100]}...")
        
        # æµ‹è¯•ä¸åŒå†…å®¹è´¨é‡ä¸‹çš„æ£€ç´¢æ•ˆæœ
        print(f"\nğŸ” æµ‹è¯•ä¸åŒå†…å®¹è´¨é‡ä¸‹çš„æ£€ç´¢æ•ˆæœ:")
        print("=" * 80)
        
        test_queries = [
            "ä¸­èŠ¯å›½é™…çš„è¥ä¸šæ”¶å…¥æƒ…å†µ",
            "å›¾1æ˜¾ç¤ºäº†ä»€ä¹ˆå†…å®¹",
            "å…¬å¸çš„æ¯›åˆ©ç‡å’Œå‡€åˆ©ç‡",
            "ä¸ªè‚¡ç›¸å¯¹æ²ªæ·±300æŒ‡æ•°çš„è¡¨ç°",
            "äº§èƒ½åˆ©ç”¨ç‡çš„å˜åŒ–è¶‹åŠ¿"
        ]
        
        for query in test_queries:
            print(f"\nğŸ“ æµ‹è¯•æŸ¥è¯¢: {query}")
            print("-" * 40)
            
            # æ‰§è¡Œæ£€ç´¢
            try:
                results = vector_store.similarity_search(query, k=5)
                
                # åˆ†ææ£€ç´¢ç»“æœ
                image_results = [doc for doc in results if doc.metadata.get('chunk_type') == 'image']
                text_results = [doc for doc in results if doc.metadata.get('chunk_type') == 'text']
                table_results = [doc for doc in results if doc.metadata.get('chunk_type') == 'table']
                
                print(f"  æ£€ç´¢åˆ°å›¾ç‰‡æ–‡æ¡£: {len(image_results)} ä¸ª")
                print(f"  æ£€ç´¢åˆ°æ–‡æœ¬æ–‡æ¡£: {len(text_results)} ä¸ª")
                print(f"  æ£€ç´¢åˆ°è¡¨æ ¼æ–‡æ¡£: {len(table_results)} ä¸ª")
                
                # åˆ†æå›¾ç‰‡æ–‡æ¡£çš„æ£€ç´¢è´¨é‡
                if image_results:
                    print(f"  ğŸ–¼ï¸  å›¾ç‰‡æ–‡æ¡£æ£€ç´¢è´¨é‡åˆ†æ:")
                    for i, doc in enumerate(image_results[:3]):  # åªåˆ†æå‰3ä¸ª
                        metadata = doc.metadata
                        page_content = doc.page_content
                        img_caption = metadata.get('img_caption', [''])
                        caption_text = ' '.join(img_caption) if img_caption else 'æ— æ ‡é¢˜'
                        
                        print(f"    {i+1}. {caption_text}")
                        print(f"       å†…å®¹é•¿åº¦: {len(page_content)} å­—ç¬¦")
                        print(f"       å†…å®¹è´¨é‡: {assess_content_quality(page_content)}")
                        print(f"       ç›¸å…³æ€§: {assess_relevance(query, page_content)}")
                
            except Exception as e:
                print(f"  æ£€ç´¢å¤±è´¥: {e}")
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        print("=" * 80)
        
        suggestions = generate_improvement_suggestions(content_quality_analysis)
        for i, suggestion in enumerate(suggestions, 1):
            print(f"  {i}. {suggestion}")
        
        # æ€»ç»“
        print(f"\nğŸ“Š æ€»ç»“:")
        print("=" * 80)
        
        total_images = len(image_docs)
        high_quality = len(content_quality_analysis.get('é«˜è´¨é‡', []))
        medium_quality = len(content_quality_analysis.get('ä¸­ç­‰è´¨é‡', []))
        low_quality = len(content_quality_analysis.get('ä½è´¨é‡', []))
        
        print(f"  æ€»å›¾ç‰‡æ•°: {total_images}")
        print(f"  é«˜è´¨é‡å†…å®¹: {high_quality} ({high_quality/total_images*100:.1f}%)")
        print(f"  ä¸­ç­‰è´¨é‡å†…å®¹: {medium_quality} ({medium_quality/total_images*100:.1f}%)")
        print(f"  ä½è´¨é‡å†…å®¹: {low_quality} ({low_quality/total_images*100:.1f}%)")
        
        if high_quality / total_images >= 0.7:
            print(f"  âœ… å›¾ç‰‡å†…å®¹è´¨é‡è‰¯å¥½ï¼ŒRAGç³»ç»Ÿæ€§èƒ½åº”è¯¥ä¸é”™")
        elif high_quality / total_images >= 0.4:
            print(f"  âš ï¸  å›¾ç‰‡å†…å®¹è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–")
        else:
            print(f"  âŒ å›¾ç‰‡å†…å®¹è´¨é‡è¾ƒå·®ï¼Œå¼ºçƒˆå»ºè®®ä¼˜åŒ–")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def analyze_content_quality(image_docs):
    """åˆ†æå›¾ç‰‡æ–‡æ¡£çš„å†…å®¹è´¨é‡"""
    
    quality_groups = {
        'é«˜è´¨é‡': [],
        'ä¸­ç­‰è´¨é‡': [],
        'ä½è´¨é‡': []
    }
    
    for doc_id, doc in image_docs:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        page_content = doc.page_content
        
        # è¯„ä¼°å†…å®¹è´¨é‡
        quality_score = assess_content_quality(page_content)
        
        if quality_score >= 0.8:
            quality_groups['é«˜è´¨é‡'].append((doc_id, doc))
        elif quality_score >= 0.5:
            quality_groups['ä¸­ç­‰è´¨é‡'].append((doc_id, doc))
        else:
            quality_groups['ä½è´¨é‡'].append((doc_id, doc))
    
    return quality_groups

def assess_content_quality(content):
    """è¯„ä¼°å†…å®¹è´¨é‡"""
    if not content or len(content.strip()) == 0:
        return 0.0
    
    score = 0.0
    
    # 1. å†…å®¹é•¿åº¦è¯„åˆ† (0-0.3åˆ†)
    length_score = min(len(content) / 200, 1.0) * 0.3
    score += length_score
    
    # 2. ä¿¡æ¯ä¸°å¯Œåº¦è¯„åˆ† (0-0.4åˆ†)
    info_keywords = ['å›¾ç‰‡æ ‡é¢˜', 'å›¾ç‰‡è„šæ³¨', 'å›¾è¡¨ç±»å‹', 'æ•°æ®', 'è¶‹åŠ¿', 'åˆ†æ']
    info_count = sum(1 for keyword in info_keywords if keyword in content)
    info_score = min(info_count / 3, 1.0) * 0.4
    score += info_score
    
    # 3. ç»“æ„åŒ–ç¨‹åº¦è¯„åˆ† (0-0.3åˆ†)
    structure_indicators = ['|', ':', ';']
    structure_count = sum(1 for indicator in structure_indicators if indicator in content)
    structure_score = min(structure_count / 2, 1.0) * 0.3
    score += structure_score
    
    return min(score, 1.0)

def assess_relevance(query, content):
    """è¯„ä¼°å†…å®¹ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"""
    if not content or not query:
        return 0.0
    
    # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
    query_words = set(query.lower().split())
    content_words = set(content.lower().split())
    
    if not query_words:
        return 0.0
    
    # è®¡ç®—è¯æ±‡é‡å åº¦
    overlap = len(query_words.intersection(content_words))
    relevance = overlap / len(query_words)
    
    return min(relevance, 1.0)

def generate_improvement_suggestions(content_quality_analysis):
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    suggestions = []
    
    low_quality_count = len(content_quality_analysis.get('ä½è´¨é‡', []))
    medium_quality_count = len(content_quality_analysis.get('ä¸­ç­‰è´¨é‡', []))
    
    if low_quality_count > 0:
        suggestions.append(f"æœ‰ {low_quality_count} ä¸ªå›¾ç‰‡æ–‡æ¡£å†…å®¹è´¨é‡è¾ƒä½ï¼Œå»ºè®®å¢å¼ºå›¾ç‰‡æè¿°ç”Ÿæˆ")
    
    if medium_quality_count > 0:
        suggestions.append(f"æœ‰ {medium_quality_count} ä¸ªå›¾ç‰‡æ–‡æ¡£å†…å®¹è´¨é‡ä¸€èˆ¬ï¼Œå¯ä»¥è€ƒè™‘æ·»åŠ æ›´å¤šè¯­ä¹‰ä¿¡æ¯")
    
    suggestions.append("è€ƒè™‘ä½¿ç”¨æ›´å…ˆè¿›çš„å›¾åƒç†è§£æ¨¡å‹ï¼ˆå¦‚ONE-PEACEï¼‰æ¥ç”Ÿæˆæ›´ä¸°å¯Œçš„å›¾ç‰‡æè¿°")
    suggestions.append("åœ¨å›¾ç‰‡æè¿°ä¸­æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚æ•°æ®è¶‹åŠ¿ã€å…³é”®æŒ‡æ ‡ç­‰")
    suggestions.append("å»ºç«‹å›¾ç‰‡å†…å®¹è´¨é‡è¯„ä¼°æœºåˆ¶ï¼Œå®šæœŸæ£€æŸ¥å’Œä¼˜åŒ–")
    suggestions.append("è€ƒè™‘ä¸ºä¸åŒç±»å‹çš„å›¾ç‰‡ï¼ˆå›¾è¡¨ã€ç…§ç‰‡ã€è¡¨æ ¼ï¼‰ä½¿ç”¨ä¸åŒçš„æè¿°ç­–ç•¥")
    
    return suggestions

if __name__ == "__main__":
    test_enhanced_image_content()
