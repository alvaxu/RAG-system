[toc]

# 1. N-Gramå’ŒTF-IDFï¼šé€šä¿—æ˜“æ‡‚çš„è§£æ

## 1.1 N-Gramï¼šè®©AIå­¦ä¼š"çŒœè¯"çš„æŠ€æœ¯

### 1.1.1 åŸºæœ¬æ¦‚å¿µ
N-Gramæ˜¯ä¸€ç§è®©è®¡ç®—æœºç†è§£è¯­è¨€è§„å¾‹çš„åŸºç¡€æ–¹æ³•ï¼Œä¸»è¦ç”¨äºé¢„æµ‹æ–‡æœ¬ä¸­ä¸‹ä¸€ä¸ªå¯èƒ½å‡ºç°çš„è¯ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³å¾ˆç®€å•ï¼šå‡è®¾ä¸€ä¸ªè¯çš„å‡ºç°åªå’Œå‰é¢çš„å‡ ä¸ªè¯æœ‰å…³ã€‚

ä¸¾ä¸ªä¾‹å­ï¼š
- "æˆ‘æƒ³åƒ"åé¢æ¥"è‹¹æœ"çš„æ¦‚ç‡ï¼Œå¯èƒ½æ¯”æ¥"æ¸¸æ³³"æ›´é«˜
- è¾“å…¥æ³•åœ¨ä½ æ‰“å‡º"dddd"æ—¶æ¨è"å¸¦å¸¦å¼Ÿå¼Ÿ"å°±æ˜¯åŸºäºè¿™ç§åŸç†

### 1.1.2 å·¥ä½œåŸç†
1. **åˆ†æ®µç»Ÿè®¡**ï¼šæŠŠæ–‡æœ¬æ‹†æˆè¿ç»­çš„è¯ç»„åˆï¼ˆæ¯”å¦‚2ä¸ªè¯çš„"æˆ‘åƒ"ï¼Œ3ä¸ªè¯çš„"æˆ‘æƒ³åƒ"ï¼‰ï¼Œç»Ÿè®¡æ¯ä¸ªç»„åˆå‡ºç°çš„æ¬¡æ•°
2. **è®¡ç®—æ¦‚ç‡**ï¼šç”¨"ä¸‹ä¸€ä¸ªè¯å‡ºç°çš„æ¬¡æ•°é™¤ä»¥å½“å‰ç»„åˆå‡ºç°çš„æ€»æ¬¡æ•°"å¾—åˆ°æ¡ä»¶æ¦‚ç‡
3. **å¤„ç†é›¶æ¦‚ç‡**ï¼šç»™ä»æœªå‡ºç°è¿‡çš„ç»„åˆåˆ†é…å¾ˆå°çš„æ¦‚ç‡ï¼Œé¿å…å®Œå…¨æ’é™¤å¯èƒ½æ€§

### 1.1.3 å¸¸è§ç±»å‹
- **Unigramï¼ˆä¸€å…ƒç»„ï¼‰**ï¼šå•ä¸ªè¯ä¸ºä¸€ç»„ï¼ˆå¦‚"æˆ‘"ã€"å–œæ¬¢"ï¼‰
- **Bigramï¼ˆäºŒå…ƒç»„ï¼‰**ï¼šä¸¤ä¸ªè¿ç»­è¯ä¸ºä¸€ç»„ï¼ˆå¦‚"æˆ‘å–œæ¬¢"ã€"å–œæ¬¢å­¦ä¹ "ï¼‰
- **Trigramï¼ˆä¸‰å…ƒç»„ï¼‰**ï¼šä¸‰ä¸ªè¿ç»­è¯ä¸ºä¸€ç»„ï¼ˆå¦‚"æˆ‘å–œæ¬¢å­¦ä¹ "ï¼‰

### 1.1.4 åº”ç”¨åœºæ™¯
- æ‰‹æœºè¾“å…¥æ³•å€™é€‰è¯é¢„æµ‹
- æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚è‡ªåŠ¨è¡¥å…¨å¥å­ï¼‰
- æ‹¼å†™æ£€æŸ¥ï¼ˆåˆ¤æ–­è¯è¯­ç»„åˆæ˜¯å¦åˆç†ï¼‰
- æœç´¢å¼•æ“æŸ¥è¯¢æ‰©å±•

### 1.1.5 ä¼˜ç¼ºç‚¹
âœ… ä¼˜ç‚¹ï¼š
- ç®€å•æ˜“å®ç°ï¼Œè®¡ç®—æ•ˆç‡é«˜
- å¯è§£é‡Šæ€§å¼ºï¼Œæ˜“äºè°ƒè¯•

âŒ ç¼ºç‚¹ï¼š
- åªèƒ½è®°ä½æœ‰é™ä¸Šä¸‹æ–‡ï¼ˆé•¿å¥å­å®¹æ˜“å‡ºé”™ï¼‰
- éœ€è¦å¤§é‡æ•°æ®è®­ç»ƒ
- å¯¹æœªè§è¿‡çš„æ–°è¯ç»„åˆé¢„æµ‹èƒ½åŠ›å·®

---

## 1.2 TF-IDFï¼šè¡¡é‡è¯è¯­é‡è¦æ€§çš„å°ºå­

### 1.2.1 åŸºæœ¬æ¦‚å¿µ
TF-IDFï¼ˆè¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ï¼‰æ˜¯ä¸€ç§è¯„ä¼°è¯è¯­é‡è¦æ€§çš„æ–¹æ³•ï¼Œå®ƒè€ƒè™‘ä¸¤ä¸ªå› ç´ ï¼š
1. **è¯é¢‘ï¼ˆTFï¼‰**ï¼šè¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡
2. **é€†æ–‡æ¡£é¢‘ç‡ï¼ˆIDFï¼‰**ï¼šè¯åœ¨æ•´ä¸ªæ–‡æ¡£é›†åˆä¸­çš„ç½•è§ç¨‹åº¦

ç®€å•è¯´ï¼šä¸€ä¸ªè¯åœ¨æœ¬æ–‡ä¸­å‡ºç°è¶Šå¤šï¼ˆTFé«˜ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ«çš„æ–‡ç« ä¸­å‡ºç°è¶Šå°‘ï¼ˆIDFé«˜ï¼‰ï¼Œå°±è¶Šé‡è¦ã€‚

### 1.2.2 è®¡ç®—å…¬å¼
```
TF-IDF = TF Ã— IDF
```
å…¶ä¸­ï¼š
- TF = è¯åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°æ¬¡æ•° / æ–‡æ¡£æ€»è¯æ•°
- IDF = log(æ–‡æ¡£æ€»æ•° / åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°)

### 1.2.3 ä¸ºä»€ä¹ˆéœ€è¦TF-IDFï¼Ÿ
ç›´æ¥ç»Ÿè®¡è¯é¢‘ä¼šæœ‰ä¸€ä¸ªé—®é¢˜ï¼šåƒ"çš„"ã€"æ˜¯"è¿™ç§è¯è™½ç„¶å‡ºç°å¾ˆå¤šï¼Œä½†å¯¹ç†è§£å†…å®¹æ²¡å¸®åŠ©ã€‚TF-IDFé€šè¿‡IDFé™ä½äº†è¿™ç±»è¯çš„æƒé‡ã€‚

### 1.2.4 åº”ç”¨åœºæ™¯
- æœç´¢å¼•æ“æ’åºï¼ˆæ‰¾å‡ºæ–‡æ¡£çœŸæ­£é‡è¦çš„è¯ï¼‰
- æ–‡æœ¬åˆ†ç±»ï¼ˆå¦‚æ–°é—»åˆ†ç±»ï¼‰
- å…³é”®è¯è‡ªåŠ¨æå–
- æ¨èç³»ç»Ÿï¼ˆåˆ†æç”¨æˆ·å…´è¶£ï¼‰

### 1.2.5 å®é™…æ¡ˆä¾‹
å¦‚æœåˆ†æä¸“åˆ©æ–‡æ¡£ï¼š
- "ä¸­å›½"å¯èƒ½è¯é¢‘é«˜ä½†IDFä½ï¼ˆå¾ˆå¤šæ–‡æ¡£éƒ½æåˆ°ï¼‰
- "ä¸“åˆ©"è¯é¢‘é€‚ä¸­ä½†IDFé«˜ï¼ˆè¾ƒå°‘æ–‡æ¡£æåˆ°ï¼‰
â†’ "ä¸“åˆ©"çš„TF-IDFå€¼ä¼šæ›´é«˜ï¼Œæ›´èƒ½ä»£è¡¨ä¸»é¢˜

### 1.2.6 ä¼˜ç¼ºç‚¹
âœ… ä¼˜ç‚¹ï¼š
- ç®€å•æœ‰æ•ˆï¼Œæ˜“äºè®¡ç®—
- èƒ½è‡ªåŠ¨è¿‡æ»¤å¸¸è§æ— æ„ä¹‰è¯

âŒ ç¼ºç‚¹ï¼š
- ä¸è€ƒè™‘è¯è¯­é¡ºåºå’Œè¯­ä¹‰å…³ç³»
- å¯¹åŒä¹‰è¯å¤„ç†ä¸å¥½ï¼ˆå¦‚"ç”µè„‘"å’Œ"è®¡ç®—æœº"ï¼‰

---

## 1.3 æ€»ç»“å¯¹æ¯”

| ç‰¹æ€§        | N-Gram                          | TF-IDF                          |
|------------|--------------------------------|--------------------------------|
| **ä¸»è¦ç”¨é€”** | é¢„æµ‹ä¸‹ä¸€ä¸ªè¯/ç”Ÿæˆæ–‡æœ¬          | è¯„ä¼°è¯è¯­é‡è¦æ€§/æ–‡æ¡£ç‰¹å¾æå–     |
| **æ ¸å¿ƒæ€æƒ³** | è¯è¯­å‡ºç°çš„æ¦‚ç‡ä¾èµ–å‰å‡ ä¸ªè¯     | é‡è¦=åœ¨æœ¬æ–‡æ¡£å¤šè§+åœ¨å…¶å®ƒæ–‡æ¡£å°‘è§ |
| **å…¸å‹åº”ç”¨** | è¾“å…¥æ³•ã€æœºå™¨ç¿»è¯‘ã€æ‹¼å†™æ£€æŸ¥     | æœç´¢å¼•æ“ã€æ–‡æœ¬åˆ†ç±»ã€å…³é”®è¯æå–  |
| **ä¼˜åŠ¿**    | ä¿æŒè¯­è¨€è¿è´¯æ€§                 | è¯†åˆ«æ–‡æ¡£å…³é”®ä¸»é¢˜è¯             |
| **å±€é™**    | é•¿è·ç¦»ä¾èµ–å·®ã€éœ€è¦å¤§é‡è®­ç»ƒæ•°æ® | å¿½ç•¥è¯è¯­é¡ºåºå’Œè¯­ä¹‰å…³ç³»         |

ä¸¤è€…å¸¸ç»“åˆä½¿ç”¨ï¼Œæ¯”å¦‚å…ˆç”¨TF-IDFæå–é‡è¦è¯ï¼Œå†ç”¨N-Gramåˆ†æè¿™äº›è¯çš„å…³ç³»ã€‚

## ç®€å•ç¤ºä¾‹
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ç¤ºä¾‹è¯åº“
words = [
    "è‹¹æœ", "é¦™è•‰", "æ©™å­", "è‘¡è„", "è è", 
    "èŠ’æœ", "è¥¿ç“œ", "è‰è“", "è“è“", "æ¨±æ¡ƒ",
    "è‹¹æœæ‰‹æœº", "è‹¹æœç”µè„‘", "è‹¹æœæ±", "çº¢è‹¹æœ", "é’è‹¹æœ"
]

# å®šä¹‰n-gramå‡½æ•°ï¼ˆè¿™é‡Œä½¿ç”¨2-gramï¼‰
def get_ngrams(word, n=2):
    return [word[i:i+n] for i in range(len(word)-n+1)]

# ä¸ºæ¯ä¸ªè¯ç”Ÿæˆn-gramç‰¹å¾
word_ngrams = [" ".join(get_ngrams(word)) for word in words]
print("è¯è¯­çš„2-gramè¡¨ç¤ºç¤ºä¾‹:")
for word, ngram in zip(words[:5], word_ngrams[:5]):
    print(f"{word} â†’ {ngram}")

# ä½¿ç”¨TF-IDFå‘é‡åŒ–
vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split())
tfidf_matrix = vectorizer.fit_transform(word_ngrams)

# å®šä¹‰æŸ¥æ‰¾ç›¸ä¼¼è¯çš„å‡½æ•°
def find_similar_words(target_word, top_n=5):
    # ç”Ÿæˆç›®æ ‡è¯çš„n-gram
    target_ngram = " ".join(get_ngrams(target_word))
    # è½¬æ¢ä¸ºTF-IDFå‘é‡
    target_vec = vectorizer.transform([target_ngram])
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = cosine_similarity(target_vec, tfidf_matrix)
    # è·å–æœ€ç›¸ä¼¼çš„è¯
    similar_indices = np.argsort(similarities[0])[::-1][1:top_n+1]  # æ’é™¤è‡ªå·±
    print(f"\nä¸'{target_word}'æœ€ç›¸ä¼¼çš„{top_n}ä¸ªè¯:")
    for idx in similar_indices:
        print(f"{words[idx]}: {similarities[0][idx]:.3f}")

# æµ‹è¯•ç¤ºä¾‹
find_similar_words("è‹¹æœ", top_n=5)
find_similar_words("è è", top_n=3)
find_similar_words("è‹¹æœæ‰‹æœº", top_n=3)
```

# 2. Word Embeddingï¼ˆè¯åµŒå…¥ï¼‰

### 2.1 ğŸŒ æŠŠè¯è¯­å˜æˆ"åæ ‡"

æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªå¤–æ˜Ÿäººï¼Œç¬¬ä¸€æ¬¡æ¥åœ°çƒå­¦ä¹ äººç±»çš„è¯­è¨€ã€‚ä½ å‘ç°ï¼š

*   å•è¯ **"çŒ«"** å’Œ **"ç‹—"** ç»å¸¸ä¸€èµ·å‡ºç°ï¼ˆå› ä¸ºå®ƒä»¬éƒ½æ˜¯å® ç‰©ï¼‰ã€‚
*   å•è¯ **"è‹¹æœ"** å’Œ **"é¦™è•‰"** ä¹Ÿç»å¸¸ä¸€èµ·å‡ºç°ï¼ˆå› ä¸ºå®ƒä»¬éƒ½æ˜¯æ°´æœï¼‰ã€‚
*   ä½† **"çŒ«"** å’Œ **"è‹¹æœ"** å‡ ä¹ä¸ä¼šåŒæ—¶å‡ºç°ï¼ˆå› ä¸ºå®ƒä»¬å±äºä¸åŒç±»åˆ«ï¼‰ã€‚

äºæ˜¯ï¼Œä½ å†³å®šç»™æ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ª **"åæ ‡"**ï¼ˆæ¯”å¦‚åœ¨ä¸‰ç»´ç©ºé—´é‡Œçš„ä½ç½®ï¼‰ï¼š

*   **"çŒ«"** â†’ `[0.8, 0.2, 0.1]`
*   **"ç‹—"** â†’ `[0.7, 0.3, 0.1]`
*   **"è‹¹æœ"** â†’ `[0.1, 0.9, 0.4]`
*   **"é¦™è•‰"** â†’ `[0.2, 0.8, 0.3]`

è¿™æ ·ï¼š\
âœ… **ç›¸ä¼¼çš„è¯**ï¼ˆæ¯”å¦‚çŒ«å’Œç‹—ï¼‰åæ ‡æ¥è¿‘ã€‚\
âŒ **ä¸ç›¸ä¼¼çš„è¯**ï¼ˆæ¯”å¦‚çŒ«å’Œè‹¹æœï¼‰åæ ‡è¿œç¦»ã€‚

***

### 2.2 ğŸ” Word Embedding æ˜¯ä»€ä¹ˆï¼Ÿ

Word Embedding å°±æ˜¯é€šè¿‡æ•°å­¦æ–¹æ³•ï¼ŒæŠŠå•è¯å˜æˆ **ä¸€ä¸²æ•°å­—ï¼ˆå‘é‡ï¼‰**ï¼Œè®©è®¡ç®—æœºèƒ½é€šè¿‡è¿™äº›æ•°å­—ï¼š

1.  **ç†è§£è¯è¯­çš„æ„æ€**ï¼ˆæ¯”å¦‚"çŒ«"å’Œ"ç‹—"éƒ½æ˜¯åŠ¨ç‰©ï¼‰ã€‚
2.  **è®¡ç®—è¯è¯­çš„å…³ç³»**ï¼ˆæ¯”å¦‚"å›½ç‹ - ç”· + å¥³ â‰ˆ å¥³ç‹"ï¼‰ã€‚

***

### 2.3 ğŸ’¡ ä¸ºä»€ä¹ˆè¦ç”¨ Word Embeddingï¼Ÿ

ç›´æ¥ç»™å•è¯ç¼–å·ï¼ˆæ¯”å¦‚"çŒ«=1ï¼Œç‹—=2"ï¼‰ä¼šä¸¢å¤±è¯­ä¹‰ä¿¡æ¯ã€‚è€Œ Word Embedding èƒ½ï¼š

*   **å‹ç¼©ä¿¡æ¯**ï¼šç”¨å°‘æ•°å‡ ä¸ªæ•°å­—è¡¨ç¤ºå¤æ‚å«ä¹‰ã€‚
*   **å‘ç°è§„å¾‹**ï¼šè‡ªåŠ¨å­¦ä¹ "çŒ«â†’ç‹—"å’Œ"è‹¹æœâ†’é¦™è•‰"çš„ç›¸ä¼¼å…³ç³»ã€‚
*   **å…¼å®¹ç®—æ³•**ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰åªèƒ½å¤„ç†æ•°å­—ï¼Œä¸èƒ½ç›´æ¥å¤„ç†æ–‡å­—ã€‚

***

### 2.4 ğŸ› ï¸ ä¸¾ä¸ªå®é™…ä¾‹å­

å‡è®¾ç”¨ 3 ç»´å‘é‡è¡¨ç¤ºè¯è¯­ï¼š

*   **"ç§‘æŠ€"** â†’ `[0.9, 0.1, 0.2]`
*   **"æ‰‹æœº"** â†’ `[0.8, 0.2, 0.3]`
*   **"æ°´æœ"** â†’ `[0.1, 0.9, 0.4]`

è®¡ç®—æœºçœ‹åˆ°ï¼š

*   `"ç§‘æŠ€"` å’Œ `"æ‰‹æœº"` çš„å‘é‡æ¥è¿‘ â†’ å®ƒä»¬ç›¸å…³ã€‚
*   `"ç§‘æŠ€"` å’Œ `"æ°´æœ"` çš„å‘é‡è¿œç¦» â†’ å®ƒä»¬æ— å…³ã€‚

***

### 2.5 ğŸ“š å¸¸è§çš„ Word Embedding æ–¹æ³•

1.  **Word2Vec**ï¼šé€šè¿‡ä¸Šä¸‹æ–‡é¢„æµ‹è¯è¯­ï¼ˆæ¯”å¦‚"çŒ«çˆ±åƒ\_\_" â†’ é¢„æµ‹"é±¼"ï¼‰ã€‚
2.  **GloVe**ï¼šç»Ÿè®¡è¯è¯­å…±åŒå‡ºç°çš„é¢‘ç‡ï¼ˆæ¯”å¦‚"çŒ«"å’Œ"ç‹—"ç»å¸¸ä¸€èµ·å‡ºç°ï¼‰ã€‚
3.  **BERT**ï¼ˆç°ä»£æ–¹æ³•ï¼‰ï¼šç»“åˆä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´å‘é‡ï¼ˆæ¯”å¦‚"è‹¹æœ"åœ¨"åƒè‹¹æœ"å’Œ"è‹¹æœæ‰‹æœº"ä¸­å«ä¹‰ä¸åŒï¼‰ã€‚

***

### 2.6 â“ ç®€å•æ€»ç»“

Word Embedding å°±æ˜¯ **è®©è®¡ç®—æœºé€šè¿‡æ•°å­—"ç†è§£"è¯è¯­**ï¼Œåƒäººç±»ä¸€æ ·çŸ¥é“"çŒ«å’Œç‹—ç›¸ä¼¼ï¼Œä½†å’Œè‹¹æœæ— å…³"ã€‚å®ƒæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„åŸºç¡€æŠ€æœ¯ï¼Œç”¨äºç¿»è¯‘ã€æœç´¢ã€èŠå¤©æœºå™¨äººç­‰åœºæ™¯ã€‚

# 3. Word2Vec

## 3.1 Word2Vecçš„ä¸¤ç§æ¨¡å‹

### (1) CBOW (Continuous Bag of Words)

*   **ç›®æ ‡**ï¼šç”¨ä¸Šä¸‹æ–‡è¯è¯­é¢„æµ‹ä¸­å¿ƒè¯ï¼ˆé€‚åˆå°å‹æ•°æ®é›†ï¼‰ã€‚
*   **ä¾‹å­**ï¼š\
    å¥å­ï¼š`"æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†"`\
    å‡è®¾çª—å£å¤§å°ä¸º 2ï¼ˆå·¦å³å„ 2 ä¸ªè¯ï¼‰ï¼Œåˆ™ï¼š
    *   è¾“å…¥ï¼š`["æˆ‘", "çˆ±", "è¯­è¨€", "å¤„ç†"]`ï¼ˆä¸Šä¸‹æ–‡ï¼‰
    *   è¾“å‡ºï¼š`"è‡ªç„¶"`ï¼ˆä¸­å¿ƒè¯ï¼‰

### (2) Skip-gram

*   **ç›®æ ‡**ï¼šç”¨ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡è¯è¯­ï¼ˆé€‚åˆå¤§å‹æ•°æ®é›†ï¼‰ã€‚
*   **ä¾‹å­**ï¼š\
    åŒä¸€å¥å­ `"æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†"`ï¼Œçª—å£å¤§å°ä¸º 2ï¼š
    *   è¾“å…¥ï¼š`"è‡ªç„¶"`ï¼ˆä¸­å¿ƒè¯ï¼‰
    *   è¾“å‡ºï¼š`["æˆ‘", "çˆ±", "è¯­è¨€", "å¤„ç†"]`ï¼ˆä¸Šä¸‹æ–‡ï¼‰

> **CBOW vs Skip-gram**ï¼š
>
> *   CBOW è®­ç»ƒæ›´å¿«ï¼Œé€‚åˆé«˜é¢‘è¯ã€‚
> *   Skip-gram å¯¹ä½é¢‘è¯æ•ˆæœæ›´å¥½ï¼Œä½†éœ€è¦æ›´å¤šæ•°æ®ã€‚

***

## 3.2 Word2Vecçš„å®ç°æ­¥éª¤

### Step 1: æ•°æ®é¢„å¤„ç†

*   åˆ†è¯ï¼ˆå¦‚ç”¨ `jieba` å¯¹ä¸­æ–‡åˆ†è¯ï¼‰ã€‚
*   æ„å»ºè¯æ±‡è¡¨ï¼ˆç»™æ¯ä¸ªè¯åˆ†é…å”¯ä¸€ IDï¼Œå¦‚ `æˆ‘=0, çˆ±=1, è‡ªç„¶=2...`ï¼‰ã€‚

### Step 2: æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹

Word2Vec æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª **å•éšå±‚ç¥ç»ç½‘ç»œ**ï¼Œç»“æ„å¦‚ä¸‹ï¼š

    è¾“å…¥å±‚ â†’ éšè—å±‚ï¼ˆEmbedding å±‚ï¼‰ â†’ è¾“å‡ºå±‚ï¼ˆSoftmaxï¼‰

*   **è¾“å…¥å±‚**ï¼šè¯è¯­çš„ one-hot ç¼–ç ï¼ˆå¦‚ `"è‡ªç„¶" = [0, 0, 1, 0, 0]`ï¼‰ã€‚
*   **éšè—å±‚**ï¼šæƒé‡çŸ©é˜µï¼ˆå³è¯å‘é‡è¡¨ï¼‰ï¼Œç»´åº¦ = `[è¯æ±‡è¡¨å¤§å°, åµŒå…¥ç»´åº¦]`ï¼ˆå¦‚ 300 ç»´ï¼‰ã€‚
*   **è¾“å‡ºå±‚**ï¼šé¢„æµ‹ä¸Šä¸‹æ–‡è¯çš„æ¦‚ç‡ï¼ˆSoftmax å½’ä¸€åŒ–ï¼‰ã€‚

### Step 3: è®­ç»ƒæ¨¡å‹

*   è¾“å…¥ä¸€ä¸ªè¯ï¼ˆå¦‚ `"è‡ªç„¶"` çš„ one-hot å‘é‡ `[0, 0, 1, 0, 0]`ï¼‰ã€‚
*   ä¹˜ä»¥æƒé‡çŸ©é˜µï¼Œå¾—åˆ°éšè—å±‚çš„ **è¯å‘é‡**ï¼ˆå¦‚ `[0.2, -0.5, 0.7, ...]`ï¼‰ã€‚
*   ç”¨ Softmax è®¡ç®—é¢„æµ‹çš„ä¸Šä¸‹æ–‡è¯æ¦‚ç‡ã€‚
*   é€šè¿‡åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰æ›´æ–°æƒé‡ï¼Œä½¿é¢„æµ‹æ›´å‡†ã€‚

### Step 4: æå–è¯å‘é‡

è®­ç»ƒå®Œæˆåï¼Œ**éšè—å±‚çš„æƒé‡çŸ©é˜µå°±æ˜¯è¯å‘é‡è¡¨**ï¼

*   ä¾‹å¦‚ï¼Œ`"è‡ªç„¶"` çš„è¯å‘é‡æ˜¯æƒé‡çŸ©é˜µçš„ç¬¬ 3 è¡Œï¼ˆå‡è®¾ `"è‡ªç„¶"` çš„ ID=2ï¼‰ã€‚

***

## 3.3 å…³é”®ä¼˜åŒ–æŠ€æœ¯

ç›´æ¥è®¡ç®— Softmax å¯¹å¤§è§„æ¨¡è¯æ±‡è¡¨æ•ˆç‡æä½ï¼Œå› æ­¤ Word2Vec ç”¨ä¸¤ç§ä¼˜åŒ–æ–¹æ³•ï¼š

### (1) è´Ÿé‡‡æ ·ï¼ˆNegative Samplingï¼‰

*   **é—®é¢˜**ï¼šSoftmax è¦è®¡ç®—æ‰€æœ‰è¯çš„æ¦‚ç‡ï¼Œè®¡ç®—é‡å¤ªå¤§ã€‚
*   **è§£å†³**ï¼šæ¯æ¬¡è®­ç»ƒåªé‡‡æ ·å°‘é‡è´Ÿæ ·æœ¬ï¼ˆéšæœºé€‰éä¸Šä¸‹æ–‡è¯ï¼‰ï¼Œä¼˜åŒ–ç›®æ ‡å˜ä¸ºï¼š
    *   æœ€å¤§åŒ–çœŸå®ä¸Šä¸‹æ–‡è¯çš„æ¦‚ç‡ã€‚
    *   æœ€å°åŒ–è´Ÿæ ·æœ¬è¯çš„æ¦‚ç‡ã€‚

### (2) å±‚æ¬¡ Softmaxï¼ˆHierarchical Softmaxï¼‰

*   ç”¨å“ˆå¤«æ›¼æ ‘ï¼ˆHuffman Treeï¼‰ç¼–ç è¯æ±‡è¡¨ï¼Œå°†è®¡ç®—å¤æ‚åº¦ä» `O(N)` é™åˆ° `O(log N)`ã€‚
*   æ¯ä¸ªè¯å¯¹åº”æ ‘çš„ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œé¢„æµ‹æ—¶åªéœ€è®¡ç®—è·¯å¾„ä¸Šçš„èŠ‚ç‚¹æ¦‚ç‡ã€‚

***

## 3.4 ä»£ç ç¤ºä¾‹ï¼ˆPythonï¼‰

ç”¨ `gensim` åº“å¿«é€Ÿè®­ç»ƒ Word2Vecï¼š

```python
from gensim.models import Word2Vec

# ç¤ºä¾‹æ•°æ®ï¼ˆå·²åˆ†è¯çš„å¥å­ï¼‰
sentences = [
    ["æˆ‘", "çˆ±", "è‡ªç„¶", "è¯­è¨€", "å¤„ç†"],
    ["æ·±åº¦", "å­¦ä¹ ", "çœŸ", "æœ‰è¶£"]
]

# è®­ç»ƒæ¨¡å‹ï¼ˆSkip-gram + è´Ÿé‡‡æ ·ï¼‰
model = Word2Vec(
    sentences,
    vector_size=100,  # è¯å‘é‡ç»´åº¦
    window=5,         # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    min_count=1,      # å¿½ç•¥ä½é¢‘è¯
    sg=1,             # 1=Skip-gram, 0=CBOW
    negative=5,       # è´Ÿé‡‡æ ·æ•°
    epochs=10         # è®­ç»ƒè½®æ¬¡
)

# è·å–è¯å‘é‡
vector = model.wv["è‡ªç„¶"]  # "è‡ªç„¶"çš„è¯å‘é‡
print(vector)

# æ‰¾ç›¸ä¼¼è¯
similar_words = model.wv.most_similar("è‡ªç„¶", topn=3)
print(similar_words)  # è¾“å‡ºï¼š[('è¯­è¨€', 0.92), ('å­¦ä¹ ', 0.88), ...]
```

***

## 3.5 æ€»ç»“

*   **æ ¸å¿ƒæ€æƒ³**ï¼šç”¨ä¸Šä¸‹æ–‡å­¦ä¹ è¯å‘é‡ï¼ˆCBOW/Skip-gramï¼‰ã€‚
*   **å…³é”®æ­¥éª¤**ï¼š
    1.  åˆ†è¯ â†’ æ„å»ºè¯æ±‡è¡¨ â†’ one-hot ç¼–ç ã€‚
    2.  è®­ç»ƒå•éšå±‚ç¥ç»ç½‘ç»œï¼Œæå–éšè—å±‚æƒé‡ä½œä¸ºè¯å‘é‡ã€‚
*   **ä¼˜åŒ–æ–¹æ³•**ï¼šè´Ÿé‡‡æ ·ã€å±‚æ¬¡ Softmax åŠ é€Ÿè®­ç»ƒã€‚
*   **åº”ç”¨åœºæ™¯**ï¼šè¯­ä¹‰æœç´¢ã€æ¨èç³»ç»Ÿã€æœºå™¨ç¿»è¯‘ç­‰ã€‚

Word2Vec çš„ä¼˜ç‚¹æ˜¯ç®€å•é«˜æ•ˆï¼Œä½†ç¼ºç‚¹æ˜¯æ— æ³•å¤„ç†å¤šä¹‰è¯ï¼ˆå¦‚"è‹¹æœ"åœ¨æ°´æœå’Œå…¬å¸è¯­å¢ƒä¸­å«ä¹‰ä¸åŒï¼‰ã€‚åç»­çš„ **GloVe**ã€**BERT** ç­‰æ¨¡å‹å¯¹æ­¤åšäº†æ”¹è¿›ã€‚

# 4. å®æ“
## 4.1 ç”¨N-Gramå’ŒTF-IDFä¸ºé…’åº—å»ºç«‹å†…å®¹æ¨èç³»ç»Ÿ
### 4.1.1 å‡†å¤‡
è¥¿é›…å›¾é…’åº—æ•°æ®é›†ï¼š
- ä¸‹è½½åœ°å€ï¼šhttps://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Seattle_Hotels.csv
- å­—æ®µï¼šname,address,desc
- ç›®æ ‡ï¼šåŸºäºç”¨æˆ·é€‰æ‹©çš„é…’åº—ï¼Œæ¨èç›¸ä¼¼åº¦é«˜çš„Top10ä¸ªå…¶ä»–é…’åº—
- æ–¹æ³•ï¼šè®¡ç®—å½“å‰é…’åº—ç‰¹å¾å‘é‡ä¸æ•´ä¸ªé…’åº—ç‰¹å¾çŸ©é˜µçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå–ç›¸ä¼¼åº¦æœ€å¤§çš„Top-kä¸ª
  
### 4.1.2 æ­¥éª¤
- Step1ï¼Œå¯¹é…’åº—æè¿°ï¼ˆDescï¼‰è¿›è¡Œç‰¹å¾æå–
	- N-Gramï¼Œæå–Nä¸ªè¿ç»­å­—çš„é›†åˆï¼Œä½œä¸ºç‰¹å¾
	- TF-IDFï¼ŒæŒ‰ç…§(min_df,max_df)æå–å…³é”®è¯ï¼Œå¹¶ç”ŸæˆTFIDFçŸ©é˜µ
- Step2ï¼Œè®¡ç®—é…’åº—ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
	- ä½™å¼¦ç›¸ä¼¼åº¦
-  Step3ï¼Œå¯¹äºæŒ‡å®šçš„é…’åº—ï¼Œé€‰æ‹©ç›¸ä¼¼åº¦æœ€å¤§çš„Top-Kä¸ªé…’åº—è¿›è¡Œè¾“å‡º

### 4.1.3 ç¤ºä¾‹ä»£ç 
```python
import pandas as pd
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import re
pd.options.display.max_columns = 30
import matplotlib.pyplot as plt
# æ”¯æŒä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
df = pd.read_csv('Seattle_Hotels.csv', encoding="latin-1")
# æ•°æ®æ¢ç´¢
# print(df.head())
print('æ•°æ®é›†ä¸­çš„é…’åº—ä¸ªæ•°ï¼š', len(df))

# åˆ›å»ºè‹±æ–‡åœç”¨è¯åˆ—è¡¨
ENGLISH_STOPWORDS = {
    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 
    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', 
    "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 
    'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 
    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 
    'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 
    'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 
    'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 
    'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 
    "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', 
    "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', 
    "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 
    'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"
}

def print_description(index):
    example = df[df.index == index][['desc', 'name']].values[0]
    if len(example) > 0:
        print('Name:', example[1])
        print(example[0])
print('ç¬¬10ä¸ªé…’åº—çš„æè¿°ï¼š')
print_description(10)

# å¾—åˆ°é…’åº—æè¿°ä¸­n-gramç‰¹å¾ä¸­çš„TopKä¸ªç‰¹å¾,é»˜è®¤n=1å³1-gram,k=Noneï¼Œè¡¨ç¤ºæ‰€æœ‰çš„ç‰¹å¾)
def get_top_n_words(corpus, n=1, k=None):
    # ç»Ÿè®¡ngramè¯é¢‘çŸ©é˜µï¼Œä½¿ç”¨è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨
    vec = CountVectorizer(ngram_range=(n, n), stop_words=list(ENGLISH_STOPWORDS)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    """
    print('feature names:')
    print(vec.get_feature_names())
    print('bag of words:')
    print(bag_of_words.toarray())
    """
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    # æŒ‰ç…§è¯é¢‘ä»å¤§åˆ°å°æ’åº
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:k]
# ç”Ÿæˆn=1.k=20çš„å¯è§†å›¾
# n_gram=1
# common_words = get_top_n_words(df['desc'], n=n_gram,k=20)
# # ç”Ÿæˆn=3.k=20çš„å¯è§†å›¾
n_gram=3
common_words = get_top_n_words(df['desc'], n=n_gram,k=20)
# common_words = get_top_n_words(df['desc'], 3, 20)
print(f"comon_words are \n {common_words}")
df1 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
df1.groupby('desc').sum()['count'].sort_values().plot(kind='barh', title=f'å»æ‰åœç”¨è¯åï¼Œé…’åº—æè¿°ä¸­çš„Top20-{n_gram}å•è¯')
plt.savefig(f'./top20-{n_gram}words.png')
plt.show()


# æ–‡æœ¬é¢„å¤„ç†
REPLACE_BY_SPACE_RE = re.compile(r'[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
# ä½¿ç”¨è‡ªå®šä¹‰çš„è‹±æ–‡åœç”¨è¯åˆ—è¡¨æ›¿ä»£nltkçš„stopwords
STOPWORDS = ENGLISH_STOPWORDS
# å¯¹æ–‡æœ¬è¿›è¡Œæ¸…æ´—
def clean_text(text):
    # å…¨éƒ¨å°å†™
    text = text.lower()
    # ç”¨ç©ºæ ¼æ›¿ä»£ä¸€äº›ç‰¹æ®Šç¬¦å·ï¼Œå¦‚æ ‡ç‚¹
    text = REPLACE_BY_SPACE_RE.sub(' ', text)
    # ç§»é™¤BAD_SYMBOLS_RE
    text = BAD_SYMBOLS_RE.sub('', text)
    # ä»æ–‡æœ¬ä¸­å»æ‰åœç”¨è¯
    text = ' '.join(word for word in text.split() if word not in STOPWORDS)
    return text
# å¯¹descå­—æ®µè¿›è¡Œæ¸…ç†ï¼Œapplyé’ˆå¯¹æŸåˆ—
df['desc_clean'] = df['desc'].apply(clean_text)
#print(df['desc_clean'])

# å»ºæ¨¡
df.set_index('name', inplace = True)
# ä½¿ç”¨TF-IDFæå–æ–‡æœ¬ç‰¹å¾ï¼Œä½¿ç”¨è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨,min_df=0.01ï¼šå¦‚æœæœ‰1000ç¯‡æ–‡æ¡£ï¼Œåªä¿ç•™è‡³å°‘åœ¨10ç¯‡æ–‡æ¡£ä¸­å‡ºç°çš„è¯(1000Ã—1%)
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0.01, stop_words=list(ENGLISH_STOPWORDS))
# é’ˆå¯¹desc_cleanæå–tfidf
tfidf_matrix = tf.fit_transform(df['desc_clean'])
# print('TFIDF feature names:')
# print(tf.get_feature_names_out())
print('length of feature_names_out:')
print(len(tf.get_feature_names_out()))
# print('tfidf_matrix:')
# print(tfidf_matrix)
print('tfidf_matrix shape=')
print(tfidf_matrix.shape)
# è®¡ç®—é…’åº—ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆçº¿æ€§æ ¸å‡½æ•°ï¼‰
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)
# print(f'cosine_similaritiesä¸º\n {cosine_similarities}')
print("conine_similarities.shape=")
print(cosine_similarities.shape)
indices = pd.Series(df.index) #df.indexæ˜¯é…’åº—åç§°

# åŸºäºç›¸ä¼¼åº¦çŸ©é˜µå’ŒæŒ‡å®šçš„é…’åº—nameï¼Œæ¨èTOP10é…’åº—
def recommendations(name, cosine_similarities = cosine_similarities):
    recommended_hotels = []
    # æ‰¾åˆ°æƒ³è¦æŸ¥è¯¢é…’åº—åç§°çš„idx
    idx = indices[indices == name].index[0]
    # print('idx=', idx)
    # å¯¹äºidxé…’åº—çš„ä½™å¼¦ç›¸ä¼¼åº¦å‘é‡æŒ‰ç…§ä»å¤§åˆ°å°è¿›è¡Œæ’åº
    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)
    # å–ç›¸ä¼¼åº¦æœ€å¤§çš„å‰10ä¸ªï¼ˆé™¤äº†è‡ªå·±ä»¥å¤–ï¼‰
    top_10_indexes = list(score_series.iloc[1:11].index)
    # æ”¾åˆ°æ¨èåˆ—è¡¨ä¸­
    for i in top_10_indexes:
        recommended_hotels.append(list(df.index)[i])
    return recommended_hotels
hotel_name='Hilton Seattle Airport & Conference Center'
recommended=recommendations(hotel_name)
print(f"top 10 similar to {hotel_name} are\n")
for i in range(len(recommended)):
    print (f"top{(i+1):02d}        {recommended[i]}")
# print(recommendations('Hilton Seattle Airport & Conference Center'))
# print(recommendations('The Bacon Mansion Bed and Breakfast'))
# #print(result)

```

### 4.1.4 ç»“æœ
```bash
æ•°æ®é›†ä¸­çš„é…’åº—ä¸ªæ•°ï¼š 152
ç¬¬10ä¸ªé…’åº—çš„æè¿°ï¼š
Name: W Seattle
Soak up the vibrant scene in the Living Room Bar and get in the mix with our live music and DJ series before heading to a memorable dinner at TRACE. Offering inspired seasonal fare in an award-winning atmosphere, it's a not-to-be-missed culinary experience in downtown Seattle. Work it all off the next morning at FITÂ®, our state-of-the-art fitness center before wandering out to explore many of the area's nearby attractions, including Pike Place Market, Pioneer Square and the Seattle Art Museum. As always, we've got you covered during your time at W Seattle with our signature Whatever/WheneverÂ® service - your wish is truly our command.
comon_words are 
 [('pike place market', 85), ('seattle tacoma international', 21), ('tacoma international airport', 21), ('free wi fi', 19), ('washington state convention', 17), ('seattle art museum', 16), ('place market seattle', 16), ('state convention center', 15), ('within walking distance', 14), ('high speed internet', 14), ('space needle pike', 12), ('needle pike place', 11), ('south lake union', 11), ('downtown seattle hotel', 10), ('sea tac airport', 10), ('home away home', 9), ('heart downtown seattle', 8), ('link light rail', 8), ('free high speed', 8), ('24 hour fitness', 7)]
length of feature_names_out:
3347
tfidf_matrix shape=
(152, 3347)
conine_similarities.shape=
(152, 152)
top 10 similar to Hilton Seattle Airport & Conference Center are

top01        Embassy Suites by Hilton Seattle Tacoma International Airport
top02        DoubleTree by Hilton Hotel Seattle Airport
top03        Seattle Airport Marriott
top04        Four Points by Sheraton Downtown Seattle Center
top05        Motel 6 Seattle Sea-Tac Airport South
top06        Hampton Inn Seattle/Southcenter
top07        Radisson Hotel Seattle Airport
top08        Knights Inn Tukwila
top09        Hotel Hotel
top10        Home2 Suites by Hilton Seattle Airport
```
![top20-1words.png](top20-1words.png)
![top20-3words.png](top20-3words.png)
## 4.2 ç”¨Word Embeddingä¸ºä¸‰å›½æ¼”ä¹‰æ‰¾ç›¸ä¼¼è¯
### 4.2.1 å‡†å¤‡
- å‡†å¤‡ä¸‰å›½æ¼”ä¹‰çš„txtæ–‡ä»¶
### 4.2.2 æ­¥éª¤
- Step1ï¼Œå…ˆå¯¹æ–‡ä»¶è¿›è¡Œåˆ†è¯ï¼ˆç”¨jiebaåŒ…ï¼‰
- Step2ï¼Œè®¾ç½®æ¨¡å‹å‚æ•°è¿›è¡Œè®­ç»ƒ
-  Step3ï¼Œè®¡ç®—ä¸¤ä¸ªè¯çš„ç›¸ä¼¼åº¦ã€æ‰¾å‡ºä¸€ä¸ªè¯æˆ–å‡ ä¸ªè¯åŠ å‡åçš„æœ€ç›¸è¿‘è¯ã€‚
### 4.2.3 ç¤ºä¾‹ä»£ç 
- word_seg.py
```python
# -*-coding: utf-8 -*-
# å¯¹txtæ–‡ä»¶è¿›è¡Œä¸­æ–‡åˆ†è¯
import jieba
import os
from utils import files_processing

# æºæ–‡ä»¶æ‰€åœ¨ç›®å½•
source_folder = './three_kingdoms/source'
segment_folder = './three_kingdoms/segment'
# å­—è¯åˆ†å‰²ï¼Œå¯¹æ•´ä¸ªæ–‡ä»¶å†…å®¹è¿›è¡Œå­—è¯åˆ†å‰²
def segment_lines(file_list,segment_out_dir,stopwords=[]):
    for i,file in enumerate(file_list):
        segment_out_name=os.path.join(segment_out_dir,'segment_{}.txt'.format(i))
        with open(file, 'rb') as f:
            document = f.read()
            document_cut = jieba.cut(document)
            sentence_segment=[]
            for word in document_cut:
                if word not in stopwords:
                    sentence_segment.append(word)
            result = ' '.join(sentence_segment)
            result = result.encode('utf-8')
            with open(segment_out_name, 'wb') as f2:
                f2.write(result)

# å¯¹sourceä¸­çš„txtæ–‡ä»¶è¿›è¡Œåˆ†è¯ï¼Œè¾“å‡ºåˆ°segmentç›®å½•ä¸­
file_list=files_processing.get_files_list(source_folder, postfix='*.txt')
segment_lines(file_list, segment_folder)
```

- word_similarity_three_kingdoms.py

```python
# -*-coding: utf-8 -*-
# å…ˆè¿è¡Œ word_segè¿›è¡Œä¸­æ–‡åˆ†è¯ï¼Œç„¶åå†è¿›è¡Œword_similarityè®¡ç®—
# å°†Wordè½¬æ¢æˆVecï¼Œç„¶åè®¡ç®—ç›¸ä¼¼åº¦ 
from gensim.models import word2vec
import multiprocessing

# å¦‚æœç›®å½•ä¸­æœ‰å¤šä¸ªæ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨PathLineSentences
segment_folder = './three_kingdoms/segment'
# åˆ‡åˆ†ä¹‹åçš„å¥å­åˆé›†
sentences = word2vec.PathLineSentences(segment_folder)
#=============== è®¾ç½®æ¨¡å‹å‚æ•°ï¼Œè¿›è¡Œè®­ç»ƒ
model = word2vec.Word2Vec(sentences, vector_size=100, window=3, min_count=1)
model.save('./three_kingdoms/model/word2Vec.model')
print(model.wv.similarity('æ›¹æ“', 'åˆ˜å¤‡'))
print(model.wv.similarity('æ›¹æ“', 'å¼ é£'))
query_name = "æ›¹æ“"
similar_words = model.wv.most_similar(query_name, topn=5)
print(f"ä¸{query_name}æœ€ç›¸ä¼¼çš„5ä¸ªè¯:")
for word, similarity in similar_words:
    print(f"{word}: {similarity:.3f}")
print("æ›¹æ“+åˆ˜å¤‡-å¼ é£=?")
similar_words = model.wv.most_similar(positive=['æ›¹æ“', 'åˆ˜å¤‡'], negative=['å¼ é£'], topn=5)
for word, similarity in similar_words:
    print(f"{word}: {similarity:.3f}")
#================= è®¾ç½®æ¨¡å‹å‚æ•°ï¼Œè¿›è¡Œè®­ç»ƒ
model2 = word2vec.Word2Vec(sentences, vector_size=128, window=5, min_count=5, workers=multiprocessing.cpu_count())
# ä¿å­˜æ¨¡å‹
model2.save('./three_kingdoms/model/word2Vec.model2')
print(model2.wv.similarity('æ›¹æ“', 'åˆ˜å¤‡'))
print(model2.wv.similarity('æ›¹æ“', 'å¼ é£'))

query_name = "æ›¹æ“"
similar_words = model2.wv.most_similar(query_name, topn=5)
print(f"ä¸{query_name}æœ€ç›¸ä¼¼çš„5ä¸ªè¯:")
for word, similarity in similar_words:
    print(f"{word}: {similarity:.3f}")
print("æ›¹æ“+åˆ˜å¤‡-å¼ é£=?")
similar_words = model2.wv.most_similar(positive=['æ›¹æ“', 'åˆ˜å¤‡'], negative=['å¼ é£'], topn=5)
for word, similarity in similar_words:
    print(f"{word}: {similarity:.3f}")
```

### 4.2.4 ç»“æœ
```bash
0.9805809
0.9755627
ä¸æ›¹æ“æœ€ç›¸ä¼¼çš„5ä¸ªè¯:
å­™æƒ: 0.988
å¸é©¬æ‡¿: 0.987
å·²: 0.986
å­”æ˜: 0.986
æ²®æˆ: 0.986
æ›¹æ“+åˆ˜å¤‡-å¼ é£=?
æŸ: 0.992
ä¸ç›¸: 0.991
è‡£: 0.990
æ—¢: 0.989
å¤§å«: 0.989
0.82772493
0.7733702
ä¸æ›¹æ“æœ€ç›¸ä¼¼çš„5ä¸ªè¯:
å­™æƒ: 0.959
å–: 0.953
å›æŠ¥: 0.953
å¤§å«: 0.952
å…¶äº‹: 0.950
æ›¹æ“+åˆ˜å¤‡-å¼ é£=?
è‡£: 0.976
ä½•ä¸º: 0.964
ä¸ç›¸: 0.962
æœ•: 0.960
ä¸»å…¬: 0.959

Process finished with exit code 0
```