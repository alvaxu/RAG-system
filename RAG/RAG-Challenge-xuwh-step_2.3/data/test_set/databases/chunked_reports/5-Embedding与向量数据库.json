{
  "metainfo": {
    "sha1": "Aitraining",
    "sha1_name": "Aitraining",
    "pages_amount": 39,
    "text_blocks_amount": 170,
    "tables_amount": 2,
    "pictures_amount": 0,
    "equations_amount": 0,
    "footnotes_amount": 0,
    "company_name": "AI应用开发",
    "file_name": "5-Embedding与向量数据库"
  },
  "content": {
    "chunks": [
      {
        "page": 1,
        "length_tokens": 5,
        "text": "Embedding与向量数据库",
        "id": 0,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 4,
        "text": "今天的学习目标",
        "id": 1,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 3,
        "text": "什么是Embedding",
        "id": 2,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 45,
        "text": "CASE：基于内容的推荐  \n什么是N-Gram  \n余弦相似度计算  \n为酒店建立内容推荐系统  \nWord Embedding  \n什么是Embedding  \nWord2Vec进行词向量训练",
        "id": 3,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 3,
        "text": "向量数据库",
        "id": 4,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 25,
        "text": "·什么是向量数据库FAISS，Milvus,Pinecone的特点向量数据库与传统数据库的对比",
        "id": 5,
        "type": "text"
      },
      {
        "page": 3,
        "length_tokens": 3,
        "text": "什么是Embedding",
        "id": 6,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 5,
        "text": "基于内容的推荐",
        "id": 7,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 6,
        "text": "基于内容的推荐：",
        "id": 8,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 19,
        "text": "·依赖性低，不需要动态的用户行为，只要有内容就可以进行推荐",
        "id": 9,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 6,
        "text": "系统不同阶段都可以应用",
        "id": 10,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 44,
        "text": "系统冷启动，内容是任何系统天生的属性，可以从中挖掘到特征，实现推荐系统的冷启动。一个复杂的推荐系统是从基于内容的推荐成长起来的",
        "id": 11,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 27,
        "text": "商品冷启动，不论什么阶段，总会有新的物品加入，这时只要有内容信息，就可以帮它进行推荐",
        "id": 12,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 12,
        "text": "了解Embedding可以从了解物体的特征表达开始",
        "id": 13,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 5,
        "text": "基于内容的推荐",
        "id": 14,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 0,
        "text": "",
        "id": 15,
        "type": "image"
      },
      {
        "page": 5,
        "length_tokens": 15,
        "text": "·物品表示 Item Representation:为每个item抽取出features",
        "id": 16,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 8,
        "text": "·特征学习ProfileLearning: ",
        "id": 17,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 27,
        "text": "利用一个用户过去喜欢（不喜欢）的item的特征数据，来学习该用户的喜好特征（profile）；",
        "id": 18,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 8,
        "text": "·生成推荐列表Recommendation Generation: ",
        "id": 19,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 17,
        "text": "通过用户profile与候选item的特征，推荐相关性最大的item。",
        "id": 20,
        "type": "text"
      },
      {
        "page": 6,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 21,
        "type": "text"
      },
      {
        "page": 6,
        "length_tokens": 7,
        "text": "西雅图酒店数据集：",
        "id": 22,
        "type": "text"
      },
      {
        "page": 6,
        "length_tokens": 89,
        "text": "·下载地址: https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Seattle_Hotels.csv  \n·字段:name,address,desc  \n·基于用户选择的酒店，推荐相似度高的Top10个其他酒店  \n·方法：计算当前酒店特征向量与整个酒店特征矩阵的余弦相似度，取相似度最大的Top-k个",
        "id": 23,
        "type": "text"
      },
      {
        "page": 6,
        "length_tokens": 0,
        "text": "",
        "id": 24,
        "type": "table"
      },
      {
        "page": 7,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 25,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 7,
        "text": "余弦相似度：",
        "id": 26,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 25,
        "text": "·通过测量两个向量的夹角的余弦值来度量它们之间的相似性",
        "id": 27,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 64,
        "text": "·判断两个向量大致方向是否相同，方向相同时，余弦相似度为1；两个向量夹角为90°时，余弦相似度的值为0，方向完全相反时，余弦相似度的值为-1。",
        "id": 28,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 19,
        "text": "·两个向量之间夹角的余弦值为[-1,1] ",
        "id": 29,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 32,
        "text": "给定属性向量A和B，A和B之间的夹角0余弦值可以通过点积和向量长度计算得出",
        "id": 30,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 0,
        "text": "",
        "id": 31,
        "type": "image"
      },
      {
        "page": 8,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 32,
        "type": "text"
      },
      {
        "page": 8,
        "length_tokens": 12,
        "text": "计算A和B的余弦相似度：",
        "id": 33,
        "type": "text"
      },
      {
        "page": 8,
        "length_tokens": 162,
        "text": "·句子A：这个程序代码太乱，那个代码规范  \n·句子B：这个程序代码不规范，那个更规范  \n·Step1，分词  \n句子A：这个/程序/代码/太乱，那个/代码/规范  \n句子B：这个/程序/代码/不/规范，那个/更/规范  \n·Step2，列出所有的词  \n这个，程序，代码，太乱，那个，规范，不，更  \n·Step3，计算词频  \n句子A：这个1，程序1，代码2，太乱1，那个1，规范1，不0，更0  \n句子B：这个1，程序1，代码1，太乱0，那个1，规范2，不1，更1",
        "id": 34,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 35,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 12,
        "text": "计算A和B的余弦相似度：",
        "id": 36,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 74,
        "text": "Step4，计算词频向量的余弦相似度句子A: (1，1，2，1，1，1，0，0)句子B: $( 1 , \\ 1 , \\ 1 , \\ 0 , \\ 1 , \\ 2 , \\ 1 , \\ 1 )$ ",
        "id": 37,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 174,
        "text": "cos( $g ) = { \\frac { 1 \\times 1 + 1 \\times 1 + 2 \\times 1 + 1 \\times 0 + 1 \\times 1 + 1 \\times 2 + 0 \\times 1 + 0 \\times 1 } { { \\sqrt { 1 ^ { 2 } + 1 ^ { 2 } + 2 ^ { 2 } + 1 ^ { 2 } + 1 ^ { 2 } + 1 ^ { 2 } + 0 ^ { 2 } + 0 ^ { 2 } } } \\times { \\sqrt { 1 ^ { 2 } + 1 ^ { 2 } + 1 ^ { 2 } + 0 ^ { 2 } +",
        "id": 38,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 74,
        "text": " { 1 ^ { 2 } + 1 ^ { 2 } + 1 ^ { 2 } + 0 ^ { 2 } + 1 ^ { 2 } + 2 ^ { 2 } } } } }$ 2+1²+1² $= 0 . 7 3 8$ ",
        "id": 39,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 17,
        "text": "结果接近1，说明句子A与句子B是相似的",
        "id": 40,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 41,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 11,
        "text": "什么是N-Gram（N元语法）：",
        "id": 42,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 94,
        "text": "·基于一个假设：第n个词出现与前n-1个词相关，而与其他任何词不相关$N = 1$ 时为unigram， $N = 2$ 为bigram， $N = 3$ 为trigram·N-Gram指的是给定一段文本，其中的N个item的序列比如文本：ABCDE，对应的Bi-Gram为AB,BC,CD,DE",
        "id": 43,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 68,
        "text": "·当一阶特征不够用时，可以用N-Gram做为新的特征。比如在处理文本特征时，一个关键词是一个特征，但有些情况不够用，需要提取更多的特征，采用N-Gram $\\Rightarrow$ 可以理解是相邻两个关键词的特征组合",
        "id": 44,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 18,
        "text": "如何了解事物的特征表达？N-Gram就是最基本的一种方式",
        "id": 45,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 46,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 115,
        "text": "plt.rcParams['font.sans-serif'] $\\mathbf { \\tau } = \\mathbf { \\tau }$ ['SimHei']#用来正常显示中文标签  \ndf $\\mathbf { \\tau } = \\mathbf { \\tau }$ pd.read_csv('Seattle_Hotels.csv',encoding $\\ c =$ \"latin-1\")  \n# 得到酒店描述中n-gram特征中的TopK个  \ndef get_top_n_words(corpus, $\\mathsf { n } = 1$ ， $\\mathrel { \\mathop : } =",
        "id": 47,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 25,
        "text": "s, $\\mathsf { n } = 1$ ， $\\mathrel { \\mathop : } =$ None):",
        "id": 48,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 0,
        "text": "",
        "id": 49,
        "type": "image"
      },
      {
        "page": 11,
        "length_tokens": 97,
        "text": "#统计ngram词频矩阵 vec $\\mathbf { \\tau } = \\mathbf { \\tau }$ CountVectorizer(ngram_range $\\ c =$ (n,n), stop_words $\\ c =$ english').fit(corpus) bag_of_words $\\mathbf { \\tau } = \\mathbf { \\tau }$ vec.transform(corpus) sum_words $\\mathbf { \\tau } = \\mathbf { \\tau }$ bag_of_words.sum(axis ${ \\boldsymbol { \\",
        "id": 50,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 103,
        "text": "{ \\tau }$ bag_of_words.sum(axis ${ \\boldsymbol { \\mathbf { \\mathit { \\sigma } } } } = 0$ ） words_freq $\\mathbf { \\tau } = \\mathbf { \\tau }$ [(word,sum_words[0, idx]) for word, idx in vec.vocabulary_.items(] #按照词频从大到小排序 words_freq $\\mathbf { \\tau } = \\mathbf { \\tau }$ sorted(words_freq, key $\\mathbf ",
        "id": 51,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 102,
        "text": "\\mathbf { \\tau }$ sorted(words_freq, key $\\mathbf { \\tau } = \\mathbf { \\tau }$ lambda x: x[1],reverse $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ True) return words_freq[:k] common_words $\\mathbf { \\tau } = \\mathbf { \\tau }$ get_top_n_words(df['desc'],1, 20) df1 $\\mathbf { \\tau } = \\mathbf { \\tau }$ p",
        "id": 52,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 92,
        "text": "1, 20) df1 $\\mathbf { \\tau } = \\mathbf { \\tau }$ pd.DataFrame(common_words, columns $\\mathbf { \\tau } = \\mathbf { \\tau }$ ['desc','count']) df1.groupby('desc').sum()['count'].sort_values().plot(kind $\\ c =$ 'barh', title $\\ c =$ 去掉停用词后，酒店描述中的Top20单词\") plt.show0 ",
        "id": 53,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 54,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 5,
        "text": "# Bi-Gram ",
        "id": 55,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 20,
        "text": "common words  = get top n words(df['des c'], 2, 20) ",
        "id": 56,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 0,
        "text": "",
        "id": 57,
        "type": "image"
      },
      {
        "page": 12,
        "length_tokens": 5,
        "text": "# Tri-Gram ",
        "id": 58,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 19,
        "text": "common words = get top n words(df['des c'], 3, 20) ",
        "id": 59,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 60,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 6,
        "text": "def clean_text(text): ",
        "id": 61,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 141,
        "text": "(151. 13732) 0.02288547901274695   \n(151, 21971) 0.03682289049851129   \n(151. 19896) 0.017052721912118395   \n(151, 22212) 0.016660175897670552   \n(151, 13482) 0.024148380094538605   \n(151, 12132) 0.024170114517665667   \n(151, 11079) 0.07169842086767955   \n(151. 11324) 0.013227229761895392   \n(152, 2",
        "id": 62,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 27,
        "text": "55   \n(151. 11324) 0.013227229761895392   \n(152, 26879) ",
        "id": 63,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 109,
        "text": "#全部小写 text $\\mathbf { \\tau } = \\mathbf { \\tau }$ text.lower() return text df['desc_clean'] $\\mathbf { \\tau } = \\mathbf { \\tau }$ df['desc'].apply(clean_text) > # 使用TF-IDF提取文本特征 tf $\\mathbf { \\tau } = \\mathbf { \\tau }$ TfidfVectorizer(analyzer $\\ c =$ word',ngram_range $\\ c =$ (1,3),min_df ${ } = 0$ ",
        "id": 64,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 103,
        "text": " word',ngram_range $\\ c =$ (1,3),min_df ${ } = 0$ ,stop_words $\\ c =$ en tfidf_matrix $\\mathbf { \\tau } = \\mathbf { \\tau }$ tf.fit_transform(df['desc_clean']) print(tfidf_matrix) print(tfidf_matrix.shape) # 计算酒店之间的余弦相似度 (线性核函数) cosine_similarities $\\mathbf { \\tau } = \\mathbf { \\tau }$ linear_kernel(",
        "id": 65,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 32,
        "text": "mathbf { \\tau } = \\mathbf { \\tau }$ linear_kernel(tfidf_matrix,tfidf_matrix) print(cosine_similarities) ",
        "id": 66,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 167,
        "text": "0.01161917 0.02656894 0.01184587 0.00244782 0.005835891 [0.01161917 1 0.015586 0.01625083 0.00313105 0.00797999 [0.02656894 0.015586 1. 0.02071479 0.00748781 0.01028037] [0.01184587 0.01625083 0.02071479 0.01066904 0.0079114 [0.00244782 0.00313105 0.00748781 0.01066904 1. 0.00257955] [0.00583589 0.0",
        "id": 67,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 59,
        "text": "00748781 0.01066904 1. 0.00257955] [0.00583589 0.00797999 0.01028037 0.0079114 0.00257955 1. (152, 152) ",
        "id": 68,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 24,
        "text": "152家酒店，之间的相似度矩阵(1-Gram,2-Gram,3-Gram)",
        "id": 69,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 70,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 102,
        "text": "#基于相似度矩阵和指定的酒店name，推荐TOP10酒店   \ndef recommendations(name,cosine_similarities $\\mathbf { \\tau } = \\mathbf { \\tau }$ cosine_similarities): recommended_hotels = [] #找到想要查询酒店名称的idx idx $\\mathbf { \\tau } = \\mathbf { \\tau }$ indices[indices $\\scriptstyle = =$ name].index[0] print('idx $\\ c =$ ,idx) #对于idx",
        "id": 71,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 116,
        "text": " =$ name].index[0] print('idx $\\ c =$ ,idx) #对于idx酒店的余弦相似度向量按照从大到小进行排序 score_series $\\mathbf { \\tau } = \\mathbf { \\tau }$ pd.Series(cosine_similarities[idx]).sort_values(ascending $\\mathbf { \\tau } = \\mathbf { \\tau }$ False) # 取相似度最大的前10个 (除了自己以外) top_10_indexes $\\mathbf { \\tau } = \\mathbf { \\tau }$",
        "id": 72,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 80,
        "text": "p_10_indexes $\\mathbf { \\tau } = \\mathbf { \\tau }$ list(score_series.iloc[1:11].index) #放到推荐列表中 for iin top_10_indexes: recommended_hotels.append(list(df.index)[i) return recommended_hotels   \nprint(recommendations('Hilton Seattle Airport & Conference Center'))   \nprint(recommendations('The Bacon Ma",
        "id": 73,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 17,
        "text": "e Center'))   \nprint(recommendations('The Bacon Mansion Bed and Breakfast')) ",
        "id": 74,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 5,
        "text": "idx= 49 ",
        "id": 75,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 71,
        "text": "[\"Enbassy Suites by Hilton Seattle Tacoma International Airport'   \n'DoubleTree by Hilton Hotel Seattle Airport', 'Seattle Airport   \nMarriott', 'Motel 6 Seattle Sea-Tac Airport South'， \"Econo Lodge   \nSeaTac Airport North'， Four Points by Sheraton Downtown Seattle   \nCenter', 'Knights Inn Tukwila',",
        "id": 76,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 82,
        "text": "owntown Seattle   \nCenter', 'Knights Inn Tukwila', “Econo Lodge Renton-Bellevue'   \n'Hampton Inn Seattle/Southcenter\", “Radisson Hotel Seattle   \nAirport\"]   \nidx= 116   \n['11th Avenue Inn Bed and Breakfast', 'Shafer Baillie Mansion Bed   \n& Breakfast'， ‘Chittenden House Bed and Breakfast\"， ‘Gasligh",
        "id": 77,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 62,
        "text": "t'， ‘Chittenden House Bed and Breakfast\"， ‘Gaslight   \nInn'， 'Bed and Breakfast Inn Seattle'， 'Silver Cloud Hotel   \nSeattle Broadway', ‘Hyatt House Seattle\", ‘Mozart Guest House'   \n\"Quality Inn & Suites Seattle Center', 'MarQueen Hotel\"] ",
        "id": 78,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 14,
        "text": "查找和指定酒店相似度最高的Top10家酒店",
        "id": 79,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 80,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 5,
        "text": "CountVectorizer: ",
        "id": 81,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 116,
        "text": "将文本中的词语转换为词频矩阵  \nfittransform：计算各个词语出现的次数  \ngetfeaturenames：可获得所有文本的关键词  \ntoarray(：查看词频矩阵的结果。  \nvec $\\mathbf { \\tau } = \\mathbf { \\tau }$ CountVectorizer(ngram_range=(n,n), stop_words $\\mathbf { \\Phi } = \\mathbf { \\Phi } ^ { \\prime }$ english').fit(corpus)  \nbag_of_words $\\mathbf { \\tau } = \\mathbf ",
        "id": 82,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 56,
        "text": "orpus)  \nbag_of_words $\\mathbf { \\tau } = \\mathbf { \\tau }$ vec.transform(corpus)  \nprint('feature names:')  \nprint(vec.get_feature_names())  \nprint('bag of words:\")  \nprint(bag_of_words.toarray())",
        "id": 83,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 0,
        "text": "",
        "id": 84,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 4,
        "text": "feature names: ",
        "id": 85,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 87,
        "text": "['00 night plus', '000 crystals marble', ‘000 sq ft', '000 square feet', ‘000 redesigned venues', '10 unique guestrooms' , '100 meters away', ‘100 non smokin '10best georgetown inn'， '11 km emerald'， \"11 km seattle'， “11 miles downtown' '120 luxury guestrooms', '120 sqft 11sqm'， '1200 people range°，",
        "id": 86,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 94,
        "text": "uestrooms', '120 sqft 11sqm'， '1200 people range°， \"12pm noon dai: minute walk'， ‘15 minutes drive'， “15 minutes water'， ‘15 people doesn'， ‘150 property', '178 guest rooms'， '18 acre retreat'， '18 acres natural'， '188th sti landmark building', “1906 located street'， '1909 cecil bacon', '1910 landma",
        "id": 87,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 40,
        "text": " located street'， '1909 cecil bacon', '1910 landmark executive hotel', '1928 inn tucked', '1930s art deco'， \"1960s renamed mason' ",
        "id": 88,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 59,
        "text": "bag of words: 0 0 0 0 0 0 0 0 0] 00 0 0 0 0] 0 00 0 0 0] [0 0 0 0 0 0]] ",
        "id": 89,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 90,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 5,
        "text": "TF-IDF: ",
        "id": 91,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 106,
        "text": "单词次数TF  \n·TF：Term Frequency，词频 文档中总单词数  \n一个单词的重要性和它在文档中出现的次数呈正比。  \n· IDF： Inverse Document Frequency，逆向文档频率  \n一个单词在文档中的区分度。这个单词出现的文档数越少，区分文档总数  \n度越大，IDF越大 IDF =log单词出现的文档数+1",
        "id": 92,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 93,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 7,
        "text": "TfidfVectorizer: ",
        "id": 94,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 111,
        "text": "#使用TF-IDF提取文本特征   \ntf $\\mathbf { \\tau } = \\mathbf { \\tau }$ TfidfVectorizer(analyzer $= ^ { \\mathsf { 1 } }$ word',ngram_range $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ (1,3),min_df=0,   \nstop_words $\\mathbf { \\lambda } = \\mathbf { \\vec { \\lambda } }$ english')   \ntfidf_matrix $\\mathbf { \\tau } = \\m",
        "id": 95,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 43,
        "text": "$ english')   \ntfidf_matrix $\\mathbf { \\tau } = \\mathbf { \\tau }$ tf.fit_transform(df['desc_clean'])   \nprint(tfidf_matrix)   \nprint(tfidf_matrix.shape) ",
        "id": 96,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 140,
        "text": "将文档集合转化为tf-idf特征值的矩阵构造函数analyzer：word或者char，即定义特征为词（word）或n-gram字符ngram_range:参数为二元组(min_n,max_n)，即要提取的n-gram的下限和上限范围max_df：最大词频，数值为小数[0.0,1.0],或者是整数，默认为1.0min_df：最小词频，数值为小数[0.0,1.0],或者是整数，默认为1.0stop_words：停用词，数据类型为列表",
        "id": 97,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 143,
        "text": "(151. 13732) 0.02288547901274695   \n151. 21971) 0.036827:0010851129   \n(151. 19896) 0.017052721912118395   \n(151, 22212) 0.016660175897670552   \n(151, 13482) 0.0241 8380094538605   \n(151, 12132) 0.024170114517665667   \n(151, 11079) 0.07169842086767955   \n(151, 11324) 0.013227220761895392   \n(152, 26",
        "id": 98,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 27,
        "text": "5   \n(151, 11324) 0.013227220761895392   \n(152, 26879) ",
        "id": 99,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 3,
        "text": "功能函数：",
        "id": 100,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 37,
        "text": "·fit_transform：进行tf-idf训练，学习到一个字典，并返回Document-term的矩阵，也就是词典中的词在该文档中出现的频次",
        "id": 101,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 6,
        "text": "为酒店建立内容推荐系统",
        "id": 102,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 6,
        "text": "基于内容的推荐：",
        "id": 103,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 93,
        "text": "·Step1，对酒店描述（Desc）进行特征提取·N-Gram，提取N个连续字的集合，作为特征·TF-IDF，按照(min_df,max_df)提取关键词，并生成TFIDF矩阵  \n·Step2，计算酒店之间的相似度矩阵·余弦相似度  \n·Step3，对于指定的酒店，选择相似度最大的Top-K个酒店进行输出",
        "id": 104,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 37,
        "text": "Thinking :N-Gram $\\cdot$ TF-IDF的特征表达会让特征矩阵非常系数，计算量大，有没有更适合的方式？",
        "id": 105,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 4,
        "text": "Word Embedding ",
        "id": 106,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 4,
        "text": "什么是Embedding:",
        "id": 107,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 93,
        "text": "·一种降维方式，将不同特征转换为维度相同的向量  \n离线变量转换成one-hot $\\Rightarrow$ 维度非常高，可以将它转换为固定size的embedding向量  \n·任何物体，都可以将它转换成为向量的形式，从Trait#1到#N  \n·向量之间，可以使用相似度进行计算  \n·当我们进行推荐的时候，可以选择相似度最大的",
        "id": 108,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 0,
        "text": "",
        "id": 109,
        "type": "image"
      },
      {
        "page": 19,
        "length_tokens": 0,
        "text": "",
        "id": 110,
        "type": "image"
      },
      {
        "page": 20,
        "length_tokens": 4,
        "text": "Word Embedding ",
        "id": 111,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 6,
        "text": "将Word进行Embedding: ",
        "id": 112,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 25,
        "text": "·如果我们将King这个单词，通过维基百科的学习，进行GloVe向量化，可以表示成",
        "id": 113,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 195,
        "text": "[ 0.50451，0.68607，-0.59517，-0.022801，0.60046，-0.13498，-0.08813，0.47377，-0.61798，-0.31012  \n-0.076666，1.493，-0.034189，-0.98173，0.68229，0.81722，-0.51874，-0.31503，-0.55809，0.66421，0.1961  \n，-0.13495，-0.11476，-0.30344，0.41177，-2.223，-1.0756，-1.0783，-0.34354，0.33505，1.9927  \n-0.04234，-0.64319，0.71125，0.4",
        "id": 114,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 118,
        "text": "354，0.33505，1.9927  \n-0.04234，-0.64319，0.71125，0.49159，0.16754，0.34344，-0.25663，-0.8523，0.1661，0.40102，1.1685  \n-1.0137 ， -0.21585， -0.15155，0.78321， -0.91241， -1.6106， -0.64426， -0.51042]",
        "id": 115,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 21,
        "text": "这50维度的权重大小在[-2,2]，按照颜色的方式来表示",
        "id": 116,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 0,
        "text": "",
        "id": 117,
        "type": "image"
      },
      {
        "page": 21,
        "length_tokens": 4,
        "text": "Word Embedding ",
        "id": 118,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 6,
        "text": "将Word进行Embedding: ",
        "id": 119,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 41,
        "text": "·我们将King与其他单词进行比较，可以看到Man和Woman更相近同样有了向量，我们还可以进行运算king-man+woman与queen的相似度最高",
        "id": 120,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 18,
        "text": "model.most_similar(positive=[\"king\",\"woman\"]，negative=[\"man\"]) ",
        "id": 121,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 0,
        "text": "",
        "id": 122,
        "type": "image"
      },
      {
        "page": 22,
        "length_tokens": 4,
        "text": "Word Embedding ",
        "id": 123,
        "type": "text"
      },
      {
        "page": 22,
        "length_tokens": 119,
        "text": "Softmax ClassifierWord2Vec: Hidden Layer Probabilltythat the word atInputVector LinearNeurons £ ranitimiyhabandn通过Embedding，把原先词所在空间映射到一个新的空间中去，使得语义上相似的单词在该空间内距离相近。 £ ∑ abilityWord Embedding $\\Rightarrow$ 学习隐藏层的权重矩阵 ∑A ‘1’ in the £输入测是one-hot编码 correspondi隐藏层的神经元数量为hidden_size(Embedding Size)回 £对于输入",
        "id": 124,
        "type": "text"
      },
      {
        "page": 22,
        "length_tokens": 77,
        "text": "spondi隐藏层的神经元数量为hidden_size(Embedding Size)回 £对于输入层和隐藏层之间的权值矩阵W，大小为 10.,00300 neurons £ one\"[vocab_size,hidden_size]10,000neurons输出层为[vocab_size]大小的向量，每一个值代表着输出一个词的概率",
        "id": 125,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 4,
        "text": "Word Embedding ",
        "id": 126,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 7,
        "text": "对于输入的one-hot编码：",
        "id": 127,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 84,
        "text": "在矩阵相乘的时候，选取出矩阵中的某一行，而这一  \n行就是输入词语的word2vec表示  \n隐含层的节点个数 $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ 词向量的维数  \n隐层的输出是每个输入单词的Word Embedding  \nword2vec，实际上就是一个查找表",
        "id": 128,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 0,
        "text": "",
        "id": 129,
        "type": "image"
      },
      {
        "page": 24,
        "length_tokens": 4,
        "text": "Word Embedding ",
        "id": 130,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 8,
        "text": "Word2Vec的两种模式：",
        "id": 131,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 12,
        "text": "·Skip-Gram，给定inputword预测上下文",
        "id": 132,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 0,
        "text": "",
        "id": 133,
        "type": "image"
      },
      {
        "page": 24,
        "length_tokens": 18,
        "text": ".Bereftoflifeherestsinpeace!Ifyouhadn'tnailedhir ",
        "id": 134,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 20,
        "text": "CBOW，给定上下文，预测inputword（与 Skip-Gram相反) ",
        "id": 135,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 0,
        "text": "",
        "id": 136,
        "type": "image"
      },
      {
        "page": 24,
        "length_tokens": 19,
        "text": "Bereft of life herestsin peace! Ifyouhadn'tnailed hi=.. ",
        "id": 137,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 0,
        "text": "",
        "id": 138,
        "type": "image"
      },
      {
        "page": 25,
        "length_tokens": 4,
        "text": "Word2Vec工具",
        "id": 139,
        "type": "text"
      },
      {
        "page": 25,
        "length_tokens": 4,
        "text": "Gensim工具",
        "id": 140,
        "type": "text"
      },
      {
        "page": 25,
        "length_tokens": 68,
        "text": "pip install gensim  \n开源的Python工具包  \n可以从非结构化文本中，无监督地学习到隐层的主题向  \n量表达  \n每一个向量变换的操作都对应着一个主题模型  \n支持TF-IDF，LDA，LSA，Word2vec等多种主题模型算法",
        "id": 141,
        "type": "text"
      },
      {
        "page": 25,
        "length_tokens": 3,
        "text": "使用方法：",
        "id": 142,
        "type": "text"
      },
      {
        "page": 25,
        "length_tokens": 73,
        "text": "：建立词向量模型：word2vec.Word2Vec(sentences)window,句子中当前单词和被预测单词的最大距离mincount,需要训练词语的最小出现次数，默认为5size,向量维度，默认为100worker训练使用的线程数，默认为1即不使用多线程",
        "id": 143,
        "type": "text"
      },
      {
        "page": 25,
        "length_tokens": 14,
        "text": "模型保存 model.save(fname) 模型加载 model.load(fname) ",
        "id": 144,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 4,
        "text": "Word2Vec工具",
        "id": 145,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 5,
        "text": "数据集：西游记",
        "id": 146,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 32,
        "text": "journey_to_the_west.txt  \n计算小说中的人物相似度，比如孙悟空与猪八戒，孙悟空与  \n孙行者",
        "id": 147,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 3,
        "text": "方案步骤:",
        "id": 148,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 55,
        "text": "Step1，使用分词工具进行分词，比如NLTK,JIEBA：Step2，将训练语料转化成一个sentence的迭代器：Step3，使用word2vec进行训练  \nStep4，计算两个单词的相似度",
        "id": 149,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 0,
        "text": "",
        "id": 150,
        "type": "table"
      },
      {
        "page": 27,
        "length_tokens": 4,
        "text": "Word2Vec工具",
        "id": 151,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 14,
        "text": "#字词分割，对整个文件内容进行字词分割",
        "id": 152,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 87,
        "text": "def segment_lines(file_list,segment_out_dir,stopwords=[]):for i,file in enumerate(file_list):segment_out_name $\\scriptstyle 1 = 0 s$ .path.join(segment_out_dir,'segment_0.txt'.format(i)with open(file,'rb') as f:document $\\mathbf { \\tau } = \\mathbf { \\tau }$ f.read()document_cut $\\mathbf { \\tau } = \\",
        "id": 153,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 85,
        "text": "\\tau }$ f.read()document_cut $\\mathbf { \\tau } = \\mathbf { \\tau }$ jieba.cut(document)sentence_segment=[]for word in document_cut:if word not in stopwords:sentence_segment.append(word)result $\\mathbf { \\tau } = \\mathbf { \\tau }$ ''join(sentence_segment)result $\\mathbf { \\tau } = \\mathbf { \\tau }$ re",
        "id": 154,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 97,
        "text": "nt)result $\\mathbf { \\tau } = \\mathbf { \\tau }$ result.encode('utf-8)with open(segment_out_name,'wb') as f2:f2.write(result)  \n# 对source中的txt文件进行分词，输出到segment目录中  \nfile_list $\\ c =$ files_processing.get_files_list('./source',postfix $\\mathbf { \\tau } = \\mathbf { \\dot { \\tau } }$ \\*.txt')  \nsegment_l",
        "id": 155,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 199,
        "text": " = \\mathbf { \\dot { \\tau } }$ \\*.txt')  \nsegment_lines(file_list,'./segment')  \n西游记（吴承思  \n西游记  \n作者：吴承思西游记却 以 丰富 瑰奇 的 想象 描写 了 师徒 四众的精怪生动地表现了无情的山川的险阻，并以  \n第第第第第第第第附第第第第第第第第第第第第第第 001 回 灵根育孕 源流 出 心性他持大道生002 I回 悟彻 菩提 真 妙理 断 魔 归本 合元神003 回 四海 千山 皆拱伏 九山 十类 尽 除名004 回 官封弼 马心 何足 注 齐 天意 未宁005 I回 乱 蟠桃 大圣 偷丹 反 天",
        "id": 156,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 239,
        "text": "尽 除名004 回 官封弼 马心 何足 注 齐 天意 未宁005 I回 乱 蟠桃 大圣 偷丹 反 天宫 诣神 捉 怪006 回 观音 赴 会问 原田 小至  薛 大至007 I回回 八卦 炉中 逃 大圣 五行 山下 定心 猿008 我佛造经传极乐 观音 奉旨 上 长安录 陈光正 赴任 逢文 江流 僧 复九 报士009 回回 袁守迪 过算 无 曲 老 龙王 拙计 犯 天条010 二 将军 宫门 镇鬼 唐大宗 地府 还望011 回 还 受生 唐王 逗香果 度孤魂萧璃正空门012 回 玄里 秉迪里 大= 观音 显象 化 金蝉013 [回回 陷 虎穴 金星 解厄 双此 伯钦 留圖014 心猿 归正 六",
        "id": 157,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 211,
        "text": "= 观音 显象 化 金蝉013 [回回 陷 虎穴 金星 解厄 双此 伯钦 留圖014 心猿 归正 六吨 无踪015 回 蛇 盘山 诸神 暗佑 应 涧 意马 收组016 I回 观音院 僧谋 宝贝 呈日山 怪古017 回 孙行者 大闹 黑凡山 现世音 收代 琵 景 怪018 回 观音院 唐僧脱 难 高老王 行者 薛登019 回 云栈 洞 悟空收 八戒 浮屠 山 玄奘 受心经020 回 昔此 居僧 有主 半山 中 八戒 争先021 回 护法设庄留 大圣 须弥 灵吉定 口度022 回 戒 士战 流」可 木又 奉法收悟净",
        "id": 158,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 0,
        "text": "",
        "id": 159,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 4,
        "text": "Word2Vec工具",
        "id": 160,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 106,
        "text": "#将Word转换成Vec，然后计算相似度   \nfrom gensim.models import word2vec   \nimport multiprocessing   \n#如果目录中有多个文件，可以使用PathLineSentences   \nsentences $\\mathbf { \\tau } = \\mathbf { \\tau }$ word2vec.PathLineSentences('./segment') 0.96224165 0.98238677   \n#设置模型参数，进行训练 0.9193349 0.9293375   \nmodel $\\mathbf { \\tau } = ",
        "id": 161,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 129,
        "text": " 0.9193349 0.9293375   \nmodel $\\mathbf { \\tau } = \\mathbf { \\tau }$ word2vec.Word2Vec(sentences, size $= 1 0 0$ ,window $^ { \\cdot = 3 }$ ,min_count $\\mathbf { \\tau } = \\mathbf { \\dot { \\tau } }$ 1)   \nprint(model.wv.similarity('孙悟空'，'猪八戒))   \nprint(model.wv.similarity('孙悟空'，'孙行者))   \n#设置模型参数，进行训练  ",
        "id": 162,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 104,
        "text": "model.wv.similarity('孙悟空'，'孙行者))   \n#设置模型参数，进行训练   \nmodel2 $\\mathbf { \\tau } = \\mathbf { \\tau }$ word2vec.Word2Vec(sentences, size $\\ c =$ 128, window $^ { = 5 }$ min_count=5,   \nworkers $\\ c =$ multiprocessing.cpu_count()   \n#保存模型   \nmodel2.save('./models/word2Vec.model\")   \nprint(model2.wv.similar",
        "id": 163,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 77,
        "text": "models/word2Vec.model\")   \nprint(model2.wv.similarity('孙悟空','猪八戒))   \nprint(model2.wv.similarity('孙悟空','孙行者))   \nprint(model2.wv.most_similar(positive $\\ c =$ ['孙悟空'，'唐僧'],negative $\\ c =$ ['孙行者')) ",
        "id": 164,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 141,
        "text": "[(菩萨\\*, 0.9546594619750977)， ('何干', 0.9491345286369324)(长老'， 0.9437956809997559)， ('沙 僧见°, 0.943665623664856),(悟净'， 0.9424765706062317)， ('手指', 0.9390666484832764),(银角', 0.937991201877594)， ( 大子 '， 0.9359638690948486)， (',0.9339208006858826), (\\*祥师', 0.9319193363189697)]",
        "id": 165,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 14,
        "text": "Thinking: 得到了特征表达 (itemrepresentation），有什么用?",
        "id": 166,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 6,
        "text": "基于内容的推荐：",
        "id": 167,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 63,
        "text": "将你看的item，相似的item推荐给你通过物品表示Item Representation $\\Rightarrow$ 抽取特征TF-IDF=>返回给某个文本的“关键词-TFIDF值”的词数对TF-IDF可以帮我们抽取文本的重要特征，做成itemembedding",
        "id": 168,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 17,
        "text": "Thinking: 如何使用事物的特征表达， 进行相似推荐？",
        "id": 169,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 24,
        "text": "计算item之间的相似度矩阵对于指定的item，选择相似度最大的Top-K个进行输出",
        "id": 170,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 4,
        "text": "Embedding的理解：",
        "id": 171,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 92,
        "text": "·Embedding指某个对象X被嵌入到另外一个对象Y中，映射 $f : X \\to \\mathbb { Y }$   \n，一种降维方式，转换为维度相同的向量  \n·矩阵分解中的User矩阵，第i行可以看成是第i个user的Embedding。Item矩阵中的第j列可以看成是对第j个Item的Embedding",
        "id": 172,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 0,
        "text": "",
        "id": 173,
        "type": "image"
      },
      {
        "page": 31,
        "length_tokens": 7,
        "text": "Word2Vec工具的使用：",
        "id": 174,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 144,
        "text": "·Word Embedding就是将Word嵌入到一个数学空间里，Word2vec，就是词嵌入的一种·可以将sentence中的word转换为固定大小的向量表达（Vector Respresentations），·其中意义相近的词将被映射到向量空间中相近的位置。  \n·将待解决的问题转换成为单词word和文章doc的对应关系  \n大V推荐中，大 $\\mathsf { V } = >$ 单词，将每一个用户关注大V的顺序 $\\Rightarrow$ 文章  \n商品推荐中，商品 $\\Rightarrow$ 单词，用户对商品的行为顺序 $\\Rightarrow$ 文章",
        "id": 175,
        "type": "text"
      },
      {
        "page": 32,
        "length_tokens": 3,
        "text": "向量数据库",
        "id": 176,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 3,
        "text": "向量数据库",
        "id": 177,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 8,
        "text": "Thinking: 什么是向量数据库？",
        "id": 178,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 57,
        "text": "一种专门用于存储和检索高维向量数据的数据库。它将数据（如文本、图像、音频等）通过嵌入模型转换为向量形式，并通过高效的索引和搜索算法实现快速检索。",
        "id": 179,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 71,
        "text": "向量数据库的核心作用是实现相似性搜索，即通过计算向量之间的距离（如欧几里得距离、余弦相似度等）来找到与目标向量最相似的其他向量。它特别适合处理非结构化数据，支持语义搜索、内容推荐等场景。",
        "id": 180,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 15,
        "text": "Thinking: 如何存储和检索嵌入向量？",
        "id": 181,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 43,
        "text": "存储：向量数据库将嵌入向量存储为高维空间中的点，并为每个向量分配唯一标识符（ID），同时支持存储元数据。",
        "id": 182,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 48,
        "text": "检索：通过近似最近邻（ANN）算法（如PQ等）对向量进行索引和快速搜索。比如，FAISS和Milvus等数据库通过高效的索引结构加速检索。",
        "id": 183,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 6,
        "text": "常见的向量数据库",
        "id": 184,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 5,
        "text": "1.FAISS ",
        "id": 185,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 26,
        "text": "特点：由Facebook开发，专注于高性能的相似性搜索，适合大规模静态数据集。",
        "id": 186,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 13,
        "text": "优势：检索速度快，支持多种索引类型",
        "id": 187,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 17,
        "text": "局限性：主要用于静态数据，更新和删除操作较复杂。",
        "id": 188,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 5,
        "text": "2.Milvus ",
        "id": 189,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 16,
        "text": "特点：开源，支持分布式架构和动态数据更新。",
        "id": 190,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 16,
        "text": "优势：具备强大的扩展性和灵活的数据管理功能，",
        "id": 191,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 5,
        "text": "3.Pinecone ",
        "id": 192,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 20,
        "text": "特点：托管的云原生向量数据库，支持高性能的向量搜索。",
        "id": 193,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 17,
        "text": "优势：完全托管，易于部署，适合大规模生产环境。",
        "id": 194,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 9,
        "text": "向量数据库与传统数据库的对比",
        "id": 195,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 4,
        "text": "1.数据类型",
        "id": 196,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 17,
        "text": "传统数据库：存储结构化数据（如表格、行、列）。",
        "id": 197,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 19,
        "text": "向量数据库：存储高维向量数据，适合非结构化数据。",
        "id": 198,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 4,
        "text": "2.查询方式",
        "id": 199,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 17,
        "text": "传统数据库：依赖精确匹配（如=、<、>）",
        "id": 200,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 28,
        "text": "向量数据库：基于相似度或距离度量（如欧几里得距离、余弦相似度）。",
        "id": 201,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 5,
        "text": "3.应用场景",
        "id": 202,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 12,
        "text": "传统数据库：适合事务记录和结构化信息管理",
        "id": 203,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 22,
        "text": "向量数据库：适合语义搜索、内容推荐等需要相似性计算的场景。",
        "id": 204,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 5,
        "text": "讨论与呈现 ",
        "id": 205,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 22,
        "text": "结合你的业务场景，你认为事物的特征提取重要么？有什么应用场景？",
        "id": 206,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 22,
        "text": "非结构化数据的理解，pdf，word等结构化数据的理解，excel，数据表等",
        "id": 207,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 8,
        "text": "如何对这些特征进行理解？",
        "id": 208,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 52,
        "text": "Thinking1：假设一个小说网站，有N部小说，每部小说都有摘要描述。如何针对该网站制定基于内容的推荐系统，即用户看了某部小说后，推荐其他相关的小说。原理和步骤是怎样的",
        "id": 209,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 11,
        "text": "Thinking2：Word2Vec的应用场景有哪些",
        "id": 210,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 45,
        "text": "Action1：使用Gensim中的Word2Vec对三国演义进行Word Embedding，分析和曹操最相近的词有哪些，曹操+刘备-张飞 $= ?$ ",
        "id": 211,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 10,
        "text": "数据集：three_kingdoms.txt ",
        "id": 212,
        "type": "text"
      },
      {
        "page": 39,
        "length_tokens": 8,
        "text": "Thank You Using data to solve problems ",
        "id": 213,
        "type": "text"
      }
    ]
  }
}