{
  "metainfo": {
    "sha1": "Aitraining",
    "sha1_name": "Aitraining",
    "pages_amount": 42,
    "text_blocks_amount": 210,
    "tables_amount": 9,
    "pictures_amount": 0,
    "equations_amount": 0,
    "footnotes_amount": 0,
    "company_name": "AI应用开发",
    "file_name": "2.1-DeepSeek使用"
  },
  "content": {
    "chunks": [
      {
        "page": 1,
        "length_tokens": 3,
        "text": "DeepSeek使用",
        "id": 0,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 4,
        "text": "今天的学习目标",
        "id": 1,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 3,
        "text": "DeepSeek使用",
        "id": 2,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 47,
        "text": "·DeepSeek的创新   \nCASE：小球碰撞试验 (Cursor $+$ DeepSeek-R1)   \nDeepSeek私有化部署选择   \n·Ollama部署DeepSeek-R1   \nAPI调用DeepSeek ",
        "id": 3,
        "type": "text"
      },
      {
        "page": 3,
        "length_tokens": 5,
        "text": "DeepSeek的创新 ",
        "id": 4,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 5,
        "text": "DeepSeek-V3模型",
        "id": 5,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 0,
        "text": "",
        "id": 6,
        "type": "image"
      },
      {
        "page": 4,
        "length_tokens": 55,
        "text": "DeepSeek-V3在推理速度上相较历史模型有了大幅提升。在目前大模型主流榜单中，DeepSeek-V3在开源模型中位列榜首，与世界上最先进的闭源模型不分伯仲。",
        "id": 7,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 7,
        "text": "DeepSeek-V3的训练成本",
        "id": 8,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 0,
        "text": "",
        "id": 9,
        "type": "table"
      },
      {
        "page": 5,
        "length_tokens": 21,
        "text": "DeepSeek-V3的训练成本，假设H800的租用是\\$2/小时 ",
        "id": 10,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 80,
        "text": "DeepSeek-V3的推出是2024年12月，并没有太大波澜DeepSeek-R1火出圈，通过新的奖励机制GRPO (group relative policy optimization)，并使用规则类验证机制自动对输出进行打分。以V3为基础模型，一个多月内训练出了性能堪比GPT-o1的R1模型，成果非常亮眼。",
        "id": 11,
        "type": "text"
      },
      {
        "page": 6,
        "length_tokens": 5,
        "text": "DeepSeek-R1模型",
        "id": 12,
        "type": "text"
      },
      {
        "page": 6,
        "length_tokens": 0,
        "text": "",
        "id": 13,
        "type": "image"
      },
      {
        "page": 6,
        "length_tokens": 0,
        "text": "",
        "id": 14,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 5,
        "text": "DeepSeek-R1模型",
        "id": 15,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 5,
        "text": "DeepSeek-R1 ",
        "id": 16,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 96,
        "text": "DeepSeek-R1遵循MIT License，允许用户通过蒸馏技术借助R1训练其他模型。  \nDeepSeek-R1上线API，对用户开放思维链输出，通过设置model $\\models ^ { \\prime }$ deepseek-reasoner'即可调用。  \n2024-12-26 发布V3  \n2025-1-15 发布APP  \n2025-1-20 发布R1  \n多家企业宣布融合DeepSeek",
        "id": 17,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 0,
        "text": "",
        "id": 18,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 83,
        "text": "MIT License是一种非常宽松的开源许可协议，允许用户自由地使用、修改、分发和商业化软件或模型。相比之下，Meta Llama的License相对严格，虽然LLaMA3是开源的，但许可协议限制了商业用途和对模型的修改，比如新的模型如果使用LLaMA，需要名称上带有LLaMA标识。",
        "id": 19,
        "type": "text"
      },
      {
        "page": 8,
        "length_tokens": 14,
        "text": "蒸馏小模型超越 OpenAl o1-mini ",
        "id": 20,
        "type": "text"
      },
      {
        "page": 8,
        "length_tokens": 64,
        "text": "在开源 DeepSeek-R1-Zero 和 DeepSeek-R1两个 660B 模型的同时，通过 DeepSeek-R1的输出，蒸馏了6 个小模型，其中32B和70B 模型在多项能力上实现了对标OpenAlo1-mini的效果。",
        "id": 21,
        "type": "text"
      },
      {
        "page": 8,
        "length_tokens": 0,
        "text": "",
        "id": 22,
        "type": "table"
      },
      {
        "page": 9,
        "length_tokens": 7,
        "text": "DeepSeek的创新: MLA ",
        "id": 23,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 11,
        "text": "MLA (Multi-Head Latent Attention) ",
        "id": 24,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 5,
        "text": "Multi-Head Attention ",
        "id": 25,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 71,
        "text": "在\"Allyouneedisattention\"的背景下，传统的多头注意力（MHA，Multi-HeadAttention）的键值（KV）缓存机制事实上对计算效率形成了较大阻碍。缩小KV缓存（KVCache）大小，并提高性能，在之前的模型架构中并未很好的解决。",
        "id": 26,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 36,
        "text": "DeepSeek引入了MLA，一种通过低秩键值联合压缩的注意力机制， 在显著减小KV缓存的同时提高计算效率。",
        "id": 27,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 25,
        "text": "低秩近似是快速矩阵计算的常用方法，在MLA之前很少用于大模型计算。",
        "id": 28,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 66,
        "text": "从大模型架构的演进情况来看，Prefill和KVCache容量瓶颈的问题正一步步被新的模型架构攻克，巨大的KVCache正逐渐成为历史。（实际上在2024年6月发布的DeepSeek-V2就已经很好的降低了KVCache的大小)",
        "id": 29,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 0,
        "text": "",
        "id": 30,
        "type": "image"
      },
      {
        "page": 10,
        "length_tokens": 11,
        "text": "DeepSeek的创新： DeepSeek-MoE ",
        "id": 31,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 0,
        "text": "",
        "id": 32,
        "type": "image"
      },
      {
        "page": 10,
        "length_tokens": 54,
        "text": "V3使用了61个MoE (Mixof Expert混合专家)block，虽然总参数量很大，但每次训练或推理时只激活了很少链路，训练成本大大降低，推理速度显著提高。",
        "id": 33,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 85,
        "text": "MoE类比为医院的分诊台，在过去所有病人都要找全科医生，效率很低。但是MoE模型相当于有一个分诊台将病人分配到不同的专科医生那里。DeepSeek在这方面也有创新，之前分诊是完全没有医学知识的保安，而现在用的是有医学知识的本科生来处理分流任务",
        "id": 34,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 12,
        "text": "DeepSeek的创新： 混合精度框架",
        "id": 35,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 0,
        "text": "",
        "id": 36,
        "type": "image"
      },
      {
        "page": 11,
        "length_tokens": 109,
        "text": "整个混合精度框架使用了FP8数据格式，但为了简化说明，只展示了线性算子（Linear Operator）的部分采用了混合精度框架，即在不同的区块里使用不同的精度来存储数据。我们知道精度越高，内存占用越多，运算复杂度越大。DeepSeek在一些不需要很高精度的模块，使用很低的精度FP8储存数据，极大的降低了训练计算量。",
        "id": 37,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 13,
        "text": "Thinking: 为什么DeepSeek计算速度快， 成本低?",
        "id": 38,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 5,
        "text": "·架构设计方面",
        "id": 39,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 30,
        "text": "DeepSeekMoE架构：在推理时仅激活部分专家，避免了激活所有参数带来的计算资源浪费。",
        "id": 40,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 22,
        "text": "MLA架构：MLA通过降秩KV矩阵，减少了显存消耗.",
        "id": 41,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 4,
        "text": "·训练策略方面",
        "id": 42,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 41,
        "text": "多token预测（MTP）目标：在训练过程中采用多token预测目标，即在每个位置上预测多个未来token，增加了训练信号的密度，提高了数据效率。",
        "id": 43,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 68,
        "text": "混合精度训练框架：在训练中，对于占据大量计算量的通用矩阵乘法（GEMM）操作，采用FP8精度执行。同时，通过细粒度量化策略和高精度累积过程，解决了低精度训练中出现的量化误差问题。",
        "id": 44,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 2,
        "text": "Summary ",
        "id": 45,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 14,
        "text": "Thinking: 为什么DeepSeek-R1的推理能力强大？",
        "id": 46,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 51,
        "text": "·强化学习驱动：DepSeek-R1通过大规模强化学习技术显著提升了推理能力。在数学、代码和自然语言推理等任务上表现出色，性能与OpenAi的o1正式版相当。",
        "id": 47,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 50,
        "text": "·长链推理（CoT）技术：DeepSeek-R1采用长链推理技术，其思维链长度可达数万字，能够逐步分解复杂问题，通过多步骤的逻辑推理来解决问题",
        "id": 48,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 91,
        "text": "强化学习的作用：训练大模型，结合少量SFT，引入少量高质量监督数据（如数千个CoT示例）进行微调提升模型初始推理能力，再通过RL进一步优化，最终达到与OpenAIo1相当的性能长链推理CoT：CoT让AI模型逐步分解复杂问题，比如在智能客服、市场分析报告、AI辅助编程领域",
        "id": 49,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 2,
        "text": "Summary ",
        "id": 50,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 0,
        "text": "",
        "id": 51,
        "type": "table"
      },
      {
        "page": 14,
        "length_tokens": 146,
        "text": "DeepSeek-R1-Zero首次验证了纯强化学习在LLM中能显著增强推理能力的可行性，即无需SFT，仅通过RL即可激励模型学会长链推理和反思。提出了多阶段训练策略 (冷启动 ${ \\tt > R L - > S F T - > }$ 全场景RL），有效兼顾准确率与可读性，产出 DeepSeek-R1，性能比肩 OpenAl-o1-1217。展示了知识蒸馏在提升小模型推理能力方面的潜力，并开源多个大小不一的蒸馏模型（1.5B\\~70B)",
        "id": 52,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 0,
        "text": "",
        "id": 53,
        "type": "image"
      },
      {
        "page": 15,
        "length_tokens": 14,
        "text": "DeepSeek +Cursor使用:物理世界中的小球碰撞",
        "id": 54,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 5,
        "text": "DeepSeek +Cursor使用",
        "id": 55,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 22,
        "text": "在File $- >$ Preferences $$ Cursor Settings 中设置deepseek-r1和deepseek-v3模型",
        "id": 56,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 2,
        "text": "× ",
        "id": 57,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 22,
        "text": "在OpenAIAPIKey中进行设置，这里是采用OpenAI的协议，可以使用自定义的模型",
        "id": 58,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 1,
        "text": "区",
        "id": 59,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 1,
        "text": "区",
        "id": 60,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 3,
        "text": "Model Names ",
        "id": 61,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 21,
        "text": "Add new models to Cursor. Often used to configure the latest OpenAl models or OpenRouter models. ",
        "id": 62,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 0,
        "text": "",
        "id": 63,
        "type": "image"
      },
      {
        "page": 16,
        "length_tokens": 9,
        "text": "claude-3.5-sonnet ",
        "id": 64,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 0,
        "text": "",
        "id": 65,
        "type": "image"
      },
      {
        "page": 16,
        "length_tokens": 2,
        "text": "√ ",
        "id": 66,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 5,
        "text": "deepseek-r1 ",
        "id": 67,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 0,
        "text": "",
        "id": 68,
        "type": "image"
      },
      {
        "page": 16,
        "length_tokens": 5,
        "text": "deepseek-v3 ",
        "id": 69,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 37,
        "text": "OpenAl Key = skQ2gN9CgZOz9jrzjCCHkijalkUyaXpHS6xssmmkl327kkib0G ",
        "id": 70,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 30,
        "text": "OpenAl Base URL $\\mathbf { \\tau } = \\mathbf { \\tau }$ http://chatapi.littlewheat.com/v1 ",
        "id": 71,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 23,
        "text": "设置好deepseek-r1和deepseek-v3模型之后，可以在Cursor中选择该模型进行编程",
        "id": 72,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 3,
        "text": "cursor-small ",
        "id": 73,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 28,
        "text": "+ Add context一个红色的小球在三角区域内运动，碰到边界就反弹，编写一个HTML网页",
        "id": 74,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 5,
        "text": "OpenAl API Key ",
        "id": 75,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 5,
        "text": "deepseek-r1 ",
        "id": 76,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 5,
        "text": "normal/agentsubmit ",
        "id": 77,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 1,
        "text": "：",
        "id": 78,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 24,
        "text": "With your Cursor Pro subscription, you do not need to use your own OpenAl key unless you're using custom models. ",
        "id": 79,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 2,
        "text": "Dismiss ",
        "id": 80,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 6,
        "text": "Turn OffOpenAlKey ",
        "id": 81,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 32,
        "text": "You can putin your OpenAl key to use Cursor at public API costs.Note: this can cost more than pro and won't work for custom model features. ",
        "id": 82,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 0,
        "text": "",
        "id": 83,
        "type": "image"
      },
      {
        "page": 16,
        "length_tokens": 6,
        "text": "Override OpenAl Base URL γ",
        "id": 84,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 12,
        "text": "http://chatapi.littlewheat.com/v1 ",
        "id": 85,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 11,
        "text": "CASE: 物理世界中的小球碰撞",
        "id": 86,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 4,
        "text": "<think> ",
        "id": 87,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 24,
        "text": "个红色的小球在三角区域内运动，碰到边界就反弹编写一个HTML网页",
        "id": 88,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 66,
        "text": "好的，我现在需要帮助用户创建一个在三角区域内反弹的小球的HTML页面。首先，我得考虑如何实现这个效果。用户想要一个红色小球在三角形内运动，碰到边界反弹。我需要用HTML5的Canvas来绘制图形和动画。",
        "id": 89,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 72,
        "text": "首先，我得确定三角形的边界。三角形通常有三个边，但检测碰撞可能会比较复杂。因为Canvas的路径是矢量，可能需要用数学方法来判断小球是否碰到三角形的边。或者，我可以将三角形视为三个线段，分别检测碰撞。",
        "id": 90,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 66,
        "text": "接下来是小球的运动。小球有位置(x,y）和速度（dx,dy)，每次动画帧更新位置。当碰到边界时，速度方向改变。但是，三角形的边界是斜边，所以反弹方向不能简单取反，需要计算反射向量。",
        "id": 91,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 64,
        "text": "这可能会比较复杂。用户可能希望简化处理，比如将三角形视为三个墙壁，每次碰撞后根据碰到的边反转速度的相应分量。或者，可能需要计算小球到各边的距离，判断是否发生碰撞，然后调整速度方向。",
        "id": 92,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 0,
        "text": "",
        "id": 93,
        "type": "image"
      },
      {
        "page": 17,
        "length_tokens": 69,
        "text": "考虑到用户可能希望代码简单，我先考虑一个等边三角形，顶点在上方，底边水平。这样三条边分别是左斜边、右斜边和底边。对于左右斜边，碰撞检测可能需要计算点是否在边的附近，并调整速度方向。",
        "id": 94,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 35,
        "text": "但这样做可能数学计算较多。或者，可以使用三个直线方程，检测小球的中心是否接近某条边，并计算反射方向。",
        "id": 95,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 94,
        "text": "另一个思路是使用边界框，将三角形的三个边作为反射面，当小球碰撞时，根据边的方向调整速度。例如，底边是水平的，碰撞时反转dy；左右边是斜的，碰撞时需要计算反射。这可能涉及到向量运算，比如计算反射向量。假设边的法线向量，然后根据入射速度计算反射速度。",
        "id": 96,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 51,
        "text": "不过，这样计算起来可能有点复杂，特别是对于初学者。有没有更简单的方法？比如，将三角形视为由三个线段组成，每次检测小球与这三个线段的碰撞，然后反弹。",
        "id": 97,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 41,
        "text": "或者，使用边界的斜率和小球的运动方向来判断碰撞。例如，当小球碰到左斜边时，根据边的斜率调整速度分量。",
        "id": 98,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 89,
        "text": "deepseek-r1会进行很长的思考，然后编写出HTML代码。编写的HTML代码运行可能不符合要求，需要进一步调整不过，这样可能需要较多的几何计算。为了简化，我可以假设三角形是一个等边三角形，三个边的方程已知，然后检测小球的中心是否接近某条边，并在碰撞时调整速度方向。",
        "id": 99,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 0,
        "text": "",
        "id": 100,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 70,
        "text": "现在，我需要考虑如何表示三角形的边。例如，左斜边可能是从左上角到右下角，右斜边是从右上角到右下角，底边是水平的。假设三角形的顶点在画布的上中点，底边在底部。",
        "id": 101,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 39,
        "text": "接下来，小球的初始位置应该在三角形内部。当小球移动时，检查它是否超出了三角形的任何一边。如果超出，则反弹。",
        "id": 102,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 30,
        "text": "但如何判断点是否在三角形内？或者，当小球的中心碰到边时，视为碰撞。这可能需要进",
        "id": 103,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 11,
        "text": "CASE: 物理世界中的小球碰撞",
        "id": 104,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 11,
        "text": "基于之前的思考，帮我完善HTML",
        "id": 105,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 0,
        "text": "",
        "id": 106,
        "type": "image"
      },
      {
        "page": 18,
        "length_tokens": 19,
        "text": "HTML样式有了调整，但是小球还是会飞出三角区域，需要进一步调整",
        "id": 107,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 4,
        "text": "<think> ",
        "id": 108,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 54,
        "text": "好吧，用户希望完善之前的HTML，但当前的代码已经实现了基本功能。首先，我需要仔细检查代码，看看哪些地方可以改进。注意到用户提到之前的功能是正确的，所以不能破坏现有逻辑。",
        "id": 109,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 48,
        "text": "首先，界面方面，原生的canvas边框比较单调，可以考虑添加背景色或者渐变效果，让画面更吸引人。比如给画布加个渐变背景，这样视觉效果更好。",
        "id": 110,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 58,
        "text": "然后，动画流畅度方面，当前的移动速度是固定的，用户可能想要更自然的运动。可以考虑调整速度参数，比如使用小数，让运动更平滑。或者添加随机的速度变化，但要注意不能改变原有反弹逻辑。",
        "id": 111,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 46,
        "text": "界面文字提示也是一个改进点。可以在页面下方添加说明文字，比如使用CSS在body中添加一个div，说明这是三角区域反弹球，这样用户更清楚演示内容。",
        "id": 112,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 37,
        "text": "另外，小球的外观可以优化，比如添加阴影效果或者高光，让它看起来更立体。在drawBall函数里，用ctx.shadow来增加立体感。",
        "id": 113,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 55,
        "text": "代码可读性方面，可以添加更多注释，尤其是碰撞检测的部分，虽然用户可能已经了解，但清晰的注释有助于维护。比如在checkCollisionWithEdge函数中解释参数A和B代表边的两个端点。",
        "id": 114,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 43,
        "text": "检查是否存在中文乱码，确保HTML的meta标签有设置UTF-8。原来的代码没有<metacharset>，需要补充，这样中文注释和可能的文本就不会乱码。",
        "id": 115,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 32,
        "text": "最后，考虑响应式布局，让画布在不同设备上居中显示。可以通过CSS的flex布局让canvas居中，提升用户体验。",
        "id": 116,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 56,
        "text": "综合这些点，逐步修改代码，确保每一处改动都不影响原有功能。比如添加meta标签，修改背景样式，增加文字提示，美化小球外观，并调整速度参数。同时保持代码结构清晰，注释准确。",
        "id": 117,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 4,
        "text": "</think> ",
        "id": 118,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 16,
        "text": "我将在原有功能不变的基础上做以下改进优化：",
        "id": 119,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 0,
        "text": "",
        "id": 120,
        "type": "image"
      },
      {
        "page": 19,
        "length_tokens": 11,
        "text": "CASE: 物理世界中的小球碰撞",
        "id": 121,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 15,
        "text": "小球弹了之后，弹出去了啊，帮我检查代码",
        "id": 122,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 4,
        "text": "<think> ",
        "id": 123,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 48,
        "text": "好的，用户反馈说小球碰撞后弹出去了，我需要仔细检查代码中的碰撞检测逻辑。首先，查看checkCollisionWithEdge函数，注意到计算法线向量的方式可能有问题。",
        "id": 124,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 133,
        "text": "原计算法线方向的部分，使用的是最近的点的坐标减去起点A的坐标，再除以边长度，但这种计算是否垂直于边呢？假设边的向量是(B.x-A.x, B.y-A.y)，那么与其垂直的向量应该是（B.y - A.y，-(B.x - A.x)）或者相反方向。原代码中计算normal的x分量是(nearest.y -A.y)/edgeLength，这等于边的y分量除以长度，这可能错误地将边的方向当成了法线方向。",
        "id": 125,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 0,
        "text": "",
        "id": 126,
        "type": "image"
      },
      {
        "page": 19,
        "length_tokens": 82,
        "text": "正确的法线应该使用边的垂直向量。比如，边向量是 (edgeX,edgeY)，则法线应该是(edgeY，-edgeX）方向，并进行单位化。原代码中使用的是从最近点减去A点后的处理，这可能导致了法线方向错误，导致反射后的速度方向不正确，出现弹出问题。",
        "id": 127,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 41,
        "text": "需要修改normal的计算方式，使用边向量取垂直向量，确保法线方向正确，从而正确反射速度，这样小球就不会弹出边界了。",
        "id": 128,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 4,
        "text": "</think> ",
        "id": 129,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 28,
        "text": "经过检查，碰撞检测中的法线方向计算存在微小误差。我们需要修正法线向量计算方式：",
        "id": 130,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 0,
        "text": "",
        "id": 131,
        "type": "image"
      },
      {
        "page": 19,
        "length_tokens": 7,
        "text": "问题原因与修正说明：",
        "id": 132,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 9,
        "text": "1.原法线方向计算错误：",
        "id": 133,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 24,
        "text": "直接告诉deepseek-r1遇到的错误，修正后，小球会在三角区域内进行运动",
        "id": 134,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 35,
        "text": "旧代码使用的计算方式会导致法线方向与实际边向量方向不一致修正后直接通过边向量计算法向量，确保方向准确",
        "id": 135,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 9,
        "text": "2.反射公式正确性验证：",
        "id": 136,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 17,
        "text": "normal.x = edge.y / edgeLength 和 normal.y = -edge.x / ",
        "id": 137,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 11,
        "text": "CASE: 物理世界中的小球碰撞",
        "id": 138,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 21,
        "text": "考虑小球的重力，以及三角区域的支撑力，小球的弹力进行完善",
        "id": 139,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 15,
        "text": "每次运行，小球可以从随机的任意方向抛出",
        "id": 140,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 7,
        "text": "小球的弹力大一些",
        "id": 141,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 24,
        "text": "最后到地面都垂直弹跳了，不是应该还会滚动么，就是有水平的速度",
        "id": 142,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 10,
        "text": "下面增加一个刷新按钮，可以重新执行这个HTML",
        "id": 143,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 17,
        "text": "中文显示是乱码，另外考虑到手机显示， 页面需要做自适应",
        "id": 144,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 0,
        "text": "",
        "id": 145,
        "type": "image"
      },
      {
        "page": 21,
        "length_tokens": 26,
        "text": "'hinking：当我们写程序的时候，什么时候用DeepSeek-V3-0324，什么时候用DeepSeek-R1?",
        "id": 146,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 25,
        "text": "V3-0324：日常编程、快速开发、前端代码生成、常规脚本任务。",
        "id": 147,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 26,
        "text": "R1：数学密集型计算、复杂算法、代码逻辑深度优化、需要推理过程的任务。",
        "id": 148,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 20,
        "text": "$\\Rightarrow$ 更擅长复杂算法实现，能优化逻辑并减少错误 ",
        "id": 149,
        "type": "text"
      },
      {
        "page": 22,
        "length_tokens": 6,
        "text": "DeepSeek私有化部署",
        "id": 150,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 9,
        "text": "私有化部署 (DeepSeek模型)",
        "id": 151,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 0,
        "text": "",
        "id": 152,
        "type": "table"
      },
      {
        "page": 23,
        "length_tokens": 9,
        "text": "DeepSeek-R1蒸馏模型",
        "id": 153,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 0,
        "text": "",
        "id": 154,
        "type": "table"
      },
      {
        "page": 24,
        "length_tokens": 9,
        "text": "私有化部署 (DeepSeek模型)",
        "id": 155,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 46,
        "text": "放出来的1.5B、7B、14B等模型是Qwen/Ilama借助R1推理强化调出来的\"蒸馏\"版本，不是真正的R1。真正",
        "id": 156,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 106,
        "text": "的DeepSeek-R1是671B全量版 deepseek-r1:1.5b--1-2G显存 deepseek-r1:7b--6-8G显存 deepseek-r1:8b--8G显存 deepseek-r1:14b--10-12G显存 deepseek-r1:32b--24G-48显存 deepseek-r1:70b--96G-128显存 deepseek-r1:671b--496GB ",
        "id": 157,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 0,
        "text": "",
        "id": 158,
        "type": "image"
      },
      {
        "page": 24,
        "length_tokens": 14,
        "text": "https://modelscope.cn/search?search $\\ c =$ deepseek ",
        "id": 159,
        "type": "text"
      },
      {
        "page": 25,
        "length_tokens": 9,
        "text": "私有化部署 (DeepSeek模型)",
        "id": 160,
        "type": "text"
      },
      {
        "page": 25,
        "length_tokens": 0,
        "text": "",
        "id": 161,
        "type": "table"
      },
      {
        "page": 26,
        "length_tokens": 9,
        "text": "私有化部署 (DeepSeek模型)",
        "id": 162,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 0,
        "text": "",
        "id": 163,
        "type": "table"
      },
      {
        "page": 27,
        "length_tokens": 9,
        "text": "私有化部署 (DeepSeek模型)",
        "id": 164,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 0,
        "text": "",
        "id": 165,
        "type": "table"
      },
      {
        "page": 28,
        "length_tokens": 7,
        "text": "私有化部署 （代码模型",
        "id": 166,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 0,
        "text": "",
        "id": 167,
        "type": "table"
      },
      {
        "page": 28,
        "length_tokens": 49,
        "text": "基于DeepSeekMoE框架发布了拥有160亿和2360亿参数的DeepSeek-Coder-V2。  \n其中，激活参数仅为24亿和210亿，这包括了基础模型和指令模型。",
        "id": 168,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 3,
        "text": "VIIm使用",
        "id": 169,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 92,
        "text": "VIIm使用：是由伯克利大学LMSYS组织开源的LLM高速推理框架，用于提升LLM的吞吐量与内存使用效率。它通过PagedAttention技术高效管理注意力键和值的内存，并结合连续批处理技术优化推理性能。vLLM支持量化技术、分布式推理、与HuggingFace模型无缝集成等功能",
        "id": 170,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 38,
        "text": ":rve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B--tensor-paralel-size2 --max-model-len32768--enforce-eager ",
        "id": 171,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 15,
        "text": "vllmserve，启动vLLM推理服务的命令",
        "id": 172,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 127,
        "text": "·deepseek-ai/DeepSeek-R1-Distill-Qwen-32B，Hugging Face 模型库中的模型名称，vLLM会尝试从 HF下载模型·--tensor-parallel-size 2，启用张量并行，在2个GPU上分布式运行模型 (适合32B大模型)·--max-model-len32768，设置模型的最大上下文长度（32K tokens），确保能处理长文本。·--enforce-eager，禁用cUDA Graph 优化 (可能在某些环境下更稳定，但性能稍低)",
        "id": 173,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 3,
        "text": "VIIm使用",
        "id": 174,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 48,
        "text": "Thinking：如果我在本地的ubuntu下面有/root/autodl-tmp/models/tclf90/deepseek-r1-distillqwen-32b-gptq-int4,如何使用vIIm进行推理？",
        "id": 175,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 59,
        "text": "vllmserve/root/autodl-tmp/models/tclf90/deepseek-r1-distll-qwen-32b-gptq-int4--tensor-parallel-size1--max-model-len 32768 --enforce-eager --quantization gptq --dtype half",
        "id": 176,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 20,
        "text": "关键改动：指定本地路径：替换HF模型名为你的本地路径。",
        "id": 177,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 16,
        "text": "--quantization gptq:显式声明使用GPTQ量化。",
        "id": 178,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 34,
        "text": "--dtype：设为half（FP16）或auto（自动选择），因为GPTQ本身是4-bit，但计算时需指定中间精度。",
        "id": 179,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 108,
        "text": "Loading safetensors checkpoint shards: 0% Completed 0/4 [00:00<?, ?it/s] Loading safetensors checkpoint shards: 25% Completed 1/4 [00:00<00:02, 1.13it/s] Loading safetensors checkpoint shards: 50% Completed 2/4 [00:02<00:02, 1. 22s/it] Loading safetensors checkpoint shards: 75% Completed 3/4 [00:03<",
        "id": 180,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 96,
        "text": "nsors checkpoint shards: 75% Completed 3/4 [00:03<00:01, 1.22s/it] Loading safetensors checkpoint shards: 100% Completed 4/4 [00:04<00:00, 1.17s/it] Loading safetensors checkpoint shards: 100% Completed 4/4 [00:04<00:00, 1. 17s/it] ",
        "id": 181,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 94,
        "text": "ERROR 03-25 1:02:05 [core.py:340] RuntimeError: CUDA out of memory occurred when warming up sampler ase try lowering max_num_seqs\" orgpu_memory_utilization' when initializing the engine.   \nERROR 03-25 11:02:05 [c0re.py:340]   \nCRITICAL 03-25 11:02:05 [core_client.py:269] Got fatal signal from worke",
        "id": 182,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 22,
        "text": "5 [core_client.py:269] Got fatal signal from worker processes， shutting down. causeissue. ",
        "id": 183,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 4,
        "text": "VIlm使用",
        "id": 184,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 75,
        "text": "ierve /root/autodl-tmp/models/tclf90/deepseek-r1-distil-qwen-32b-gptq-int4-tensor-paralel-size1--maxmodel-len 4096 --quantization gptq --dtype half--gpu-memory-utilization 0.8--max-num-seqs8--enforce-eager ",
        "id": 185,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 107,
        "text": "INFO 03-25 11:06:45 [loader.py:429] Loading weights took 4.84 seconds   \nINFO_03-25_11:06:45_[gpu_model_runner.py:1176] Model loading took 18.1678 GB and 5.752492 seconds   \nERROR 03-25 11:06:47 [core.py:340] EngineCore hit an exception: Traceback (most recent call last):   \nEROR03-2511:06:47core.py",
        "id": 186,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 112,
        "text": "ost recent call last):   \nEROR03-2511:06:47core.py:340]File\"/rot/miniconda3/b/python3.10/site-packages/vlm//engie/core.py\"，ine32inrn _engine_core   \nERROR 03-25 11:06:47 [core.py:340] engine_core = EngineCoreProc(\\*args，\\*\\*kwargs)   \nERROR 03-25 11:06:47 [core.py:340] File \"/root/miniconda3/lib/pyt",
        "id": 187,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 111,
        "text": "06:47 [core.py:340] File \"/root/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\"，line 287,in__i nit   \nERROR 03-25 11:06:47 [core.py:340] super(.__init_(vllm_config，executor_class，log_stats)   \nERROR 03-25 11:06:47 [core.py:340] File \"/root/miniconda3/lib/python3.10/site-packages/vllm",
        "id": 188,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 109,
        "text": "/root/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\"，line 62, in __in it   \nERROR 03-25 11:06:47 [c0re.py:340] num_gpu_blocks，num_cpu_blocks = self._initialize_kv_caches(   \nERROR 03-25 11:06:47 [core.py:340] File \"/root/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py",
        "id": 189,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 109,
        "text": "ib/python3.10/site-packages/vllm/v1/engine/core.py\"，line 124，in _in itialize_kv_caches   \nERROR 03-25 11:06:47 [core.py:340] kv_cache_configs = get_kv_cache_configs(vllm_config,kv_cache_specs,   \nERROR 03-25 11:06:47 [core.py:340] File \"/root/miniconda3/lib/python3.10/site-packages/vllm/v1/core/kv_c",
        "id": 190,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 109,
        "text": "da3/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py\"， line 576 in get_kv_cache_configs   \nERROR 03-25 11:06:47 [c0re.py:340] check_enough_kv_cache_memory(vllm_config， kv_cache_spec,   \nERROR 03-25 11:06:47 [core.py:340] File \"/root/miniconda3/lib/python3.10/site-packages/vllm/v1/core/kv_",
        "id": 191,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 94,
        "text": "nda3/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py\"，line 468 ，in check_enough_kv_cache_memory   \nERROR 03-25 11:06:47 [core.py:340] raise ValueError(\"No available memory for the cache blocks.   \nEROR03-2511:06:47core.py:340]ValueError:Noavailablememoryforthecacheblocks.Tryincreasinggpu",
        "id": 192,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 83,
        "text": "oavailablememoryforthecacheblocks.Tryincreasinggpu_memory_utiiationw hen initializing the engine.   \nERROR 03-25 11:06:47 [c0re.py:340]   \nCRITICAL 03-2511:06:47[core_client.py:269]Gotfatal signalfrom worker processesshuttingdown.Seestack traceabove forroot cause issue.   \nKilled ",
        "id": 193,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 68,
        "text": "尽管我们已经将参数调整到非常保守的配置(max-model-len $\\scriptstyle 1 = 4 0 9 6$ 、gpu-memory-utilization $_ { | = 0 . 8 }$ ），但32BGPTQ量化模型仍然无法在24GB GPU 上运行。",
        "id": 194,
        "type": "text"
      },
      {
        "page": 32,
        "length_tokens": 14,
        "text": "CASE: DeepSeek-R1-7B使用(GPU部署）",
        "id": 195,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 10,
        "text": "CASE: DeepSeek-R1-7B使用",
        "id": 196,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 106,
        "text": "#导入必要的库   \nfrom modelscope import AutoModelForCausalLM, AutoTokenizer   \n#设置模型路径   \nmodel_name $\\mathbf { \\tau } = \\mathbf { \\tau }$ \"/root/autodl-tmp/models/deepseek-ai/DeepSeek  \nR1-Distill-Qwen-7B\"   \n#加载模型   \n# torch_dtype $\\ c =$ \"auto\"自动选择合适的数据类型   \n#device_map=\"cuda\"指定使用GPU加速   \nmodel $\\mathb",
        "id": 197,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 98,
        "text": "类型   \n#device_map=\"cuda\"指定使用GPU加速   \nmodel $\\mathbf { \\tau } = \\mathbf { \\tau }$ AutoModelForCausalLM.from_pretrained( model_name, torch_dtype $\\ c =$ \"auto\", device_map=\"cuda\"#也可以设置为\"auto\"自动选择设备   \n）   \n#加载对应的分词器   \ntokenizer $\\mathbf { \\tau } = \\mathbf { \\tau }$ AutoTokenizer.from_pretrained(model",
        "id": 198,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 112,
        "text": "thbf { \\tau }$ AutoTokenizer.from_pretrained(model_name)   \n#设置用户输入的提示词   \nprompt $\\mathbf { \\tau } = \\mathbf { \\tau }$ \"帮我写一个二分查找法\"   \n#构建对话消息列表   \n$\\mathsf { m e s s a g e s } = [$ {\"role\": \"system\",\"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": prompt}   \n]   \n#将消息转换为模型可接",
        "id": 199,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 75,
        "text": "e\": \"user\", \"content\": prompt}   \n]   \n#将消息转换为模型可接受的格式   \ntext $\\mathbf { \\tau } = \\mathbf { \\tau }$ tokenizer.apply_chat_template( messages, tokenize $\\ c =$ False, add_generation_prompt $\\mathbf { \\lambda } = \\mathbf { \\vec { \\lambda } }$ True ",
        "id": 200,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 0,
        "text": "",
        "id": 201,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 10,
        "text": "CASE: DeepSeek-R1-7B使用",
        "id": 202,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 96,
        "text": "#将文本转换为模型输入格式并移动到正确的设备上   \nmodel_inputs $\\mathbf { \\tau } = \\mathbf { \\tau }$ tokenizer([text],   \nreturn_tensors $\\ c =$ \"pt\").to(model.device)   \n#将生成的token ID解码为文本   \n# skip_special_tokens $\\ c =$ True跳过特殊token   \nresponse $\\mathbf { \\tau } = \\mathbf { \\tau }$ tokenizer.batch_decode(generated_ids",
        "id": 203,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 77,
        "text": "hbf { \\tau }$ tokenizer.batch_decode(generated_ids,   \nskip_special_tokens $\\ c =$ True)[0]   \n#打印生成的回复   \nprint(response)   \n#生成回复   \ngenerated_ids $\\mathbf { \\tau } = \\mathbf { \\tau }$ model.generate( \\*\\*model_inputs, max_new_tokens $\\ c =$ 2000   \n） ",
        "id": 204,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 0,
        "text": "",
        "id": 205,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 0,
        "text": "",
        "id": 206,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 56,
        "text": "嗯，我现在要学习一下二分查找法，也就是二分查找。我对这个算法不是很熟悉，但我知道它是一种高效的查找方法，特别是在有序数组中。让我先理清思路，然后一步步来。",
        "id": 207,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 63,
        "text": "首先，二分查找的基本思想是将一个数组不断对半分，然后在分好的两部分中继续查找，直到找到目标元素。这种方法的时间复杂度是0(1ogn)，这比线性查找要快很多，特别是当数组很大时。",
        "id": 208,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 85,
        "text": "那具体步骤是什么呢？我记得应该是先确定数组的中点，然后比较中点处的元素和目标元素的大小。如果中点处的元素等于目标，就找到了；如果目标比中点小，那么在左边继续查找；如果目标比中点大，则在右边继续查找。这个过程不断重复，直到找到目标或者数组遍历完毕。",
        "id": 209,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 165,
        "text": "举个例子，假设有一个有序数组：[1,3,5,7,9,1,13,15]，我要找数字7。首先，数组的长度是8，中点应该是第4个元素，也就是7的位置。所以直接找到了。但如果我要找的是数字6，那么中点是7，比6大，所以应该在左边继续查找。然后新的子数组是[1,3,5]，中点是3的位置，元素是5。6比5大，所以继续在右边查找。新的子数组是[5,7]，中点是6的位置，元素是7。但是6比7小，所以继续在左边查找，但左边只有一个元素5，比较后发现6不在数组中。",
        "id": 210,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 46,
        "text": "那么，二分查找的关键点在于数组必须是有序的，并且每次查找都要计算中点，并根据比较结果调整左右边界。那具体怎么在代码中实现呢？",
        "id": 211,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 97,
        "text": "我记得通常会用一个循环，初始时左边界left=0，右边界right=n-1。然后计算中点mid=(left+right)/2。如果数组[mid]等于目标，返回mid；否则，如果目标小于数组[mid]，则right=mid-1，继续在左边查找；否则，left=mid+1，继续在右边查找。直到left>right时，说明没找到。",
        "id": 212,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 11,
        "text": "那我来试着写一个伪代码：",
        "id": 213,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 58,
        "text": "#提取生成的文本部分 (去除输入部分)   \ngenerated_ids $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ [ output_ids[len(input_ids):] for input_ids, output_ids in   \nzip(model_inputs.input_ids,generated_ids)   \n] ",
        "id": 214,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 91,
        "text": "function binary_search(array，target): left = 0 right $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ len(array)-1 while left $< =$ right: mid $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ (left $^ +$ right）//2 if array[mid] $\\scriptstyle = =$ target: return mid elifarray[mid]<target: left $\\mathbf { \\sigma } = \\m",
        "id": 215,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 53,
        "text": "ifarray[mid]<target: left $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ mid +1 else: right $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ mid -1 return -1 # 表示没找到 ",
        "id": 216,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 4,
        "text": "Ollama使用",
        "id": 217,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 16,
        "text": "Ollama官方主要支持macOs和Linux，但Windows用户也可以安装",
        "id": 218,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 6,
        "text": "方法1： 使用 WSL",
        "id": 219,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 79,
        "text": "Step1,打开PowerShell（管理员权限），运行：wsl--install  \n重启电脑后，WSL会自动完成安装（默认安装Ubuntu）  \nStep2,安装 Ollama  \n在 WSL 终端（Ubuntu） 中运行:curl -fsSL https://ollama.com/install.sh丨 sh  \nStep3，启动Ollama 服务  \nollama serve",
        "id": 220,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 18,
        "text": "(保持此终端运行，另开一个新终端进行后续操作)",
        "id": 221,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 7,
        "text": "方法2:直接下载Windows版",
        "id": 222,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 7,
        "text": "https://ollama.com/ ",
        "id": 223,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 18,
        "text": "安装后，OIlama会作为服务运行（可在任务管理器查看）",
        "id": 224,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 10,
        "text": "Get up and running with large language models. ",
        "id": 225,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 33,
        "text": "Run Llama 3.3, DeepSeek-R1, Phi-4, Mistral, Gemma 3, and other models, locally. ",
        "id": 226,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 0,
        "text": "",
        "id": 227,
        "type": "image"
      },
      {
        "page": 38,
        "length_tokens": 4,
        "text": "Ollama使用",
        "id": 228,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 130,
        "text": "Ollama官方模型库 https://ollama.com/library  \n下载deepseek-r1:1.5b 模型  \nollama pull deepseek-r1:1.5b  \n如果要删除该模型，可以使用  \nollama rm deepseek-r1:1.5b  \n运行该模型，使用  \nollama run deepseek-r1:1.5b  \nPS C:\\Windows\\System32> ollama run deepseek-r1:1.5b  \nvvv 你好啊  \n<think>  \n</think>  \n你好！很高兴见到你，有什么我可以帮忙的吗？",
        "id": 229,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 0,
        "text": "",
        "id": 230,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 3,
        "text": "qwq ",
        "id": 231,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 14,
        "text": "QwQ is the reasoning model of the Qwen series. ",
        "id": 232,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 5,
        "text": "tools 32b ",
        "id": 233,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 35,
        "text": "↓ 1.1M Pulls $0$ 8 Tags $\\textcircled { \\scriptsize { \\perp } }$ Updated 2 weeks ago ",
        "id": 234,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 5,
        "text": "deepseek-r1 ",
        "id": 235,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 36,
        "text": "DeepSeek's first-generation of reasoning models with comparable performance to OpenAl-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen. ",
        "id": 236,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 57,
        "text": "1.5b 7b 8b 14b 32b 70b 671b ↓ 31.5M Pulls $0$ 29 Tags $\\textcircled { \\scriptsize { \\perp } }$ Updated 7 weeks ago ",
        "id": 237,
        "type": "text"
      },
      {
        "page": 39,
        "length_tokens": 10,
        "text": "CASE: DeepSeek-R1使用(API调用)",
        "id": 238,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 11,
        "text": "CASE: DeepSeek-R1使用-阿里代理",
        "id": 239,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 5,
        "text": "1、基本设置：",
        "id": 240,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 45,
        "text": "import dashscope   \nfrom dashscope.api_entities.dashscope_response import Role   \n# 设置 API key   \ndashscope.api_key $\\mathbf { \\tau } = \\mathbf { \\tau }$ \"your-api-key' ",
        "id": 241,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 7,
        "text": "2、模型调用封装:",
        "id": 242,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 95,
        "text": "def get_response(messages):response $\\mathbf { \\tau } = \\mathbf { \\tau }$ dashscope.Generation.call(model $\\mathsf { I } \\mathop { = } \\frac { \\boldsymbol { \\mathsf { I } } } { \\boldsymbol { \\mathsf { I } } }$ deepseek-r1'，#使用 deepseek-r1 模型messages $\\mathbf { \\tau } =$ messages,result_format $= ^ {",
        "id": 243,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 29,
        "text": "$\\mathbf { \\tau } =$ messages,result_format $= ^ { \\prime }$ message'#将输出设置为message形式）return response",
        "id": 244,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 7,
        "text": "3、情感分析实现:",
        "id": 245,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 115,
        "text": "review $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ '这款音效特别好给你意想不到的音质。‘   \nmessages =[ {\"role\":\"system\",\"content\":\"你是一名舆情分析师，帮我判断   \n产品口碑的正负向，回复请用一个词语：正向或者负向\"}, {\"role\": \"user\",\"content\": review}   \n]   \nresponse $\\mathbf { \\tau } = \\mathbf { \\tau }$ get_response(messages)   \nprint(response.output.choi",
        "id": 246,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 17,
        "text": "t_response(messages)   \nprint(response.output.choices[O].message.content) ",
        "id": 247,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 2,
        "text": "正向",
        "id": 248,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 2,
        "text": "Summary ",
        "id": 249,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 38,
        "text": "·大模型API是连接AI能力的桥梁，让开发者无需关注底层架构即可调用前沿AI能力，极大拓展了技术应用的边界。",
        "id": 250,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 90,
        "text": "·Prompt工程是激活大模型潜力的钥匙  \n结构化设计 (角色定义/分步指令/示例规范)  \n业务场景对齐(需求分析→Prompt迭代→效果验证)  \n性能优化技巧 (温度系数/输出限制/上下文管理)  \n·LLM正在重塑开发范式，通过大模型API接口覆盖NLP/CV/多模态任务",
        "id": 251,
        "type": "text"
      },
      {
        "page": 42,
        "length_tokens": 8,
        "text": "Thank You Using data to solve problems ",
        "id": 252,
        "type": "text"
      }
    ]
  }
}