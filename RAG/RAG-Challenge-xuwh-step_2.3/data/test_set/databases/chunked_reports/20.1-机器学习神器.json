{
  "metainfo": {
    "sha1": "Aitraining",
    "sha1_name": "Aitraining",
    "pages_amount": 85,
    "text_blocks_amount": 322,
    "tables_amount": 25,
    "pictures_amount": 0,
    "equations_amount": 15,
    "footnotes_amount": 0,
    "company_name": "AI应用开发",
    "file_name": "20.1-机器学习神器"
  },
  "content": {
    "chunks": [
      {
        "page": 1,
        "length_tokens": 3,
        "text": "机器学习神器",
        "id": 0,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 2,
        "text": "学习方法",
        "id": 1,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 13,
        "text": "· Thinking: behind the theory,original from the real problem ",
        "id": 2,
        "type": "text"
      },
      {
        "page": 2,
        "length_tokens": 12,
        "text": "·Action: solve problems by tools,present the results ",
        "id": 3,
        "type": "text"
      },
      {
        "page": 3,
        "length_tokens": 4,
        "text": "今天的学习目标",
        "id": 4,
        "type": "text"
      },
      {
        "page": 3,
        "length_tokens": 5,
        "text": "预测全家桶 ",
        "id": 5,
        "type": "text"
      },
      {
        "page": 3,
        "length_tokens": 48,
        "text": "Project A: 员工离职预测 ·ProjectB：男女声音识别 ·分类算法：LR，SVM，KNN ·树模型：GBDT,XGBoost, LightGBM, CatBoost， NGBoost ",
        "id": 6,
        "type": "text"
      },
      {
        "page": 3,
        "length_tokens": 3,
        "text": "机器学习神器",
        "id": 7,
        "type": "text"
      },
      {
        "page": 3,
        "length_tokens": 49,
        "text": "·什么是集成学习GBDT原理  \nXGBoost  \nLightGBMCatBoost  \n·在Project中使用机器学习神器  \n·AI大赛：二手车价格预测  \n·如何防止模型过拟合",
        "id": 8,
        "type": "text"
      },
      {
        "page": 4,
        "length_tokens": 9,
        "text": "1/2 预测全家桶 ",
        "id": 9,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 10,
        "text": "Project A: 员工离职预测 ",
        "id": 10,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 5,
        "text": "员工离职预测 ",
        "id": 11,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 4,
        "text": "In Class Competition ",
        "id": 12,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 17,
        "text": "https://www.kaggle.com/c/bi-attrition-predict/ ",
        "id": 13,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 48,
        "text": "·我们有员工的各种统计信息，以及该员工是否已经离职，统计的信息包括了（工资、出差、工作环境满意度、工作投入度、是否加班、是否升职、工资提升比例等）",
        "id": 14,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 35,
        "text": "·现在需要你来通过训练数据得出员工离职预测，并给出你在测试集上的预测结果。我们将给出课程上公开的榜单",
        "id": 15,
        "type": "text"
      },
      {
        "page": 5,
        "length_tokens": 0,
        "text": "",
        "id": 16,
        "type": "image"
      },
      {
        "page": 6,
        "length_tokens": 10,
        "text": "Project A: 员工离职预测 ",
        "id": 17,
        "type": "text"
      },
      {
        "page": 6,
        "length_tokens": 0,
        "text": "",
        "id": 18,
        "type": "table"
      },
      {
        "page": 7,
        "length_tokens": 10,
        "text": "Project A: 员工离职预测 ",
        "id": 19,
        "type": "text"
      },
      {
        "page": 7,
        "length_tokens": 0,
        "text": "",
        "id": 20,
        "type": "table"
      },
      {
        "page": 8,
        "length_tokens": 5,
        "text": "预测全家桶 ",
        "id": 21,
        "type": "text"
      },
      {
        "page": 8,
        "length_tokens": 11,
        "text": "常用预测（分类，回归）模型：",
        "id": 22,
        "type": "text"
      },
      {
        "page": 8,
        "length_tokens": 32,
        "text": "·分类算法：LR，SVM，KNN · 树模型：GBDT,XGBoost, LightGBM,CatBoost，NGBoost ",
        "id": 23,
        "type": "text"
      },
      {
        "page": 8,
        "length_tokens": 0,
        "text": "",
        "id": 24,
        "type": "image"
      },
      {
        "page": 8,
        "length_tokens": 14,
        "text": "·特征工程：好的特征工程是拿分的关键",
        "id": 25,
        "type": "text"
      },
      {
        "page": 8,
        "length_tokens": 10,
        "text": "·模型：懂原理，会调参 ",
        "id": 26,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 4,
        "text": "常用预测模型",
        "id": 27,
        "type": "text"
      },
      {
        "page": 9,
        "length_tokens": 33,
        "text": "数据预处理  \n分类算法：LR，SVM，KNN  \n树模型：GBDT,XGBoost, LightGBM, CatBoost， NGBoost",
        "id": 28,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 6,
        "text": "Project A: 数据预处理",
        "id": 29,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 6,
        "text": "Step1，对数据进行探索",
        "id": 30,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 96,
        "text": "#工离职预测   \nimport pandas as pd   \ntrain $\\ c =$ pd.read_csv('train.csv',index_col $\\scriptstyle 1 = 0$ ）   \ntest=pd.read_csv('test.csv',index_col=0)   \nprint(train['Attrition'].value_counts())   \n#处理Attrition字段   \ntrain['Attrition']=train['Attrition'].map(lambda x:1 if $\\ x = = 1$ Yes' else O)   \n#查看",
        "id": 31,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 38,
        "text": ".map(lambda x:1 if $\\ x = = 1$ Yes' else O)   \n#查看数据中每列是否有空值   \nprint(train.isna().sum()) ",
        "id": 32,
        "type": "text"
      },
      {
        "page": 10,
        "length_tokens": 87,
        "text": "No 988   \nYes 188   \nName: Attrition, dtype: int64   \nAge 0   \nAttrition 0   \nBusinessTravel 0   \nDailyRate 0   \nDepartment 0   \nDistanceFromHome 0   \nEducation 0   \nEducationField 0   \nEmployeeCount 0   \nYearslnCurrentRole 0   \nYearsSinceLastPromotion 0   \nYearsWithCurrManager 0   \ndtype: int64 ",
        "id": 33,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 6,
        "text": "Project A: 数据预处理",
        "id": 34,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 14,
        "text": "Step2， 去掉无用特征，处理分类特征",
        "id": 35,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 108,
        "text": "#去掉没用的列员工号码，标准工时（ $\\scriptstyle \\left. = 8 0 \\right.$ ））   \ntrain $\\mathbf { \\tau } = \\mathbf { \\tau }$ train.drop(['EmployeeNumber','StandardHours'],axis ${ \\mathfrak { s } } = 1$ ）   \ntest $\\mathbf { \\tau } = \\mathbf { \\tau }$ test.drop(['EmployeeNumber','StandardHours'],axis $_ { \\cdot = 1 }$ ）  ",
        "id": 36,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 68,
        "text": "umber','StandardHours'],axis $_ { \\cdot = 1 }$ ）   \n#对于分类特征进行特征值编码   \nfrom sklearn.preprocessing import LabelEncoder   \nattr=['Age','BusinessTravel','Department','Education','EducationField','Gender'   \n,'JobRole','MaritalStatus','Over18','OverTime'] ",
        "id": 37,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 6,
        "text": "for feature in attr: ",
        "id": 38,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 42,
        "text": "Ibe $\\ c =$ LabelEncoder() train[feature]=lbe.fit_transform(train[feature]) test[feature] $= |$ be.transform(test[feature]) train.to_csv('temp.csv') ",
        "id": 39,
        "type": "text"
      },
      {
        "page": 11,
        "length_tokens": 0,
        "text": "",
        "id": 40,
        "type": "table"
      },
      {
        "page": 12,
        "length_tokens": 6,
        "text": "Project A: 数据预处理",
        "id": 41,
        "type": "text"
      },
      {
        "page": 12,
        "length_tokens": 0,
        "text": "",
        "id": 42,
        "type": "table"
      },
      {
        "page": 12,
        "length_tokens": 0,
        "text": "",
        "id": 43,
        "type": "table"
      },
      {
        "page": 13,
        "length_tokens": 2,
        "text": "LR工具",
        "id": 44,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 3,
        "text": "LR工具：",
        "id": 45,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 10,
        "text": "from sklearn.linear_model.logistic import LogisticRegression ",
        "id": 46,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 2,
        "text": "参数:",
        "id": 47,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 189,
        "text": "penalty，惩罚项，正则化参数，防止过拟合，I1或l2，默认为l2·C，正则化系数入的倒数，float类型，默认为1.0solver，损失函数优化方法，liblinear（默认），Ibfgs，newton-cg，sagrandom_state，随机数种子max_iter，算法收敛的最大迭代次数，默认为100$\\scriptstyle \\mathbf { \\ t o } 1 = 0 . 0 0 0 1$ ：优化算法停止条件，迭代前后函数差小于tol则终止verbose $= 0$ ：日志冗长度int：冗长度；0：不输出训练过程；1：偶尔输出； ${ > } 1$ ：对每个子模型都输出n_jo",
        "id": 48,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 88,
        "text": "int：冗长度；0：不输出训练过程；1：偶尔输出； ${ > } 1$ ：对每个子模型都输出n_jobs $\\mathord { \\left. \\vert { \\begin{array} { r l } \\end{array} } \\right. }$ ：并行数，int：个数；-1：跟CPU核数一致；1:默认值",
        "id": 49,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 4,
        "text": "常用方法：",
        "id": 50,
        "type": "text"
      },
      {
        "page": 13,
        "length_tokens": 91,
        "text": "fit(X,y,sample_weight $\\ c =$ None)  \nfit_transform(X,y=None,\\*\\*fit_params)predict(X)，用来预测样本，也就是分类predict_proba(X)，输出分类概率。返回每种类别的概率，按照分类类别顺序给出。score(X,y, sample_weight $\\mathop { \\bf { \\phi } } =$ None)，返回给定测试集合的平均准确率（mean accuracy）",
        "id": 51,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 2,
        "text": "LR工具",
        "id": 52,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 8,
        "text": "Step3, 模型参数配置 ",
        "id": 53,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 128,
        "text": "model $\\mathbf { \\tau } = \\mathbf { \\tau }$ LogisticRegression(max_iter=100, verbose $\\mathbf { \\tau } = \\mathbf { \\dot { \\tau } }$ True, random_state $\\scriptstyle = 3 3$ ， tol=1e-4 iter 25 act 1.052e+01 pre 1.050e+01 delta 2.865e-01 f 3.291e+02 g 3.106e+02 CG 9 t 5.049e-02 pre 5.014e-02 delta 2.86",
        "id": 54,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 163,
        "text": ".106e+02 CG 9 t 5.049e-02 pre 5.014e-02 delta 2.865e-01 f 3.186e+02 lgl 5.462e+04 CG 1 t 8.230e-03 pre 8.184e-03 delta 2.865e-01 f 3.185e+02 lgl 5.377e+03 CG cg reaches trust region boundary ） t 9.298e+00 pre 9.223e+00 delta 4.300e-01 f 3.185e+02 |lgl 2.770e+02 CG 1: iter 29 act 2.898e-02 pre 2.878e",
        "id": 55,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 127,
        "text": "l 2.770e+02 CG 1: iter 29 act 2.898e-02 pre 2.878e-02 delta 4.300e-01 f 3.092e+02 lgl 3.736e+04 CG 2   \nmodel.fit(X_train, y_train) iter act1epreedeta4elCG 6 2 cg reaches trust region boundary   \npredict $\\mathbf { \\tau } = \\mathbf { \\tau }$ model.predict_proba(test)[:,1] act 7e2preedeta47 B 5.929e+",
        "id": 56,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 153,
        "text": "edict_proba(test)[:,1] act 7e2preedeta47 B 5.929e+04 CG 12 2 act 3.256e+00 pre 3.259e+00 delta 4.477e-01 f 3.007e+02 lgl 8.364e+02 CG 8 iter 35 act 3.303e-02 pre 3.265e-02 delta 4.477e-01 f 2.974e+02 lgl 3.015e+04 CG 2   \ntest['Attrition'] $\\ c =$ predict iter act 3.761e-01 pre 3.697e-01 delta 4.477",
        "id": 57,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 144,
        "text": "edict iter act 3.761e-01 pre 3.697e-01 delta 4.477e-01 f 2.974e+02 lgl 3.958e+02 CG 6 iter 37 act 2.707e-04 pre 2.705e-04 delta 4.477e-01 f 2.970e+02 gl 3.163e+03 CG 2   \n#转化为二分类输出   \ntest['Attrition']=test['Attrition'].map(lambda x:1 if $x > = 0 . 5$ else 0)   \ntest[['Attrition']].to_csv('submit_Ir",
        "id": 58,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 21,
        "text": "$ else 0)   \ntest[['Attrition']].to_csv('submit_Ir.csv') ",
        "id": 59,
        "type": "text"
      },
      {
        "page": 14,
        "length_tokens": 0,
        "text": "",
        "id": 60,
        "type": "table"
      },
      {
        "page": 15,
        "length_tokens": 4,
        "text": "SVM工具：",
        "id": 61,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 131,
        "text": "sklearn中支持向量分类主要有三种方法：SVC、NuSVC、LinearSVC   \nsklearn.svm.SVC( $\\ c = 1 . 0$ ，kernel $\\mathbf { \\Phi } = \\mathbf { \\Phi } ^ { \\prime }$ rbf', degree $^ { = 3 }$ ,gamma $= ^ { \\prime }$ auto', coef $) { = } 0 . 0$ shrinking $\\mathop :$ True,probability=False,to $\\mathsf { I } { = } 0 . 0 0 1$ ,cache_s",
        "id": 62,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 116,
        "text": "=False,to $\\mathsf { I } { = } 0 . 0 0 1$ ,cache_size $= 2 0 0$ class_weight $\\ c =$ None,verbose $\\ c =$ False,max_iter=-1, decision_function_shape $= \"$ ovr',random_state $\\ c =$ None)   \nsklearn.svm.NuSVC( $\\mathsf { n u } { = } 0 . 5$ ,kernel='rbf',degree $\\mathbf { \\lambda } = :$ 3,gamma $= ^ {",
        "id": 63,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 106,
        "text": "f',degree $\\mathbf { \\lambda } = :$ 3,gamma $= ^ { \\vert }$ auto', coef $\\scriptstyle \\mathtt { \\lambda } = 0 . 0$ ，shrinking $\\varXi$ True,probability=False, to $\\mathsf { l } { = } 0 . 0 0 1$ ,cache_size=200, class_weight $\\ c =$ None,verbose $\\ c =$ False,max_iter=-1, decision_function_shape $= ^",
        "id": 64,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 122,
        "text": "=$ False,max_iter=-1, decision_function_shape $= ^ { \\mathsf { I } }$ ovr',random_state $\\circleddash$ None)   \nsklearn.svm.LinearSvC(penalty $= ^ { \\prime } \\vert { 2 ^ { \\prime } }$ loss $= ^ { \\prime }$ squared_hinge', dual=True, t $_ { \\cdot 0 1 = 0 . 0 0 0 1 }$ ， ${ \\mathsf { C } } { = } 1 . 0$",
        "id": 65,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 115,
        "text": "= 0 . 0 0 0 1 }$ ， ${ \\mathsf { C } } { = } 1 . 0$ ，multi_class $= ^ { \\prime }$ ovr',fit_intercept $\\risingdotseq$ True, intercept_scaling $\\scriptstyle = 1$ ,class_weight $\\mathbf { \\Psi } : =$ None,verbose $_ { = 0 }$ random_state $\\ c =$ None,max_iter $\\mathbf { \\tau } = \\mathbf { \\tau }$ 1000) ",
        "id": 66,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 4,
        "text": "常用参数：",
        "id": 67,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 180,
        "text": "·C，惩罚系数，类似于LR中的正则化系数，C越大惩罚越大nu，代表训练集训练的错误率的上限（用于NuSVC）kernel，核函数类型，RBF,Linear,Poly,Sigmoid,precomputed，默认为RBF径向基核（高斯核函数）gamma，核函数系数，默认为autodegree，当指定kernel为'poly'时，表示选择的多项式的最高次数，默认为三次多项式probability，是否使用概率估计shrinking，是否进行启发式，SVM只用少量训练样本进行计算penalty，正则化参数，L1和L2两种参数可选，仅LinearSVC有loss，损失函数，有‘hinge’和‘squa",
        "id": 68,
        "type": "text"
      },
      {
        "page": 15,
        "length_tokens": 76,
        "text": "参数，L1和L2两种参数可选，仅LinearSVC有loss，损失函数，有‘hinge’和‘squared_hinge’两种可选，前者又称L1损失，后者称为L2损失  \n·tol:残差收敛条件，默认是0.0001，与LR中的一致",
        "id": 69,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 3,
        "text": "SVM工具",
        "id": 70,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 5,
        "text": "SVM工具: ",
        "id": 71,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 167,
        "text": "·SVC，SupportVector Classification，支持向量机用于分类 libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核  \n·SVR，Support Vector Regression，支持向量机用于回归 Kernel核的选择技巧的：  \n·sklearn中支持向量分类主要有三种方法：sVC、NusVC、LinearSVc·如果样本数量<特征数：  \n·基于libsvm工具包实现，台湾大学林智仁教授在200年开发的一个简 方法1：简单的使用线性核就可以，不用选择非线性核单易用的SVM工具包 方法2：可以先对数据进行降维，然后使用非线性核  \n·SV",
        "id": 72,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 127,
        "text": "以，不用选择非线性核单易用的SVM工具包 方法2：可以先对数据进行降维，然后使用非线性核  \n·SVC，C-Support Vector Classification，支持向量分类 ·如果样本数量 $> =$ 特征数  \n·NuSVC，Nu-Support VectorClasification，核支持向量分类，和SVc类 可以使用非线性核，将样本映射到更高维度，可以得到比较好的结果似，不同的是可以使用参数来控制支持向量的个数  \n·LinearSVC， Linear Support Vector Classification",
        "id": 73,
        "type": "text"
      },
      {
        "page": 16,
        "length_tokens": 13,
        "text": "线性支持向量分类，使用的核函数是linear",
        "id": 74,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 67,
        "text": "SVM思想：一些线性不可分的问题可能是非线性可分的，也就是在高维空间中存在分离超平面（separating hyperplane）使用非线性函数从原始的特征空间映射至更高维的空间，转化为线性可分问题",
        "id": 75,
        "type": "text"
      },
      {
        "page": 17,
        "length_tokens": 0,
        "text": "",
        "id": 76,
        "type": "image"
      },
      {
        "page": 18,
        "length_tokens": 3,
        "text": "SVM工具",
        "id": 77,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 8,
        "text": "Step3, 模型参数配置 ",
        "id": 78,
        "type": "text"
      },
      {
        "page": 18,
        "length_tokens": 0,
        "text": "",
        "id": 79,
        "type": "image"
      },
      {
        "page": 18,
        "length_tokens": 0,
        "text": "",
        "id": 80,
        "type": "table"
      },
      {
        "page": 19,
        "length_tokens": 8,
        "text": "Project B： 男女声音识别 ",
        "id": 81,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 4,
        "text": "男女声音识别",
        "id": 82,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 65,
        "text": "·数据集：3168个录制的声音样本（来自男性和女性演讲者），采集的频率范围是0hz-280hz，已经对数据进行了预处理  \n·一共有21个属性值，请判断该声音是男还是女？  \n·使用Accuracy作为评价标准",
        "id": 83,
        "type": "text"
      },
      {
        "page": 19,
        "length_tokens": 2,
        "text": "B ",
        "id": 84,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 8,
        "text": "Project B: 男女声音识别 ",
        "id": 85,
        "type": "text"
      },
      {
        "page": 20,
        "length_tokens": 0,
        "text": "",
        "id": 86,
        "type": "table"
      },
      {
        "page": 20,
        "length_tokens": 0,
        "text": "",
        "id": 87,
        "type": "table"
      },
      {
        "page": 21,
        "length_tokens": 8,
        "text": "Project B： 男女声音识别 ",
        "id": 88,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 4,
        "text": "男女声音识别",
        "id": 89,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 119,
        "text": "·Step1，数据加载  \nStep2，数据预处理  \n分离特征X和Target y  \n使用标签编码，male $- > 1$ ,female $- > 0$   \n将特征X矩阵进行规范化  \n#标准差标准化，处理后的数据符合标准正态分布  \nscaler $\\mathbf { \\tau } = \\mathbf { \\tau }$ StandardScaler()  \nStep3，数据集切分，train_test_split  \nStep4，模型训练  \nSVM， Linear SVM  \nStep5，模型预测",
        "id": 90,
        "type": "text"
      },
      {
        "page": 21,
        "length_tokens": 0,
        "text": "",
        "id": 91,
        "type": "text"
      },
      {
        "page": 22,
        "length_tokens": 4,
        "text": "常用预测模型",
        "id": 92,
        "type": "text"
      },
      {
        "page": 22,
        "length_tokens": 33,
        "text": "数据预处理  \n分类算法：LR，SVM，KNN  \n树模型：GBDT,XGBoost,LightGBM,CatBoost，NGBoost",
        "id": 93,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 9,
        "text": "每种模型都有适用的场景",
        "id": 94,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 4,
        "text": "LR优点：",
        "id": 95,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 38,
        "text": "实现简单，广泛的应用于工业问题上;·分类时计算量非常小，速度很快，使用资源低；·方便观测样本概率分数；",
        "id": 96,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 5,
        "text": "LR缺点: ",
        "id": 97,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 83,
        "text": "·当特征空间很大时，LR的性能不是很好；  \n·容易欠拟合，准确度不太高；  \n·不能很好地处理大量多类特征或变量;  \n·通常只处理二分类问题，多分类需要使用softmax（LR在多分类的推广），且必须线性可分；  \n·对于非线性特征，需要进行转换；",
        "id": 98,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 5,
        "text": "SVM优点：",
        "id": 99,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 58,
        "text": "·可以解决高维问题，即大型特征空间；  \n·能够处理非线性特征的相互作用；  \n·需要先对数据进行归一化，因为计算是基于距离的模型，所以SVM和LR都需要对数据进行归一化处理",
        "id": 100,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 6,
        "text": "SVM缺点: ",
        "id": 101,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 35,
        "text": "·当样本很多时，效率并不是很高；  \n·对非线性问题没有通用解决方案，可能会很难找到合适核函数",
        "id": 102,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 7,
        "text": "对缺失数据敏感；",
        "id": 103,
        "type": "text"
      },
      {
        "page": 23,
        "length_tokens": 31,
        "text": "SVM核的选择是有技巧的，样本数量<特征数，线性核，大于特征数使用非线性核",
        "id": 104,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 9,
        "text": "每种模型都有适用的场景",
        "id": 105,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 141,
        "text": "·可以使用LR模型作为预测的Baseline  \n·FM衍生模型在推荐系统，尤其是CTR预估中有广泛应用，弥补了LR模型的不足（需要人工组合特征，耗费大量时间和人力)  \n·Attention机制，对于Diversity多样性的情况，Attention机制可以提升效率，并且得出更好的结果  \n·TreeEnsemble模型，比如GBDT，使用广泛，因为训练模型更可控  \n对于LR模型，如果欠拟合，需要增加feature，才能提高  \n准确率。而对于tree-ensemble来说，解决方法是训练更  \n多的决策树tree。Kaggle比赛中使用很多",
        "id": 106,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 5,
        "text": "常用预测模型：",
        "id": 107,
        "type": "text"
      },
      {
        "page": 24,
        "length_tokens": 76,
        "text": "·分类算法：LR，SVM，KNN  \n矩阵分解：FunkSVD，BiasSVD，SVD++  \n· FM模型：FM,FFM,DeepFM,NFM，AFM  \n树模型：GBDT,XGBoost, LightGBM,CatBoost，NGBoost  \nAttention模型： DIN,DIEN,DSIN",
        "id": 108,
        "type": "text"
      },
      {
        "page": 25,
        "length_tokens": 6,
        "text": "2/2机器学习神器",
        "id": 109,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 5,
        "text": "什么是集成学习",
        "id": 110,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 4,
        "text": "集成学习：",
        "id": 111,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 31,
        "text": "·思想，将多个弱分类器按照某种方式组合起来，形成一个强分类器（三个臭皮匠赛过诸葛亮）",
        "id": 112,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 54,
        "text": "·Bagging，把数据集通过有放回的抽样方式，划分为多个数据集，分别训练多个模型。针对分类问题，按照少数服从多数原则进行投票，针对回归问题，求多个测试结果的平均值",
        "id": 113,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 51,
        "text": "·Stacking，通常是不同的模型，而且每个分类都用了全部训练数据，得到预测结果y1,y2,.,yk，然后再训练一个分类器 Meta Classifier,将这些预测结果作为输入，得到最终的预测结果",
        "id": 114,
        "type": "text"
      },
      {
        "page": 26,
        "length_tokens": 0,
        "text": "",
        "id": 115,
        "type": "image"
      },
      {
        "page": 27,
        "length_tokens": 5,
        "text": "什么是集成学习",
        "id": 116,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 4,
        "text": "集成学习：",
        "id": 117,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 142,
        "text": "·Boosting，与Bagging一样，使用的相同的弱学习器，不过是以自适应的方法顺序地学习这些弱学习器，即每个新学习器都依赖于前面的模型，并按照某种确定性的策略将它们组合起来  \n·两个重要的Boosting算法：AdaBoost（自适应提升）和Gradient Boosting（梯度提升）  \n·AdaBoost，使用前面的学习器用简单的模型去适配数据，然后分析错误。然后会给予错误预测的数据更高权重，然后用后面的学习器去修复  \n·Boosting通过把一些列的弱学习器串起来，组成一个强学",
        "id": 118,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 2,
        "text": "习器",
        "id": 119,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 0,
        "text": "",
        "id": 120,
        "type": "image"
      },
      {
        "page": 27,
        "length_tokens": 2,
        "text": "268 ",
        "id": 121,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 35,
        "text": "集成学习：训练弱学习器，并添加到集成模型中更新：基于当前的集成学习结果，更新训练集（值或权重）",
        "id": 122,
        "type": "text"
      },
      {
        "page": 27,
        "length_tokens": 0,
        "text": "",
        "id": 123,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 5,
        "text": "什么是集成学习",
        "id": 124,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 7,
        "text": "Boosting与Bagging: ",
        "id": 125,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 166,
        "text": "·结构上，Bagging是基分类器并行处理，而Boosting是串行处理·训练集，Bagging的基分类器训练是独立的，而Boosting的训练集是依赖于之前的模型  \n·作用，Bagging的作用是减少variance，而Boosting在于减少bias对于Bagging，对样本进行重采样，通过重采样得到的子样本集训练模型，最后取平均。因为子样本集的相似性，而且使用相同的弱学习器，因此每个学习器有近似相等的bias和variance，因为每个学习器相互独立，所以可以显著降低variance，但是无法降低bias  \n对于Boosting，采用顺序的方式最小化损失函数，所以bias自然是逐步下",
        "id": 126,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 37,
        "text": "是无法降低bias  \n对于Boosting，采用顺序的方式最小化损失函数，所以bias自然是逐步下降，子模型之和不能显著降低variance",
        "id": 127,
        "type": "text"
      },
      {
        "page": 28,
        "length_tokens": 0,
        "text": "",
        "id": 128,
        "type": "image"
      },
      {
        "page": 29,
        "length_tokens": 3,
        "text": "机器学习神器",
        "id": 129,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 7,
        "text": "GradientBoosting集成学习：",
        "id": 130,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 35,
        "text": "·XGBoost,LightGBM,CatBoost,NGBoost实际上是对GBDT方法的不同实现，针对同一目标、做了不同的优化处理",
        "id": 131,
        "type": "text"
      },
      {
        "page": 29,
        "length_tokens": 0,
        "text": "",
        "id": 132,
        "type": "image"
      },
      {
        "page": 30,
        "length_tokens": 5,
        "text": "XGBoost: ",
        "id": 133,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 16,
        "text": "https://arxiv.0rg/abs/1603.02754 ",
        "id": 134,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 16,
        "text": "· 对于一个问题，INPUT X: age,gender,occupation,… ",
        "id": 135,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 12,
        "text": "Target y: How does the person like computer games? ",
        "id": 136,
        "type": "text"
      },
      {
        "page": 30,
        "length_tokens": 0,
        "text": "",
        "id": 137,
        "type": "image"
      },
      {
        "page": 30,
        "length_tokens": 0,
        "text": "",
        "id": 138,
        "type": "image"
      },
      {
        "page": 30,
        "length_tokens": 10,
        "text": "基学习器，采用CART回归树",
        "id": 139,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 5,
        "text": "XGBoost: ",
        "id": 140,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 23,
        "text": "目标函数 $\\dot { \\bf \\Phi } =$ 损失函数 $^ +$ 正则化项",
        "id": 141,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 39,
        "text": "$$\nO b j ( \\Theta ) = \\underset { \\ Y } { \\cal L } ( \\Theta ) + \\boldsymbol { \\Omega } ( \\Theta )\n$$",
        "id": 142,
        "type": "equation"
      },
      {
        "page": 31,
        "length_tokens": 17,
        "text": "损失函数：拟合数据 正则化项：惩罚复杂模型",
        "id": 143,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 20,
        "text": "·误差函数尽量拟合训练数据，正则化项鼓励简单的模型",
        "id": 144,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 41,
        "text": "· $\\Omega ( f _ { t } )$ 用于控制树的复杂度，防止过拟合，使得模型更简化，也使得最终的模型的预测结果更稳定",
        "id": 145,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 61,
        "text": "$$\n\\Omega \\big ( f _ { t } \\big ) = \\gamma T + \\frac { 1 } { 2 } \\lambda \\sum _ { j = 1 } ^ { T } w _ { \\underbrace { j } } ^ { 2 }\n$$",
        "id": 146,
        "type": "equation"
      },
      {
        "page": 31,
        "length_tokens": 19,
        "text": "T：叶子数量 wj：叶子分数的L2正则项 ",
        "id": 147,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 14,
        "text": "γ：加入新叶子节点引入的复杂度代价",
        "id": 148,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 46,
        "text": "$f _ { t } \\left( x \\right) = w _ { q \\left( x \\right) } \\quad \\mathsf { w }$ 代表叶子向量，q表示树的结构",
        "id": 149,
        "type": "text"
      },
      {
        "page": 31,
        "length_tokens": 0,
        "text": "",
        "id": 150,
        "type": "image"
      },
      {
        "page": 31,
        "length_tokens": 44,
        "text": "$$\n\\Omega = \\gamma \\times 3 + \\frac { 1 } { 2 } \\lambda \\big ( 4 + 0 . 0 1 + 1 \\big )\n$$",
        "id": 151,
        "type": "equation"
      },
      {
        "page": 32,
        "length_tokens": 7,
        "text": "TreeEnsemble集成学习：",
        "id": 152,
        "type": "text"
      },
      {
        "page": 32,
        "length_tokens": 89,
        "text": "单个CART回归树过于简单，可以通过多个CART回归树组成一个强学习器·预测函数，样本的预测结果 $\\mathbf { \\Psi } =$ 每棵树预测分数之和$\\hat { y } _ { i } = \\sum _ { k = 1 } ^ { K } f _ { k } \\big ( x _ { i } \\big )$ ",
        "id": 153,
        "type": "text"
      },
      {
        "page": 32,
        "length_tokens": 0,
        "text": "",
        "id": 154,
        "type": "image"
      },
      {
        "page": 32,
        "length_tokens": 3,
        "text": "目标函数优化",
        "id": 155,
        "type": "text"
      },
      {
        "page": 32,
        "length_tokens": 125,
        "text": "$$\n\\begin{array} { c } { { O b j \\displaystyle \\big ( \\Theta \\big ) = \\sum _ { i } l \\big ( y _ { i } , \\hat { y } _ { i } \\big ) + \\sum _ { k } \\Omega \\big ( f _ { k } \\big ) } } \\\\ { { \\Omega \\big ( f \\big ) = \\gamma T + \\displaystyle \\frac { 1 } { 2 } \\lambda \\| w \\| ^ { 2 } } } \\end{array}\n$$",
        "id": 156,
        "type": "equation"
      },
      {
        "page": 32,
        "length_tokens": 22,
        "text": "正则项是由叶子结点的数量和叶子结点权重的平方和决定",
        "id": 157,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 7,
        "text": "XGBoost的目标函数：",
        "id": 158,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 99,
        "text": "$$\nO b j ^ { t } = \\sum _ { i = 1 } ^ { n } l \\big ( y _ { i } , \\hat { y } _ { i } ^ { ( t - 1 ) } + f _ { t } \\big ( x _ { i } \\big ) \\big ) + \\Omega \\big ( f _ { t } \\big ) + \\mathrm { c o n s t a n t }\n$$",
        "id": 159,
        "type": "equation"
      },
      {
        "page": 33,
        "length_tokens": 15,
        "text": "·对目标函数改进，进行二阶泰勒展开： ",
        "id": 160,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 92,
        "text": "$$\nf { \\big ( } x + \\Delta x { \\big ) } \\approx f { \\big ( } x { \\big ) } + f ^ { \\prime } { \\big ( } x { \\big ) } \\Delta x + { \\frac { 1 } { 2 } } f ^ { \\prime } { \\big ( } x { \\big ) } \\Delta x ^ { 2 }\n$$",
        "id": 161,
        "type": "equation"
      },
      {
        "page": 33,
        "length_tokens": 3,
        "text": "·定义 ",
        "id": 162,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 0,
        "text": "",
        "id": 163,
        "type": "image"
      },
      {
        "page": 33,
        "length_tokens": 186,
        "text": "$$\n\\begin{array} { r l } & { \\hat { y } _ { i } ^ { ( 0 ) } = 0 } \\\\ & { \\hat { y } _ { i } ^ { ( 1 ) } = f _ { 1 } ( x _ { i } ) = \\hat { y } ^ { ( 0 ) } + f _ { 1 } ( x _ { i } ) } \\\\ & { \\hat { y } _ { i } ^ { ( 2 ) } = f _ { 1 } ( x _ { i } ) + f _ { 2 } ( x _ { i } ) = \\hat { y } _ { i } ^ { ( 1 ) } + f _ { 2 } ( x _ { i } ) } \\\\ & { \\cdots } \\end{array}\n$$",
        "id": 164,
        "type": "equation"
      },
      {
        "page": 33,
        "length_tokens": 80,
        "text": "$$\n\\hat { \\cal y } _ { i } ^ { ( t ) } = \\sum _ { k = 1 } ^ { t } f _ { k } ( x _ { i } ) = \\hat { \\cal y } _ { i } ^ { ( t - 1 ) } + f _ { t } ( x _ { i } )\n$$",
        "id": 165,
        "type": "equation"
      },
      {
        "page": 33,
        "length_tokens": 15,
        "text": "保留前t-1轮的模型预测 加入新的预测函数",
        "id": 166,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 10,
        "text": "如何选择每一轮的预测函数f？",
        "id": 167,
        "type": "text"
      },
      {
        "page": 33,
        "length_tokens": 24,
        "text": "选取一个f来使得目标函数尽量降低，即加入f后的预测结果与实际结果误差减少",
        "id": 168,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 7,
        "text": "XGBoost的目标函数：",
        "id": 169,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 384,
        "text": "$$\n\\begin{array} { l } { { \\displaystyle O b j ^ { \\prime } \\approx \\sum _ { i = 1 } ^ { n } \\biggl [ g _ { i } f _ { i } \\biggl ( x _ { i } \\biggr ) + \\frac { 1 } { 2 } h _ { i } f _ { i } ^ { 2 } \\bigl ( x _ { i } \\bigr ) \\biggr ] + \\Omega \\bigl ( f _ { i } \\bigr ) } } \\\\ { { \\displaystyle \\qquad = \\sum _ { i = 1 } ^ { n } \\biggl [ g _ { i } ^ { \\mathrm { \\tiny \\dag } } w _ { q ( x _ { i } ) } + \\frac { 1 } { 2 } h _ { i } w _ { q ( x _ { i } ) } ^ { 2 } \\biggr ] + \\gamma + \\lambda \\frac { 1 } { 2 } \\sum _ { j = 1 } ^ { T } w _ { j } ^ { 2 } } } \\\\ { { \\displaystyle \\qquad = \\sum _ { j = 1 } ^ { T } \\biggl [ \\biggl ( \\sum _ { i \\in I _ { i } } g _ { i } \\biggr ) w _ { j } + \\frac { 1 } { 2 } \\biggl ( \\sum _ { i \\in I _ { i } } h _ { i } + \\lambda \\biggr ) w _ { j } ^ { 2 } \\biggr ] + \\gamma { \\cal T } } } \\end{array}\n$$",
        "id": 170,
        "type": "equation"
      },
      {
        "page": 34,
        "length_tokens": 7,
        "text": "·T为叶子节点数量",
        "id": 171,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 113,
        "text": "$I _ { j }$ 定义为每个叶子节点里面的样本集合 $I _ { j } = \\left\\{ i \\vert q ( x _ { i } ) = j \\right\\}$ $f _ { t } \\left( x _ { i } \\right) = w _ { q ( x _ { i } ) }$ 即每个样本所在叶子节点索引的分数（叶子权重 $\\boldsymbol { \\mathsf { w } } )$ （204号",
        "id": 172,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 35,
        "text": "$G _ { j } , H _ { j }$ 分别表示每个叶子节点的一阶梯度的和，与二阶梯度的和：",
        "id": 173,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 84,
        "text": "$$\n\\begin{array} { c } { { G _ { j } = \\displaystyle \\sum _ { i \\in I _ { j } } g _ { i } } } \\\\ { { { } } } \\\\ { { H _ { j } = \\displaystyle \\sum _ { i \\in I _ { j } } h _ { i } } } \\end{array}\n$$",
        "id": 174,
        "type": "equation"
      },
      {
        "page": 34,
        "length_tokens": 6,
        "text": "目标函数改写为：",
        "id": 175,
        "type": "text"
      },
      {
        "page": 34,
        "length_tokens": 207,
        "text": "$$\n\\begin{array} { l } { { \\displaystyle O b j ^ { t } = \\sum _ { j = 1 } ^ { T } \\left[ \\left( \\sum _ { i \\in I _ { j } } g _ { i } \\right) w _ { j } + \\frac { 1 } { 2 } \\left( \\sum _ { i \\in I _ { j } } h _ { i } + \\lambda \\right) w _ { j } ^ { 2 } \\right] + \\gamma T } } \\\\ { { \\displaystyle \\ = \\sum _ { j = 1 } ^ { T } \\left[ G _ { j } w _ { j } + \\frac { 1 } { 2 } \\big ( H _ { j } + \\lambda \\big ) w _ { j } ^ { 2 } \\right] + \\gamma T } } \\end{array}\n$$",
        "id": 176,
        "type": "equation"
      },
      {
        "page": 34,
        "length_tokens": 175,
        "text": "$$\n\\begin{array} { r l } { { } } & { { \\displaystyle \\frac { \\partial O b j } { \\partial w _ { j } } = G _ { j } + \\big ( H _ { j } + \\lambda \\big ) w _ { j } = 0 } } \\\\ { { } } & { { w _ { j } = - \\displaystyle \\frac { G _ { j } } { H _ { j } + \\lambda } \\quad O b j = - \\displaystyle \\frac { 1 } { 2 } \\sum _ { j = 1 } ^ { T } \\displaystyle \\frac { G _ { j } ^ { 2 } } { H _ { j } + \\lambda } + \\gamma T } } \\end{array}\n$$",
        "id": 177,
        "type": "equation"
      },
      {
        "page": 35,
        "length_tokens": 7,
        "text": "XGBoost的目标函数：",
        "id": 178,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 31,
        "text": "·Obj目标函数也称为结构分数（打分函数），代表当指定一个树的结构的时候，我们在目标上最多可以减少多少",
        "id": 179,
        "type": "text"
      },
      {
        "page": 35,
        "length_tokens": 0,
        "text": "",
        "id": 180,
        "type": "image"
      },
      {
        "page": 36,
        "length_tokens": 7,
        "text": "XGBoost的目标函数：",
        "id": 181,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 21,
        "text": "·求Obj分数最小的树结构，可以穷举所有可能，但计算量太大",
        "id": 182,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 60,
        "text": "$$\nO b j = - \\frac { 1 } { 2 } \\sum _ { j = 1 } ^ { T } \\frac { G _ { j } ^ { 2 } } { H _ { j } + \\lambda } + \\gamma T\n$$",
        "id": 183,
        "type": "equation"
      },
      {
        "page": 36,
        "length_tokens": 16,
        "text": "·使用贪心法，即利用打分函数（计算增益）",
        "id": 184,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 39,
        "text": "以Gain作为是否分割的条件，Gain看作是未分割前的Obj减去分割后的左右Obj，加入新叶子节点引入的复杂度代价",
        "id": 185,
        "type": "text"
      },
      {
        "page": 36,
        "length_tokens": 0,
        "text": "",
        "id": 186,
        "type": "image"
      },
      {
        "page": 36,
        "length_tokens": 39,
        "text": "如果 $\\mathsf { G a i n } { < } 0$ ，则此叶节点不做分割，分割方案个数很多，计算量依然很大",
        "id": 187,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 9,
        "text": "XGBoost的分裂节点算法：",
        "id": 188,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 125,
        "text": "·贪心方法，获取最优分割节点（splitpoint）  \n将所有样本按照gi从小到大排序，通过遍历，查看每个  \n节点是否需要分割  \n·对于特征值的个数为n时，总共有n-1种划分  \n·Step1，对样本扫描一遍，得出GL，GR  \n·Step2，根据Gain的分数进行分割通过贪心法，计算效率得到大幅提升，XGBoost重新定义划分属性，即Gain，而Gain的计算是由目标损失函数obj决定的",
        "id": 189,
        "type": "text"
      },
      {
        "page": 37,
        "length_tokens": 0,
        "text": "",
        "id": 190,
        "type": "table"
      },
      {
        "page": 37,
        "length_tokens": 0,
        "text": "",
        "id": 191,
        "type": "table"
      },
      {
        "page": 38,
        "length_tokens": 19,
        "text": "XGBoost的分裂节点算法（近似算法，Histogram 2016 paper）：",
        "id": 192,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 200,
        "text": "·对于连续型特征值，样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合  \n·方法，在寻找split节点的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，然后形成若干个bucket(桶)，只将bucket边界上的特征值作为split节点的候选，从而获得性能提升  \n·从算法伪代码中该流程还可以分为两种，全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部近似就是在具体的某一次分裂节点的过程中采用近似算法  \nfor $k = 1$ to m doPropose $S _ { k } = \\{ s _ { k 1",
        "id": 193,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 127,
        "text": "r $k = 1$ to m doPropose $S _ { k } = \\{ s _ { k 1 } , s _ { k 2 } , \\cdot \\cdot \\cdot s _ { k l } \\}$ by percentiles on feature $k$ Proposal can be done per tree (global), or per split(local).  \nend  \nfor $k = 1$ to m do（20 $\\begin{array} { r } { G _ { k v }  = \\sum _ { j \\in \\{ j | s _ { k , v } \\",
        "id": 194,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 137,
        "text": "_ { k v }  = \\sum _ { j \\in \\{ j | s _ { k , v } \\geq \\mathbf { x } _ { j k } > s _ { k , v - 1 } \\} } g _ { j } } \\\\ { H _ { k v }  = \\sum _ { j \\in \\{ j | s _ { k , v } \\geq \\mathbf { x } _ { j k } > s _ { k , v - 1 } \\} } h _ { j } } \\end{array}$ （204号  \nend  \nFollow same step as in previous sect",
        "id": 195,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 23,
        "text": "（204号  \nend  \nFollow same step as in previous section to find max  \nscore only among proposed splits.",
        "id": 196,
        "type": "text"
      },
      {
        "page": 38,
        "length_tokens": 0,
        "text": "",
        "id": 197,
        "type": "text"
      },
      {
        "page": 39,
        "length_tokens": 6,
        "text": "XGBoost算法特点：",
        "id": 198,
        "type": "text"
      },
      {
        "page": 39,
        "length_tokens": 213,
        "text": "·XGBoost将树模型的复杂度加入到正则项中，从而避免过拟合，泛化性能好  \n·损失函数是用泰勒展开式展开的，用到了一阶导和二阶导，可以加快优化速度  \n·在寻找最佳分割点时，采用近似贪心算法，用来加速计算  \n·不仅支持CART作为基分类器，还支持线性分类器，在使用线性分类器的时候可以使用L1，L2正则化  \n·支持并行计算，XGBoost的并行是基于特征计算的并行，将特征列排序后以block的形式存储在内存中，在后面的迭代中重复使用这个结构。在进行节点分裂时，计算每个特征的增益，选择增益最大的特征作为分割节点，各个特征的增益计算可以使用多线程并行  \n·优点：速度快、效果好、能处理大规模",
        "id": 199,
        "type": "text"
      },
      {
        "page": 39,
        "length_tokens": 68,
        "text": "大的特征作为分割节点，各个特征的增益计算可以使用多线程并行  \n·优点：速度快、效果好、能处理大规模数据、支持自定义损失函数等  \n·缺点：算法参数过多，调参复杂，不适合处理超高维特征数据",
        "id": 200,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 4,
        "text": "XGBoost工具",
        "id": 201,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 6,
        "text": "XGBoost工具: ",
        "id": 202,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 49,
        "text": "https://github.com/dmlc/xgboost  \n参数分为：  \n通用参数：对系统进行控制Booster参数：控制每一步的booster(tree/regression)  \n·学习目标参数：控制训练目标的表现",
        "id": 203,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 4,
        "text": "通用参数：",
        "id": 204,
        "type": "text"
      },
      {
        "page": 40,
        "length_tokens": 142,
        "text": "·booster，模型选择，gbtree或者gblinear。gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算。[default $\\mathop { \\bf { \\phi } } =$ gbtree]  \n·silent，缄默方式，0表示打印运行时，1表示以缄默方式运行，不打印运行时信息。[default $\\scriptstyle : = 0 ]$   \n·nthread，XGBoost运行时的线程数，[default=缺省值是当前系统可以获得的最大线程数]  \n·num_feature，boosting过程中用到的特征个数，XGBoost会自动设置",
        "id": 205,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 4,
        "text": "XGBoost工具",
        "id": 206,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 4,
        "text": "Booster参数：",
        "id": 207,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 80,
        "text": "·eta [default $\\left. = 0 . 3 \\right]$ 1，为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。eta通过缩减特征的权重使提升计算过程更加保守，取值范围为[0,1]",
        "id": 208,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 78,
        "text": "·gamma [default $\\left[ = 0 \\right]$ ，分裂节点时，损失函数减小值只有大于等于gamma节点才分裂，gamma值越大，算法越保守，越不容易过拟合，但性能就不一定能保证，需要tradeoff，取值范围 $[ 0 , \\infty ]$ ",
        "id": 209,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 49,
        "text": "· max_depth [default $\\mathrel { \\mathop = } 6$ 5]，树的最大深度，取值范围为 $[ 1 , \\infty ]$ ，典型值为3-10",
        "id": 210,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 85,
        "text": "·min_child_weight[default $_ { : = 1 ] }$ ，一个子集的所有观察值的最小权重和。如果新分裂的节点的样本权重和小于min_child_weight则停止分裂。这个可以用来减少过拟合，但是也不能太高，会导致欠拟合，取值范围为 $[ 0 , \\infty ]$ ",
        "id": 211,
        "type": "text"
      },
      {
        "page": 41,
        "length_tokens": 137,
        "text": "$$\nG a i n = { \\frac { 1 } { 2 } } \\Biggl [ { \\frac { G _ { L } ^ { 2 } } { H _ { L } + \\lambda } } + { \\frac { G _ { R } ^ { 2 } } { H _ { R } + \\lambda } } - { \\frac { ( G _ { L } + G _ { R } ) ^ { 2 } } { H _ { L } + H _ { R } + [ \\lambda ] } } \\Biggr ] - [ \\gamma ] \\Biggr \\} \\quad \n$$",
        "id": 212,
        "type": "equation"
      },
      {
        "page": 42,
        "length_tokens": 4,
        "text": "XGBoost工具",
        "id": 213,
        "type": "text"
      },
      {
        "page": 42,
        "length_tokens": 4,
        "text": "Booster参数：",
        "id": 214,
        "type": "text"
      },
      {
        "page": 42,
        "length_tokens": 146,
        "text": "·subsample[default $: = 1$ 1，构建每棵树对样本的采样率，如果设置成0.5，XGBoost会随机选择 $50 \\%$ 的样本作为训练集  \n·colsample_bytree [default $\\scriptstyle : = 1 ]$ ，列采样率，也就是特征采样率  \n· lambda[default ${ \\tt \\Psi } = 1 { \\tt \\Psi }$ ,alias:reg_lambda]，L2正则化，用来控制XGBoost的正则化部分  \n·alpha[default ${ \\bf \\bar { \\Lambda } } { \\bf \\Lambda }",
        "id": 215,
        "type": "text"
      },
      {
        "page": 42,
        "length_tokens": 82,
        "text": "[default ${ \\bf \\bar { \\Lambda } } { \\bf \\Lambda } = 0$ ,alias:reg_alpha]，L1正则化，增加该值会让模型更加收敛  \n·scale_pos_weight[default $\\scriptstyle : = 1 ]$ ，在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛",
        "id": 216,
        "type": "text"
      },
      {
        "page": 43,
        "length_tokens": 4,
        "text": "XGBoost工具",
        "id": 217,
        "type": "text"
      },
      {
        "page": 43,
        "length_tokens": 4,
        "text": "学习目标参数：",
        "id": 218,
        "type": "text"
      },
      {
        "page": 43,
        "length_tokens": 118,
        "text": "·objective[default $\\mathbf { \\Psi } : =$ reg:linear]，定义学习目标，reg:linear，reg:logistic，binary:logistic，binary:logitraw，count:poisson，multi:softmax,multi:softprob， rank:pairwise  \n·eval_metric，评价指标，包括rmse，logloss，error，merror，mlogloss，auc，ndcg，map等  \n·seed[ default $_ { : = 0 }$ ]，随机数的种子  \n·dtrain，训练的数",
        "id": 219,
        "type": "text"
      },
      {
        "page": 43,
        "length_tokens": 139,
        "text": "eed[ default $_ { : = 0 }$ ]，随机数的种子  \n·dtrain，训练的数据  \n·num_boost_round，提升迭代的次数，也就是生成多少基模型  \n·early_stopping_rounds，早停法迭代次数  \n·evals：这是一个列表，用于对训练过程中进行评估列表中的元素。形式是evals $\\mathbf { \\tau } = \\mathbf { \\tau }$ [(dtrain,'train'),(dval,'val')]或者是ev $\\mathsf { a l s } = [ ( \\mathsf { d t r a i n } , ^ { \\pr",
        "id": 220,
        "type": "text"
      },
      {
        "page": 43,
        "length_tokens": 110,
        "text": " { a l s } = [ ( \\mathsf { d t r a i n } , ^ { \\prime } \\mathsf { t r a i n } ^ { \\prime } )$ ］对于第一种情况，它使得我们可以在训练过程中观察验证集的效果  \n·verbose_eval，如果为True，则对evals中元素的评估输出在结果中；如果输入数字，比如5，则每隔5个迭代输出一次  \n·learning_rates：每一次提升的学习率的列表",
        "id": 221,
        "type": "text"
      },
      {
        "page": 44,
        "length_tokens": 4,
        "text": "XGBoost工具",
        "id": 222,
        "type": "text"
      },
      {
        "page": 44,
        "length_tokens": 15,
        "text": "#天猫用户复购预测（XGBoost使用示意）",
        "id": 223,
        "type": "text"
      },
      {
        "page": 44,
        "length_tokens": 119,
        "text": "X_train,X_valid,y_train,Y_valid $\\mathbf { \\tau } = \\mathbf { \\tau }$ train_test_split(train_X,train_y,test_size=.2)  \n#使用XGBoost  \nmodel $\\mathbf { \\tau } = \\mathbf { \\tau }$ xgb.XGBClassifier(max_depth $^ { = 8 }$ ，#树的最大深度n_estimators $_ { \\mathsf { 3 } } = 1 0 0 0$ ，#提升迭代的次数，也就是生成多少基模型min_child_w",
        "id": 224,
        "type": "text"
      },
      {
        "page": 44,
        "length_tokens": 171,
        "text": "{ 3 } } = 1 0 0 0$ ，#提升迭代的次数，也就是生成多少基模型min_child_weight $: = 3 0 0$ ,#一个子集的所有观察值的最小权重和colsample_bytree=0.8,#列采样率，也就是特征采样率subsample $\\scriptstyle : = 0 . 8$ ，#构建每棵树对样本的采样率eta $= 0 . 3$ ，#eta通过缩减特征的权重使提升计算过程更加保守，防止过拟合seed=42 #随机数种子  \n）  \nmodel.fit(X_train, y_train,eval_metric $= ^ { \\prime }$ auc',eva",
        "id": 225,
        "type": "text"
      },
      {
        "page": 44,
        "length_tokens": 98,
        "text": "ain, y_train,eval_metric $= ^ { \\prime }$ auc',eval_set $\\ c =$ [(X_train,y_train),(X_valid,y_valid)],verbose=True,#早停法，如果auc在10epoch没有进步就stopearly_stopping_round $_ { : = 1 0 }$   \n】  \nmodel.fit(X_train, y_train)  \nprob $\\mathbf { \\tau } = \\mathbf { \\tau }$ model.predict_proba(test_data)",
        "id": 226,
        "type": "text"
      },
      {
        "page": 44,
        "length_tokens": 0,
        "text": "",
        "id": 227,
        "type": "text"
      },
      {
        "page": 45,
        "length_tokens": 4,
        "text": "XGBoost工具",
        "id": 228,
        "type": "text"
      },
      {
        "page": 45,
        "length_tokens": 8,
        "text": "Step3, 模型参数配置 ",
        "id": 229,
        "type": "text"
      },
      {
        "page": 45,
        "length_tokens": 133,
        "text": "param $\\mathbf { \\tau } = \\mathbf { \\tau }$ {'boosting_type':'gbdt', 'objective' :'binary:logistic',#任务目标 'eval_metric':'auc',#评估指标 'eta' :0.01,#学习率 'max_depth':15,#树最大深度 'colsample_bytree':0.8,#设置在每次迭代中使用特征的比例 'subsample':0.9,#样本采样比例 'subsample_freq':8,#bagging的次数 'alpha': 0.6,#L1正则 'lambda': 0,#L2",
        "id": 230,
        "type": "text"
      },
      {
        "page": 45,
        "length_tokens": 30,
        "text": "':8,#bagging的次数 'alpha': 0.6,#L1正则 'lambda': 0,#L2正则 ",
        "id": 231,
        "type": "text"
      },
      {
        "page": 46,
        "length_tokens": 4,
        "text": "XGBoost工具",
        "id": 232,
        "type": "text"
      },
      {
        "page": 46,
        "length_tokens": 11,
        "text": "Step4, 模型训练，得出预测结果",
        "id": 233,
        "type": "text"
      },
      {
        "page": 46,
        "length_tokens": 104,
        "text": "X_train, X_valid, y_train, y_valid $\\mathbf { \\tau } = \\mathbf { \\tau }$ train_test_split(train.drop('Attrition',axis $_ { \\cdot = 1 }$ ), train['Attrition'],   \ntest_size ${ \\tt \\Psi } = 0 . 2$ ,random_state $= 4 2$ ）   \ntrain_data $\\mathbf { \\tau } = \\mathbf { \\tau }$ xgb.DMatrix(X_train,label=y_t",
        "id": 234,
        "type": "text"
      },
      {
        "page": 46,
        "length_tokens": 98,
        "text": " = \\mathbf { \\tau }$ xgb.DMatrix(X_train,label=y_train)   \nvalid_data $\\mathbf { \\tau } = \\mathbf { \\tau }$ xgb.DMatrix(X_valid, label=y_valid)   \ntest_data $\\mathbf { \\tau } = \\mathbf { \\tau }$ xgb.DMatrix(test)   \nmodel $\\mathbf { \\tau } = \\mathbf { \\tau }$ xgb.train(param, train_data, evals $\\ c ",
        "id": 235,
        "type": "text"
      },
      {
        "page": 46,
        "length_tokens": 108,
        "text": "{ \\tau }$ xgb.train(param, train_data, evals $\\ c =$ [(train_data,'train'),(valid_data,'valid')],   \nnum_boost_round $= 1 0 0 0 0$ ,early_stopping_round $s = 2 0 0$ ,verbose_eva $1 = 2 5$ ）   \npredict $\\mathbf { \\tau } = \\mathbf { \\tau }$ model.predict(test_data)   \ntest['Attrition'] $\\ c =$ predict",
        "id": 236,
        "type": "text"
      },
      {
        "page": 46,
        "length_tokens": 69,
        "text": "ct(test_data)   \ntest['Attrition'] $\\ c =$ predict   \n#转化为二分类输出   \ntest['Attrition']=test['Attrition'].map(lambda x:1 if $x > = 0 . 5$ else 0)   \ntest[['Attrition']].to_csv('submit_lgb.csv') ",
        "id": 237,
        "type": "text"
      },
      {
        "page": 46,
        "length_tokens": 0,
        "text": "",
        "id": 238,
        "type": "table"
      },
      {
        "page": 47,
        "length_tokens": 5,
        "text": "LightGBM: ",
        "id": 239,
        "type": "text"
      },
      {
        "page": 47,
        "length_tokens": 60,
        "text": "2017年经微软推出，XGBoost的升级版  \nKaggle竞赛使用最多的模型之一，必备机器学习神器  \nLight $\\Rightarrow$ 在大规模数据集上运行效率更高（20 $G B M = 2$ Gradient Boosting Machine",
        "id": 240,
        "type": "text"
      },
      {
        "page": 48,
        "length_tokens": 4,
        "text": "Motivation: ",
        "id": 241,
        "type": "text"
      },
      {
        "page": 48,
        "length_tokens": 142,
        "text": "·常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制  \n·GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。对于工业级海量的数据，普通的GBDT算法是不能满足其需求的  \n·LightGBM的提出是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业场景",
        "id": 242,
        "type": "text"
      },
      {
        "page": 48,
        "length_tokens": 0,
        "text": "",
        "id": 243,
        "type": "table"
      },
      {
        "page": 48,
        "length_tokens": 0,
        "text": "",
        "id": 244,
        "type": "table"
      },
      {
        "page": 49,
        "length_tokens": 9,
        "text": "LightGBM与XGBoost: ",
        "id": 245,
        "type": "text"
      },
      {
        "page": 49,
        "length_tokens": 100,
        "text": "·模型精度：两个模型相当训练速度：LightGBM训练速度更快 $\\Rightarrow 1 / 1 0$ ·内存消耗：LightGBM占用内存更小 $\\Rightarrow 1 / 6$ ·特征缺失值：两个模型都可以自动处理特征缺失值·分类特征：XGBoost不支持类别特征，需要对其进行OneHot编码，而LightGBM支持分类特征",
        "id": 246,
        "type": "text"
      },
      {
        "page": 49,
        "length_tokens": 0,
        "text": "",
        "id": 247,
        "type": "image"
      },
      {
        "page": 50,
        "length_tokens": 8,
        "text": "XGBoost模型的复杂度：",
        "id": 248,
        "type": "text"
      },
      {
        "page": 50,
        "length_tokens": 161,
        "text": "·模型复杂度 $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ 树的棵数×每棵树的叶子数量×每片叶子生成复杂度   \n·每片叶子生成复杂度 $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ 特征数量 $\\mathsf { x }$ 候选分裂点数量×样本的数量   \n针对XGBoost的优化：   \n·Histogram算法，直方图算法 $\\Rightarrow$ 减少候选分裂点数量   \n·GOSS算法，基于梯度的单边采样算法 $\\Rightarrow$ 减少样本的数量   \n·EFB算法，互斥特征捆绑算法 $\\Rightar",
        "id": 249,
        "type": "text"
      },
      {
        "page": 50,
        "length_tokens": 72,
        "text": "$\\Rightarrow$ 减少样本的数量   \n·EFB算法，互斥特征捆绑算法 $\\Rightarrow$ 减少特征的数量   \n· LightGBM $\\mathbf { \\tau } = \\mathbf { \\tau }$ XGBoost $^ +$ Histogram $+$ GOSS + EFB ",
        "id": 250,
        "type": "text"
      },
      {
        "page": 50,
        "length_tokens": 0,
        "text": "",
        "id": 251,
        "type": "image"
      },
      {
        "page": 51,
        "length_tokens": 13,
        "text": "XGBoost的预排序（pre-sorted）算法：",
        "id": 252,
        "type": "text"
      },
      {
        "page": 51,
        "length_tokens": 71,
        "text": "·将样本按照特征取值排序，然后从全部特征取值中找到最优的分裂点位·预排序算法的候选分裂点数量 $\\mathbf { \\bar { \\rho } } = \\mathbf { \\bar { \\rho } }$ 样本特征不同取值个数减1",
        "id": 253,
        "type": "text"
      },
      {
        "page": 51,
        "length_tokens": 0,
        "text": "",
        "id": 254,
        "type": "table"
      },
      {
        "page": 51,
        "length_tokens": 146,
        "text": "$$\nG a i n { = } \\frac { 1 } { 2 } \\Biggl [ \\frac { G _ { { L } } ^ { 2 } } { H _ { { L } } + \\lambda } { + } \\frac { G _ { { R } } ^ { 2 } } { H _ { { R } } + \\lambda } { - } \\frac { \\left( G _ { { L } } + G _ { { R } } \\right) ^ { 2 } } { H _ { { L } } + H _ { { R } } + \\lambda } \\Biggr ] { - } \\gamma\n$$",
        "id": 255,
        "type": "equation"
      },
      {
        "page": 52,
        "length_tokens": 7,
        "text": "LightGBM的Histogram算法：",
        "id": 256,
        "type": "text"
      },
      {
        "page": 52,
        "length_tokens": 9,
        "text": "替代XGBoost的预排序算法",
        "id": 257,
        "type": "text"
      },
      {
        "page": 52,
        "length_tokens": 135,
        "text": "思想是先连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图，即将连续特征值离散化到k个bins上（比如 $k = 2 5 5$ ），当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点XGBoost需要遍历所有离散化的值，LightGBM只要遍历k个直方图的值·候选分裂点数量 $= k - 1$ ",
        "id": 258,
        "type": "text"
      },
      {
        "page": 52,
        "length_tokens": 0,
        "text": "",
        "id": 259,
        "type": "table"
      },
      {
        "page": 52,
        "length_tokens": 0,
        "text": "",
        "id": 260,
        "type": "image"
      },
      {
        "page": 52,
        "length_tokens": 0,
        "text": "",
        "id": 261,
        "type": "table"
      },
      {
        "page": 53,
        "length_tokens": 4,
        "text": "GOSS算法：",
        "id": 262,
        "type": "text"
      },
      {
        "page": 53,
        "length_tokens": 122,
        "text": "·Gradient-based One-Side Sampling，基于梯度的单边采样算法  \n思想是通过样本采样，减少目标函数增益Gain的计算复杂度  \n·单边采样，只对梯度绝对值较小的样本按照一定比例进行采样，而保留了梯度绝对值较大的样本  \n·因为目标函数增益主要来自于梯度绝对值较大的样本 $\\Rightarrow { \\mathsf { G O S S } }$ 算法在性能和精度之间进行了很好的tradeoff",
        "id": 263,
        "type": "text"
      },
      {
        "page": 53,
        "length_tokens": 0,
        "text": "",
        "id": 264,
        "type": "table"
      },
      {
        "page": 53,
        "length_tokens": 72,
        "text": "gi：梯度绝对值，假设阈值为0.1对gi $< 0 . 1$ 的样本 $\\Rightarrow 1 / 3$ 概率进行采样对gi> $_ { \\cdot = 0 . 1 }$ 的样本 $\\Rightarrow$ 全部保留",
        "id": 265,
        "type": "text"
      },
      {
        "page": 53,
        "length_tokens": 2,
        "text": "V ",
        "id": 266,
        "type": "text"
      },
      {
        "page": 53,
        "length_tokens": 0,
        "text": "",
        "id": 267,
        "type": "table"
      },
      {
        "page": 54,
        "length_tokens": 4,
        "text": "EFB算法：",
        "id": 268,
        "type": "text"
      },
      {
        "page": 54,
        "length_tokens": 197,
        "text": "·ExclusiveFeatureBundling，互斥特征绑定算法  \n·思想是特征中包含大量稀疏特征的时候，减少构建直方图的特征数量，从而降低计算复杂度  \n·数据集中通常会有大量的稀疏特征（大部分为0，少量为非0）我们认为这些稀疏特征是互斥的，即不会同时取非零值  \n·EFB算法可以通过对某些特征的取值重新编码，将多个这样互斥的特征绑定为一个新的特征  \n·类别特征可以转换成onehot编码，这些多个特征的onehot编码是互斥的，可以使用EFB将他们绑定为一个特征  \n·在LightGBM中，可以直接将每个类别取值和一个bin关联，从而自动地处理它们，也就无需预处理成onehot编码",
        "id": 269,
        "type": "text"
      },
      {
        "page": 54,
        "length_tokens": 0,
        "text": "",
        "id": 270,
        "type": "image"
      },
      {
        "page": 55,
        "length_tokens": 5,
        "text": "LightGBM工具：",
        "id": 271,
        "type": "text"
      },
      {
        "page": 55,
        "length_tokens": 28,
        "text": "import lightgbm as lgb   \n官方文档:http://lightgbm.readthedocs.io/en/latest/PythonIntro.html ",
        "id": 272,
        "type": "text"
      },
      {
        "page": 55,
        "length_tokens": 2,
        "text": "参数:",
        "id": 273,
        "type": "text"
      },
      {
        "page": 55,
        "length_tokens": 77,
        "text": "boosting_type，训练方式，gbdt  \n。objective，目标函数，可以是binary，regressionmetric，评估指标，可以选择auc,mae，mse，binary_logloss,multi_loglossmax_depth，树的最大深度，当模型过拟合时，可以降低max_depth  \nmindata in leaf，叶子节点最小记录数，默认20",
        "id": 274,
        "type": "text"
      },
      {
        "page": 55,
        "length_tokens": 16,
        "text": "Bagging参数：bagging_fraction+bagging_freq（需要同时设置）",
        "id": 275,
        "type": "text"
      },
      {
        "page": 55,
        "length_tokens": 129,
        "text": "·bagging_fraction，每次迭代时用的数据比例，用于加快训练速度和减小过拟合  \nbagging_freq：bagging的次数。默认为o，表示禁用bagging，非零值表示执行k次bagging，可以设置为3-5  \nfeature_fraction，设置在每次迭代中使用特征的比例，例如为0.8时，意味着在每次迭代中随机选择 $80 \\%$ 的参数来建树  \nearly_stopping_round，如果一次验证数据的一个度量在最近的round中没有提高，模型将停止训练",
        "id": 276,
        "type": "text"
      },
      {
        "page": 56,
        "length_tokens": 2,
        "text": "参数:",
        "id": 277,
        "type": "text"
      },
      {
        "page": 56,
        "length_tokens": 137,
        "text": "·lambda，正则化项，范围为 $0 { \\sim } 1$   \n·min_gain_to_split，描述分裂的最小gain，控制树的有用的分裂  \n：max_cat_group，在group 边界上找到分割点，当类别数量很多时，找分割点很容易过拟合时  \n·num_boost_round，迭代次数，通常 $1 0 0 +$   \n·num_leaves，默认31  \n·device，指定cpu 或者gpu  \n·max_bin，表示 feature将存入的bin 的最大数量  \n·categorical_feature，如果 categorical_features $\\mathrel",
        "id": 278,
        "type": "text"
      },
      {
        "page": 56,
        "length_tokens": 63,
        "text": "egorical_feature，如果 categorical_features $\\mathrel { \\mathop : } = 0 , 1 , 2$ ，则列0，1，2是categorical变量  \n·ignore_column，与categorical_features类似，只不过不是将特定的列视为categorical，而是完全忽略",
        "id": 279,
        "type": "text"
      },
      {
        "page": 57,
        "length_tokens": 8,
        "text": "Step3, 模型参数配置 ",
        "id": 280,
        "type": "text"
      },
      {
        "page": 57,
        "length_tokens": 128,
        "text": "m $\\mathbf { \\tau } = \\mathbf { \\tau }$ {'boosting_type':'gbdt', 'objective':'binary',#任务类型 'metric':'auc',#评估指标 'learning_rate' :O.01,#学习率 'max_depth':15,#树的最大深度 'feature_fraction':0.8,#设置在每次迭代中使用特征的比例 'bagging_fraction':0.9,#样本采样比例 'bagging_freq':8,#bagging的次数 'lambda_I1': O.6,#L1正则 'lambda_l2':0,",
        "id": 281,
        "type": "text"
      },
      {
        "page": 57,
        "length_tokens": 30,
        "text": ",#bagging的次数 'lambda_I1': O.6,#L1正则 'lambda_l2':0,#L2正则 ",
        "id": 282,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 4,
        "text": "LightGBM工具",
        "id": 283,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 11,
        "text": "Step4, 模型训练， 得出预测结果",
        "id": 284,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 75,
        "text": "X_train,X_valid,y_train,Y_valid $\\mathbf { \\tau } = \\mathbf { \\tau }$ train_test_split(train.drop('Attrition',axis $_ { \\cdot = 1 }$ ), train['Attrition'], test_size ${ \\tt \\Psi } = 0 . 2$ ,random_state $= 4 2$ ） ",
        "id": 285,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 28,
        "text": "trn_data $\\mathbf { \\tau } = \\mathbf { \\tau }$ Igb.Dataset(X_train, label=y_train) ",
        "id": 286,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 27,
        "text": "val_data $\\mathbf { \\tau } = \\mathbf { \\tau }$ Igb.Dataset(X_valid, label=y_valid) ",
        "id": 287,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 100,
        "text": "model $\\mathbf { \\tau } = \\mathbf { \\tau }$ lgb.train(param,train_data,valid_sets $\\ c =$ [train_data,valid_data],num_boost_round $\\mathbf { \\tau } = \\mathbf { \\tau }$ 10000,early_stopping_round $s = 2 0 0$ ,verbose_eva $\\mathsf { I } = 2 5$ , categorical_feature $\\mathbf { \\sigma } =$ attr) ",
        "id": 288,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 9,
        "text": "predict $\\ c =$ model.predict(test) ",
        "id": 289,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 10,
        "text": "test['Attrition'] $\\ c =$ predict ",
        "id": 290,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 7,
        "text": "#转化为二分类输出",
        "id": 291,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 31,
        "text": "test['Attrition']=test['Attrition'].map(lambda x:1 if $x > = 0 . 5$ else 0) ",
        "id": 292,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 15,
        "text": "test[['Attrition']].to_csv('submit_lgb.csv') ",
        "id": 293,
        "type": "text"
      },
      {
        "page": 58,
        "length_tokens": 0,
        "text": "",
        "id": 294,
        "type": "table"
      },
      {
        "page": 59,
        "length_tokens": 3,
        "text": "参数对比",
        "id": 295,
        "type": "text"
      },
      {
        "page": 59,
        "length_tokens": 0,
        "text": "",
        "id": 296,
        "type": "table"
      },
      {
        "page": 60,
        "length_tokens": 9,
        "text": "祖传参数 (LightGBM) ",
        "id": 297,
        "type": "text"
      },
      {
        "page": 60,
        "length_tokens": 0,
        "text": "",
        "id": 298,
        "type": "text"
      },
      {
        "page": 60,
        "length_tokens": 130,
        "text": "LGBMClassifier经验参数   \nclf $\\mathbf { \\tau } = \\mathbf { \\tau }$ Igb.LGBMClassifier( num_leaves $= 2 ^ { * * } 5 - 1$ ,reg_alpha $= 0 . 2 5$ , reg_lambda=0.25,   \nobjective $= ^ { 1 }$ binary', max_depth $= - 1$ ,learning_rate $= 0 . 0 0 5$ ,min_child_samples $^ { = 3 }$   \nrandom_state $= 2 0 2 1$ n",
        "id": 299,
        "type": "text"
      },
      {
        "page": 60,
        "length_tokens": 123,
        "text": "_samples $^ { = 3 }$   \nrandom_state $= 2 0 2 1$ n_estimators $\\ c =$ 2000,subsample=1,colsample_bytree $\\scriptstyle = 1$ 1 ）   \nnum_leave $1 = 2 ^ { * * } 5 - 1$ #树的最大叶子数，对比×GBoost一般为   \n2^(max_depth)   \nreg_alpha，L1正则化系数   \nreglambda，L2正则化系数   \nmax_depth，最大树的深度 ",
        "id": 300,
        "type": "text"
      },
      {
        "page": 60,
        "length_tokens": 0,
        "text": "",
        "id": 301,
        "type": "text"
      },
      {
        "page": 60,
        "length_tokens": 47,
        "text": "n_estimators，树的个数，相当于训练的轮数 subsample，训练样本采样率 (行采样) colsample_bytree，训练特征采样率 (列采样) ",
        "id": 302,
        "type": "text"
      },
      {
        "page": 61,
        "length_tokens": 8,
        "text": "祖传参数 (XGBoost ",
        "id": 303,
        "type": "text"
      },
      {
        "page": 61,
        "length_tokens": 8,
        "text": "XGBoost VS LightGBM ",
        "id": 304,
        "type": "text"
      },
      {
        "page": 61,
        "length_tokens": 13,
        "text": "XGBoost效果相对LightGBM可能会好一些",
        "id": 305,
        "type": "text"
      },
      {
        "page": 61,
        "length_tokens": 114,
        "text": "xgb $\\mathbf { \\tau } = \\mathbf { \\tau }$ xgb.XGBClassifier( max_depth $= 6$ ,learning_rate $_ { = 0 . 0 5 }$ ,n_estimators=2000, objective $= ^ { \\mathsf { I } }$ binary:logistic', tree_method $= ^ { \\prime }$ gpu_hist', subsample $= 0 . 8$ , colsample_bytree $\\scriptstyle : = 0 . 8$ min_child_samp",
        "id": 306,
        "type": "text"
      },
      {
        "page": 61,
        "length_tokens": 40,
        "text": "ple_bytree $\\scriptstyle : = 0 . 8$ min_child_samples=3,eval_metric $u ^ { \\prime }$ auc',reg_lambda=0.! ） ",
        "id": 307,
        "type": "text"
      },
      {
        "page": 61,
        "length_tokens": 118,
        "text": "max_depth，树的最大深度  \nlearning_rate,学习率  \nreg_lambda，L2正则化系数  \nn_estimators，树的个数，相当于训练的轮数  \nobjective，目标函数,binary:logistic用于二分类任务  \ntree_method，使用功能的树的构建方法，hist代表使用直方图优  \n化的近似贪婪算法  \nsubsample，训练样本采样率 (行采样)  \ncolsample_bytree，训练特征采样率 (列采样)",
        "id": 308,
        "type": "text"
      },
      {
        "page": 61,
        "length_tokens": 0,
        "text": "",
        "id": 309,
        "type": "image"
      },
      {
        "page": 61,
        "length_tokens": 38,
        "text": "subsample，colsample_bytree是个值得调参的参数典型的取值为0.5-0.9 (取0.7效果可能更好)",
        "id": 310,
        "type": "text"
      },
      {
        "page": 62,
        "length_tokens": 4,
        "text": "CatBoost算法：",
        "id": 311,
        "type": "text"
      },
      {
        "page": 62,
        "length_tokens": 149,
        "text": "·俄罗斯科技公司Yandex开源的机器学习库（2017年）  \n·https://arxiv.0rg/pdf/1706.09516.pdf  \n· CatBoost $\\mathbf { \\tau } = \\mathbf { \\tau }$ Catgorical $^ +$ Boost  \n·高效的处理分类特征（categoricalfeatures），首先对分类特征做统计，计算某个分类特征（category）出现的频率，然后加上超参数，生成新的数值型特征（numericalfeatures）  \n·同时使用组合类别特征，丰富了特征维度  \n·采用的基模型是对称决策树，算法的参数少、支持分类变",
        "id": 312,
        "type": "text"
      },
      {
        "page": 62,
        "length_tokens": 43,
        "text": " \n·同时使用组合类别特征，丰富了特征维度  \n·采用的基模型是对称决策树，算法的参数少、支持分类变量，通过可以防止过拟合",
        "id": 313,
        "type": "text"
      },
      {
        "page": 63,
        "length_tokens": 13,
        "text": "CatBoost， LightGBM， XGBoost对比:",
        "id": 314,
        "type": "text"
      },
      {
        "page": 63,
        "length_tokens": 148,
        "text": "·2015年航班延误数据，包含分类和数值变量  \n:https://www.kaggle.com/usdot/flight-delays/data  \n一共有约500条记录，使用 $10 \\%$ 的数据，即50条记录CatBoost过拟合程度最小，在测试集上准确度最高0.816，同时预测用时最短，但这个表现仅仅在有分类特征，而且调节了one-hot最大量时才会出现  \n如果不利用CatBoost算法在这些特征上的优势，表现效果就会变成最差，AUC0.752  \n使用CatBoost需要数据中包含分类变量，同时适当地调节这些变量时，才会表现不错",
        "id": 315,
        "type": "text"
      },
      {
        "page": 63,
        "length_tokens": 0,
        "text": "",
        "id": 316,
        "type": "table"
      },
      {
        "page": 64,
        "length_tokens": 13,
        "text": "CatBoost， LightGBM， XGBoost对比:",
        "id": 317,
        "type": "text"
      },
      {
        "page": 64,
        "length_tokens": 91,
        "text": "处理特征为分类的神器  \n·支持即用的分类特征，因此我们不需要对分类特征进行预处理（比如使用LabelEncoding或OneHotEncoding)  \nCatBoost设计了一种算法验证改进，避免了过拟合。因此处理分类数据比LightGBM和XGBoost 强  \n：准确性比XGBoost更高，同时训练时间更短  \n支持 GPU训练可以处理缺失的值",
        "id": 318,
        "type": "text"
      },
      {
        "page": 64,
        "length_tokens": 0,
        "text": "",
        "id": 319,
        "type": "table"
      },
      {
        "page": 64,
        "length_tokens": 64,
        "text": "实验1，分类模型MNIST识别（6万数据，784特征)实验2，回归模型，预测纽约出租车票价 (6万数据，7个特征)实验3，回归模型，预测纽约出租车票价 (200万数据，7个特征)",
        "id": 320,
        "type": "text"
      },
      {
        "page": 65,
        "length_tokens": 3,
        "text": "CatBoost工具",
        "id": 321,
        "type": "text"
      },
      {
        "page": 65,
        "length_tokens": 5,
        "text": "CatBoost工具: ",
        "id": 322,
        "type": "text"
      },
      {
        "page": 65,
        "length_tokens": 27,
        "text": "· https://github.com/dmlc/xgboost ·https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html ",
        "id": 323,
        "type": "text"
      },
      {
        "page": 65,
        "length_tokens": 4,
        "text": "构造参数：",
        "id": 324,
        "type": "text"
      },
      {
        "page": 65,
        "length_tokens": 128,
        "text": "·learning_rate，学习率  \n· depth，树的深度  \n·l2_leaf_reg，L2正则化系数  \n·n_estimators，树的最大数量，即迭代次数  \n·one_hot_max_size，one-hot编码最大规模，默认值根据数据和训练环境的不同而不同  \n·loss_function，损失函数，包括Logloss，RMSE，MAE，CrossEntropy，回归任务默认RMSE，分类任务默认Logloss  \neval_metric，优化目标，包括RMSE，Logloss，MAE，CrossEntropy，Recall，Precision，F1，Accuracy，AU",
        "id": 325,
        "type": "text"
      },
      {
        "page": 65,
        "length_tokens": 22,
        "text": "s，MAE，CrossEntropy，Recall，Precision，F1，Accuracy，AUC，R2",
        "id": 326,
        "type": "text"
      },
      {
        "page": 66,
        "length_tokens": 3,
        "text": "CatBoost工具",
        "id": 327,
        "type": "text"
      },
      {
        "page": 66,
        "length_tokens": 4,
        "text": "fit函数参数：",
        "id": 328,
        "type": "text"
      },
      {
        "page": 66,
        "length_tokens": 108,
        "text": "·X，输入数据数据类型可以是：list;pandas.DataFrame;pandas.y=Nonecat features=None，用于处理分类特征sample_weight=None，输入数据的样本权重  \n·logging_level=None，控制是否输出日志信息，或者其他信息  \n·plot=False，训练过程中，绘制，度量值，所用时间等  \neval_set=None，验证集合，数据类型list(X,y)tuples  \nbaseline $\\ c =$ Noneuse_best_model $\\ c =$ None  \nverbose $\\ c =$ None",
        "id": 329,
        "type": "text"
      },
      {
        "page": 67,
        "length_tokens": 49,
        "text": "model $\\mathbf { \\tau } = \\mathbf { \\tau }$ CatBoostClassifier(iteration $\\scriptstyle \\mathsf { s } = 1 0 0 0$ ，#最大树数，即迭代次数",
        "id": 330,
        "type": "text"
      },
      {
        "page": 67,
        "length_tokens": 129,
        "text": "depth $= 6$ ，#树的深度  \nlearning_rate $\\mathbf { \\varepsilon } = 0 . 0 3$ ,#学习率  \ncustom_ ${ \\mathsf { I o s s } } = ^ { \\prime } { \\mathsf { A u c } } ^ { \\prime }$ ,#训练过程中，用户自定义的损失函数  \neval_metric $\\because \\mathsf { A U C } ^ { \\prime }$ ,#过拟合检验（设置True）的评估指标，用于优化  \nbagging_temperatur $\\scriptstyle \\",
        "id": 331,
        "type": "text"
      },
      {
        "page": 67,
        "length_tokens": 144,
        "text": "ue）的评估指标，用于优化  \nbagging_temperatur $\\scriptstyle \\mathtt { \\underline { { \\sigma } } } = 0 . 8 3$ ，#贝叶斯bootstrap强度设置  \n$r s \\mathsf { m } = 0 . 7 8 ,$ #随机子空间  \nod_type $\\mathrel { \\mathop : }$ Iter',#过拟合检查类型  \nod_wait $_ { : = 1 5 0 }$ ，#使用Iter时，表示达到指定次数后，停止训练  \nmetric_period $= 4 0 0$ ，#计算优化评估值的频率 ",
        "id": 332,
        "type": "text"
      },
      {
        "page": 67,
        "length_tokens": 88,
        "text": "指定次数后，停止训练  \nmetric_period $= 4 0 0$ ，#计算优化评估值的频率  \nl2_leaf_ $\\displaystyle \\mathsf { r e g } = 5$ ,#2正则参数  \nthread_count $= 2 0$ ，#并行线程数量  \nrandom_seed $= 9 6 7$ #随机数种子  \n）",
        "id": 333,
        "type": "text"
      },
      {
        "page": 68,
        "length_tokens": 3,
        "text": "CatBoost工具",
        "id": 334,
        "type": "text"
      },
      {
        "page": 68,
        "length_tokens": 8,
        "text": "Step3, 模型参数配置 ",
        "id": 335,
        "type": "text"
      },
      {
        "page": 68,
        "length_tokens": 107,
        "text": "model $\\mathbf { \\tau } = \\mathbf { \\tau }$ cb.CatBoostClassifier(iterations=1000, depth $\\scriptstyle 1 = { \\overline { { \\jmath } } }$ 7， learning_rate $_ { ! = 0 . 0 1 }$ ， loss_function $= ^ { \\mathsf { 1 } }$ Logloss', eval_metric $= ^ { \\prime }$ AUC', logging_level $\\mathbf { \\tau } = \\mathbf",
        "id": 336,
        "type": "text"
      },
      {
        "page": 68,
        "length_tokens": 35,
        "text": "}$ AUC', logging_level $\\mathbf { \\tau } = \\mathbf { \\tau }$ Verbose', metric_period $\\scriptstyle = 5 0$ ",
        "id": 337,
        "type": "text"
      },
      {
        "page": 68,
        "length_tokens": 45,
        "text": "#得到分类特征的列号 categorical_features_indices =[] fori in range(len(X_train.columns)): if X_train.columns.values[i] in attr: categorical_features_indices.append(i) print(categorical_features_indices) ",
        "id": 338,
        "type": "text"
      },
      {
        "page": 68,
        "length_tokens": 32,
        "text": "attr=['Age','BusinessTravel','Department','Education','EducationField','Gend er','JobRole','MaritalStatus','Over18','OverTime'] ",
        "id": 339,
        "type": "text"
      },
      {
        "page": 68,
        "length_tokens": 22,
        "text": "[0,1,3,5,6,9,13,15,19,20] ",
        "id": 340,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 3,
        "text": "CatBoost工具",
        "id": 341,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 11,
        "text": "Step4, 模型训练，得出预测结果",
        "id": 342,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 70,
        "text": "model.fit(X_train,y_train,eval_set=(X_valid,y_valid),   \ncat_features $\\ c =$ categorical_features_indices)   \npredict $\\mathbf { \\tau } = \\mathbf { \\tau }$ model.predict(test)   \ntest['Attrition'] $\\ c =$ predict   \ntest[['Attrition']].to_csv('submit_cb.csv') ",
        "id": 343,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 146,
        "text": "0： test: 0.6390374 best: 0.6390374 (0) t0tal: 80.6ms remaining: 1m 20s   \n50: test: 0.7998472 best: 0.7998472 (50) total: 805ms remaining: 15s   \n100: test: 0.8054131 best: 0.8054131 (100) total: 1.54s remaining: 13.8s   \n150 : test: 0.8053039 be5t: 0.8054131 (100) total: 2.51s remaining: 14.1s   \n2",
        "id": 344,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 141,
        "text": "0.8054131 (100) total: 2.51s remaining: 14.1s   \n200 : test: 0.8075958 best: 0.8075958 (200) total: 3.55s remaining: 14.1s   \n250 : test: 0.8062862 best: 0.8075958 (200) total: 4.55s remaining: 13.6s   \n300: test: 0.8045400 best: 0.8075958 (200) total: 5.51s remaining: 12.8s   \n350 : test: 0.8059587",
        "id": 345,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 147,
        "text": "l: 5.51s remaining: 12.8s   \n350 : test: 0.8059587 be5t: 0.8075958 (200) total: 6.45s remaining: 11.9s   \n400 : test: 0.8065044 best: 0.8075958 (200) total: 7.41s remaining: 11.1s   \n450 : test: 0.8065044 be5t: 0.8075958 (200) total: 8.44s remaining: 10.3s   \n500: test: 0.8077049 best: 0.8077049 (50",
        "id": 346,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 146,
        "text": " 10.3s   \n500: test: 0.8077049 best: 0.8077049 (500) total: 9.46s remaining: 9.43s   \n550 : test: 0.8090145 best: 0.8090145 (550) total: 10.6s remaining: 8.6s   \n600: test: 0.8106515 best: 0.8106515 (600) total: 11.5s remaining: 7.62s   \n650 : test: 0.8113063 be5t: 0.8113063 (650) total: 12.4s remai",
        "id": 347,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 143,
        "text": "0.8113063 be5t: 0.8113063 (650) total: 12.4s remaining: 6.67s   \n700 : test: 0.8125068 best: 0.8125068 (700) total: 13.4s remaining: 5.71s   \n750 : test: 0.8126160 best: 0.8126160 (750) total: 14.4s remaining: 4.77s   \n800: test: 0.8116337 best: 0.8126160 (750) total: 15.5s remaining: 3.84s   \n850 :",
        "id": 348,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 139,
        "text": "26160 (750) total: 15.5s remaining: 3.84s   \n850 : test: 0.8121794 be5t: 0.8126160 (750) total: 16.5s remaining: 2.89s   \n900: test: 0.8116337 best: 0.8126160 (750) total: 17.6s remaining: 1.94s   \n950 : test: 0.8111972 best: 0.8126160 (750) total: 18.7s remaining: 965ms   \n999 : test: 0.8108698 bes",
        "id": 349,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 61,
        "text": "8.7s remaining: 965ms   \n999 : test: 0.8108698 best: 0.8126160 (750) total: 19.8s remaining: 0us   \nbestTest = 0.8126159555   \nbestIteration = 750 ",
        "id": 350,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 0,
        "text": "",
        "id": 351,
        "type": "text"
      },
      {
        "page": 69,
        "length_tokens": 0,
        "text": "",
        "id": 352,
        "type": "table"
      },
      {
        "page": 70,
        "length_tokens": 93,
        "text": "·LighGBM效率高，在Kaggle比赛中应用多  \n·CatBoost对于分类特征多的数据，可以高效的处理，过拟合程度小，效果好  \n·XGBoost,LightGBM,CatBoost参数较多，调参需要花大量时间  \n·Boosting集成学习包括AdaBoosting和Gradient Boosting  \n·Boosting只是集成学习中的一种（Bagging,Stacking）",
        "id": 353,
        "type": "text"
      },
      {
        "page": 71,
        "length_tokens": 8,
        "text": "打卡：二手车价格预测",
        "id": 354,
        "type": "text"
      },
      {
        "page": 71,
        "length_tokens": 42,
        "text": "针对AI大赛：二手车价格预测，编写AI算法，进行预测，挑战分数<550 https://tianchi.aliyun.com/competition/entrance/231784/information ",
        "id": 355,
        "type": "text"
      },
      {
        "page": 71,
        "length_tokens": 12,
        "text": "训练集: used_car_train_20200313.csv ",
        "id": 356,
        "type": "text"
      },
      {
        "page": 71,
        "length_tokens": 12,
        "text": "测试集： used_car_testB_20200421.csv",
        "id": 357,
        "type": "text"
      },
      {
        "page": 71,
        "length_tokens": 11,
        "text": "XGBoost模型LightGBM模型CatBoost模型",
        "id": 358,
        "type": "text"
      },
      {
        "page": 71,
        "length_tokens": 9,
        "text": "你觉得哪个模型的模型效果好？",
        "id": 359,
        "type": "text"
      },
      {
        "page": 72,
        "length_tokens": 7,
        "text": "如何防止模型过拟合",
        "id": 360,
        "type": "text"
      },
      {
        "page": 72,
        "length_tokens": 10,
        "text": "Thinking: 如何防止模型过拟合？",
        "id": 361,
        "type": "text"
      },
      {
        "page": 72,
        "length_tokens": 21,
        "text": "1、模型层面优化  \n2、数据层面优化  \n3、业务逻辑约束",
        "id": 362,
        "type": "text"
      },
      {
        "page": 72,
        "length_tokens": 11,
        "text": "学习赛第二季:466/780.2130",
        "id": 363,
        "type": "text"
      },
      {
        "page": 72,
        "length_tokens": 88,
        "text": "[19995] validation_0-mae:493.30894   \n[19996] validation_0-mae:493.30957   \n[19997] validation_0-mae:493.30842   \n[19998] validation 0-mae:493.30761   \n[19999] validation_0-mae:493.30777   \n模型3 验证集 MAE:493.31 ",
        "id": 364,
        "type": "text"
      },
      {
        "page": 72,
        "length_tokens": 19,
        "text": "7.2 集成模型预测集成模型验证集 MAE: 492.49",
        "id": 365,
        "type": "text"
      },
      {
        "page": 72,
        "length_tokens": 6,
        "text": "8.评估模型性能",
        "id": 366,
        "type": "text"
      },
      {
        "page": 72,
        "length_tokens": 91,
        "text": "日期：2025-06-06 00:09:57  \n分数：818.1239  \n日期：2025-06-05 12:55:31  \n分数：826.2079  \n日期：2025-06-05 12:28:42  \n分数：826.2079  \n日期：2025-06-05 12:08:09  \n分数：780.2130",
        "id": 367,
        "type": "text"
      },
      {
        "page": 73,
        "length_tokens": 7,
        "text": "如何防止模型过拟合",
        "id": 368,
        "type": "text"
      },
      {
        "page": 73,
        "length_tokens": 10,
        "text": "Thinking: 如何在模型层面进行优化？",
        "id": 369,
        "type": "text"
      },
      {
        "page": 73,
        "length_tokens": 5,
        "text": "正则化技术：",
        "id": 370,
        "type": "text"
      },
      {
        "page": 73,
        "length_tokens": 73,
        "text": "树模型：调整max_depth(建议5-8)、min_samples_leaf(建议10-20，代表每个叶子节点至少包含的样本数)  \n神经网络：添加dropout层(0.2-0.5)+ L2正则化  \n线性模型：增大L1/L2正则化系数",
        "id": 371,
        "type": "text"
      },
      {
        "page": 73,
        "length_tokens": 4,
        "text": "早停机制：",
        "id": 372,
        "type": "text"
      },
      {
        "page": 73,
        "length_tokens": 36,
        "text": "监控验证集loss，设置patience=10-20个epoch，patience代表允许验证集损失validation loss连续不改善的轮次（epochs）数量",
        "id": 373,
        "type": "text"
      },
      {
        "page": 73,
        "length_tokens": 4,
        "text": "集成方法：",
        "id": 374,
        "type": "text"
      },
      {
        "page": 73,
        "length_tokens": 21,
        "text": "使用Blending（用70%训练基模型， $30 \\%$ 训练元模型）",
        "id": 375,
        "type": "text"
      },
      {
        "page": 74,
        "length_tokens": 11,
        "text": "如何防止模型过拟合 （树模型）",
        "id": 376,
        "type": "text"
      },
      {
        "page": 74,
        "length_tokens": 6,
        "text": "XGBoost中的超参数",
        "id": 377,
        "type": "text"
      },
      {
        "page": 74,
        "length_tokens": 163,
        "text": "$\\mathsf { p a r a m s } = \\left\\{ \\begin{array} { r l } \\end{array} \\right.$ （2'objective':'reg:squarederror'，#目标函数，回归问题用平方误差'eval_metric': 'mae', # 评估指标，平均绝对误差'learning_rate': 0.01, # 学习率，控制每棵树对最终结果的影响，越小越保守'max_depth': 6, # 树的最大深度，防止过拟合'subsample': 0.8, # 每棵树随机采样的样本比例，防止过拟合'colsample_bytree': 0.8",
        "id": 378,
        "type": "text"
      },
      {
        "page": 74,
        "length_tokens": 87,
        "text": ": 0.8, # 每棵树随机采样的样本比例，防止过拟合'colsample_bytree': 0.8, # 每棵树随机采样的特征比例，防止过拟合'seed': 42, # 随机种子，保证结果可复现'nthread': -1 #使用全部CPU线程加速训练  \n}",
        "id": 379,
        "type": "text"
      },
      {
        "page": 75,
        "length_tokens": 11,
        "text": "如何防止模型过拟合 （树模型）",
        "id": 380,
        "type": "text"
      },
      {
        "page": 75,
        "length_tokens": 13,
        "text": "在sklearn决策树中，可以使用min_samples_leaf参数",
        "id": 381,
        "type": "text"
      },
      {
        "page": 75,
        "length_tokens": 104,
        "text": "from sklearn.ensemble import RandomForestRegressor  \n#尝试不同值观察效果  \nmodel $\\mathbf { \\tau } = \\mathbf { \\tau }$ RandomForestRegressor(min_samples_leaf $\\dot { \\mathbf { \\eta } } = 1 0$ ，# 初始建议值n_estimator ${ \\mathsf { s } } { = } 1 0 0$ ，random_state $= 4 2$   \n）  \nmodel.fit(X_train, y_train)",
        "id": 382,
        "type": "text"
      },
      {
        "page": 75,
        "length_tokens": 72,
        "text": "XGBoost没有min_samples_leaf 这个参数，在XGBoost 中，类似的参数是：min_child_weight，即一个叶子节点上最小的样本权重和（对于回归问题就是样本个数的和）。如果一个叶子节点的样本权重和小于这个值，则不会再分裂。",
        "id": 383,
        "type": "text"
      },
      {
        "page": 76,
        "length_tokens": 12,
        "text": "如何防止模型过拟合 （神经网络）",
        "id": 384,
        "type": "text"
      },
      {
        "page": 76,
        "length_tokens": 12,
        "text": "如果使用神经网络，可以增加layers.Dropout 层",
        "id": 385,
        "type": "text"
      },
      {
        "page": 76,
        "length_tokens": 84,
        "text": "from tensorflow import keras   \nfrom tensorflow.keras import layers   \nmodel $\\mathbf { \\tau } = \\mathbf { \\tau }$ keras.Sequential([ layers.Dense(128, activation $= ^ { \\mathsf { \\Gamma } }$ relu',input_shape $\\ c =$ (X_train.shape[1],)), layers.Dropout(0.2), layers.Dense(64, activation $= ^ { \\mat",
        "id": 386,
        "type": "text"
      },
      {
        "page": 76,
        "length_tokens": 46,
        "text": "pout(0.2), layers.Dense(64, activation $= ^ { \\mathsf { \\Gamma } }$ relu'), layers.Dropout(0.1), layers.Dense(1)   \n1) ",
        "id": 387,
        "type": "text"
      },
      {
        "page": 76,
        "length_tokens": 56,
        "text": "神经网络是一种受人脑神经元结构启发的机器学习模型。核心思想是通过大量“神经元”节点的层层连接和非线性变换，自动学习输入特征与输出目标之间的复杂映射关系。",
        "id": 388,
        "type": "text"
      },
      {
        "page": 77,
        "length_tokens": 12,
        "text": "如何防止模型过拟合 （线性模型）",
        "id": 389,
        "type": "text"
      },
      {
        "page": 77,
        "length_tokens": 22,
        "text": "如果使用线性模型，比如逻辑回归，可以使用L1或L2正则化系数",
        "id": 390,
        "type": "text"
      },
      {
        "page": 77,
        "length_tokens": 133,
        "text": "model $\\mathbf { \\tau } = \\mathbf { \\tau }$ LogisticRegression(max_iter=100,verbose $\\mathbf { \\tau } = \\mathbf { \\dot { \\tau } }$ True,random_state $= 4 2$ ，$\\scriptstyle { \\mathrm { t o l } } = 1 \\ e - 4 .$ penalty $= ^ { 1 } 1 2 ^ { 1 }$ ，#或'I1'${ \\mathsf { C } } { = } 1 . 0 ,$ # 正则化强度倒数，越小正则化越强 ",
        "id": 391,
        "type": "text"
      },
      {
        "page": 77,
        "length_tokens": 35,
        "text": " \\mathsf { C } } { = } 1 . 0 ,$ # 正则化强度倒数，越小正则化越强  \n）",
        "id": 392,
        "type": "text"
      },
      {
        "page": 77,
        "length_tokens": 126,
        "text": "L1正则化项是模型权重的绝对值之和： （20 $\\begin{array} { l } { { \\displaystyle { \\cal L } 1 = \\lambda \\sum _ { i = 1 } ^ { n } \\left| w _ { i } \\right| } } \\\\ { { \\displaystyle { \\cal L } 2 = \\lambda \\sum _ { i = 1 } ^ { n } w _ { i } ^ { 2 } } } \\end{array}$ L2正则化项是模型权重的平方和 ",
        "id": 393,
        "type": "text"
      },
      {
        "page": 78,
        "length_tokens": 12,
        "text": "如何防止模型过拟合 （早停机制）",
        "id": 394,
        "type": "text"
      },
      {
        "page": 78,
        "length_tokens": 13,
        "text": "使用早停机制， 可以让模型初始化训练次数更多一些",
        "id": 395,
        "type": "text"
      },
      {
        "page": 78,
        "length_tokens": 101,
        "text": "model $\\mathbf { \\tau } = \\mathbf { \\tau }$ xgb.train( params, dtrain, num_boost_round=2000, evals $\\ c =$ evals, early_stopping_rounds $= 5 2 0$ verbose_eval=100   \nT   \nearly_stop $\\mathbf { \\tau } = \\mathbf { \\tau }$ keras.callbacks.EarlyStopping(monitor='val_loss',   \npatience $: = 2 0$ , restor",
        "id": 396,
        "type": "text"
      },
      {
        "page": 78,
        "length_tokens": 102,
        "text": "monitor='val_loss',   \npatience $: = 2 0$ , restore_best_weights $\\ c =$ True)   \nhistory $\\mathbf { \\tau } = \\mathbf { \\tau }$ model.fit( X_train_scaled,y_train, validation_data $\\ c =$ (X_val_scaled, y_val), epoch $s { = } 2 0 0$ ， batch_size $\\mathtt { = 6 4 }$ 冏， callbacks $\\ c =$ [early_stop], ",
        "id": 397,
        "type": "text"
      },
      {
        "page": 78,
        "length_tokens": 26,
        "text": "htt { = 6 4 }$ 冏， callbacks $\\ c =$ [early_stop], verbose=2   \n） ",
        "id": 398,
        "type": "text"
      },
      {
        "page": 78,
        "length_tokens": 0,
        "text": "",
        "id": 399,
        "type": "text"
      },
      {
        "page": 78,
        "length_tokens": 45,
        "text": "早停是一种正则化技术，用于防止模型在训练过程中过拟合。核心思想是：在验证集性能不再提升时提前终止训练，而不是一直训练到收敛。",
        "id": 400,
        "type": "text"
      },
      {
        "page": 79,
        "length_tokens": 6,
        "text": "Blending集成学习方法",
        "id": 401,
        "type": "text"
      },
      {
        "page": 79,
        "length_tokens": 7,
        "text": "Thinking: Blending 是什么？",
        "id": 402,
        "type": "text"
      },
      {
        "page": 79,
        "length_tokens": 18,
        "text": "Blending是一种分层集成学习技术，通过以下两步组合多个模型：",
        "id": 403,
        "type": "text"
      },
      {
        "page": 79,
        "length_tokens": 76,
        "text": "·基模型（BaseModels）：用训练集的 $70 \\%$ 训练多个不同模型（如随机森林、XGBoost、神经网络等）。·元模型（Meta Model）：用剩下的30%数据生成基模型的预测结果作为新特征，训练一个次级模型（通常为线性模型）进行最终预测。",
        "id": 404,
        "type": "text"
      },
      {
        "page": 79,
        "length_tokens": 56,
        "text": "神经网络是一种受人脑神经元结构启发的机器学习模型。核心思想是通过大量“神经元”节点的层层连接和非线性变换，自动学习输入特征与输出目标之间的复杂映射关系。",
        "id": 405,
        "type": "text"
      },
      {
        "page": 80,
        "length_tokens": 6,
        "text": "Blending集成学习方法",
        "id": 406,
        "type": "text"
      },
      {
        "page": 80,
        "length_tokens": 29,
        "text": "假设数据集有15,000条二手车记录，特征包括车龄、里程、品牌等，目标为价格",
        "id": 407,
        "type": "text"
      },
      {
        "page": 80,
        "length_tokens": 72,
        "text": "import numpy as np   \nfrom sklearn.model_selection import train_test_split   \nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor   \nfrom sklearn.linear_model import LinearRegression   \n#原始数据   \nX,y= load_car_data()#假设已加载数据   \nX_train,X_val,y_train,y_val $\\mathbf { \\tau } =",
        "id": 408,
        "type": "text"
      },
      {
        "page": 80,
        "length_tokens": 57,
        "text": "  \nX_train,X_val,y_train,y_val $\\mathbf { \\tau } = \\mathbf { \\tau }$ train_test_split(X,y, test_size $\\scriptstyle \\mathtt { = 0 . 3 }$ ,random_state $= 4 2$ ",
        "id": 409,
        "type": "text"
      },
      {
        "page": 80,
        "length_tokens": 0,
        "text": "",
        "id": 410,
        "type": "text"
      },
      {
        "page": 80,
        "length_tokens": 89,
        "text": "# 进一步划分训练集： $70 \\%$ 基模型 $1 3 0 \\%$ 元模型 X_base,X_meta,y_base,y_meta $\\mathbf { \\tau } = \\mathbf { \\tau }$ train_test_split( X_train,γ_train, test_size $\\scriptstyle \\mathtt { = 0 . 3 }$ , random_state $: = 4 2$ 1 ",
        "id": 411,
        "type": "text"
      },
      {
        "page": 81,
        "length_tokens": 6,
        "text": "Blending集成学习方法",
        "id": 412,
        "type": "text"
      },
      {
        "page": 81,
        "length_tokens": 7,
        "text": "Step1: 训练基模型",
        "id": 413,
        "type": "text"
      },
      {
        "page": 81,
        "length_tokens": 110,
        "text": "# 定义3个基模型 model_rf $\\mathbf { \\tau } = \\mathbf { \\tau }$ RandomForestRegressor(n_estimators $_ { \\mathbf { \\lambda } = 1 0 0 }$ , random_state $= 4 2$ model_gb $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ GradientBoostingRegressor(n_estimators $_ { \\mathbf { \\lambda } = 1 0 0 }$ , random_state $= 4 2$ ",
        "id": 414,
        "type": "text"
      },
      {
        "page": 81,
        "length_tokens": 54,
        "text": "hbf { \\lambda } = 1 0 0 }$ , random_state $= 4 2$ model_nn $\\mathbf { \\tau } = \\mathbf { \\tau }$ make_neural_network()#自定义的神经网络 ",
        "id": 415,
        "type": "text"
      },
      {
        "page": 81,
        "length_tokens": 39,
        "text": "#在 $70 \\%$ 数据上训练基模型 model_rf.fit(X_base,y_base) model_gb.fit(X_base,y_base) model_nn.fit(X_base, y_base) ",
        "id": 416,
        "type": "text"
      },
      {
        "page": 82,
        "length_tokens": 6,
        "text": "Blending集成学习方法",
        "id": 417,
        "type": "text"
      },
      {
        "page": 82,
        "length_tokens": 7,
        "text": "Step2：生成元特征",
        "id": 418,
        "type": "text"
      },
      {
        "page": 82,
        "length_tokens": 17,
        "text": "用基模型预测剩余 $30 \\%$ 数据，生成新特征",
        "id": 419,
        "type": "text"
      },
      {
        "page": 82,
        "length_tokens": 53,
        "text": "#获取基模型对元数据集的预测  \nmeta_features $\\mathbf { \\tau } = \\mathbf { \\tau }$ np.column_stack([model_rf.predict(X_meta),model_gb.predict(X_meta),model_nn.predict(X_meta)  \n1)",
        "id": 420,
        "type": "text"
      },
      {
        "page": 82,
        "length_tokens": 26,
        "text": "#元特征示例（每条样本的3个基模型预测值） print(meta_features[:3]) ",
        "id": 421,
        "type": "text"
      },
      {
        "page": 82,
        "length_tokens": 48,
        "text": "#输出类似： #[[12.5,13.1, 11.8], # [8.2,7.9,8.5], # [20.1,19.7,21.3]] ",
        "id": 422,
        "type": "text"
      },
      {
        "page": 83,
        "length_tokens": 6,
        "text": "Blending集成学习方法",
        "id": 423,
        "type": "text"
      },
      {
        "page": 83,
        "length_tokens": 7,
        "text": "Step3: 训练元模型",
        "id": 424,
        "type": "text"
      },
      {
        "page": 83,
        "length_tokens": 44,
        "text": "# 用基模型的预测结果作为输入，真实价格作为目标  \nmeta_model $\\mathbf { \\tau } = \\mathbf { \\tau }$ LinearRegression()  \nmeta_model.fit(meta_features, y_meta)",
        "id": 425,
        "type": "text"
      },
      {
        "page": 83,
        "length_tokens": 7,
        "text": "Step4: 预测新数据",
        "id": 426,
        "type": "text"
      },
      {
        "page": 83,
        "length_tokens": 66,
        "text": "通过Blending学习，可以得到三个模型（rf,gb,nn）的线性回归系数，比如为：权重系数 $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ [0.4,0.5,0.1],偏置 $= 0 . 2$ ",
        "id": 427,
        "type": "text"
      },
      {
        "page": 83,
        "length_tokens": 92,
        "text": "#对验证集生成基模型预测   \nval_meta_features $\\mathbf { \\tau } = \\mathbf { \\tau }$ np.column_stack([ model_rf.predict(X_val), model_gb.predict(X_val), model_nn.predict(X_val)   \n1)   \n#用元模型做最终预测   \nfinal_predictions $\\mathbf { \\tau } = \\mathbf { \\tau }$ meta_model.predict(val_meta_features)   \n#计算MAE   \nprint(",
        "id": 428,
        "type": "text"
      },
      {
        "page": 83,
        "length_tokens": 29,
        "text": "del.predict(val_meta_features)   \n#计算MAE   \nprint(\"Blending MAE:\",mean_absolute_error(y_val, final_predictions)) ",
        "id": 429,
        "type": "text"
      },
      {
        "page": 83,
        "length_tokens": 52,
        "text": "最终预测 $= w _ { 1 } \\times$ 模型1预测 $+ w _ { 2 } \\times$ 模型2预测 $+ w _ { 3 } \\times$ 模型3预测 $+ b$ ",
        "id": 430,
        "type": "text"
      },
      {
        "page": 84,
        "length_tokens": 110,
        "text": "过拟合（Overfitting）是导致模型在实际应用中表现糟糕的主要原因之一。在二手车价格预测这类数据噪声大、特征复杂的任务中，过拟合会直接影响模型的商业价值和可靠性。训练 $M A E = 4 0 0$ （模型似乎很准），测试 $M A E = 8 0 0$ （实际预测误差翻倍） $\\Rightarrow$ 典型的过拟合",
        "id": 431,
        "type": "text"
      },
      {
        "page": 84,
        "length_tokens": 0,
        "text": "",
        "id": 432,
        "type": "table"
      },
      {
        "page": 85,
        "length_tokens": 8,
        "text": "Thank You Using data to solve problems ",
        "id": 433,
        "type": "text"
      }
    ]
  }
}