# 机器学习神器

# 学习方法

· Thinking: behind the theory,original from the real problem

·Action: solve problems by tools,present the results

# 今天的学习目标

# 预测全家桶

Project A: 员工离职预测 ·ProjectB：男女声音识别 ·分类算法：LR，SVM，KNN ·树模型：GBDT,XGBoost, LightGBM, CatBoost， NGBoost

# 机器学习神器

·什么是集成学习GBDT原理  
XGBoost  
LightGBMCatBoost  
·在Project中使用机器学习神器  
·AI大赛：二手车价格预测  
·如何防止模型过拟合

# 1/2 预测全家桶

# Project A: 员工离职预测

员工离职预测

In Class Competition

https://www.kaggle.com/c/bi-attrition-predict/

·我们有员工的各种统计信息，以及该员工是否已经离职，统计的信息包括了（工资、出差、工作环境满意度、工作投入度、是否加班、是否升职、工资提升比例等）

·现在需要你来通过训练数据得出员工离职预测，并给出你在测试集上的预测结果。我们将给出课程上公开的榜单

![](images/3f1438d829101534e0ba3d640a10c09cfd4d49b4013da4888e6f2cd90680406d.jpg)

# Project A: 员工离职预测

数据表字段：  

<html><body><table><tr><td>字段 定义</td><td></td><td>字段</td><td>定义</td></tr><tr><td>Age</td><td>员工年龄</td><td>Joblnvolvement</td><td>员工工作投入度，从1到4，1为投入度最低，4为投入 度最高</td></tr><tr><td>Attrition</td><td>员工是否已经离职，Yes表示离职，No表示未离职</td><td>JobLevel</td><td>职业级别，从1到5，1为最低级别，5为最高级别</td></tr><tr><td>BusinessTravel</td><td>商务差旅频率，Non-Travel不出差，TravelRarely不 经常出差，TravelFrequently经常出差</td><td>JobRole</td><td>工作角色：Sales Executive销售主管，Research Scientist科学研究员，LaboratoryTechnician实验室技</td></tr><tr><td>DailyRate Department</td><td>平均日工资 员工所在部门，Sales销售部，Research&</td><td></td><td>术员，Manufacturing Director制造总监，Healthcare Representative医疗代表，Manager经理，Sales</td></tr><tr><td></td><td>Development研发部，Human Resources人力资源部 DistanceFromHome公司跟家庭住址的距离，从1到29，1表示最近，29表</td><td></td><td>Representative销售代表，ResearchDirector研究总 监，Human Resources人力资源</td></tr><tr><td>Education</td><td>示最远 员工的教育程度，从1到5，5表示教育程度最高</td><td>JobSatisfaction</td><td>工作满意度，从1到4，1代表满意度最低，4代表最高</td></tr><tr><td>EducationField</td><td>员工所学习的专业领域，Life Sciences表示生命科学，</td><td>MaritalStatus</td><td>员工婚姻状况，Single单身，Married已婚，Divorced 离婚</td></tr><tr><td></td><td>Medical表示医疗，Marketing表示市场营销 表示人力资源，Other表示其他 TechnicalDegree表示技术学位，Human ResourcesNumCompaniesWorked员工曾经工作过的公司数</td><td>Monthlylncome</td><td>员工月收入，范围在1009到19999之间</td></tr><tr><td>EmployeeNumber员工号码</td><td></td><td>Over18</td><td>年龄是否超过18岁</td></tr><tr><td></td><td>EnvironmentSatisfa 员工对于工作环境的满意程度，从1到4，1的满意程OverTime</td><td></td><td>是否加班，Yes表示加班，No表示不加班</td></tr><tr><td>ction</td><td>度最低，4的满意程度最高</td><td>PercentSalaryHike</td><td>工资提高的百分比</td></tr><tr><td>Gender</td><td>员工性别，Male表示男性，Female表示女性</td><td></td><td></td></tr></table></body></html>

# Project A: 员工离职预测

数据表字段：  

<html><body><table><tr><td>字段</td><td>定义</td></tr><tr><td>PerformanceRating</td><td>绩效评估</td></tr><tr><td>RelationshipSatisfaction</td><td>关系满意度，从1到4，1表示满意度最低，4表示满意度最高</td></tr><tr><td>StandardHours</td><td>标准工时</td></tr><tr><td>StockOptionLevel</td><td>股票期权水平</td></tr><tr><td>TotalWorkingYears</td><td>总工龄</td></tr><tr><td>TrainingTimesLastYear</td><td>上一年的培训时长，从0到6，0表示没有培训，6表示培训时 间最长</td></tr><tr><td>WorkLifeBalance</td><td>工作与生活平衡程度，从1到4，1表示平衡程度最低，4表示 平衡程度最高</td></tr><tr><td>YearsAtCompany</td><td>在目前公司工作年数</td></tr><tr><td>YearsInCurrentRole</td><td>在目前工作职责的工作年数</td></tr><tr><td>YearsSinceLastPromotion</td><td>距离上次升职时长</td></tr><tr><td>YearsWithCurrManager</td><td>跟目前的管理者共事年数</td></tr></table></body></html>

# 预测全家桶

常用预测（分类，回归）模型：

·分类算法：LR，SVM，KNN · 树模型：GBDT,XGBoost, LightGBM,CatBoost，NGBoost

![](images/0a5a7266ef8b140be5c68b8b90ab817bf4bbfc741e2ef3cf364dc30a1d8806f9.jpg)

·特征工程：好的特征工程是拿分的关键

·模型：懂原理，会调参

# 常用预测模型

数据预处理  
分类算法：LR，SVM，KNN  
树模型：GBDT,XGBoost, LightGBM, CatBoost， NGBoost

# Project A: 数据预处理

# Step1，对数据进行探索

#工离职预测   
import pandas as pd   
train $\ c =$ pd.read_csv('train.csv',index_col $\scriptstyle 1 = 0$ ）   
test=pd.read_csv('test.csv',index_col=0)   
print(train['Attrition'].value_counts())   
#处理Attrition字段   
train['Attrition']=train['Attrition'].map(lambda x:1 if $\ x = = 1$ Yes' else O)   
#查看数据中每列是否有空值   
print(train.isna().sum())

No 988   
Yes 188   
Name: Attrition, dtype: int64   
Age 0   
Attrition 0   
BusinessTravel 0   
DailyRate 0   
Department 0   
DistanceFromHome 0   
Education 0   
EducationField 0   
EmployeeCount 0   
YearslnCurrentRole 0   
YearsSinceLastPromotion 0   
YearsWithCurrManager 0   
dtype: int64

# Project A: 数据预处理

Step2， 去掉无用特征，处理分类特征

#去掉没用的列员工号码，标准工时（ $\scriptstyle \left. = 8 0 \right.$ ））   
train $\mathbf { \tau } = \mathbf { \tau }$ train.drop(['EmployeeNumber','StandardHours'],axis ${ \mathfrak { s } } = 1$ ）   
test $\mathbf { \tau } = \mathbf { \tau }$ test.drop(['EmployeeNumber','StandardHours'],axis $_ { \cdot = 1 }$ ）   
#对于分类特征进行特征值编码   
from sklearn.preprocessing import LabelEncoder   
attr=['Age','BusinessTravel','Department','Education','EducationField','Gender'   
,'JobRole','MaritalStatus','Over18','OverTime']

for feature in attr:

Ibe $\ c =$ LabelEncoder() train[feature]=lbe.fit_transform(train[feature]) test[feature] $= |$ be.transform(test[feature]) train.to_csv('temp.csv')

<html><body><table><tr><td>uesr_id</td><td>Age</td><td>Attritior BusinessTravel DailyRate</td><td></td><td>Department</td><td>DistanceFromHome</td><td>Education</td><td>Educatior</td></tr><tr><td>1374</td><td>40</td><td>0</td><td>2</td><td>605</td><td>2 21</td><td>2</td><td>1</td></tr><tr><td>1092</td><td>27</td><td>0</td><td>2</td><td>950</td><td>1</td><td>28 2</td><td>5</td></tr><tr><td>768</td><td>22</td><td>0</td><td>2</td><td>300</td><td>2</td><td>26 2</td><td>2</td></tr><tr><td>569</td><td>18</td><td>0</td><td>0</td><td>1434</td><td>2</td><td>8 3</td><td>1</td></tr><tr><td>911</td><td>7</td><td>1</td><td>1</td><td>599</td><td>2 24</td><td>0</td><td>1</td></tr><tr><td>408</td><td>34</td><td>0</td><td>2</td><td>1490</td><td>1</td><td>4 1</td><td>1</td></tr><tr><td>1321</td><td>29</td><td>0</td><td>2</td><td>207</td><td>1</td><td>9 3</td><td>1</td></tr><tr><td>1224</td><td>8</td><td>0</td><td>2</td><td>390</td><td>1</td><td>17 3</td><td>3</td></tr><tr><td>1061</td><td>6</td><td>0</td><td>0</td><td>830</td><td>2</td><td>13 1</td><td>1</td></tr><tr><td>530</td><td>9</td><td>0</td><td>2</td><td>608</td><td>1</td><td>1 1</td><td>1</td></tr><tr><td>1233</td><td>12</td><td>0</td><td>2</td><td>793</td><td>1</td><td>16 0</td><td>1</td></tr><tr><td>1303</td><td>29</td><td>0</td><td>2</td><td>1001</td><td>1</td><td>4 2</td><td>1</td></tr><tr><td>347</td><td>29</td><td>0</td><td>1</td><td>1309</td><td>2</td><td>4</td><td>0 3</td></tr><tr><td>440</td><td>16</td><td>1</td><td>1</td><td>988</td><td>0</td><td>23 2</td><td>0</td></tr><tr><td>1160</td><td>27</td><td>0</td><td>2</td><td>1329</td><td>1</td><td>2 1</td><td>4</td></tr><tr><td>826</td><td>20</td><td>0</td><td>2</td><td>433</td><td>0</td><td>1</td><td>2 0</td></tr><tr><td>61</td><td>20</td><td>0</td><td>1</td><td>653</td><td>1</td><td>29 4</td><td>1</td></tr><tr><td>519</td><td>11</td><td>0</td><td>1</td><td>806</td><td>1</td><td>1 3</td><td></td></tr><tr><td>1027</td><td>16</td><td>0</td><td>2</td><td>401</td><td>1</td><td>1</td><td>1</td></tr><tr><td>92</td><td>12</td><td>0</td><td>2</td><td>1334</td><td>2</td><td>2 4 1</td><td>1</td></tr><tr><td>1169</td><td>9</td><td>0</td><td>2</td><td>486</td><td>1</td><td></td><td>3</td></tr><tr><td>620</td><td>17</td><td>0</td><td>2</td><td>1343</td><td>1</td><td>8</td><td>2 3 0</td></tr><tr><td>682</td><td>14</td><td>0</td><td>0</td><td>1184</td><td>1</td><td>27</td><td>3</td></tr><tr><td>40</td><td>17</td><td>0</td><td>2</td><td>464</td><td>1</td><td>1</td><td>2 1</td></tr><tr><td>977</td><td>16</td><td>0</td><td>0</td><td>999</td><td>1</td><td>4</td><td>1 4</td></tr><tr><td>75</td><td>13</td><td>0</td><td>2</td><td>746</td><td>1</td><td>26</td><td>0 5</td></tr><tr><td>422</td><td>1</td><td>1</td><td>2</td><td>489</td><td>0</td><td>8</td><td>3 1</td></tr><tr><td>1264</td><td>37</td><td>0</td><td>2</td><td>478</td><td>1</td><td>2</td><td>1 5 2</td></tr><tr><td>703</td><td>20</td><td>0</td><td>0</td><td>152</td><td>2</td><td>2</td><td>3</td></tr><tr><td>662</td><td>2</td><td>1</td><td>2</td><td>500</td><td>2</td><td>10</td><td>2 5</td></tr><tr><td>247</td><td>16</td><td>0</td><td>2</td><td>470</td><td>1</td><td>2 2</td><td>2 3 3</td></tr><tr><td>621</td><td>18</td><td>0</td><td>2</td><td>928</td><td>2</td><td>1</td><td>1 1</td></tr><tr><td>1192</td><td>31</td><td>0</td><td>2</td><td>464</td><td>1</td><td>16</td><td>1 2 3</td></tr><tr><td>553</td><td>22</td><td>0</td><td>2</td><td>804</td><td>1</td><td>2</td><td>0 3</td></tr><tr><td>186</td><td>22</td><td>0</td><td>2</td><td>989</td><td>1</td><td>4</td><td>0 3</td></tr><tr><td>156</td><td>33</td><td>0</td><td>2</td><td>1169</td><td>1</td><td>7</td><td>3 3</td></tr><tr><td>879</td><td>42</td><td>0</td><td>2</td><td>696</td><td>2</td><td>7</td><td>3 2</td></tr><tr><td>312</td><td>13</td><td>0</td><td>2</td><td>192</td><td>1</td><td>2</td><td>3 1</td></tr><tr><td>8</td><td>20</td><td>0</td><td>1</td><td>216</td><td>1</td><td>23</td><td>2 1</td></tr></table></body></html>

# Project A: 数据预处理

处理前，处理后数据对比  

<html><body><table><tr><td>uesr_id Age</td><td></td><td></td><td>Attrition BusinessTravel</td><td>DailyRateDepartmerDistanceFEducation</td><td></td><td></td><td>Educatior</td></tr><tr><td>1374</td><td>58No</td><td></td><td>Travel_Rarely</td><td>605 Sales</td><td>21</td><td></td><td>3Life Scie</td></tr><tr><td>1092</td><td>45No</td><td></td><td>Travel_Rarely</td><td>950 Research</td><td></td><td>28</td><td>3Technical</td></tr><tr><td>768</td><td>40No</td><td></td><td>Travel_Rarely</td><td>300 Sales</td><td></td><td>26</td><td>3Marketing</td></tr><tr><td>569</td><td>36No</td><td></td><td>Non-Travel</td><td>1434 Sales</td><td></td><td>8</td><td>4Life Scie</td></tr><tr><td>911</td><td></td><td>25Yes</td><td>Travel_Frequentl</td><td>599Sales</td><td></td><td>24</td><td>1 Life Scie</td></tr><tr><td>408</td><td>52No</td><td></td><td>Travel_Rarely</td><td>1490Research</td><td></td><td>4</td><td>2 Life Scie</td></tr><tr><td>1321</td><td>47No</td><td></td><td>Travel_Rarely</td><td>207 Research</td><td></td><td>9</td><td>4 Life Scie</td></tr><tr><td>1224</td><td>26No</td><td></td><td>Travel_Rarely</td><td>390 Research</td><td></td><td>17</td><td>4 Medical</td></tr><tr><td>1061</td><td>24 No</td><td></td><td>Non-Travel</td><td>830 Sales</td><td></td><td>13</td><td>2Life Scie</td></tr><tr><td>530</td><td>27No</td><td></td><td>Travel_Rarely</td><td>608 Research</td><td></td><td>1</td><td>2 Life Scie</td></tr><tr><td>1233</td><td>30No</td><td></td><td>Travel_Rarely</td><td>793Research</td><td></td><td>16</td><td>1 Life Scie</td></tr><tr><td>1303</td><td>47No</td><td></td><td>Travel_Rarely</td><td>1001 Research</td><td></td><td>4</td><td>3Life Scie</td></tr><tr><td>347</td><td>47No</td><td></td><td>Travel_Frequent1</td><td>1309 Sales</td><td></td><td>4</td><td>1 Medical</td></tr><tr><td>440</td><td>34 Yes</td><td></td><td>Travel_Frequent1</td><td>988 Human Res</td><td></td><td>23</td><td>3 Human Res</td></tr><tr><td>1160</td><td>45No</td><td></td><td>Travel_Rarely</td><td>1329 Research</td><td></td><td>2</td><td>20ther</td></tr><tr><td>826</td><td>38No</td><td></td><td>Travel_Rarely</td><td>433 Human Res</td><td></td><td>1</td><td>3 Human Res</td></tr><tr><td>61</td><td>38No</td><td></td><td>Travel_Frequent1</td><td>653 Research</td><td></td><td>29</td><td>5Life Scie</td></tr><tr><td>519</td><td>29No</td><td></td><td>Travel_Frequentl</td><td>806Research</td><td></td><td>1</td><td>4Life Scie</td></tr><tr><td>1027</td><td>34 No</td><td></td><td>Travel_Rarely</td><td>401 Research</td><td></td><td>1</td><td>3Life Scie</td></tr><tr><td>92</td><td>30No</td><td></td><td>Travel_Rarely</td><td>1334 Sales</td><td></td><td>4</td><td>2 Medical</td></tr><tr><td>1169</td><td>27No</td><td></td><td>Travel_Rarely</td><td>486 Research</td><td></td><td>8</td><td>3 Medical</td></tr><tr><td>620</td><td>35No</td><td></td><td>Travel_Rarely</td><td>1343Research</td><td></td><td>27</td><td>1 Medical</td></tr><tr><td>682</td><td>32No</td><td></td><td>Non-Travel</td><td>1184 Research</td><td></td><td>1</td><td>3Life Scie</td></tr><tr><td>40</td><td>35No</td><td></td><td>Travel_Rarely</td><td>464 Research</td><td></td><td>4</td><td>2Other</td></tr><tr><td>977</td><td>34 No</td><td></td><td>Non-Travel</td><td>999 Research</td><td></td><td>26</td><td>1 Technical</td></tr><tr><td>75</td><td>31 No</td><td></td><td>Travel_Rarely</td><td>746Research</td><td></td><td>8</td><td>4Life Scie</td></tr><tr><td>422</td><td>19 Yes</td><td></td><td>Travel_Rarely</td><td>489 Human Res</td><td></td><td>2</td><td>2 Technical</td></tr><tr><td>1264</td><td>55No</td><td></td><td>Travel_Rarely</td><td>478Research</td><td></td><td>2</td><td>3Medical</td></tr><tr><td>703 662</td><td>38 No</td><td></td><td>Non-Travel</td><td>152 Sales</td><td></td><td>10</td><td>3Technical</td></tr><tr><td>247</td><td>20 Yes</td><td></td><td>Travel_Rarely</td><td>500 Sales</td><td></td><td>2</td><td>3 Medical</td></tr><tr><td>621</td><td>34 No</td><td></td><td>Travel_Rarely</td><td>470 Research</td><td></td><td>2</td><td>4 Life Scie</td></tr><tr><td>1192</td><td>36No</td><td></td><td>Travel_Rarely</td><td>928 Sales</td><td></td><td>1</td><td>2 Life Scie</td></tr><tr><td></td><td>49 No</td><td></td><td>Travel_Rarely</td><td>464Research</td><td></td><td>16</td><td>3Medical</td></tr><tr><td>553</td><td>40No</td><td></td><td>Travel_Rarely</td><td>804 Research</td><td></td><td>2</td><td>1 Medical</td></tr><tr><td>186</td><td>40No</td><td></td><td>Travel_Rarely</td><td>989 Research</td><td></td><td>4</td><td>1 Medical</td></tr><tr><td>156 879</td><td>51No</td><td></td><td>Travel_Rarely</td><td>1169 Research</td><td></td><td>7 7</td><td>4 Medical 4Marketing</td></tr><tr><td>312</td><td>60No 31 No</td><td></td><td>Travel_Rarely Travel_Rarely</td><td>696 Sales 192 Research</td><td></td><td>2</td><td>4Life Scie</td></tr><tr><td>8</td><td>38 No</td><td></td><td>Travel_Frequentl</td><td>216 Research</td><td></td><td>23</td><td>3Life Scie</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

<html><body><table><tr><td>uesr_id</td><td>Age</td><td>Attritior BusinessTravel DailyRate</td><td></td><td>Department</td><td>DistanceFromHome</td><td>Education</td><td>Educatior</td></tr><tr><td>1374</td><td>40</td><td>0</td><td>2</td><td>605</td><td>2 21</td><td>2</td><td>1</td></tr><tr><td>1092</td><td>27</td><td>0</td><td>2</td><td>950</td><td>1</td><td>28 2</td><td>5</td></tr><tr><td>768</td><td>22</td><td>0</td><td>2</td><td>300</td><td>2</td><td>26 2</td><td>2</td></tr><tr><td>569</td><td>18</td><td>0</td><td>0</td><td>1434</td><td>2</td><td>8 3</td><td>1</td></tr><tr><td>911</td><td>7</td><td>1</td><td>1</td><td>599</td><td>2 24</td><td>0</td><td>1</td></tr><tr><td>408</td><td>34</td><td>0</td><td>2</td><td>1490</td><td>1</td><td>4 1</td><td>1</td></tr><tr><td>1321</td><td>29</td><td>0</td><td>2</td><td>207</td><td>1</td><td>9 3</td><td>1</td></tr><tr><td>1224</td><td>8</td><td>0</td><td>2</td><td>390</td><td>1</td><td>17 3</td><td>3</td></tr><tr><td>1061</td><td>6</td><td>0</td><td>0</td><td>830</td><td>2</td><td>13 1</td><td>1</td></tr><tr><td>530</td><td>9</td><td>0</td><td>2</td><td>608</td><td>1</td><td>1 1</td><td>1</td></tr><tr><td>1233</td><td>12</td><td>0</td><td>2</td><td>793</td><td>1</td><td>16 0</td><td>1</td></tr><tr><td>1303</td><td>29</td><td>0</td><td>2</td><td>1001</td><td>1</td><td>4 2</td><td>1</td></tr><tr><td>347</td><td>29</td><td>0</td><td>1</td><td>1309</td><td>2</td><td>4</td><td>0 3</td></tr><tr><td>440</td><td>16</td><td>1</td><td>1</td><td>988</td><td>0</td><td>23 2</td><td>0</td></tr><tr><td>1160</td><td>27</td><td>0</td><td>2</td><td>1329</td><td>1</td><td>2 1</td><td>4</td></tr><tr><td>826</td><td>20</td><td>0</td><td>2</td><td>433</td><td>0</td><td>1</td><td>2 0</td></tr><tr><td>61</td><td>20</td><td>0</td><td>1</td><td>653</td><td>1</td><td>29 4</td><td>1</td></tr><tr><td>519</td><td>11</td><td>0</td><td>1</td><td>806</td><td>1</td><td>1 3</td><td></td></tr><tr><td>1027</td><td>16</td><td>0</td><td>2</td><td>401</td><td>1</td><td>1</td><td>1</td></tr><tr><td>92</td><td>12</td><td>0</td><td>2</td><td>1334</td><td>2</td><td>2 4 1</td><td>1</td></tr><tr><td>1169</td><td>9</td><td>0</td><td>2</td><td>486</td><td>1</td><td></td><td>3</td></tr><tr><td>620</td><td>17</td><td>0</td><td>2</td><td>1343</td><td>1</td><td>8</td><td>2 3 0</td></tr><tr><td>682</td><td>14</td><td>0</td><td>0</td><td>1184</td><td>1</td><td>27</td><td>3</td></tr><tr><td>40</td><td>17</td><td>0</td><td>2</td><td>464</td><td>1</td><td>1</td><td>2 1</td></tr><tr><td>977</td><td>16</td><td>0</td><td>0</td><td>999</td><td>1</td><td>4</td><td>1 4</td></tr><tr><td>75</td><td>13</td><td>0</td><td>2</td><td>746</td><td>1</td><td>26</td><td>0 5</td></tr><tr><td>422</td><td>1</td><td>1</td><td>2</td><td>489</td><td>0</td><td>8</td><td>3 1</td></tr><tr><td>1264</td><td>37</td><td>0</td><td>2</td><td>478</td><td>1</td><td>2</td><td>1 5 2</td></tr><tr><td>703</td><td>20</td><td>0</td><td>0</td><td>152</td><td>2</td><td>2</td><td>3</td></tr><tr><td>662</td><td>2</td><td>1</td><td>2</td><td>500</td><td>2</td><td>10</td><td>2 5</td></tr><tr><td>247</td><td>16</td><td>0</td><td>2</td><td>470</td><td>1</td><td>2 2</td><td>2 3 3</td></tr><tr><td>621</td><td>18</td><td>0</td><td>2</td><td>928</td><td>2</td><td>1</td><td>1 1</td></tr><tr><td>1192</td><td>31</td><td>0</td><td>2</td><td>464</td><td>1</td><td>16</td><td>1 2 3</td></tr><tr><td>553</td><td>22</td><td>0</td><td>2</td><td>804</td><td>1</td><td>2</td><td>0 3</td></tr><tr><td>186</td><td>22</td><td>0</td><td>2</td><td>989</td><td>1</td><td>4</td><td>0 3</td></tr><tr><td>156</td><td>33</td><td>0</td><td>2</td><td>1169</td><td>1</td><td>7</td><td>3 3</td></tr><tr><td>879</td><td>42</td><td>0</td><td>2</td><td>696</td><td>2</td><td>7</td><td>3 2</td></tr><tr><td>312</td><td>13</td><td>0</td><td>2</td><td>192</td><td>1</td><td>2</td><td>3 1</td></tr><tr><td>8</td><td>20</td><td>0</td><td>1</td><td>216</td><td>1</td><td>23</td><td>2 1</td></tr></table></body></html>

# LR工具

# LR工具：

from sklearn.linear_model.logistic import LogisticRegression

# 参数:

penalty，惩罚项，正则化参数，防止过拟合，I1或l2，默认为l2·C，正则化系数入的倒数，float类型，默认为1.0solver，损失函数优化方法，liblinear（默认），Ibfgs，newton-cg，sagrandom_state，随机数种子max_iter，算法收敛的最大迭代次数，默认为100$\scriptstyle \mathbf { \ t o } 1 = 0 . 0 0 0 1$ ：优化算法停止条件，迭代前后函数差小于tol则终止verbose $= 0$ ：日志冗长度int：冗长度；0：不输出训练过程；1：偶尔输出； ${ > } 1$ ：对每个子模型都输出n_jobs $\mathord { \left. \vert { \begin{array} { r l } \end{array} } \right. }$ ：并行数，int：个数；-1：跟CPU核数一致；1:默认值

# 常用方法：

fit(X,y,sample_weight $\ c =$ None)  
fit_transform(X,y=None,\*\*fit_params)predict(X)，用来预测样本，也就是分类predict_proba(X)，输出分类概率。返回每种类别的概率，按照分类类别顺序给出。score(X,y, sample_weight $\mathop { \bf { \phi } } =$ None)，返回给定测试集合的平均准确率（mean accuracy）

# LR工具

# Step3, 模型参数配置

model $\mathbf { \tau } = \mathbf { \tau }$ LogisticRegression(max_iter=100, verbose $\mathbf { \tau } = \mathbf { \dot { \tau } }$ True, random_state $\scriptstyle = 3 3$ ， tol=1e-4 iter 25 act 1.052e+01 pre 1.050e+01 delta 2.865e-01 f 3.291e+02 g 3.106e+02 CG 9 t 5.049e-02 pre 5.014e-02 delta 2.865e-01 f 3.186e+02 lgl 5.462e+04 CG 1 t 8.230e-03 pre 8.184e-03 delta 2.865e-01 f 3.185e+02 lgl 5.377e+03 CG cg reaches trust region boundary ） t 9.298e+00 pre 9.223e+00 delta 4.300e-01 f 3.185e+02 |lgl 2.770e+02 CG 1: iter 29 act 2.898e-02 pre 2.878e-02 delta 4.300e-01 f 3.092e+02 lgl 3.736e+04 CG 2   
model.fit(X_train, y_train) iter act1epreedeta4elCG 6 2 cg reaches trust region boundary   
predict $\mathbf { \tau } = \mathbf { \tau }$ model.predict_proba(test)[:,1] act 7e2preedeta47 B 5.929e+04 CG 12 2 act 3.256e+00 pre 3.259e+00 delta 4.477e-01 f 3.007e+02 lgl 8.364e+02 CG 8 iter 35 act 3.303e-02 pre 3.265e-02 delta 4.477e-01 f 2.974e+02 lgl 3.015e+04 CG 2   
test['Attrition'] $\ c =$ predict iter act 3.761e-01 pre 3.697e-01 delta 4.477e-01 f 2.974e+02 lgl 3.958e+02 CG 6 iter 37 act 2.707e-04 pre 2.705e-04 delta 4.477e-01 f 2.970e+02 gl 3.163e+03 CG 2   
#转化为二分类输出   
test['Attrition']=test['Attrition'].map(lambda x:1 if $x > = 0 . 5$ else 0)   
test[['Attrition']].to_csv('submit_Ir.csv')

<html><body><table><tr><td>user_id</td><td>Attrition</td></tr><tr><td>442</td><td>0</td></tr><tr><td>1091</td><td>0</td></tr><tr><td>981</td><td>0</td></tr><tr><td>785</td><td>0</td></tr><tr><td>1332</td><td>1</td></tr><tr><td>501</td><td>0</td></tr><tr><td>1058</td><td>1</td></tr><tr><td>1253</td><td>0</td></tr><tr><td>751</td><td>0</td></tr><tr><td>122</td><td>0</td></tr><tr><td>268</td><td>0</td></tr><tr><td>940</td><td>0</td></tr><tr><td>1125</td><td>0</td></tr><tr><td>540</td><td>0</td></tr><tr><td>1034</td><td>0</td></tr><tr><td>432</td><td>0</td></tr><tr><td>794</td><td>0</td></tr><tr><td>666</td><td>0</td></tr><tr><td>942</td><td>0</td></tr><tr><td>1114</td><td>0</td></tr><tr><td>853</td><td>0</td></tr><tr><td>1330</td><td>0</td></tr><tr><td>692</td><td>0</td></tr><tr><td>548</td><td>0</td></tr><tr><td>54</td><td>1</td></tr><tr><td>319</td><td>0</td></tr><tr><td>1147</td><td>0</td></tr><tr><td>1235</td><td>0</td></tr><tr><td>1436</td><td>1</td></tr></table></body></html>

# SVM工具：

sklearn中支持向量分类主要有三种方法：SVC、NuSVC、LinearSVC   
sklearn.svm.SVC( $\ c = 1 . 0$ ，kernel $\mathbf { \Phi } = \mathbf { \Phi } ^ { \prime }$ rbf', degree $^ { = 3 }$ ,gamma $= ^ { \prime }$ auto', coef $) { = } 0 . 0$ shrinking $\mathop :$ True,probability=False,to $\mathsf { I } { = } 0 . 0 0 1$ ,cache_size $= 2 0 0$ class_weight $\ c =$ None,verbose $\ c =$ False,max_iter=-1, decision_function_shape $= "$ ovr',random_state $\ c =$ None)   
sklearn.svm.NuSVC( $\mathsf { n u } { = } 0 . 5$ ,kernel='rbf',degree $\mathbf { \lambda } = :$ 3,gamma $= ^ { \vert }$ auto', coef $\scriptstyle \mathtt { \lambda } = 0 . 0$ ，shrinking $\varXi$ True,probability=False, to $\mathsf { l } { = } 0 . 0 0 1$ ,cache_size=200, class_weight $\ c =$ None,verbose $\ c =$ False,max_iter=-1, decision_function_shape $= ^ { \mathsf { I } }$ ovr',random_state $\circleddash$ None)   
sklearn.svm.LinearSvC(penalty $= ^ { \prime } \vert { 2 ^ { \prime } }$ loss $= ^ { \prime }$ squared_hinge', dual=True, t $_ { \cdot 0 1 = 0 . 0 0 0 1 }$ ， ${ \mathsf { C } } { = } 1 . 0$ ，multi_class $= ^ { \prime }$ ovr',fit_intercept $\risingdotseq$ True, intercept_scaling $\scriptstyle = 1$ ,class_weight $\mathbf { \Psi } : =$ None,verbose $_ { = 0 }$ random_state $\ c =$ None,max_iter $\mathbf { \tau } = \mathbf { \tau }$ 1000)

# 常用参数：

·C，惩罚系数，类似于LR中的正则化系数，C越大惩罚越大nu，代表训练集训练的错误率的上限（用于NuSVC）kernel，核函数类型，RBF,Linear,Poly,Sigmoid,precomputed，默认为RBF径向基核（高斯核函数）gamma，核函数系数，默认为autodegree，当指定kernel为'poly'时，表示选择的多项式的最高次数，默认为三次多项式probability，是否使用概率估计shrinking，是否进行启发式，SVM只用少量训练样本进行计算penalty，正则化参数，L1和L2两种参数可选，仅LinearSVC有loss，损失函数，有‘hinge’和‘squared_hinge’两种可选，前者又称L1损失，后者称为L2损失  
·tol:残差收敛条件，默认是0.0001，与LR中的一致

# SVM工具

# SVM工具:

·SVC，SupportVector Classification，支持向量机用于分类 libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核  
·SVR，Support Vector Regression，支持向量机用于回归 Kernel核的选择技巧的：  
·sklearn中支持向量分类主要有三种方法：sVC、NusVC、LinearSVc·如果样本数量<特征数：  
·基于libsvm工具包实现，台湾大学林智仁教授在200年开发的一个简 方法1：简单的使用线性核就可以，不用选择非线性核单易用的SVM工具包 方法2：可以先对数据进行降维，然后使用非线性核  
·SVC，C-Support Vector Classification，支持向量分类 ·如果样本数量 $> =$ 特征数  
·NuSVC，Nu-Support VectorClasification，核支持向量分类，和SVc类 可以使用非线性核，将样本映射到更高维度，可以得到比较好的结果似，不同的是可以使用参数来控制支持向量的个数  
·LinearSVC， Linear Support Vector Classification

线性支持向量分类，使用的核函数是linear

SVM思想：一些线性不可分的问题可能是非线性可分的，也就是在高维空间中存在分离超平面（separating hyperplane）使用非线性函数从原始的特征空间映射至更高维的空间，转化为线性可分问题

![](images/321cc644739726a94352376f051e33b87d4d974e15b51db6657b1d44e189f435.jpg)  
complex in low dimensions   
simple in higher dimensions

# SVM工具

# Step3, 模型参数配置

![](images/8297924aa0468cd95f5b4fd3e8afb15793c3b0154e6b4d5cc7cd8fa7bfa0d64d.jpg)

<html><body><table><tr><td>user_id</td><td>Attrition</td></tr><tr><td>442</td><td>0</td></tr><tr><td>1091</td><td>0</td></tr><tr><td>981</td><td>0</td></tr><tr><td>785</td><td>0</td></tr><tr><td>1332</td><td>1</td></tr><tr><td>501</td><td>0</td></tr><tr><td>1058</td><td>1</td></tr><tr><td>1253</td><td>0</td></tr><tr><td>751</td><td>0</td></tr><tr><td>122</td><td>0</td></tr><tr><td>268</td><td>0</td></tr><tr><td>940</td><td>0</td></tr><tr><td>1125</td><td>0</td></tr><tr><td>540</td><td>1</td></tr><tr><td>1034</td><td>0</td></tr><tr><td>432</td><td>0</td></tr><tr><td>794</td><td>0</td></tr><tr><td>666</td><td>0</td></tr><tr><td>942</td><td>0</td></tr><tr><td>1114</td><td>0</td></tr><tr><td>853</td><td>0</td></tr><tr><td>1330</td><td>0</td></tr><tr><td>692</td><td>0</td></tr><tr><td>548</td><td>0</td></tr><tr><td>54</td><td>1</td></tr><tr><td>319</td><td>0</td></tr><tr><td>1147</td><td>0</td></tr><tr><td>1235</td><td>0</td></tr><tr><td>1436</td><td>1</td></tr></table></body></html>

# Project B： 男女声音识别

# 男女声音识别

·数据集：3168个录制的声音样本（来自男性和女性演讲者），采集的频率范围是0hz-280hz，已经对数据进行了预处理  
·一共有21个属性值，请判断该声音是男还是女？  
·使用Accuracy作为评价标准

B

# Project B: 男女声音识别

数据表字段：  

<html><body><table><tr><td>字段+</td><td>定义</td></tr><tr><td>meanfreq</td><td>平均频率（单位kHz)</td></tr><tr><td>sd</td><td>频率标准差</td></tr><tr><td>median</td><td>中位频率(单位kHz)</td></tr><tr><td>Q25</td><td>频率第一个四分位数(单位kHz)</td></tr><tr><td>Q75</td><td>频率第三个四分位数(单位kHz)</td></tr><tr><td>IQR</td><td>四分位数间距</td></tr><tr><td>skew</td><td>歪斜</td></tr><tr><td>kurt</td><td>峰度</td></tr><tr><td>sfm</td><td>频谱平坦度</td></tr><tr><td>mode</td><td>波模频率</td></tr><tr><td>centroid</td><td>质心频率</td></tr></table></body></html>

<html><body><table><tr><td>字段</td><td>定义</td></tr><tr><td>peakf</td><td>峰值频率</td></tr><tr><td>meanfun</td><td>测量声信号的基频平均值</td></tr><tr><td>minfun</td><td>测量声信号的最小基频</td></tr><tr><td>maxfun</td><td>测量声波信号的最大基频</td></tr><tr><td>meandom</td><td>通过声学信号测量的主导频率的平均值</td></tr><tr><td>mindom</td><td>通过声学信号测量的最小频率</td></tr><tr><td>maxdom</td><td>通过声学信号测量的最大频率</td></tr><tr><td>dfrange</td><td>声学信号测量的主频范围</td></tr><tr><td>modindx</td><td>调制指数（计算基频相邻测量值之间的累积 绝对差除以频率范围)</td></tr><tr><td>label</td><td>男or女</td></tr></table></body></html>

# Project B： 男女声音识别

# 男女声音识别

·Step1，数据加载  
Step2，数据预处理  
分离特征X和Target y  
使用标签编码，male $- > 1$ ,female $- > 0$   
将特征X矩阵进行规范化  
#标准差标准化，处理后的数据符合标准正态分布  
scaler $\mathbf { \tau } = \mathbf { \tau }$ StandardScaler()  
Step3，数据集切分，train_test_split  
Step4，模型训练  
SVM， Linear SVM  
Step5，模型预测

# 常用预测模型

数据预处理  
分类算法：LR，SVM，KNN  
树模型：GBDT,XGBoost,LightGBM,CatBoost，NGBoost

# 每种模型都有适用的场景

# LR优点：

实现简单，广泛的应用于工业问题上;·分类时计算量非常小，速度很快，使用资源低；·方便观测样本概率分数；

# LR缺点:

·当特征空间很大时，LR的性能不是很好；  
·容易欠拟合，准确度不太高；  
·不能很好地处理大量多类特征或变量;  
·通常只处理二分类问题，多分类需要使用softmax（LR在多分类的推广），且必须线性可分；  
·对于非线性特征，需要进行转换；

# SVM优点：

·可以解决高维问题，即大型特征空间；  
·能够处理非线性特征的相互作用；  
·需要先对数据进行归一化，因为计算是基于距离的模型，所以SVM和LR都需要对数据进行归一化处理

# SVM缺点:

·当样本很多时，效率并不是很高；  
·对非线性问题没有通用解决方案，可能会很难找到合适核函数

对缺失数据敏感；

SVM核的选择是有技巧的，样本数量<特征数，线性核，大于特征数使用非线性核

# 每种模型都有适用的场景

·可以使用LR模型作为预测的Baseline  
·FM衍生模型在推荐系统，尤其是CTR预估中有广泛应用，弥补了LR模型的不足（需要人工组合特征，耗费大量时间和人力)  
·Attention机制，对于Diversity多样性的情况，Attention机制可以提升效率，并且得出更好的结果  
·TreeEnsemble模型，比如GBDT，使用广泛，因为训练模型更可控  
对于LR模型，如果欠拟合，需要增加feature，才能提高  
准确率。而对于tree-ensemble来说，解决方法是训练更  
多的决策树tree。Kaggle比赛中使用很多

# 常用预测模型：

·分类算法：LR，SVM，KNN  
矩阵分解：FunkSVD，BiasSVD，SVD++  
· FM模型：FM,FFM,DeepFM,NFM，AFM  
树模型：GBDT,XGBoost, LightGBM,CatBoost，NGBoost  
Attention模型： DIN,DIEN,DSIN

# 2/2机器学习神器

# 什么是集成学习

# 集成学习：

·思想，将多个弱分类器按照某种方式组合起来，形成一个强分类器（三个臭皮匠赛过诸葛亮）

·Bagging，把数据集通过有放回的抽样方式，划分为多个数据集，分别训练多个模型。针对分类问题，按照少数服从多数原则进行投票，针对回归问题，求多个测试结果的平均值

·Stacking，通常是不同的模型，而且每个分类都用了全部训练数据，得到预测结果y1,y2,.,yk，然后再训练一个分类器 Meta Classifier,将这些预测结果作为输入，得到最终的预测结果

![](images/53a1b936c7ce3478b98dab10e7c8102f58d2e43bcaf666559f909ad092233dff.jpg)

# 什么是集成学习

# 集成学习：

·Boosting，与Bagging一样，使用的相同的弱学习器，不过是以自适应的方法顺序地学习这些弱学习器，即每个新学习器都依赖于前面的模型，并按照某种确定性的策略将它们组合起来  
·两个重要的Boosting算法：AdaBoost（自适应提升）和Gradient Boosting（梯度提升）  
·AdaBoost，使用前面的学习器用简单的模型去适配数据，然后分析错误。然后会给予错误预测的数据更高权重，然后用后面的学习器去修复  
·Boosting通过把一些列的弱学习器串起来，组成一个强学

习器

![](images/599a6bff745263dd52e51b8b2b81f67acd865c24caf05e329e864f1cce72c1f8.jpg)

268

集成学习：训练弱学习器，并添加到集成模型中更新：基于当前的集成学习结果，更新训练集（值或权重）

# 什么是集成学习

# Boosting与Bagging:

·结构上，Bagging是基分类器并行处理，而Boosting是串行处理·训练集，Bagging的基分类器训练是独立的，而Boosting的训练集是依赖于之前的模型  
·作用，Bagging的作用是减少variance，而Boosting在于减少bias对于Bagging，对样本进行重采样，通过重采样得到的子样本集训练模型，最后取平均。因为子样本集的相似性，而且使用相同的弱学习器，因此每个学习器有近似相等的bias和variance，因为每个学习器相互独立，所以可以显著降低variance，但是无法降低bias  
对于Boosting，采用顺序的方式最小化损失函数，所以bias自然是逐步下降，子模型之和不能显著降低variance

![](images/edf2b3b0593f27d89cdc19d6471d7c12ac4980451cce3c2be296347880e4234f.jpg)  
Boosting学习方式

# 机器学习神器

GradientBoosting集成学习：

·XGBoost,LightGBM,CatBoost,NGBoost实际上是对GBDT方法的不同实现，针对同一目标、做了不同的优化处理

![](images/0c9cfbc50be92e890c937065c909de16d0f34c6a3fd96aee7efa1fd4942ae296.jpg)

# XGBoost:

https://arxiv.0rg/abs/1603.02754

· 对于一个问题，INPUT X: age,gender,occupation,…

Target y: How does the person like computer games?

![](images/bce0f5aeef8ae5cb68f79a6a57fa19c442de6e56d734e2272bb9419071d87b16.jpg)

![](images/bd6f762df811ee008aaf9963e63e76b3fc11e136c5f2ecdd6776ac46b799a6d6.jpg)  
每个叶子节点对应预测的分数

基学习器，采用CART回归树

# XGBoost:

目标函数 $\dot { \bf \Phi } =$ 损失函数 $^ +$ 正则化项

$$
O b j ( \Theta ) = \underset { \ Y } { \cal L } ( \Theta ) + \boldsymbol { \Omega } ( \Theta )
$$

损失函数：拟合数据 正则化项：惩罚复杂模型

·误差函数尽量拟合训练数据，正则化项鼓励简单的模型

· $\Omega ( f _ { t } )$ 用于控制树的复杂度，防止过拟合，使得模型更简化，也使得最终的模型的预测结果更稳定

$$
\Omega \big ( f _ { t } \big ) = \gamma T + \frac { 1 } { 2 } \lambda \sum _ { j = 1 } ^ { T } w _ { \underbrace { j } } ^ { 2 }
$$

T：叶子数量 wj：叶子分数的L2正则项

γ：加入新叶子节点引入的复杂度代价

$f _ { t } \left( x \right) = w _ { q \left( x \right) } \quad \mathsf { w }$ 代表叶子向量，q表示树的结构

![](images/7ff4db229a194224e78321c829b24da278132a02ba9690cdf691fb558332f95b.jpg)

$$
\Omega = \gamma \times 3 + \frac { 1 } { 2 } \lambda \big ( 4 + 0 . 0 1 + 1 \big )
$$

TreeEnsemble集成学习：

单个CART回归树过于简单，可以通过多个CART回归树组成一个强学习器·预测函数，样本的预测结果 $\mathbf { \Psi } =$ 每棵树预测分数之和$\hat { y } _ { i } = \sum _ { k = 1 } ^ { K } f _ { k } \big ( x _ { i } \big )$

![](images/82d57e6d443f3683a446d5e0c8a8e46c765a50ffebb42c3c28f4ede0a8bf9309.jpg)

目标函数优化

$$
\begin{array} { c } { { O b j \displaystyle \big ( \Theta \big ) = \sum _ { i } l \big ( y _ { i } , \hat { y } _ { i } \big ) + \sum _ { k } \Omega \big ( f _ { k } \big ) } } \\ { { \Omega \big ( f \big ) = \gamma T + \displaystyle \frac { 1 } { 2 } \lambda \| w \| ^ { 2 } } } \end{array}
$$

正则项是由叶子结点的数量和叶子结点权重的平方和决定

XGBoost的目标函数：

$$
O b j ^ { t } = \sum _ { i = 1 } ^ { n } l \big ( y _ { i } , \hat { y } _ { i } ^ { ( t - 1 ) } + f _ { t } \big ( x _ { i } \big ) \big ) + \Omega \big ( f _ { t } \big ) + \mathrm { c o n s t a n t }
$$

·对目标函数改进，进行二阶泰勒展开：

$$
f { \big ( } x + \Delta x { \big ) } \approx f { \big ( } x { \big ) } + f ^ { \prime } { \big ( } x { \big ) } \Delta x + { \frac { 1 } { 2 } } f ^ { \prime } { \big ( } x { \big ) } \Delta x ^ { 2 }
$$

·定义

![](images/0d178149732935b49856f5f5afb50e7718fe14139031ea35498ba592fb30aab4.jpg)

$$
\begin{array} { r l } & { \hat { y } _ { i } ^ { ( 0 ) } = 0 } \\ & { \hat { y } _ { i } ^ { ( 1 ) } = f _ { 1 } ( x _ { i } ) = \hat { y } ^ { ( 0 ) } + f _ { 1 } ( x _ { i } ) } \\ & { \hat { y } _ { i } ^ { ( 2 ) } = f _ { 1 } ( x _ { i } ) + f _ { 2 } ( x _ { i } ) = \hat { y } _ { i } ^ { ( 1 ) } + f _ { 2 } ( x _ { i } ) } \\ & { \cdots } \end{array}
$$

$$
\hat { \cal y } _ { i } ^ { ( t ) } = \sum _ { k = 1 } ^ { t } f _ { k } ( x _ { i } ) = \hat { \cal y } _ { i } ^ { ( t - 1 ) } + f _ { t } ( x _ { i } )
$$

保留前t-1轮的模型预测 加入新的预测函数

如何选择每一轮的预测函数f？

选取一个f来使得目标函数尽量降低，即加入f后的预测结果与实际结果误差减少

XGBoost的目标函数：

$$
\begin{array} { l } { { \displaystyle O b j ^ { \prime } \approx \sum _ { i = 1 } ^ { n } \biggl [ g _ { i } f _ { i } \biggl ( x _ { i } \biggr ) + \frac { 1 } { 2 } h _ { i } f _ { i } ^ { 2 } \bigl ( x _ { i } \bigr ) \biggr ] + \Omega \bigl ( f _ { i } \bigr ) } } \\ { { \displaystyle \qquad = \sum _ { i = 1 } ^ { n } \biggl [ g _ { i } ^ { \mathrm { \tiny \dag } } w _ { q ( x _ { i } ) } + \frac { 1 } { 2 } h _ { i } w _ { q ( x _ { i } ) } ^ { 2 } \biggr ] + \gamma + \lambda \frac { 1 } { 2 } \sum _ { j = 1 } ^ { T } w _ { j } ^ { 2 } } } \\ { { \displaystyle \qquad = \sum _ { j = 1 } ^ { T } \biggl [ \biggl ( \sum _ { i \in I _ { i } } g _ { i } \biggr ) w _ { j } + \frac { 1 } { 2 } \biggl ( \sum _ { i \in I _ { i } } h _ { i } + \lambda \biggr ) w _ { j } ^ { 2 } \biggr ] + \gamma { \cal T } } } \end{array}
$$

·T为叶子节点数量

$I _ { j }$ 定义为每个叶子节点里面的样本集合 $I _ { j } = \left\{ i \vert q ( x _ { i } ) = j \right\}$ $f _ { t } \left( x _ { i } \right) = w _ { q ( x _ { i } ) }$ 即每个样本所在叶子节点索引的分数（叶子权重 $\boldsymbol { \mathsf { w } } )$ （204号

$G _ { j } , H _ { j }$ 分别表示每个叶子节点的一阶梯度的和，与二阶梯度的和：

$$
\begin{array} { c } { { G _ { j } = \displaystyle \sum _ { i \in I _ { j } } g _ { i } } } \\ { { { } } } \\ { { H _ { j } = \displaystyle \sum _ { i \in I _ { j } } h _ { i } } } \end{array}
$$

目标函数改写为：

$$
\begin{array} { l } { { \displaystyle O b j ^ { t } = \sum _ { j = 1 } ^ { T } \left[ \left( \sum _ { i \in I _ { j } } g _ { i } \right) w _ { j } + \frac { 1 } { 2 } \left( \sum _ { i \in I _ { j } } h _ { i } + \lambda \right) w _ { j } ^ { 2 } \right] + \gamma T } } \\ { { \displaystyle \ = \sum _ { j = 1 } ^ { T } \left[ G _ { j } w _ { j } + \frac { 1 } { 2 } \big ( H _ { j } + \lambda \big ) w _ { j } ^ { 2 } \right] + \gamma T } } \end{array}
$$

$$
\begin{array} { r l } { { } } & { { \displaystyle \frac { \partial O b j } { \partial w _ { j } } = G _ { j } + \big ( H _ { j } + \lambda \big ) w _ { j } = 0 } } \\ { { } } & { { w _ { j } = - \displaystyle \frac { G _ { j } } { H _ { j } + \lambda } \quad O b j = - \displaystyle \frac { 1 } { 2 } \sum _ { j = 1 } ^ { T } \displaystyle \frac { G _ { j } ^ { 2 } } { H _ { j } + \lambda } + \gamma T } } \end{array}
$$

XGBoost的目标函数：

·Obj目标函数也称为结构分数（打分函数），代表当指定一个树的结构的时候，我们在目标上最多可以减少多少

![](images/d9f895903cd342f2b9fa2ec0f499e61c04be1dff99fc969bcc5a1a6fe1b90f64.jpg)

XGBoost的目标函数：

·求Obj分数最小的树结构，可以穷举所有可能，但计算量太大

$$
O b j = - \frac { 1 } { 2 } \sum _ { j = 1 } ^ { T } \frac { G _ { j } ^ { 2 } } { H _ { j } + \lambda } + \gamma T
$$

·使用贪心法，即利用打分函数（计算增益）

以Gain作为是否分割的条件，Gain看作是未分割前的Obj减去分割后的左右Obj，加入新叶子节点引入的复杂度代价

![](images/bca0cb5b1e08f5f8d156bac8ef1ce8738098733e82f6ee0b07f467466b08854a.jpg)

如果 $\mathsf { G a i n } { < } 0$ ，则此叶节点不做分割，分割方案个数很多，计算量依然很大

# XGBoost的分裂节点算法：

·贪心方法，获取最优分割节点（splitpoint）  
将所有样本按照gi从小到大排序，通过遍历，查看每个  
节点是否需要分割  
·对于特征值的个数为n时，总共有n-1种划分  
·Step1，对样本扫描一遍，得出GL，GR  
·Step2，根据Gain的分数进行分割通过贪心法，计算效率得到大幅提升，XGBoost重新定义划分属性，即Gain，而Gain的计算是由目标损失函数obj决定的

<html><body><table><tr><td colspan="4">a</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>g1.h1</td><td>g4,h4</td><td>g2,h2</td><td>g5,h5 g3,h3</td></tr><tr><td>G, = g1 +g4</td><td></td><td>GR=g2+g+g5</td><td></td></tr></table></body></html>

Algorithm 1: Exact Greedy Algorithm for Split Finding   

<html><body><table><tr><td>G ←∑ie1 9i,H←∑i∈1hi gain ←0</td><td>Input: d, feature dimension</td><td></td><td></td></tr><tr><td>for k =1 to m do GL←O，HL←0</td><td></td><td></td><td></td></tr><tr><td colspan="2"></td><td>for j in sorted(I, by Xjk) do</td><td></td></tr><tr><td colspan="2"></td><td>GL ←GL+gj,HL←HL+ʰj</td><td></td></tr><tr><td colspan="2">GR←G-GL,HR←H-HL</td><td></td><td></td></tr><tr><td colspan="2"></td><td></td><td></td></tr><tr><td colspan="2"></td><td>G²</td><td>Q2</td></tr><tr><td colspan="2"></td><td>G²R score ← max(score,HL+x + HR+X - H+X)</td><td></td></tr><tr><td colspan="2"></td><td></td><td></td></tr><tr><td colspan="2">end</td><td></td><td></td></tr><tr><td colspan="2"></td><td></td><td></td></tr></table></body></html>

XGBoost的分裂节点算法（近似算法，Histogram 2016 paper）：

·对于连续型特征值，样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合  
·方法，在寻找split节点的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，然后形成若干个bucket(桶)，只将bucket边界上的特征值作为split节点的候选，从而获得性能提升  
·从算法伪代码中该流程还可以分为两种，全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部近似就是在具体的某一次分裂节点的过程中采用近似算法  
for $k = 1$ to m doPropose $S _ { k } = \{ s _ { k 1 } , s _ { k 2 } , \cdot \cdot \cdot s _ { k l } \}$ by percentiles on feature $k$ Proposal can be done per tree (global), or per split(local).  
end  
for $k = 1$ to m do（20 $\begin{array} { r } { G _ { k v }  = \sum _ { j \in \{ j | s _ { k , v } \geq \mathbf { x } _ { j k } > s _ { k , v - 1 } \} } g _ { j } } \\ { H _ { k v }  = \sum _ { j \in \{ j | s _ { k , v } \geq \mathbf { x } _ { j k } > s _ { k , v - 1 } \} } h _ { j } } \end{array}$ （204号  
end  
Follow same step as in previous section to find max  
score only among proposed splits.

XGBoost算法特点：

·XGBoost将树模型的复杂度加入到正则项中，从而避免过拟合，泛化性能好  
·损失函数是用泰勒展开式展开的，用到了一阶导和二阶导，可以加快优化速度  
·在寻找最佳分割点时，采用近似贪心算法，用来加速计算  
·不仅支持CART作为基分类器，还支持线性分类器，在使用线性分类器的时候可以使用L1，L2正则化  
·支持并行计算，XGBoost的并行是基于特征计算的并行，将特征列排序后以block的形式存储在内存中，在后面的迭代中重复使用这个结构。在进行节点分裂时，计算每个特征的增益，选择增益最大的特征作为分割节点，各个特征的增益计算可以使用多线程并行  
·优点：速度快、效果好、能处理大规模数据、支持自定义损失函数等  
·缺点：算法参数过多，调参复杂，不适合处理超高维特征数据

# XGBoost工具

XGBoost工具:

https://github.com/dmlc/xgboost  
参数分为：  
通用参数：对系统进行控制Booster参数：控制每一步的booster(tree/regression)  
·学习目标参数：控制训练目标的表现

# 通用参数：

·booster，模型选择，gbtree或者gblinear。gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算。[default $\mathop { \bf { \phi } } =$ gbtree]  
·silent，缄默方式，0表示打印运行时，1表示以缄默方式运行，不打印运行时信息。[default $\scriptstyle : = 0 ]$   
·nthread，XGBoost运行时的线程数，[default=缺省值是当前系统可以获得的最大线程数]  
·num_feature，boosting过程中用到的特征个数，XGBoost会自动设置

# XGBoost工具

# Booster参数：

·eta [default $\left. = 0 . 3 \right]$ 1，为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。eta通过缩减特征的权重使提升计算过程更加保守，取值范围为[0,1]

·gamma [default $\left[ = 0 \right]$ ，分裂节点时，损失函数减小值只有大于等于gamma节点才分裂，gamma值越大，算法越保守，越不容易过拟合，但性能就不一定能保证，需要tradeoff，取值范围 $[ 0 , \infty ]$

· max_depth [default $\mathrel { \mathop = } 6$ 5]，树的最大深度，取值范围为 $[ 1 , \infty ]$ ，典型值为3-10

·min_child_weight[default $_ { : = 1 ] }$ ，一个子集的所有观察值的最小权重和。如果新分裂的节点的样本权重和小于min_child_weight则停止分裂。这个可以用来减少过拟合，但是也不能太高，会导致欠拟合，取值范围为 $[ 0 , \infty ]$

$$
G a i n = { \frac { 1 } { 2 } } \Biggl [ { \frac { G _ { L } ^ { 2 } } { H _ { L } + \lambda } } + { \frac { G _ { R } ^ { 2 } } { H _ { R } + \lambda } } - { \frac { ( G _ { L } + G _ { R } ) ^ { 2 } } { H _ { L } + H _ { R } + [ \lambda ] } } \Biggr ] - [ \gamma ] \Biggr \} \quad 
$$

# XGBoost工具

# Booster参数：

·subsample[default $: = 1$ 1，构建每棵树对样本的采样率，如果设置成0.5，XGBoost会随机选择 $50 \%$ 的样本作为训练集  
·colsample_bytree [default $\scriptstyle : = 1 ]$ ，列采样率，也就是特征采样率  
· lambda[default ${ \tt \Psi } = 1 { \tt \Psi }$ ,alias:reg_lambda]，L2正则化，用来控制XGBoost的正则化部分  
·alpha[default ${ \bf \bar { \Lambda } } { \bf \Lambda } = 0$ ,alias:reg_alpha]，L1正则化，增加该值会让模型更加收敛  
·scale_pos_weight[default $\scriptstyle : = 1 ]$ ，在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛

# XGBoost工具

# 学习目标参数：

·objective[default $\mathbf { \Psi } : =$ reg:linear]，定义学习目标，reg:linear，reg:logistic，binary:logistic，binary:logitraw，count:poisson，multi:softmax,multi:softprob， rank:pairwise  
·eval_metric，评价指标，包括rmse，logloss，error，merror，mlogloss，auc，ndcg，map等  
·seed[ default $_ { : = 0 }$ ]，随机数的种子  
·dtrain，训练的数据  
·num_boost_round，提升迭代的次数，也就是生成多少基模型  
·early_stopping_rounds，早停法迭代次数  
·evals：这是一个列表，用于对训练过程中进行评估列表中的元素。形式是evals $\mathbf { \tau } = \mathbf { \tau }$ [(dtrain,'train'),(dval,'val')]或者是ev $\mathsf { a l s } = [ ( \mathsf { d t r a i n } , ^ { \prime } \mathsf { t r a i n } ^ { \prime } )$ ］对于第一种情况，它使得我们可以在训练过程中观察验证集的效果  
·verbose_eval，如果为True，则对evals中元素的评估输出在结果中；如果输入数字，比如5，则每隔5个迭代输出一次  
·learning_rates：每一次提升的学习率的列表

# XGBoost工具

#天猫用户复购预测（XGBoost使用示意）

X_train,X_valid,y_train,Y_valid $\mathbf { \tau } = \mathbf { \tau }$ train_test_split(train_X,train_y,test_size=.2)  
#使用XGBoost  
model $\mathbf { \tau } = \mathbf { \tau }$ xgb.XGBClassifier(max_depth $^ { = 8 }$ ，#树的最大深度n_estimators $_ { \mathsf { 3 } } = 1 0 0 0$ ，#提升迭代的次数，也就是生成多少基模型min_child_weight $: = 3 0 0$ ,#一个子集的所有观察值的最小权重和colsample_bytree=0.8,#列采样率，也就是特征采样率subsample $\scriptstyle : = 0 . 8$ ，#构建每棵树对样本的采样率eta $= 0 . 3$ ，#eta通过缩减特征的权重使提升计算过程更加保守，防止过拟合seed=42 #随机数种子  
）  
model.fit(X_train, y_train,eval_metric $= ^ { \prime }$ auc',eval_set $\ c =$ [(X_train,y_train),(X_valid,y_valid)],verbose=True,#早停法，如果auc在10epoch没有进步就stopearly_stopping_round $_ { : = 1 0 }$   
】  
model.fit(X_train, y_train)  
prob $\mathbf { \tau } = \mathbf { \tau }$ model.predict_proba(test_data)

# XGBoost工具

# Step3, 模型参数配置

param $\mathbf { \tau } = \mathbf { \tau }$ {'boosting_type':'gbdt', 'objective' :'binary:logistic',#任务目标 'eval_metric':'auc',#评估指标 'eta' :0.01,#学习率 'max_depth':15,#树最大深度 'colsample_bytree':0.8,#设置在每次迭代中使用特征的比例 'subsample':0.9,#样本采样比例 'subsample_freq':8,#bagging的次数 'alpha': 0.6,#L1正则 'lambda': 0,#L2正则

# XGBoost工具

# Step4, 模型训练，得出预测结果

X_train, X_valid, y_train, y_valid $\mathbf { \tau } = \mathbf { \tau }$ train_test_split(train.drop('Attrition',axis $_ { \cdot = 1 }$ ), train['Attrition'],   
test_size ${ \tt \Psi } = 0 . 2$ ,random_state $= 4 2$ ）   
train_data $\mathbf { \tau } = \mathbf { \tau }$ xgb.DMatrix(X_train,label=y_train)   
valid_data $\mathbf { \tau } = \mathbf { \tau }$ xgb.DMatrix(X_valid, label=y_valid)   
test_data $\mathbf { \tau } = \mathbf { \tau }$ xgb.DMatrix(test)   
model $\mathbf { \tau } = \mathbf { \tau }$ xgb.train(param, train_data, evals $\ c =$ [(train_data,'train'),(valid_data,'valid')],   
num_boost_round $= 1 0 0 0 0$ ,early_stopping_round $s = 2 0 0$ ,verbose_eva $1 = 2 5$ ）   
predict $\mathbf { \tau } = \mathbf { \tau }$ model.predict(test_data)   
test['Attrition'] $\ c =$ predict   
#转化为二分类输出   
test['Attrition']=test['Attrition'].map(lambda x:1 if $x > = 0 . 5$ else 0)   
test[['Attrition']].to_csv('submit_lgb.csv')

<html><body><table><tr><td></td><td></td><td></td><td></td><td></td><td>5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Attritior</td><td></td></tr><tr><td>T</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>T</td><td>0</td><td>0</td><td>0</td><td>0</td><td></td><td>0</td><td>T</td><td>0</td><td>0</td><td>0</td><td></td><td>0</td><td></td><td>0</td><td>0</td><td>0</td><td></td><td>T</td><td></td><td>L</td><td></td><td></td><td></td><td></td></tr></table></body></html>

# LightGBM:

2017年经微软推出，XGBoost的升级版  
Kaggle竞赛使用最多的模型之一，必备机器学习神器  
Light $\Rightarrow$ 在大规模数据集上运行效率更高（20 $G B M = 2$ Gradient Boosting Machine

# Motivation:

·常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制  
·GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。对于工业级海量的数据，普通的GBDT算法是不能满足其需求的  
·LightGBM的提出是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业场景

内存消耗  

<html><body><table><tr><td>数据集</td><td>XGBoost</td><td>XGBoost_approxLightGBM</td><td></td></tr><tr><td>Higgs</td><td>4.853GB</td><td>4.875GB</td><td>0.822GB</td></tr><tr><td>Yahoo LTR</td><td>1. 907GB</td><td>2. 221GB</td><td>0.831GB</td></tr><tr><td>MSLTR</td><td>5.469GB</td><td>5. 600GB</td><td>0. 745GB</td></tr><tr><td>Expo</td><td>1. 553GB</td><td>1.560GB</td><td>0. 450GB</td></tr></table></body></html>

评估指标（AUC，NDCG）  

<html><body><table><tr><td>数据集</td><td>指标</td><td>XGBoost</td><td>XGBoost_approx LightGBM</td><td></td></tr><tr><td>Higgs</td><td></td><td>0. 839528</td><td>0.840533</td><td>0. 845123</td></tr><tr><td>Yahoo LTR</td><td>NDCG@5</td><td>0.740316</td><td>0.739363</td><td>0. 756369</td></tr><tr><td>MS LTR</td><td>NDCG@5</td><td>0. 476245</td><td>0. 474441</td><td>0.510153</td></tr><tr><td>Expo</td><td>AUC</td><td>0. 75548</td><td>0. 757071</td><td>0. 781061</td></tr></table></body></html>

# LightGBM与XGBoost:

·模型精度：两个模型相当训练速度：LightGBM训练速度更快 $\Rightarrow 1 / 1 0$ ·内存消耗：LightGBM占用内存更小 $\Rightarrow 1 / 6$ ·特征缺失值：两个模型都可以自动处理特征缺失值·分类特征：XGBoost不支持类别特征，需要对其进行OneHot编码，而LightGBM支持分类特征

![](images/1c6428f95b5a5a3dad803baaf33aa129054e127faf2a53c1d0b8556472f93b02.jpg)  
TimeCost Comparison(#machine $= 1$ lower $\mathbf { \Sigma } = \mathbf { \Sigma }$ better

# XGBoost模型的复杂度：

·模型复杂度 $\mathbf { \sigma } = \mathbf { \sigma }$ 树的棵数×每棵树的叶子数量×每片叶子生成复杂度   
·每片叶子生成复杂度 $\mathbf { \sigma } = \mathbf { \sigma }$ 特征数量 $\mathsf { x }$ 候选分裂点数量×样本的数量   
针对XGBoost的优化：   
·Histogram算法，直方图算法 $\Rightarrow$ 减少候选分裂点数量   
·GOSS算法，基于梯度的单边采样算法 $\Rightarrow$ 减少样本的数量   
·EFB算法，互斥特征捆绑算法 $\Rightarrow$ 减少特征的数量   
· LightGBM $\mathbf { \tau } = \mathbf { \tau }$ XGBoost $^ +$ Histogram $+$ GOSS + EFB

![](images/3b22866204182136fdb2226d6b1b032176d67af3deed8db8394b7829fdc63fa5.jpg)  
TimeCostComparison(#machine ${ \bf \Omega } = 1 { \bf \Omega } ^ { \prime }$ ，lower $\mathbf { \Sigma } = \mathbf { \Sigma }$ better

XGBoost的预排序（pre-sorted）算法：

·将样本按照特征取值排序，然后从全部特征取值中找到最优的分裂点位·预排序算法的候选分裂点数量 $\mathbf { \bar { \rho } } = \mathbf { \bar { \rho } }$ 样本特征不同取值个数减1

<html><body><table><tr><td>i</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>Xi</td><td>0.1</td><td>2.1</td><td>2.5</td><td>3.0</td><td>3.0</td><td>4.0</td><td>4.5</td><td>5.0</td></tr><tr><td>gi</td><td>0.01</td><td>0.03</td><td>0.06</td><td>0.05</td><td>0.04</td><td>0.7</td><td>0.6</td><td>0.07</td></tr><tr><td>hi</td><td>0.2</td><td>0.04</td><td>0.05</td><td>0.02</td><td>0.08</td><td>0.02</td><td>0.03</td><td>0.03</td></tr></table></body></html>

$$
G a i n { = } \frac { 1 } { 2 } \Biggl [ \frac { G _ { { L } } ^ { 2 } } { H _ { { L } } + \lambda } { + } \frac { G _ { { R } } ^ { 2 } } { H _ { { R } } + \lambda } { - } \frac { \left( G _ { { L } } + G _ { { R } } \right) ^ { 2 } } { H _ { { L } } + H _ { { R } } + \lambda } \Biggr ] { - } \gamma
$$

LightGBM的Histogram算法：

替代XGBoost的预排序算法

思想是先连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图，即将连续特征值离散化到k个bins上（比如 $k = 2 5 5$ ），当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点XGBoost需要遍历所有离散化的值，LightGBM只要遍历k个直方图的值·候选分裂点数量 $= k - 1$

bin1   

<html><body><table><tr><td>i</td><td>１</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>Xi</td><td>0.1</td><td>2.1</td><td>2.5</td><td>3.0</td><td>3.0</td><td>4.0</td><td>4.5</td><td>5.0</td></tr><tr><td>gi</td><td>0.01</td><td>0.03</td><td>0.06</td><td>0.05</td><td>0.04</td><td>0.7</td><td>0.6</td><td>0.07</td></tr><tr><td>hi</td><td>0.2</td><td>0.04</td><td>0.05</td><td>0.02</td><td>0.08</td><td>0.02</td><td>0.03</td><td>0.03</td></tr></table></body></html>

![](images/15a3b7243b0245620c213b033e210a05a803b91d76846d5e473666b1955b22b2.jpg)

<html><body><table><tr><td>bin(i)</td><td>bin1</td><td>bin2</td><td>bin3</td></tr><tr><td>N</td><td>3</td><td>3</td><td>2</td></tr><tr><td>G</td><td>0.1</td><td>0.79</td><td>0.67</td></tr><tr><td>H</td><td>0.29</td><td>0.12</td><td>0.06</td></tr></table></body></html>

# GOSS算法：

·Gradient-based One-Side Sampling，基于梯度的单边采样算法  
思想是通过样本采样，减少目标函数增益Gain的计算复杂度  
·单边采样，只对梯度绝对值较小的样本按照一定比例进行采样，而保留了梯度绝对值较大的样本  
·因为目标函数增益主要来自于梯度绝对值较大的样本 $\Rightarrow { \mathsf { G O S S } }$ 算法在性能和精度之间进行了很好的tradeoff

<html><body><table><tr><td>i</td><td>1</td><td>２</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>Xi</td><td>0.1</td><td>2.1</td><td>2.5</td><td>3.0</td><td>3.0</td><td>4.0</td><td>4.5</td><td>5.0</td></tr><tr><td>gi</td><td>0.01</td><td>0.03</td><td>0.06</td><td>0.05</td><td>0.04</td><td>0.7</td><td>0.6</td><td>0.07</td></tr><tr><td>hi</td><td>0.2</td><td>0.04</td><td>0.05</td><td>0.02</td><td>0.08</td><td>0.02</td><td>0.03</td><td>0.03</td></tr></table></body></html>

gi：梯度绝对值，假设阈值为0.1对gi $< 0 . 1$ 的样本 $\Rightarrow 1 / 3$ 概率进行采样对gi> $_ { \cdot = 0 . 1 }$ 的样本 $\Rightarrow$ 全部保留

V

<html><body><table><tr><td>bin(i)</td><td>bin1</td><td>bin2</td><td>bin3</td></tr><tr><td>N</td><td>3=0+1*3</td><td>4=1+1*3</td><td>1=1+0*3</td></tr><tr><td>Gi</td><td>0.09</td><td>0.85</td><td>0.6</td></tr><tr><td>H</td><td>0.12</td><td>0.08</td><td>0.03</td></tr></table></body></html>

# EFB算法：

·ExclusiveFeatureBundling，互斥特征绑定算法  
·思想是特征中包含大量稀疏特征的时候，减少构建直方图的特征数量，从而降低计算复杂度  
·数据集中通常会有大量的稀疏特征（大部分为0，少量为非0）我们认为这些稀疏特征是互斥的，即不会同时取非零值  
·EFB算法可以通过对某些特征的取值重新编码，将多个这样互斥的特征绑定为一个新的特征  
·类别特征可以转换成onehot编码，这些多个特征的onehot编码是互斥的，可以使用EFB将他们绑定为一个特征  
·在LightGBM中，可以直接将每个类别取值和一个bin关联，从而自动地处理它们，也就无需预处理成onehot编码

![](images/ea0e050109a64513d57ccaf34eae5a0fdc68d103aa666513e9959be39c7f8879.jpg)

# LightGBM工具：

import lightgbm as lgb   
官方文档:http://lightgbm.readthedocs.io/en/latest/PythonIntro.html

# 参数:

boosting_type，训练方式，gbdt  
。objective，目标函数，可以是binary，regressionmetric，评估指标，可以选择auc,mae，mse，binary_logloss,multi_loglossmax_depth，树的最大深度，当模型过拟合时，可以降低max_depth  
mindata in leaf，叶子节点最小记录数，默认20

Bagging参数：bagging_fraction+bagging_freq（需要同时设置）

·bagging_fraction，每次迭代时用的数据比例，用于加快训练速度和减小过拟合  
bagging_freq：bagging的次数。默认为o，表示禁用bagging，非零值表示执行k次bagging，可以设置为3-5  
feature_fraction，设置在每次迭代中使用特征的比例，例如为0.8时，意味着在每次迭代中随机选择 $80 \%$ 的参数来建树  
early_stopping_round，如果一次验证数据的一个度量在最近的round中没有提高，模型将停止训练

# 参数:

·lambda，正则化项，范围为 $0 { \sim } 1$   
·min_gain_to_split，描述分裂的最小gain，控制树的有用的分裂  
：max_cat_group，在group 边界上找到分割点，当类别数量很多时，找分割点很容易过拟合时  
·num_boost_round，迭代次数，通常 $1 0 0 +$   
·num_leaves，默认31  
·device，指定cpu 或者gpu  
·max_bin，表示 feature将存入的bin 的最大数量  
·categorical_feature，如果 categorical_features $\mathrel { \mathop : } = 0 , 1 , 2$ ，则列0，1，2是categorical变量  
·ignore_column，与categorical_features类似，只不过不是将特定的列视为categorical，而是完全忽略

# Step3, 模型参数配置

m $\mathbf { \tau } = \mathbf { \tau }$ {'boosting_type':'gbdt', 'objective':'binary',#任务类型 'metric':'auc',#评估指标 'learning_rate' :O.01,#学习率 'max_depth':15,#树的最大深度 'feature_fraction':0.8,#设置在每次迭代中使用特征的比例 'bagging_fraction':0.9,#样本采样比例 'bagging_freq':8,#bagging的次数 'lambda_I1': O.6,#L1正则 'lambda_l2':0,#L2正则

# LightGBM工具

# Step4, 模型训练， 得出预测结果

X_train,X_valid,y_train,Y_valid $\mathbf { \tau } = \mathbf { \tau }$ train_test_split(train.drop('Attrition',axis $_ { \cdot = 1 }$ ), train['Attrition'], test_size ${ \tt \Psi } = 0 . 2$ ,random_state $= 4 2$ ）

trn_data $\mathbf { \tau } = \mathbf { \tau }$ Igb.Dataset(X_train, label=y_train)

val_data $\mathbf { \tau } = \mathbf { \tau }$ Igb.Dataset(X_valid, label=y_valid)

model $\mathbf { \tau } = \mathbf { \tau }$ lgb.train(param,train_data,valid_sets $\ c =$ [train_data,valid_data],num_boost_round $\mathbf { \tau } = \mathbf { \tau }$ 10000,early_stopping_round $s = 2 0 0$ ,verbose_eva $\mathsf { I } = 2 5$ , categorical_feature $\mathbf { \sigma } =$ attr)

predict $\ c =$ model.predict(test)

test['Attrition'] $\ c =$ predict

#转化为二分类输出

test['Attrition']=test['Attrition'].map(lambda x:1 if $x > = 0 . 5$ else 0)

test[['Attrition']].to_csv('submit_lgb.csv')

<html><body><table><tr><td></td><td></td><td></td><td></td><td></td><td>5</td><td></td><td></td><td>.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Attritior</td></tr><tr><td>T</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>T</td><td>0</td><td>0</td><td></td><td>0</td><td>0</td><td>心</td><td>0</td><td>T</td><td></td><td></td><td>0</td><td>0</td><td>0</td><td></td><td>0</td><td></td><td>0</td><td>F</td><td></td><td></td><td></td><td></td><td>0</td><td></td><td></td></tr></table></body></html>

# 参数对比

<html><body><table><tr><td>xgb</td><td>1gb</td><td>XGBClassifier（xgb.sklearn）</td><td>LGBMClassifier（lgb.sklearn）</td></tr><tr><td>booster=' gbtree'</td><td>boosting=' gbdt'</td><td>booster=' gbtree'</td><td>boosting_type=' gbdt'</td></tr><tr><td>objective=' binary:logistic'</td><td>lapplication='binary'</td><td>objective=' binary:logistic'</td><td>objective=' binary'</td></tr><tr><td>max_depth=7</td><td>num_leaves=2**7</td><td>max_depth=7</td><td>num_leaves=2**7</td></tr><tr><td>eta=0.1</td><td>learning_rate=0.1</td><td>learning_rate=0.1</td><td>learning_rate=0.1</td></tr><tr><td>num_boost_round=10</td><td>num_boost_round=10</td><td>n_estimators=10</td><td>n_estimators=10</td></tr><tr><td>gamma=0</td><td>min_split_gain=0.0</td><td>gamma=0</td><td>min_split_gain=0.0</td></tr><tr><td>min_child_weight=5</td><td>min_child_weight=5</td><td>min_child_weight=5</td><td>min_child_weight=5</td></tr><tr><td>subsample=1</td><td>bagging_fraction=1</td><td>subsample=1. 0</td><td>subsample=1. 0</td></tr><tr><td>colsample_bytree=1. 0</td><td>feature_fraction=1</td><td>colsample_bytree=1.0</td><td>colsample_bytree=1.0</td></tr><tr><td>alpha=0</td><td>lambda_11=0</td><td>reg_alpha=0.0</td><td>reg_alpha=0.0</td></tr><tr><td>lambda=1</td><td>lambda_12=0</td><td>reg_lambda=1</td><td>reg_lambda=0. 0</td></tr><tr><td>scale_pos_weight=1</td><td>scale_pos_weight=1</td><td>scale_pos_weight=1</td><td>scale_pos_weight=1</td></tr><tr><td>seed</td><td>bagging_seed</td><td></td><td></td></tr><tr><td>feature_fraction_seed</td><td>random_state=888</td><td>random_state=888</td><td></td></tr><tr><td>nthread</td><td>num_threads</td><td>n_jobs=4</td><td>n_jobs=4</td></tr><tr><td>evals</td><td>valid_sets</td><td>eval_set</td><td>eval_set</td></tr><tr><td>eval_metric</td><td>metric</td><td>eval_metric</td><td>eval_metric</td></tr><tr><td>early_stopping_rounds</td><td>learly_stopping_rounds</td><td>early_stopping_rounds</td><td>early_stopping_rounds</td></tr><tr><td>lverbose_eval</td><td>verbose_eval</td><td>verbose</td><td>verbose</td></tr></table></body></html>

# 祖传参数 (LightGBM)

#

LGBMClassifier经验参数   
clf $\mathbf { \tau } = \mathbf { \tau }$ Igb.LGBMClassifier( num_leaves $= 2 ^ { * * } 5 - 1$ ,reg_alpha $= 0 . 2 5$ , reg_lambda=0.25,   
objective $= ^ { 1 }$ binary', max_depth $= - 1$ ,learning_rate $= 0 . 0 0 5$ ,min_child_samples $^ { = 3 }$   
random_state $= 2 0 2 1$ n_estimators $\ c =$ 2000,subsample=1,colsample_bytree $\scriptstyle = 1$ 1 ）   
num_leave $1 = 2 ^ { * * } 5 - 1$ #树的最大叶子数，对比×GBoost一般为   
2^(max_depth)   
reg_alpha，L1正则化系数   
reglambda，L2正则化系数   
max_depth，最大树的深度

n_estimators，树的个数，相当于训练的轮数 subsample，训练样本采样率 (行采样) colsample_bytree，训练特征采样率 (列采样)

# 祖传参数 (XGBoost

# XGBoost VS LightGBM

# XGBoost效果相对LightGBM可能会好一些

xgb $\mathbf { \tau } = \mathbf { \tau }$ xgb.XGBClassifier( max_depth $= 6$ ,learning_rate $_ { = 0 . 0 5 }$ ,n_estimators=2000, objective $= ^ { \mathsf { I } }$ binary:logistic', tree_method $= ^ { \prime }$ gpu_hist', subsample $= 0 . 8$ , colsample_bytree $\scriptstyle : = 0 . 8$ min_child_samples=3,eval_metric $u ^ { \prime }$ auc',reg_lambda=0.! ）

max_depth，树的最大深度  
learning_rate,学习率  
reg_lambda，L2正则化系数  
n_estimators，树的个数，相当于训练的轮数  
objective，目标函数,binary:logistic用于二分类任务  
tree_method，使用功能的树的构建方法，hist代表使用直方图优  
化的近似贪婪算法  
subsample，训练样本采样率 (行采样)  
colsample_bytree，训练特征采样率 (列采样)

![](images/6efeb5869883951972399bc0f019e090000fe84e98fc42e7c235d57775add40c.jpg)

subsample，colsample_bytree是个值得调参的参数典型的取值为0.5-0.9 (取0.7效果可能更好)

# CatBoost算法：

·俄罗斯科技公司Yandex开源的机器学习库（2017年）  
·https://arxiv.0rg/pdf/1706.09516.pdf  
· CatBoost $\mathbf { \tau } = \mathbf { \tau }$ Catgorical $^ +$ Boost  
·高效的处理分类特征（categoricalfeatures），首先对分类特征做统计，计算某个分类特征（category）出现的频率，然后加上超参数，生成新的数值型特征（numericalfeatures）  
·同时使用组合类别特征，丰富了特征维度  
·采用的基模型是对称决策树，算法的参数少、支持分类变量，通过可以防止过拟合

CatBoost， LightGBM， XGBoost对比:

·2015年航班延误数据，包含分类和数值变量  
:https://www.kaggle.com/usdot/flight-delays/data  
一共有约500条记录，使用 $10 \%$ 的数据，即50条记录CatBoost过拟合程度最小，在测试集上准确度最高0.816，同时预测用时最短，但这个表现仅仅在有分类特征，而且调节了one-hot最大量时才会出现  
如果不利用CatBoost算法在这些特征上的优势，表现效果就会变成最差，AUC0.752  
使用CatBoost需要数据中包含分类变量，同时适当地调节这些变量时，才会表现不错

<html><body><table><tr><td>模型</td><td>XGBoost</td><td colspan="2">LightGBM</td><td colspan="2">CatBoost</td></tr><tr><td>使用参数</td><td>max_depth:50 learning_rate:0.16 min_child_weight:1 n_estimators:200</td><td colspan="2">max_depth:50 learning_rate:0.1 num_leaves:900 n_estimators:300</td><td colspan="2">max_depth:10 learning_rate:0.15 12_leaf_reg=9 iteration:500 one_hot_max_size=50</td></tr><tr><td>训练集 AUC</td><td rowspan="2">0.999</td><td>没有使用分 类特征索引</td><td>征索引</td><td>使用分类特 没有使用分 类特征索引</td><td>使用分类特征索 引</td></tr><tr><td></td><td>0.992</td><td>0.999</td><td>0.842</td><td>0.887</td></tr><tr><td>测试集 AUC</td><td>0.789</td><td>0.785</td><td>0. 772</td><td>0.752</td><td>0.816</td></tr><tr><td>训练用时</td><td>970秒</td><td>153秒</td><td>326秒</td><td>180秒</td><td>390秒</td></tr><tr><td>预测用时</td><td>184秒</td><td>40秒</td><td>156秒</td><td>2秒</td><td>14秒</td></tr></table></body></html>

三种模型在flight-delays预测中的训练速度和准确度

CatBoost， LightGBM， XGBoost对比:

处理特征为分类的神器  
·支持即用的分类特征，因此我们不需要对分类特征进行预处理（比如使用LabelEncoding或OneHotEncoding)  
CatBoost设计了一种算法验证改进，避免了过拟合。因此处理分类数据比LightGBM和XGBoost 强  
：准确性比XGBoost更高，同时训练时间更短  
支持 GPU训练可以处理缺失的值

<html><body><table><tr><td></td><td>BaselineXGBoost</td><td>Baseline LightGBM</td><td>Baseline CatBoost</td><td>TunedXGBoost</td><td>Tuned LightGBM</td><td>Tuned CatBoost</td></tr><tr><td>Training Time</td><td>14m08s</td><td>2m33s</td><td>4m39s</td><td>1h 2m 34s</td><td>32m25s</td><td>23m29s</td></tr><tr><td>Prediction Time</td><td>0.84s</td><td>0.4s</td><td>0.9s</td><td>1.2s</td><td>1.1s</td><td>0.9s</td></tr><tr><td>Accuracy Score</td><td>0.8484</td><td>0.8509</td><td>0.7851</td><td>0.8717</td><td>0.8716</td><td>0.8197</td></tr><tr><td></td><td>Baseline XGBoost</td><td>Baseline LightGBM</td><td>Baseline CatBoost</td><td>TunedXGBoost</td><td>Tuned LightGBM</td><td>Tuned CatBoost</td></tr><tr><td>Training Time</td><td>5.94s</td><td>1.71s</td><td>0.55s</td><td>9.84s</td><td>2.23s</td><td>4.46s</td></tr><tr><td>Prediction Time</td><td>0.012s</td><td>0.012s</td><td>0.048s</td><td>0.0155s</td><td>0.018s</td><td>0.030s</td></tr><tr><td>R2Score</td><td>2.89</td><td>2.97</td><td>3.27</td><td>3.45</td><td>3.67</td><td>3.98</td></tr><tr><td rowspan="4"></td><td></td><td>： Baseline XGBoost</td><td>Baseline LightGBM</td><td>Baseline CatBoost</td><td></td><td></td></tr><tr><td>Training Time</td><td></td><td>43s</td><td>17s</td><td>37s</td><td></td></tr><tr><td>Prediction Time</td><td></td><td>0.003s</td><td>0.007s</td><td>0.004s</td><td></td></tr><tr><td>R2 Score</td><td>12.43</td><td></td><td>12.06</td><td>7.87</td><td></td></tr></table></body></html>

实验1，分类模型MNIST识别（6万数据，784特征)实验2，回归模型，预测纽约出租车票价 (6万数据，7个特征)实验3，回归模型，预测纽约出租车票价 (200万数据，7个特征)

# CatBoost工具

CatBoost工具:

· https://github.com/dmlc/xgboost ·https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html

# 构造参数：

·learning_rate，学习率  
· depth，树的深度  
·l2_leaf_reg，L2正则化系数  
·n_estimators，树的最大数量，即迭代次数  
·one_hot_max_size，one-hot编码最大规模，默认值根据数据和训练环境的不同而不同  
·loss_function，损失函数，包括Logloss，RMSE，MAE，CrossEntropy，回归任务默认RMSE，分类任务默认Logloss  
eval_metric，优化目标，包括RMSE，Logloss，MAE，CrossEntropy，Recall，Precision，F1，Accuracy，AUC，R2

# CatBoost工具

# fit函数参数：

·X，输入数据数据类型可以是：list;pandas.DataFrame;pandas.y=Nonecat features=None，用于处理分类特征sample_weight=None，输入数据的样本权重  
·logging_level=None，控制是否输出日志信息，或者其他信息  
·plot=False，训练过程中，绘制，度量值，所用时间等  
eval_set=None，验证集合，数据类型list(X,y)tuples  
baseline $\ c =$ Noneuse_best_model $\ c =$ None  
verbose $\ c =$ None

model $\mathbf { \tau } = \mathbf { \tau }$ CatBoostClassifier(iteration $\scriptstyle \mathsf { s } = 1 0 0 0$ ，#最大树数，即迭代次数

depth $= 6$ ，#树的深度  
learning_rate $\mathbf { \varepsilon } = 0 . 0 3$ ,#学习率  
custom_ ${ \mathsf { I o s s } } = ^ { \prime } { \mathsf { A u c } } ^ { \prime }$ ,#训练过程中，用户自定义的损失函数  
eval_metric $\because \mathsf { A U C } ^ { \prime }$ ,#过拟合检验（设置True）的评估指标，用于优化  
bagging_temperatur $\scriptstyle \mathtt { \underline { { \sigma } } } = 0 . 8 3$ ，#贝叶斯bootstrap强度设置  
$r s \mathsf { m } = 0 . 7 8 ,$ #随机子空间  
od_type $\mathrel { \mathop : }$ Iter',#过拟合检查类型  
od_wait $_ { : = 1 5 0 }$ ，#使用Iter时，表示达到指定次数后，停止训练  
metric_period $= 4 0 0$ ，#计算优化评估值的频率  
l2_leaf_ $\displaystyle \mathsf { r e g } = 5$ ,#2正则参数  
thread_count $= 2 0$ ，#并行线程数量  
random_seed $= 9 6 7$ #随机数种子  
）

# CatBoost工具

# Step3, 模型参数配置

model $\mathbf { \tau } = \mathbf { \tau }$ cb.CatBoostClassifier(iterations=1000, depth $\scriptstyle 1 = { \overline { { \jmath } } }$ 7， learning_rate $_ { ! = 0 . 0 1 }$ ， loss_function $= ^ { \mathsf { 1 } }$ Logloss', eval_metric $= ^ { \prime }$ AUC', logging_level $\mathbf { \tau } = \mathbf { \tau }$ Verbose', metric_period $\scriptstyle = 5 0$

#得到分类特征的列号 categorical_features_indices =[] fori in range(len(X_train.columns)): if X_train.columns.values[i] in attr: categorical_features_indices.append(i) print(categorical_features_indices)

attr=['Age','BusinessTravel','Department','Education','EducationField','Gend er','JobRole','MaritalStatus','Over18','OverTime']

[0,1,3,5,6,9,13,15,19,20]

# CatBoost工具

# Step4, 模型训练，得出预测结果

model.fit(X_train,y_train,eval_set=(X_valid,y_valid),   
cat_features $\ c =$ categorical_features_indices)   
predict $\mathbf { \tau } = \mathbf { \tau }$ model.predict(test)   
test['Attrition'] $\ c =$ predict   
test[['Attrition']].to_csv('submit_cb.csv')

0： test: 0.6390374 best: 0.6390374 (0) t0tal: 80.6ms remaining: 1m 20s   
50: test: 0.7998472 best: 0.7998472 (50) total: 805ms remaining: 15s   
100: test: 0.8054131 best: 0.8054131 (100) total: 1.54s remaining: 13.8s   
150 : test: 0.8053039 be5t: 0.8054131 (100) total: 2.51s remaining: 14.1s   
200 : test: 0.8075958 best: 0.8075958 (200) total: 3.55s remaining: 14.1s   
250 : test: 0.8062862 best: 0.8075958 (200) total: 4.55s remaining: 13.6s   
300: test: 0.8045400 best: 0.8075958 (200) total: 5.51s remaining: 12.8s   
350 : test: 0.8059587 be5t: 0.8075958 (200) total: 6.45s remaining: 11.9s   
400 : test: 0.8065044 best: 0.8075958 (200) total: 7.41s remaining: 11.1s   
450 : test: 0.8065044 be5t: 0.8075958 (200) total: 8.44s remaining: 10.3s   
500: test: 0.8077049 best: 0.8077049 (500) total: 9.46s remaining: 9.43s   
550 : test: 0.8090145 best: 0.8090145 (550) total: 10.6s remaining: 8.6s   
600: test: 0.8106515 best: 0.8106515 (600) total: 11.5s remaining: 7.62s   
650 : test: 0.8113063 be5t: 0.8113063 (650) total: 12.4s remaining: 6.67s   
700 : test: 0.8125068 best: 0.8125068 (700) total: 13.4s remaining: 5.71s   
750 : test: 0.8126160 best: 0.8126160 (750) total: 14.4s remaining: 4.77s   
800: test: 0.8116337 best: 0.8126160 (750) total: 15.5s remaining: 3.84s   
850 : test: 0.8121794 be5t: 0.8126160 (750) total: 16.5s remaining: 2.89s   
900: test: 0.8116337 best: 0.8126160 (750) total: 17.6s remaining: 1.94s   
950 : test: 0.8111972 best: 0.8126160 (750) total: 18.7s remaining: 965ms   
999 : test: 0.8108698 best: 0.8126160 (750) total: 19.8s remaining: 0us   
bestTest = 0.8126159555   
bestIteration = 750

<html><body><table><tr><td>user_id</td><td>Attrition</td></tr><tr><td>442</td><td>0</td></tr><tr><td>1091</td><td>0</td></tr><tr><td>981</td><td>0</td></tr><tr><td>785</td><td>0</td></tr><tr><td>1332</td><td>1</td></tr><tr><td>501</td><td>0</td></tr><tr><td>1058</td><td>0</td></tr><tr><td>1253</td><td>0</td></tr><tr><td>751</td><td>0</td></tr><tr><td>122</td><td>0</td></tr><tr><td>268</td><td>0</td></tr><tr><td>940</td><td>0</td></tr><tr><td>1125</td><td>0</td></tr><tr><td>540</td><td>1</td></tr><tr><td>1034</td><td>0</td></tr><tr><td>432</td><td>0</td></tr><tr><td>794</td><td>0</td></tr><tr><td>666</td><td>0</td></tr><tr><td>942</td><td>0</td></tr><tr><td>1114</td><td>0</td></tr><tr><td>853</td><td>0</td></tr><tr><td>1330</td><td>0</td></tr><tr><td>692</td><td>0</td></tr><tr><td>548</td><td>0</td></tr><tr><td>54</td><td>0</td></tr><tr><td>319</td><td>0</td></tr><tr><td>1147</td><td>0</td></tr><tr><td>1235</td><td>0</td></tr><tr><td>1436</td><td>1</td></tr></table></body></html>

Shrink model to first 751 iterations.

·LighGBM效率高，在Kaggle比赛中应用多  
·CatBoost对于分类特征多的数据，可以高效的处理，过拟合程度小，效果好  
·XGBoost,LightGBM,CatBoost参数较多，调参需要花大量时间  
·Boosting集成学习包括AdaBoosting和Gradient Boosting  
·Boosting只是集成学习中的一种（Bagging,Stacking）

# 打卡：二手车价格预测

针对AI大赛：二手车价格预测，编写AI算法，进行预测，挑战分数<550 https://tianchi.aliyun.com/competition/entrance/231784/information

训练集: used_car_train_20200313.csv

测试集： used_car_testB_20200421.csv

XGBoost模型LightGBM模型CatBoost模型

你觉得哪个模型的模型效果好？

# 如何防止模型过拟合

# Thinking: 如何防止模型过拟合？

1、模型层面优化  
2、数据层面优化  
3、业务逻辑约束

学习赛第二季:466/780.2130

[19995] validation_0-mae:493.30894   
[19996] validation_0-mae:493.30957   
[19997] validation_0-mae:493.30842   
[19998] validation 0-mae:493.30761   
[19999] validation_0-mae:493.30777   
模型3 验证集 MAE:493.31

7.2 集成模型预测集成模型验证集 MAE: 492.49

8.评估模型性能

日期：2025-06-06 00:09:57  
分数：818.1239  
日期：2025-06-05 12:55:31  
分数：826.2079  
日期：2025-06-05 12:28:42  
分数：826.2079  
日期：2025-06-05 12:08:09  
分数：780.2130

# 如何防止模型过拟合

Thinking: 如何在模型层面进行优化？

正则化技术：

树模型：调整max_depth(建议5-8)、min_samples_leaf(建议10-20，代表每个叶子节点至少包含的样本数)  
神经网络：添加dropout层(0.2-0.5)+ L2正则化  
线性模型：增大L1/L2正则化系数

# 早停机制：

监控验证集loss，设置patience=10-20个epoch，patience代表允许验证集损失validation loss连续不改善的轮次（epochs）数量

集成方法：

使用Blending（用70%训练基模型， $30 \%$ 训练元模型）

# 如何防止模型过拟合 （树模型）

# XGBoost中的超参数

$\mathsf { p a r a m s } = \left\{ \begin{array} { r l } \end{array} \right.$ （2'objective':'reg:squarederror'，#目标函数，回归问题用平方误差'eval_metric': 'mae', # 评估指标，平均绝对误差'learning_rate': 0.01, # 学习率，控制每棵树对最终结果的影响，越小越保守'max_depth': 6, # 树的最大深度，防止过拟合'subsample': 0.8, # 每棵树随机采样的样本比例，防止过拟合'colsample_bytree': 0.8, # 每棵树随机采样的特征比例，防止过拟合'seed': 42, # 随机种子，保证结果可复现'nthread': -1 #使用全部CPU线程加速训练  
}

# 如何防止模型过拟合 （树模型）

在sklearn决策树中，可以使用min_samples_leaf参数

from sklearn.ensemble import RandomForestRegressor  
#尝试不同值观察效果  
model $\mathbf { \tau } = \mathbf { \tau }$ RandomForestRegressor(min_samples_leaf $\dot { \mathbf { \eta } } = 1 0$ ，# 初始建议值n_estimator ${ \mathsf { s } } { = } 1 0 0$ ，random_state $= 4 2$   
）  
model.fit(X_train, y_train)

XGBoost没有min_samples_leaf 这个参数，在XGBoost 中，类似的参数是：min_child_weight，即一个叶子节点上最小的样本权重和（对于回归问题就是样本个数的和）。如果一个叶子节点的样本权重和小于这个值，则不会再分裂。

# 如何防止模型过拟合 （神经网络）

# 如果使用神经网络，可以增加layers.Dropout 层

from tensorflow import keras   
from tensorflow.keras import layers   
model $\mathbf { \tau } = \mathbf { \tau }$ keras.Sequential([ layers.Dense(128, activation $= ^ { \mathsf { \Gamma } }$ relu',input_shape $\ c =$ (X_train.shape[1],)), layers.Dropout(0.2), layers.Dense(64, activation $= ^ { \mathsf { \Gamma } }$ relu'), layers.Dropout(0.1), layers.Dense(1)   
1)

神经网络是一种受人脑神经元结构启发的机器学习模型。核心思想是通过大量“神经元”节点的层层连接和非线性变换，自动学习输入特征与输出目标之间的复杂映射关系。

# 如何防止模型过拟合 （线性模型）

如果使用线性模型，比如逻辑回归，可以使用L1或L2正则化系数

model $\mathbf { \tau } = \mathbf { \tau }$ LogisticRegression(max_iter=100,verbose $\mathbf { \tau } = \mathbf { \dot { \tau } }$ True,random_state $= 4 2$ ，$\scriptstyle { \mathrm { t o l } } = 1 \ e - 4 .$ penalty $= ^ { 1 } 1 2 ^ { 1 }$ ，#或'I1'${ \mathsf { C } } { = } 1 . 0 ,$ # 正则化强度倒数，越小正则化越强  
）

L1正则化项是模型权重的绝对值之和： （20 $\begin{array} { l } { { \displaystyle { \cal L } 1 = \lambda \sum _ { i = 1 } ^ { n } \left| w _ { i } \right| } } \\ { { \displaystyle { \cal L } 2 = \lambda \sum _ { i = 1 } ^ { n } w _ { i } ^ { 2 } } } \end{array}$ L2正则化项是模型权重的平方和

# 如何防止模型过拟合 （早停机制）

使用早停机制， 可以让模型初始化训练次数更多一些

model $\mathbf { \tau } = \mathbf { \tau }$ xgb.train( params, dtrain, num_boost_round=2000, evals $\ c =$ evals, early_stopping_rounds $= 5 2 0$ verbose_eval=100   
T   
early_stop $\mathbf { \tau } = \mathbf { \tau }$ keras.callbacks.EarlyStopping(monitor='val_loss',   
patience $: = 2 0$ , restore_best_weights $\ c =$ True)   
history $\mathbf { \tau } = \mathbf { \tau }$ model.fit( X_train_scaled,y_train, validation_data $\ c =$ (X_val_scaled, y_val), epoch $s { = } 2 0 0$ ， batch_size $\mathtt { = 6 4 }$ 冏， callbacks $\ c =$ [early_stop], verbose=2   
）

早停是一种正则化技术，用于防止模型在训练过程中过拟合。核心思想是：在验证集性能不再提升时提前终止训练，而不是一直训练到收敛。

# Blending集成学习方法

Thinking: Blending 是什么？

Blending是一种分层集成学习技术，通过以下两步组合多个模型：

·基模型（BaseModels）：用训练集的 $70 \%$ 训练多个不同模型（如随机森林、XGBoost、神经网络等）。·元模型（Meta Model）：用剩下的30%数据生成基模型的预测结果作为新特征，训练一个次级模型（通常为线性模型）进行最终预测。

神经网络是一种受人脑神经元结构启发的机器学习模型。核心思想是通过大量“神经元”节点的层层连接和非线性变换，自动学习输入特征与输出目标之间的复杂映射关系。

# Blending集成学习方法

假设数据集有15,000条二手车记录，特征包括车龄、里程、品牌等，目标为价格

import numpy as np   
from sklearn.model_selection import train_test_split   
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor   
from sklearn.linear_model import LinearRegression   
#原始数据   
X,y= load_car_data()#假设已加载数据   
X_train,X_val,y_train,y_val $\mathbf { \tau } = \mathbf { \tau }$ train_test_split(X,y, test_size $\scriptstyle \mathtt { = 0 . 3 }$ ,random_state $= 4 2$

# 进一步划分训练集： $70 \%$ 基模型 $1 3 0 \%$ 元模型 X_base,X_meta,y_base,y_meta $\mathbf { \tau } = \mathbf { \tau }$ train_test_split( X_train,γ_train, test_size $\scriptstyle \mathtt { = 0 . 3 }$ , random_state $: = 4 2$ 1

# Blending集成学习方法

# Step1: 训练基模型

# 定义3个基模型 model_rf $\mathbf { \tau } = \mathbf { \tau }$ RandomForestRegressor(n_estimators $_ { \mathbf { \lambda } = 1 0 0 }$ , random_state $= 4 2$ model_gb $\mathbf { \sigma } = \mathbf { \sigma }$ GradientBoostingRegressor(n_estimators $_ { \mathbf { \lambda } = 1 0 0 }$ , random_state $= 4 2$ model_nn $\mathbf { \tau } = \mathbf { \tau }$ make_neural_network()#自定义的神经网络

#在 $70 \%$ 数据上训练基模型 model_rf.fit(X_base,y_base) model_gb.fit(X_base,y_base) model_nn.fit(X_base, y_base)

# Blending集成学习方法

# Step2：生成元特征

用基模型预测剩余 $30 \%$ 数据，生成新特征

#获取基模型对元数据集的预测  
meta_features $\mathbf { \tau } = \mathbf { \tau }$ np.column_stack([model_rf.predict(X_meta),model_gb.predict(X_meta),model_nn.predict(X_meta)  
1)

#元特征示例（每条样本的3个基模型预测值） print(meta_features[:3])

#输出类似： #[[12.5,13.1, 11.8], # [8.2,7.9,8.5], # [20.1,19.7,21.3]]

# Blending集成学习方法

# Step3: 训练元模型

# 用基模型的预测结果作为输入，真实价格作为目标  
meta_model $\mathbf { \tau } = \mathbf { \tau }$ LinearRegression()  
meta_model.fit(meta_features, y_meta)

# Step4: 预测新数据

通过Blending学习，可以得到三个模型（rf,gb,nn）的线性回归系数，比如为：权重系数 $\mathbf { \sigma } = \mathbf { \sigma }$ [0.4,0.5,0.1],偏置 $= 0 . 2$

#对验证集生成基模型预测   
val_meta_features $\mathbf { \tau } = \mathbf { \tau }$ np.column_stack([ model_rf.predict(X_val), model_gb.predict(X_val), model_nn.predict(X_val)   
1)   
#用元模型做最终预测   
final_predictions $\mathbf { \tau } = \mathbf { \tau }$ meta_model.predict(val_meta_features)   
#计算MAE   
print("Blending MAE:",mean_absolute_error(y_val, final_predictions))

最终预测 $= w _ { 1 } \times$ 模型1预测 $+ w _ { 2 } \times$ 模型2预测 $+ w _ { 3 } \times$ 模型3预测 $+ b$

过拟合（Overfitting）是导致模型在实际应用中表现糟糕的主要原因之一。在二手车价格预测这类数据噪声大、特征复杂的任务中，过拟合会直接影响模型的商业价值和可靠性。训练 $M A E = 4 0 0$ （模型似乎很准），测试 $M A E = 8 0 0$ （实际预测误差翻倍） $\Rightarrow$ 典型的过拟合

<html><body><table><tr><td>方法</td><td>作用</td></tr><tr><td>早停(Early Stopping)</td><td>监控验证集MAE，patience=15轮无改善则停止训练</td></tr><tr><td>Dropout</td><td>在神经网络中随机丢弃20%神经元</td></tr><tr><td>L1/L2正则化</td><td>线性回归添加L1或L2正则化</td></tr><tr><td>集成方法</td><td>使用Blending组合随机森林、XGBoost和神经网络</td></tr></table></body></html>

# Thank You Using data to solve problems