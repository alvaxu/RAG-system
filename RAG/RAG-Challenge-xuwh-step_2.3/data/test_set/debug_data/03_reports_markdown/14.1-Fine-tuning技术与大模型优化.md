# Fine-tuning技术与大模型优化

# 今天的学习目标

# Fine-tuning技术与大模型优化

·如何模型微调（以ChatGLM为例）   
·李飞飞50美金复刻R1模型   
s1: Simple test-time scaling Unsloth：LLM高效微调 CASE: qwen2.5-7B微调 (alpaca-cleaned) CASE：训练垂类模型（中文医疗模型） CASE：训练自己的R1模型

模型微调 (以ChatGLM为例)

# ChatGLM-6B定制

P-Tuning:

·用自己的数据集对ChatGLM模型进行微调·P-TuningV2已将需要微调的参数减少到原来的 $0 . 1 \% - 3 \%$ 大大加快了训练速度，同时最低GPU显存要求为7G(建议16-24G)

TIPS：AdvertiseGen数据集，训练3000steps，大约需要训练2小时，需要保证你的GPU有足够多的计算单元

您已订阅"Colab Pro"。了解详情。可用：21.01个计算单元使用率：大约每小时13.16个您有2个有效会话。管理会话

# 余量不足警告！

根据您当前的使用量数据，预付费资源可能最长还能使用1.6小时。如需购买更多计算单元，请点击此处。我们可能无法保留您当前的运行时，或许会为您分配新的运行时。

# ChatGLM-6B定制 (训练)

如果是你单卡，运行的时候会报错：

Default process group has not been initialized, please make sure to call init_process_group.

<html><body><table><tr><td>704 705</td></tr><tr><td></td></tr></table></body></html>

RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.

TIPS：单卡GPU，需要修改main.py，设置 training_args.local_rank = -1

<html><body><table><tr><td>51 52- 53 54 55 56- 57 58</td><td>parser = HfArgumentParser((ModelArguments， DataTrainingArguments, if len(sys.argv) == 2 and sys.argv[1].endswith(".json"): # If we pass only one argument to the script and it's the pat # let's parse it to get our arguments. model_args， data_args， training_args = parser.parse_json_file else: model_args，data_args， training_args = parser.parse_args_into</td></tr><tr><td>60 61 62</td><td>##### 增 加 两 行 ##### training_args.local_rank = -1 print(training_args.local_rank) # Setup logging</td></tr></table></body></html>

# ChatGLM-6B定制 (训练)

# #使用P-Tuningv2进行训练!bash train.sh

{'loss': 4.4697，'learning_rate' : 0.01626666666666667，'epoch' : 0.08} {loss': 4.4755，'learning_rate': 0.016200000000000003，'epoch' :0.08} {loss':4.5359，'learning_rate' : 0.016133333333333333，'epoch' :0.08} {'loss':4.5341，'learning_rate':0.016066666666666667，'epoch' :0.08} {'loss':4.5033，'learning_rate':0.016,'epoch' :0.08} {'loss' : 4.4446, ‘learning_rate':0.015933333333333334，'epoch'：0.09} {'loss' : 4.4129, 'learning_rate':0.015866666666666668, 'epoch':0.09} {loss' : 4.4903, 'learning_rate':0.0158，'epoch':0.09} {'loss' : 4.5469, 'learning_rate':0.015733333333333332, 'epoch':0.09} {'loss' : 4.4247, 'learning_rate':0.015666666666666666, 'epoch':0.09} {'loss' : 4.5165, 'learning_rate':0.015600000000000001, 'epoch':0.09} {loss' : 4.5162，'learning_rate’: 0.015533333333333333, epoch':0.09} {loss':4.3992，'learning_rate':0.015466666666666667，'epoch' :0.09} {'loss':4.4758，'learning_rate':0.0154，'epoch':0.1} {loss' : 4.4585，'1earning_rate' :0.015333333333333334，'epoch' :0.1} {loss':4.5184，'learning_rate':0.015266666666666666，'epoch':0.1} {'loss':4.4634，'learning_rate':0.0152，'epoch':0.1} 24% 720/3000 [39:26<2:05:19, 3.30s/it]

# train.json训练集数据

{"content":"类型#裙\*图案#卡通\*裙长#连衣裙","summary":“传奇而又经典的卡通形象，米老鼠似乎已经成为了孩童风格的一种标志，大小不一的头像以及奇趣的表情设计。满版的点缀风格让整个连衣裙洋溢着独特的天真气质，加之面料小口袋的点缀，小小的造型呈现出灵巧而又可爱的格调，让宝贝俏皮萌动。"}

{"content":"类型#裤\*版型#宽松\*材质#牛仔布\*风格#街头\* 风格#休闲\*裤长#五分裤\*裤腰型#松紧腰","summary":"这 款休闲五分裤，采用亲肤软牛仔，洗水磨白形成深浅对比， 更加个性。大弹力松紧腰，舒适贴合，一点都不紧勒。裤 子门襟时尚的设计，为细节加分。立体双贴袋，腰间系带 的点缀更吸睛。精致的裁剪，或是干练整洁的走线和宽松 版型，是对街头的描写。"}

# ChatGLM-6B定制 (训练)

PRE_SEQ_LEN=128   
LR=2e-2

CUDA_VISIBLE_DEVICES=O python3 main.py\--do_train \ 训练模式--train_file AdvertiseGen/train.json\ 训练集--validation_file AdvertiseGen/dev.json \ 验证集--prompt_column content \ Prompt字段--response_column summary\ Response字段--overwrite_cache \ 覆盖缓存文件--model_name_or_path THUDM/chatglm-6b 预训练模型--output_dir output/adgen-chatglm-6b-pt- 模型输出目录  
\$PRE_SEQ_LEN-\$LR\

--overwrite_output_dir\ 覆盖输出目录--max_source_length 64\ 输入最大长度--max_target_length 64\ 输出最大长度--per_device_train_batch_size1\ 训练batch_size--per_device_eval_batch_size 1\ 验证batch_size--gradient_accumulation_steps 16\ 梯度累计步数--predict_with_generate\ 使用生成模式预测--max_steps 3000\ 最大训练步数--logging_steps 10 \ 每隔10步记录日志--save_steps 1000\ 每隔1000步保存模型--learning_rate \$LR\ 学习率--pre_seq_len \$PRE_SEQ_LEN\ 预处理序列长度--quantization_bit 4 量化位数

# ChatGLM-6B定制 (推理)

Thinking: 如何加载本地模型？

将train.sh 中的THUDM/chatglm-6b改为本地的模型路径(参考train2.sh)

Thinking: 如何使用训练好的ChatGLM模型进行  
推理  
使用evaluate.sh，这里需要修改  
model_name_or_path 和 ptuning_checkpoint

TIPS：在P-tuningv2训练时模型只保存PrefixEncoder参数，所以在推理时需要同时加载原ChatGLM-6B模型以及PrefixEncoder的权重

PRE_SEQ_LEN=128   
CHECKP0INT=adgen-chatglm-6b-pt-128-2e-2   
STEP=3000

CUDA_VISIBLE_DEVICES=0 python3 main.py \ --do_predict : -validation_file AdvertiseGen/dev.json - -test_file AdvertiseGen/dev.json \ -overwrite_cache \ - -prompt_column content \ --response_column summary - -model_name_or_path THUDM/chatglm-6b V - -ptuning_checkpoint ./output/\$CHECKPOINT/checkpoint-\$STEP - -output_dir ./output/\$CHECKPOINT V --overwrite_output_dir \ - -max_source_length 64 \ - -max_target_length 64 \ --per_device_eval_batch_size 1 - -predict_with_generate \ - -ρre_seq_Len \$PRE_SEQ_LEN - -quantization_bit 4

# ChatGLM-6B定制 (推理)

Thinking: 如果我们训练了1000Step， 如何使用这时的模型进行推理？

[INFO|modeling_utils.py:3185] 2023-05-21 06:57:55,121 $\gg$ All model checkpoint weights were used when ini [WARNING|modeling_utils.py:3187] 2023-05-2106:57:55,121 >>Some weightsof ChatGLMForConditionalGenerat You should probably TRAIN this model ona down-stream task to be able to use it for predictions and infe: [INFO|modeling_utils.py:2821] 2023-05-21 06:57:55,396 $\gg$ Generation config file not found, using a gener: Quantized to 4 bit

input_ids[3,3,3,3,3,3,3,3,3,3,3，3,3,3,3，3,3,3，3,3，3,3，3，3,3，3,3,3,3,3，5，（inputs 类型#上衣\*材质#牛仔布\*颜色#白色\*风格#简约\*图案#刺绣\*衣样式#外套\*衣款式#破洞

label_ids[5，71689，66561，67061，77257，70984，6，72194，65173，64290，64622，81549，63823，65173，642labels 简约而不简单的牛仔外套，白色的衣身十分百搭。衣身多处有做旧破洞设计，打破单调乏味，增加一丝造型看点。！05/21/2023 07:00:09 - INFO -__main__ - \*\*\* Predict \*\*\*

[INFO|configuration_utils.py:577] 2023-05-21 07:00:09,009 >> Generate config GenerationConfig{ "_from_model_config":true, "bos_token_id":130004, "eos_token_id":130005, "pad_token_id":3, "transformers_version":"4.29.2"

打开evaluate.sh，确保   
model_name_or_path $\mathbf { \tau } = \mathbf { \tau }$ THUDM/chatglm-6b   
ptuning_checkpoint   
= ./output/\$CHECKPOINT/checkpoint-\$STEP   
这时调整上面的 ${ \mathsf { S T E P } } = 1 0 0 0$   
!bash evaluate.sh

在dev.json中定义了1700个数据这里可以筛选10条，保存为dev2.json，进行预测

# ChatGLM-6B定制 (推理)

# 1000step的时候 对前10条dev进行的预测结果

\*\*\*\*\* predict metrics \*\*\*\*\*   
predict_bleu-4 二 10.0084   
predict_rouge $^ { - 1 }$ $\mathbf { \Sigma } = \mathbf { \Sigma }$ 34.2697   
predict_rouge-2 二 8.5939   
predict_rouge-l 二 25.3421   
predict_runtime 二 0:00:27.51   
predict_samples $\mathbf { \Sigma } = \mathbf { \Sigma }$ 10   
predict_samples_per_second $\mathbf { \Sigma } = \mathbf { \Sigma }$ 0.363   
predict_steps_per_second $\mathbf { \Sigma } = \mathbf { \Sigma }$ 0.363

Prompt:类型#上衣\*材质#牛仔布\*颜色#白色\*风格#简约\*图案#刺绣\*衣样式#外套\*衣款式#破洞预训练模型：这件上衣的材质是牛仔布,颜色是白色,风格是简约，图案是刺绣，衣样式是外套，衣款式是破洞。

![](images/5e9997225288ac79909586d16972b692526798db44e12a558032a767d6b8638a.jpg)

1000STEP：这一款牛仔外套，白色调的牛仔外套，简约大方，白色刺绣图案,精致美观,打破单调,凸显个性。破洞设计时尚时髦，占显个性。

# P-Tuning V2:

P-Tuning v2: Prompt Tuning Can Be Comparable to Finetuning Universally Across Scales and Tasks, 2022 https://arxiv.org/pdf/2110.07602.pdf   
·解决了PromptTuning在小模型上效果不佳的问题   
·将PromptTuning扩展到更复杂的NLU任务，比如MRC 答案抽取、NER实体抽取等序列标注任务。 P-TuningV2只需要0.1%-3%的可训练参数   
$\Rightarrow$ 减少了训练时间、内存成本和每个任务的存储成本

![](images/30cf9d953ad00d0acbec0e0b105f0444f7a0ce74d28423e19e17af89f3ce9653.jpg)

Lora (Low-Rank Adaptation of Large Language Models)

·在原始预训练模型旁边增加一个旁路，通过低秩分解（先降维再升维）来模拟参数的更新量

·将原模型与降维矩阵A，升维矩阵B分开   
训练阶段，只训练B和A   
推理阶段，将BA加到原参数，即 $h = W x + B A x = ( W + B A ) x$   
不引入额外的推理延迟

·初始化，A采用高斯分布初始化，B初始化为全0，这样训练开始时旁路为0矩阵

·多任务切换，当前任务 $\mathsf { W O } + \mathsf { B } \mathsf { 1 A } \mathsf { 1 }$ ，将lora部分减掉换成B2A2，即可实现任务切换  
·秩的选取：对于一般的任务， $\mathsf { r a n k } { = } 1 , 2 , 4 , 8$ 即可，如果任务较大，可以选择更大的rank

![](images/b9ca41f2881fc492804105346a01ecb8fb844442d972ce51b6f78314d9532997.jpg)

# Lora原理

Lora (Low-Rank Adaptation of Large Language Models)

·大模型的低秩适配器，即固定大模型，增加低秩分解的矩阵来适配下游任务

Lora的优点:

1）一个中心模型服务多个下游任务，节省参数存储量  
2）推理阶段不引入额外计算量  
3）训练任务比较稳定，效果比较好

Thinking: 如果不使用Lora进行训练会怎样？

如果训练集很小，比如20-30samples，会把大模型带跑。因为微调大模型会针对小样本进行过度拟合

![](images/8f9062349fef85950e166c51ad55976319391ac22cf6b15979ed69b516c9e80e.jpg)

# PEFT库

# Peft库:

https://github.com/huggingface/peft

# PEFT

1）LoRA:大模型的低秩适配器   
2） Prefix Tuning: Optimizing Continuous Prompts for   
Generation   
3) P-Tuning:GPT Understands, Too   
4） Prompt Tuning: The Power of Scale for Parameter  
Efficient Prompt Tuning   
模 scale PLMs is often prohibitively costly.In this regard, PEFT methods only fine-tur small number of (extra)   
tune的模型，目前支持4种策略： model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of fullfine-tuning.

# State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods

Seamlessly integrated with Inference.

Accelerate for large scale models leveraging DeepSpeed and Big Model

Supported methods:

1. LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS

2. Prefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation, P-Tuning v2: Prompt Tuning   
Can Be Comparable to Fine-tuning Universally Across Scales and Tasks   
3. P-Tuning: GPT Understands, Too   
4. Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning   
5. AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning

# 打卡：基于ChatGLM的模型微调

部署私有化大模型，并通过自定义的广告语数据集进行训练，用它来完成你的广告语生成任务吧

TIPS：AdvertiseGen数据集，训练3000steps，大约需要训练2小时，需要保证你的GPU有足够多的计算单元

TIPS：单卡GPU，需要修改main.py，设置training_args.local_rank $= - 1$

TIPS：在P-tuningv2训练时模型只保存PrefixEncoder参数，所以在推理时需要同时加载原ChatGLM-6B模型以及PrefixEncoder 的权重

# 李飞飞50美金复刻R1模型s1: Simple test-time scaling

# s1: Simple test-time scaling

s1: Simple test-time scaling https://arxiv.org/pdf/2501.19393

·李飞飞等研究人员用不到50美元的云计算费用，成功训练出了一个名为s1的推理模型  
·该模型在数学和编码能力中的表现，与OpenAi的O1和DeepSeek-R1等尖端推理模型不相上下。  
·s1模型的训练只用了1000个样本数据，具体过程是：使用Gemini对这1000个样本完善推理过程，然后对Qwen模型进行监督微调。

<html><body><table><tr><td>Model</td><td># ex.</td><td>AIME 2024</td><td>MATH 500</td><td>GPQA Diamond</td></tr><tr><td colspan="5">API only</td></tr><tr><td>o1-preview</td><td>N.A.</td><td>44.6</td><td>85.5</td><td>73.3</td></tr><tr><td>o1-mini</td><td>N.A.</td><td>70.0</td><td>90.0</td><td>60.0</td></tr><tr><td>01 Gemini 2.0</td><td>N.A.</td><td>74.4</td><td>94.8</td><td>77.3</td></tr><tr><td>Flash Think.</td><td>N.A.</td><td>60.0</td><td>N.A.</td><td>N.A.</td></tr><tr><td colspan="5">Open Weights</td></tr><tr><td>Qwen2.5- 32B-Instruct</td><td>N.A.</td><td>26.7</td><td>84.0</td><td>49.0</td></tr><tr><td>QwQ-32B</td><td>N.A.</td><td>50.0</td><td>90.6</td><td>65.2</td></tr><tr><td>r1</td><td>>800K</td><td>79.8</td><td>97.3</td><td>71.5</td></tr><tr><td>r1 -distill</td><td>800K</td><td>72.6</td><td>94.3</td><td>62.1</td></tr><tr><td colspan="5">Open Weights and Open Data</td></tr><tr><td>Sky-T1 Bespoke-32B</td><td>17K 17K</td><td>43.3 63.3</td><td>82.4 93.0</td><td>56.8 58.1</td></tr><tr><td>s1 w/o BF</td><td>1K</td><td>50.0</td><td>92.6</td><td>56.6</td></tr><tr><td>s1-32B</td><td>1K</td><td>56.7</td><td>93.0</td><td>59.6</td></tr></table></body></html>

# s1: Simple test-time scaling

S1模型方法论：

·数据集构建（s1K）：

从三个标准一一难度、 多样性和质量一一出发，挑选了1000个问题及其对应的推理路径。

数据集包括来自不同领域，如数学竞赛、物理竞赛，并且新增了两个原创的数据集：s1-prob和s1-teasers，分别涵盖了概率问题和定量交易面试中的难题。

·预算强制 (Budget Forcing） :

一种控制测试时计算的技术，通过强制终止或延长模型的思考过程（通过添加"Wait"字符串），使模型有机会重新检查答案，从而可能纠正错误的推理步骤。

Mathematical Competition PhD-Level Problem Solving Math Science Questions (MATH500) (AIME24) (GPQA Diamond) 95 60 60 e 40 50 20 40 0 512 2048 512 2048 8192 1024 4096 Average thinking time (tokens)

# s1: Simple test-time scaling

# 预算强制 (Budget Forcing） :

1、设定思考时间限制：

当模型开始处理一个问题时，首先为其设定了一个最大思考时间（以token数量衡量）。如果模型在这个时间内完成了思考并准备给出答案，则按照正常流程进行。

2、强制结束思考过程：

如果模型生成的思考token超过了预设的最大值，系统会强行终止模型的思考过程。这通常是通过添加一个特殊的end-of-thinking tokendelimiter实现的，促使模型停止进一步的推理，并转向生成最终答案。

# 3、鼓励更深入的探索：

如果希望模型花更多的时间来考虑一个问题，可以抑制end-of-thinking token delimiter的生成，并在当前的推理路径后面追加"Wait"字符串 $\Rightarrow$ 为了让模型有机会重新评估其先前的推理步骤，可能会纠正一些快速但不准确的回答。

例如，"raspberry"这个单词中有三个'r'，但最初的快速阅读导致模型错误地计数为两个。通过使用"Wait"，模型被引导去重新读取问题和单词，最终正确地计数了三个'r'。

# s1: Simple test-time scaling

Thinking：如何知道每个任务需要多少时间，如何控制这个预算限制的方向（多一些，少一些）？

# 基于训练数据的分析：

通过分析训练集中的问题，可以估计出不同复杂度的问题所需的平均思考时间。例如，在实验中，将推理路径按照长度（以token数量计）分组，并据此对模型进行提示。

# 基于实验评估的时间调整！

在初步设定的时间预算基础上运行模型，并记录其在各种问题上的表现。这包括准确性和生成的答案长度等指标。  
基于此决定增加思考时间，还是减少思考时间。

# 使用分类条件控制：

论文提到了一种叫做"class-conditional control"的方法，它允许用户通过APl参数设置不同的"reasoning_effort"级别(低、中、高）。虽然这种方法不能精确控制计算量，但可以根据期望的响应速度和细节水平调整思考时间。

# s1: Simple test-time scaling

Thinking：S1整个的训练过程是怎样的？

1、数据准备：首先，从大约59,000个问题中精心挑选并构建了一个包含1,000个高质量样本的数据集（s1k）。这些样本包含了问题、由Gemini 2.0FlashThinkingAPI生成的推理路径以及对应的答案。

2、模型选择与微调：选择了Qwen2.5-32B-Instruct作为基础模型，并在其上进行了监督微调SFT。这里是将s1K数据集中的问题及其对应的推理路径和答案输入到Qwen2.5-32B-Instruct模型中进行训练。

3、预算强制技术的应用：在测试阶段，为了控制模型的计算资源使用，采用了“预算强制当模型生成的思考token超过设定限制时，通过添加结束思考的标记强制终止思考过程。如果希望模型花费更多时间思考，则通过追加"Wait"字符串鼓励模型进一步探索。整个训练是在16个H100 GPU上进行的，耗时约26分钟。

# s1: Simple test-time scaling

-一共有2个r <lim_end|> The word "raspberry" contains 2 letter "r". <lim_end|>

<|im_start|>system   
You are Qwen, created by Alibaba Cloud. You are a helpful assistant. <lim_end|>   
<|im_start|>user   
How many r in raspberry   
<lim_end|>   
<lim_start|>assistant   
<lim_start|>think   
让我一步步分析:   
1.先写出单词：raspberry   
2.数一下字母r的出现次数：   
-第一个r在开头   
-第二个r在中间

思考过程控制：MAX_TOKENS_THINKING=32000 #控制思考token的上限NUM_IGNORE $\mathbf { \Psi } = 1$ #控制思考的次数

通过prompt template引导模型输出思考过程，通过sft.py进行训练，损失函数loss的计算包括：思考过程(<|im_start|>think后的内容)和最终答案

Unsloth：LLM高效微调

# Unsloth：LLM高效微调

Unsloth https://github.com/unslothai/unsloth

·一个高效的开源工具，专注于加速大语言模型（LLMs）的微调和训练过程。  
·高效微调：支持Llama、Mistral、Qwen等主流模型，微调速度比传统方法快2-5倍，内存使用减少 $5 0 - 8 0 \%$ ·低显存需求：仅需7GB显存即可训练1.5B参数的模型，15GB显存可支持高达15B参数的模型。  
·集成GRPO算法（群体相对策略优化）：增强模型推理能力，比如在无人工提示下解决逻辑问题·广泛的模型兼容性：支持多种模型格式（如GGUF、OIlama、vLLM）和量化方法（如QLoRA、LoRA）。·开源与免费：提供免费的colab Notebook，用户只需添加数据集并运行代码即可获得微调模型。  
·支持多种量化方法：可将模型从原始的浮动精度（FP32）转换为低精度（如BF16、QLoRA和Q4等），以减少内存占用并加速模型推理和训练过程。

# Unsloth：LLM高效微调

https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)- GRPO.ipynb#scrollTo $\ c =$ P1zyu9Ug2XEt

<html><body><table><tr><td>Unsloth supports</td><td>Free Notebooks</td><td>Performance</td><td>Memory use</td></tr><tr><td>Llama 3.2 (3B)</td><td>Start for free</td><td>2x faster</td><td>70% less</td></tr><tr><td>GRPO (reasoning)</td><td>? Start for free</td><td>2x faster</td><td>80% less</td></tr><tr><td>Phi-4 (14B)</td><td>Start for free</td><td>2x faster</td><td>70% less</td></tr><tr><td>Llama 3.2 Vision (11B)</td><td>Start for free</td><td>2x faster</td><td>50% less</td></tr><tr><td>Llama 3.1 (8B)</td><td>Start for free</td><td>2x faster</td><td>70% less</td></tr><tr><td>Gemma 2 (9B)</td><td>■ Start for free</td><td>2x faster</td><td>70% less</td></tr><tr><td>Qwen 2.5 (7B)</td><td>? Start for free</td><td>2x faster</td><td>70% less</td></tr><tr><td>Mistral v0.3 (7B)</td><td>Start for free</td><td>2.2x faster</td><td>75% less</td></tr><tr><td>Ollama</td><td>Start for free</td><td>1.9x faster</td><td>60% less</td></tr><tr><td>DPO Zephyr</td><td>Start for free</td><td>1.9x faster</td><td>50% less</td></tr></table></body></html>

CASE: qwen2.5-7B微调 (alpaca-cleaned)

# CASE: Qwen2.5-7B微调 (alpaca-cleaned)

yahma/alpaca-cleaned数据集

·Stanford发布的原始Alpaca数据集的清理版本，主要用于LLM的指令微调（InstructTuning）。  
·原始数据集：AIpaca-52k，最初用于微调LLaMA模型以生成Alpaca-7B模型，包含约52K条指令数据。  
·清理版本：yahma/alpaca-cleaned 是对原Alpaca数据集的优化版本，修复了数据中的瑕疵 (如噪声、格式问题)

2.数据格式 (JSON格式)

instruction：用户输入的指令（必填）。input：可选的上下文或补充输入（选填）。output：模型应生成的回答 (必填)

"instruction":"计算这些物品的总费用。", "input":"汽车-\$3000，衣服- $\$ 100$ ，书- $\$ 20$ 。",   
"output":"汽车、衣服和书的总费用为 $\$ 3000+5100+520=$   
\$3120。"   
}

Alpaca数据集是一个用于语言模型指令微调 (InstructionTuning）的数据集，由Stanford研究团队开发

# TRL SFTTrainer 工具

# TRLSFTTrainer工具

·HugingFace的TRL(Transformer Reinforcement Learning)库中的SFTTrainer(Supervised Fine-tuning Trainer)是专门设计用于简化Transformer模型监督微调过程的工具。

·SFTTrainer提供了一系列专门为监督微调设计的功能：

内存高效训练：支持参数高效微调技术(PEFT)，如 LoRA (Low-Rank Adaptation)，可以显著减少内存使用量。  
数据集处理简化：自动处理常见的数据格式，简化了从原始文本到模型输入的转换过程。它内置了对文本补全任务的支持，这是许多语言模型微调场景中的常见需求。  
训练优化：提供了针对语言模型训练的特定优化，如打包(packing)技术，可以将多个短序列合并成一个长序列，提高训练效率。灵活配置：保留了原始Trainer的所有功能，同时添加了针对监督微调的特定参数和选项

# CASE: Qwen2.5-7B微调 (alpaca-cleaned)

from unsloth import FastLanguageModel  
#设置模型参数  
max_seq_length $= 2 0 4 8$ #设置最大序列长度，支持RoPE缩放  
dtype $\mathbf { \tau } = \mathbf { \tau }$ None #数据类型，None表示自动检测。TeslaT4 使用 Float16,  
Ampere $^ +$ 使用 Bfloat16  
load_in_4bit $\mathbf { \sigma } = \mathbf { \sigma }$ True #使用4bit量化来减少内存使用  
#加载预训练模型和分词器  
model, tokenizer $\mathbf { \tau } = \mathbf { \tau }$ FastLanguageModel.from_pretrained(  
model_name $\mathbf { \tau } = \mathbf { \tau }$ "/root/autodl-tmp/models/Qwen/Qwen2__5-7B-lnstruct",  
#使用Qwen2.5-7B模型max_seq_length $\mathbf { \tau } = \mathbf { \tau }$ max_seq_length,dtype $\mathbf { \tau } = \mathbf { \tau }$ dtype,load_in_4bit $\mathbf { \tau } = \mathbf { \tau }$ load_in_4bit,  
）  
#添加LoRA适配器，只需要更新1-10%的参数  
model $\mathbf { \tau } = \mathbf { \tau }$ FastLanguageModel.get_peft_model(model,$r = 1 6$ ，# LoRA秩，建议使用8、16、32、64、128target_modules $\mathbf { \tau } = \mathbf { \tau }$ ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj",]，#需要应用LoRA的模块lora_alpha $= 1 6$ ，# LoRA缩放因子lora_dropout $\begin{array} { r } { { \bf \Omega } = 0 . } \end{array}$ ，#LoRAdropout率，0为优化设置bias $\mathbf { \tau } = \mathbf { \tau }$ "none"，#偏置项设置，none为优化设置use_gradient_checkpointing $\mathbf { \tau } = \mathbf { \tau }$ "unsloth"，#使用unsloth的梯度检查点，可减  
少 $30 \%$ 显存使用random_state $= 3 4 0 7$ ，#随机种子use_rslora $\mathbf { \tau } = \mathbf { \tau }$ False，# 是否使用rank stabilized LoRAloftq_config $\mathbf { \tau } = \mathbf { \tau }$ None，# LoftQ配置  
）

通过FastLanguageModel.from_pretrained()加载基础模型，然后通过FastLanguageModel.get_peft_model()添加 LoRA层

# CASE: Qwen2.5-7B微调 (alpaca-cleaned)

# #定义Alpaca格式的提示模板

alpaca_prompt $\mathbf { \tau } = \mathbf { \tau }$ "" Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

# ### Instruction:

0

### Input:

0

### Response: 0" 111

#获取结束标记 EOS_TOKEN $\mathbf { \tau } = \mathbf { \tau }$ tokenizer.eos_token

# #定义数据格式化函数

def formatting_prompts_func(examples): instructions $\mathbf { \tau } = \mathbf { \tau }$ examples["instruction"] inputs $\mathbf { \tau } = \mathbf { \tau }$ examples["input"] outputs $\mathbf { \tau } = \mathbf { \tau }$ examples["output"] texts = [] for instruction, input, output in zip(instructions, inputs, outputs): #必须添加EOS_TOKEN，否则生成会无限继续 text $\mathbf { \tau } = \mathbf { \tau }$ alpaca_prompt.format(instruction,input, output) $^ +$ EOS_TOKEN texts.append(text) return {"text" : texts, }

# CASE: Qwen2.5-7B微调 (alpaca-cleaned)

#加载Alpaca数据集   
from datasets import load_dataset   
dataset $\mathbf { \tau } = \mathbf { \tau }$ load_dataset("/root/autodl-tmp/datasets/yahma/alpaca   
cleaned",split $\mathbf { \tau } = \mathbf { \tau }$ "train")   
dataset $\mathbf { \tau } = \mathbf { \tau }$ dataset.map(formatting_prompts_func, batched $\mathbf { \tau } = \mathbf { \tau }$ True,) #设置训练参数和训练器   
from trl import SFTTrainer   
from transformers import TrainingArguments from unsloth import is_bfloat16_supported

# #定义训练参数

training_args $\mathbf { \tau } = \mathbf { \tau }$ TrainingArguments(per_device_train_batch_size $= 2$ ，#每个设备的训练批次大小gradient_accumulation_steps $= 4$ ，#梯度累积步数warmup_steps $= 5$ ，#预热步数max_steps $= 6 0$ ，#最大训练步数

learning_rate $= 2 e { - 4 }$ ，#学习率fp16 $\mathbf { \sigma } = \mathbf { \sigma }$ not is_bfloat16_supported()，#是否使用FP16$\mathsf { b f 1 6 } =$ is_bfloat16_supported()，# 是否使用BF16logging_steps $\mathbf { \Psi } = 1$ ，#日志记录步数optim $\mathbf { \tau } = \mathbf { \tau }$ "adamw_8bit"，#优化器weight_decay $\mathbf { \tau } = 0 . 0 1$ ，#权重衰减Ir_scheduler_type $\mathbf { \sigma } = \mathbf { \sigma }$ "linear"，#学习率调度器类型$\mathsf { s e e d } = 3 4 0 7$ ，#随机种子output_dir $\mathbf { \sigma } = \mathbf { \sigma }$ "outputs"，#输出目录report_to $\mathbf { \sigma } = \mathbf { \sigma }$ "none"，#报告方式）

# CASE: Qwen2.5-7B微调 (alpaca-cleaned)

#创建SFTTrainer实例  
trainer $\mathbf { \tau } = \mathbf { \tau }$ SFTTrainer(model $=$ model,tokenizer $\mathbf { \tau } = \mathbf { \tau }$ tokenizer,train_dataset $\mathbf { \tau } = \mathbf { \tau }$ dataset,dataset_text_field $\mathbf { \tau } = \mathbf { \tau }$ "text",max_seq_length $\mathbf { \tau } = \mathbf { \tau }$ max_seq_length,dataset_num_proc $= 2$ ，packing $\mathbf { \tau } = \mathbf { \tau }$ False，#对于短序列可以设置为True，训练速度提升5倍args $\mathbf { \tau } = \mathbf { \tau }$ training_args,  
#开始训练  
trainer_stats $\mathbf { \tau } = \mathbf { \tau }$ trainer.train()

Jnsloth: Will smartly offload gradients to save VRAM!   

<html><body><table><tr><td>[60/60 01:29, Epoch 0/1]</td></tr><tr><td>Step</td><td>Training Loss</td></tr><tr><td>1</td><td>1.433300</td></tr><tr><td>2 1.786000</td><td></td></tr><tr><td>３</td><td>1.542800</td></tr><tr><td>4</td><td>1.674900</td></tr><tr><td>5</td><td>1.452400</td></tr><tr><td>6</td><td>1.193900</td></tr><tr><td>7</td><td>0.900700</td></tr><tr><td>8 1.154300</td><td></td></tr><tr><td>9 1.102500</td><td></td></tr><tr><td>10</td><td></td></tr><tr><td></td><td>1.040300</td></tr><tr><td>11 0.844500 12 0.949100</td><td></td></tr><tr><td>13 0.824900</td><td></td></tr><tr><td>14 1.035500</td><td></td></tr><tr><td></td><td></td></tr><tr><td>15 0.806500</td><td></td></tr></table></body></html>

# CASE: Qwen2.5-7B微调 (alpaca-cleaned)

# #模型推理部分

FastLanguageModel.for_inference(model)#启用原生2倍速推理  
inputs $\mathbf { \tau } = \mathbf { \tau }$ tokenizer(  
Ialpaca_prompt.format("Continue the fibonnaci sequence."，#指令"1,1,2,3,5,8"，#输入""，#输出留空用于生成）  
], return_tensors $\mathbf { \tau } = \mathbf { \tau }$ "pt").to("cuda")  
#生成输出  
outputs $\mathbf { \tau } = \mathbf { \tau }$ model.generate(\*\*inputs,max_new_tokens $\mathbf { \tau } = \mathbf { \tau }$ 64,use_cache $\mathbf { \tau } = \mathbf { \tau }$ True)  
tokenizer.batch_decode(outputs)  
\*\*inputs:经过tokenizer处理后的输入数据  
max new tokens = 64,  
控制模型最多可以生成的新token数量  
这里设置为64，最多会生成64个新的token  
如果生成过程中遇到终止符（如EOStoken），会  
提前停止  
use cache $\mathbf { \sigma } = \mathbf { \sigma }$ True:  
启用KVcache（键值缓存）机制  
可以显著提升生成速度，因为不需要重复计算之前  
的token的注意力权重

[Belowisnrctiotatibssdituttatproesftotetitesposetaproatelyletete eqestciiis#spebn seqcee

# CASE: Qwen2.5-7B微调 (alpaca-cleaned)

#微调模型保存 model.save_pretrained("lora_model") tokenizer.save_pretrained("lora_model")

('lora_model/tokenizer_config.json',   
'lora_model/special_tokens_map.json',   
'lora_model/vocab.json',   
'lora_model/merges.txt',   
'lora_model/added_tokens.json',   
'lora_model/tokenizer.json')

![](images/82a6ce895185de092f746a8823b63ad2e9e6a3220bf9c655bce862bf3b6e5877.jpg)

<html><body><table><tr><td colspan="2">/unsloth_cases/lora_model /</td></tr><tr><td>名称</td><td>修改时间</td></tr><tr><td>{} adapter_config.json</td><td>2天前</td></tr><tr><td>adapter_model.safet... D</td><td>2天前</td></tr><tr><td>{} added_tokens.json</td><td>2天前</td></tr><tr><td>merges.txt</td><td>2天前</td></tr><tr><td>M README.md</td><td>2天前</td></tr><tr><td>{母} special_tokens_map.j...</td><td>2天前</td></tr><tr><td>{} tokenizer_config.json</td><td>2天前</td></tr><tr><td>{} tokenizer.json</td><td>2天前</td></tr><tr><td>{} vocab.json</td><td>2天前</td></tr></table></body></html>

CASE: 训练垂类模型（中文医疗模型）

# CASE: 训练垂类模型 （中文医疗模型

Chinesemedicaldialoguedata中文医疗对话数据集https://github.com/Toyhom/Chinese-medical-dialogue-data在Data_数据中有6个文件夹：

<Andriatria_男科>94596个问答对<IM_内科>220606个问答对<OAGD_妇产科>183751个问答对<Oncology_肿瘤科>75553个问答对<Pediatric_儿科>101602个问答对<Surgical_外科>115991个问答对

数据内容示意  

<html><body><table><tr><td>depart ment</td><td>title</td><td>question</td><td>answer</td></tr><tr><td>心血 管科</td><td>高血压患者</td><td>我有高血压这两天女婿来 的时候给我拿了些党参泡 能吃党参吗？水喝，您好高血压可以吃 党参吗？</td><td>高血压病人可以口服党参的。党 参有降血脂，降血压的作用，可 以彻底消除血液中的垃圾，从而 对冠心病以及心血管疾病的患者 都有一定的稳定预防工作作用， 因此平时口服党参能远离三高的 危害。另外党参除了益气养血， 降低中枢神经作用，调整消化系 统功能，健脾补肺的功能。感谢 您的进行咨询，期望我的解释对 你有所帮助。</td></tr><tr><td>消化 科</td><td>哪家医院能 治胃反流</td><td>烧心，打隔，咳嗽低烧， 以有4年多</td><td>建议你用奥美拉唑同时，加用吗 丁啉或莫沙必利或援生力维，另 外还可以加用达喜片</td></tr></table></body></html>

# CASE: 训练垂类模型 （中文医疗模型）

TODO：训练中文医疗垂直模型使用Data_数据中的6个科室的数据参考 Qwen2_5_(7B)_Alpaca.py

@Data_数据这里面有6个数据集，我想按照@Qwen2_5_(7B)_Alpaca.py改成写医疗领域的垂直模型的训练，帮我编写新的Python

![](images/00ff02d7011950c1f68ae0dfe8a2aaa71f48658879d115b0a094f2d57dbd76ba.jpg)

Thinking: 运行后报错， 主要是因为数据中文编码的问题如何解决？

# CASE: 训练垂类模型 （中文医疗模型

UnicodeDecodeError Traceback (most recent call last)这个错误是因为CSV文件的编码格式不是UTF-8，而是其他编码（可能是GBK或GB231  
Cell In[5], line 52是中文Windows系统常用的编码)。让我修改代码以正确处理中文编码:49return {"text": texts}medical_finetune.py No changes made 门51#加载医疗数据集 def load_medical_data(data_dir):  
--> 52 dataset $\mathbf { \tau } = \mathbf { \tau }$ load_medical_data("Data_数据") data """加载医疗对话数据""" $\mathbf { \sigma } = \mathbf { \sigma }$ []53 dataset $\mathbf { \tau } = \mathbf { \tau }$ dataset.map(formatting_prompts_func, batched $\ c =$ True) #遍历所有科室目.1istdir(data_dir):dept_path $\mathbf { \sigma } = \mathbf { \sigma }$ os.path.join(data_dir, dept)if not os.path.isdir(dept_path):  
Cell In[5], line 27,in load_medical_data(data_dir) continue25 #读取csv文件 #读取csv $\mathbf { \sigma } = \mathbf { \sigma }$ fforfins.istdirdeptath)iff.dswith26 csv_file $\mathbf { \tau } = \mathbf { \tau }$ [f for f in os.listdir(dept_path) if f.endswith('.csv')][0] try:# 首先尝试UTF-8编码  
--> 27 df $\mathbf { \tau } = \mathbf { \tau }$ pd.read_csv(os.path.join(dept_path, csv_file))29 #处理数据将错误贴给Cursor，找到原因是因为csv文件不是utf-8修正后进行运行，可能还会遇到问题，还需要进行调壺  
$\scriptstyle = = { \frac { } { } }$ 运行报错

# CASE: 训练垂类模型 （中文医疗模型）

@Data_数据这里面有很多csv，帮我编写一个新的python，读取里面的.csv，注意里面是中文的编码

![](images/f381b64b694b3a4e65f65cc739420a246fa9775304f21479264d01a731306d64.jpg)

我来帮你编写一个专门用于读取和处理医疗对话数据的Python脚本。这个脚本会处理所有科室的CSV文件，并正确处理中文编码：

![](images/7ba9b096eb1ce5c59b8689c76a72d85caa78142a121ce7a60044147007ff3f8d.jpg)

有时候在原有的文件上进行修改，不如另起一个文件，针对性的进行编码，比如这里专门写Python读取.csv的内容

开始处理医疗对话数据...

处理内科数据...

正在处理文件：内科5000-33000.csv

文件内科5000-33000.csv 的列名:['department','title','ask','answer']处理数据:

$100 \%$

220606/220606 [00:04<00:00,45523.05it/s]

处理外科数据..

正在处理文件:外科5-14000.csv

文件外科5-14000.csv的列名:['department','title','ask','answer']

# CASE: 训练垂类模型 （中文医疗模型）

@process_medical_data.py已经成功读取到了文件夹下面的.csv内容，@medical_finetune.py但是这里没有读取成功帮我参照 $@$ process_medical_data.py进行调整

我来帮你修改 medical_finetune.py 中的数据加载部分，参照 process_medical_data.py的实现:

medical_finetune.py $+ 8 1 - 2 5 \ √$ 口  
def read_csv_with_encoding(file_path):"""尝试使用不同的编码读取cSV文件"""encodings $\mathbf { \sigma } = \mathbf { \sigma }$ ['gbk'，'gb2312'，'gb18030'，'utf-8']for encoding in encodings:try:return pd.read_csv(file_path，encoding $\mathbf { \sigma } =$ encoding)except UnicodeDecodeError:continueraise ValueError(f"无法使用任何编码读取文件：{file_path}")  
def load_medical_data(data_dir):"n书 1迁数据……□

我已经修改了 medical_finetune.py 中的数据加载部分，主要改进包括：

1.添加了 read_csv_with_encoding 函数:

支持多种中文编码（GBK、GB2312、GB18030、UTF-8)自动尝试不同编码直到成功读取

处理内科数据.  
正在处理文件：内科5000-33000.csv  
文件 内科5000-33000.csv 的列名：［'department'，'title'，'ask'，'answer']处理外科数据.  
正在处理文件：外科5-14000.csv  
文件 外科5-14000.csv 的列名：['department'，'title'，'ask'，'answer']处理儿科数据.  
正在处理文件：儿科5-14000.csv  
文件儿科5-14000.csv 的列名：［'department'，'title'，'ask'，'answer']处理肿瘤科数据.  
正在处理文件：肿瘤科5-10000.csv  
文件 肿瘤科5-10000.csv 的列名：['department'，'title'，'ask'，'answer']处理妇产科数据.  
正在处理文件：妇产科6-28000.csv  
文件 妇产科6-28000.csv 的列名：['department'，'title'，'ask'，'answer']处理男科数据..  
正在处理文件：男科5-13000.csv  
文件 男科5-13000.csv 的列名：［'department'，'title'，'ask'，'answer']

成功处理 664001 条数据

# CASE: 训练垂类模型 （中文医疗模型）

<html><body><table><tr><td>Unsloth: Will smartly offload gradients to save VRAM!</td><td></td><td></td></tr><tr><td></td><td></td><td>[60/60 01:29, Epoch 0/1]</td></tr><tr><td>Step Training Loss</td><td></td><td></td></tr><tr><td>1</td><td>3.366000</td><td></td></tr><tr><td>2</td><td>2.909200</td><td></td></tr><tr><td>3</td><td>3.020400</td><td></td></tr><tr><td>4</td><td>2.949000</td><td></td></tr><tr><td>5</td><td>2.895600</td><td></td></tr><tr><td>6</td><td>2.670800</td><td></td></tr><tr><td>7</td><td>2.286200</td><td></td></tr><tr><td>8</td><td>2.528700</td><td></td></tr><tr><td>9</td><td>2.013400</td><td></td></tr><tr><td>10</td><td>2.123000</td><td></td></tr><tr><td>11</td><td>2.518200</td><td></td></tr><tr><td>12</td><td>2.066400</td><td></td></tr><tr><td>13</td><td>2.065300</td><td></td></tr><tr><td>14</td><td>2.200300</td><td></td></tr></table></body></html>

问题：我最近总是感觉头晕，应该怎么办？

回答:

你是一个专业的医疗助手。请根据患者的问题提供专业、准确的回答。

###问题:

我最近总是感觉头晕，应该怎么办？

###回答:

你好！建议你检查一下血脂和血压看看是否偏高，如果偏高的话可以吃降脂平胶囊调理，平时注意饮食清淡为主，不吃油腻的食物，多吃蔬菜水果等，少吃甜食，多喝水，不要熬夜，不要生气。<lim_end|>

# CASE: 训练垂类模型 （中文医疗模型）

#定义训练参数

training_args $\mathbf { \tau } = \mathbf { \tau }$ TrainingArguments(per_device_train_batch_size $= 2$ ，#每个设备的训练批次大小gradient_accumulation_steps $= 4$ ，#梯度累积步数warmup_steps $= 5$ ，#预热步数max_steps $\widehat { \mathbf { \xi } } = \widehat { \mathbf { \xi } } \cdot \widehat { \mathbf { \xi } } \cdot$ ，#不使用max_stepsnum_train_epochs $= 3$ ，#训练3个epoch  
）

当max_steps $= - 1$ 时，训练器会使用num_train_epochs来控制训练轮数

现在的训练设置：

每个batch处理2个样本，累积4次梯度后更新一次模型，  
实际batch_s ${ \mathsf { i } } _ { } \mathsf { z e } = 2 \times 4 = 8$   
会完整遍历数据集3次，总训练步数 $\mathbf { \sigma } = \mathbf { \sigma }$ (数据集大小/实际  
batch_size) $\times$ epochs

模型训练中，需要设置正确的max_steps，或者num_train_epochs

# CASE: 训练垂类模型 （中文医疗模型）

# #加载保存的模型进行推理

# 自动从adapter_config.json 读取基础模型路径

if True:

<html><body><table><tr><td></td><td>×</td><td></td><td></td><td>X</td><td>X</td><td>t</td><td>X</td><td>X</td><td>□×</td><td></td><td>□ ×</td><td>×</td><td>回</td><td></td><td></td><td></td></tr></table></body></html>

from unsloth import FastLanguageModel   
model, tokenizer $\mathbf { \tau } = \mathbf { \tau }$ FastLanguageModel.from_pretrained( model_name $\mathbf { \sigma } = \mathbf { \sigma }$ "lora_model_medical"，#保存的模型 max_seq_length $\mathbf { \tau } = \mathbf { \tau }$ max_seq_length, dtype $\mathbf { \tau } = \mathbf { \tau }$ dtype, load_in_4bit $\mathbf { \tau } = \mathbf { \tau }$ load_in_4bit,   
）   
FastLanguageModel.for_inference(model)#启用原生2倍速推

# 按文件名过滤

<html><body><table><tr><td>名称</td><td>修改时间</td></tr><tr><td>{)adapter_config.json</td><td>2天前</td></tr><tr><td>Dadapter_model.safet..</td><td>2天前</td></tr><tr><td>{)added_tokens.json</td><td>2天前</td></tr><tr><td>merges.txt</td><td>2天前</td></tr><tr><td>MREADME.md</td><td>2天前</td></tr><tr><td>{)special_tokens_map..</td><td>2天前</td></tr><tr><td>{}tokenizer_config.json</td><td>2天前</td></tr><tr><td>{}tokenizer.json</td><td>2天前</td></tr><tr><td>vocab.json</td><td>2天前</td></tr></table></body></html>

/./unsloth_cases/lora_model/

root: alpha_pattern:{} auto_mapping:null base_model_name_or_path:"/root/autodl-tmp/models/Qwen/Qwen2__5-7B-Instruct"

bias:"none"   
corda_config:null   
eva_config:null   
exclude_modules:null   
fan_in_fan_out:false   
inference_mode:true   
init_lora_weights:true   
layer_replication:null   
layers_pattern:null   
layers_to_transform:null   
loftq_config:{}   
lora_alpha:16

question $\mathbf { \sigma } = \mathbf { \sigma }$ "我最近总是感觉头晕，应该怎么办？" generate_medical_response(question)

# CASE: 训练自己的R1模型

# CASE: 训练自己的R1模型

# 1.安装和初始化

#安装必要的库   
!pip install unsloth vllm   
# 导入FastLanguageModel和PatchFastRL   
from unsloth import FastLanguageModel, PatchFastRL   
PatchFastRL("GRPO", FastLanguageModel)

FastLanguageModel，是unsloth库提供的一个优化过的LLM加载器和处理器，主要功能：

# 2.模型加载

model, tokenizer $\mathbf { \tau } = \mathbf { \tau }$ FastLanguageModel.from_pretrained(model_name $\mathbf { \tau } = \mathbf { \tau }$ "meta-llama/meta-Llama-3.1-8B-Instruct",max_seq_length $\mathbf { \tau } = \mathbf { \tau }$ max_seq_length,load_in_4bit $\mathbf { \tau } = \mathbf { \tau }$ True,#使用4bit量化加载fast_inference $\mathbf { \tau } = \mathbf { \tau }$ True,#启用vLLM快速推理max_lora_rank $\mathbf { \tau } = \mathbf { \tau }$ lora_rank,gpu_memory_utilization $= 0 . 6$ ，#控制GPU内存使用  
）

PatchFastRL，是一个用于强化学习(RL)的补丁函数：

提供了更快的模型加载和推理速度  
·支持4bit量化加载模型  
集成了vLLM进行快速推理  
·支持LoRA(Low-Rank Adaptation)微调  
·为语言模型添加强化学习相关的功能  
特别支持GRPO (Guided Reward Policy Optimization)算法  
·优化模型训练过程

# CASE: 训练自己的R1模型

3.数据准备使用GSM8K数据集进行训练定义了特定的系统提示格式：

SYSTEM_PROMPT=!   
Respond in the following format:   
<reasoning>   
</reasoning>   
<answer>   
·   
</answer>   
1-=

# 4.奖励函数设计

定义了多个奖励函数来评估模型输出：

correctness reward func:检查答案正确性int_reward_func:检查输出是否为整数strict_format_reward_func:严格检查输出格式soft_format_reward_func:宽松检查输出格式xmlcount_reward_func:检查xML标签的正确使用

# GSM8K数据集

GSM8K（Grade School Math 8K）是一个高质量的小学数学应用题数据集，主要用于评估和训练人工智能模型在数学推理和多步问题解决方面的能力 https://huggingface.co/datasets/openai/gsm8k

question answer   
string · lengths string ·lengths   
42 985 50 1.23k   
Natalia sold clips to 48 of her friends in April，and then she Natalia sold $4 8 / 2 = < < 4 8 / 2 = 2 4 > > 2 4$ clips in May. Natalia sold sold half as many clips in May. How many clips did Natalia sell.. $4 8 + 2 4 \ = \ < < 4 8 + 2 4 = 7 2 > > 7 2$ clips altogether in April and May. 非排排排 72 Weng earns $\$ 12$ an hour for babysitting. Yesterday，she just did Weng earns $1 2 / 6 \Theta = \left. \ S < < 1 2 / 6 \Theta = \Theta . 2 > > \Theta . 2 \right.$ per minute. Working 50 50 minutes of babysitting. How much did she earn? minutes，she earned $\Theta . 2 \ \times \ 5 \Theta \ = \ \oint < < \Theta \ : . 2 \star 5 \Theta = 1 \Theta > > 1 \Theta$ 非排排 10 Betty is saving money for a new wallet which costs $\$ 100$ .Betty In the beginning， Betty has only 1 $0 0 \vert 2 = 5 < < 1 0 0 / 2 = 5 0 > > 5 0$ ： has only half of the money she needs. Her parents decided to gis.. Betty's grandparents gave her $1 5 ~ \star ~ 2 ~ = ~ \$ < < 15 \star 2 = 30 > > 30$ .This... Julie is reading a 120-page book. Yesterday， she was able to read Maila read $1 2 \times 2 = < < 1 2 \star 2 = 2 4 > > 2 4$ pages today.So she was able to 12 pages and today， she read twice as many pages as yesterday. I... read a total of $1 2 ~ + ~ 2 4 ~ = ~ < < 1 2 + 2 4 = 3 6 > > 3 6$ pages since yesterday.. James writes a 3-page letter to 2 different friends twice a week. He writes each friend $3 \times 2 = < < 3 \times 2 = 6 > > 6$ pages a week So he writes How many pages does he write a year? （20 $6 { \star } 2 { = } < < 6 { \star } 2 { = } 1 2 { > } { > } 1 2$ pages every week That means he writes $1 2 \star 5 2 =$ ： Mark has a garden with flowers. He planted plants of three There are $8 0 / 1 0 0 \ \star \ 1 0 \ = \ < < 8 0 / 1 0 0 \star 1 0 = 8 > > 8$ more purple flowers than different colors in it. Ten of them are yellow, and there are 80.. yellow flowers. So in Mark's garden， there are $1 0 + 8 =$

# CASE: 训练自己的R1模型

# 5.训练配置

training_args $\mathbf { \tau } = \mathbf { \tau }$ GRPOConfig(use_vllm $\mathbf { \tau } = \mathbf { \tau }$ True,learning_rate $= 5 6 . - 6 .$ max_steps $\mathbf { \tau } = \mathbf { \tau }$ 250,save_steps $\mathbf { \tau } = \mathbf { \tau }$ 250,#其他训练参数...

# 6.模型训练

trainer $\mathbf { \tau } = \mathbf { \tau }$ GRPOTrainer( model $\mathbf { \tau } = \mathbf { \tau }$ model, processing_class $\mathbf { \tau } = \mathbf { \tau }$ tokenizer, reward_funcs $\mathbf { \sigma } = \mathbf { \sigma }$ 【 xmlcount_reward_func, soft_format_reward_func strict_format_reward_func int_reward_func, correctness_reward_func, ], args $\mathbf { \tau } = \mathbf { \tau }$ training_args, train_dataset $\mathbf { \tau } = \mathbf { \tau }$ dataset,   
）   
trainer.train()

# CASE: 训练自己的R1模型

7.模型保存和导出提供了多种保存格式：

16位浮点数格式  
4位量化格式  
LoRA适配器格式  
GGUF格式 （用于Illama.cpp）

# 8.推理示例

text $\mathbf { \tau } = \mathbf { \tau }$ tokenizer.apply_chat_template([ {"role" : "system","content" : SYSTEM_PROMPT}, {"role" : "user", "content" : "Calculate pi."}, ], tokenize $\mathbf { \tau } = \mathbf { \tau }$ False,add_generation_prompt $\mathbf { \tau } = \mathbf { \tau }$ True)

用于训练进行结构化推理的大模型，通过特定的格式(reasoning和answer标签）来输出推理过程和最终答案。

使用了GRPO训练方法采用了LoRA进行高效微调支持多种量化和保存格式包含了详细的奖励函数设计

# GRPO 学习

GRPO (Group Relative Policy Optimization)

组相对策略优化，是一种用于训练LLM的强化学习算法是DeepSeek-R1模型的核心技术之一GRPO的核心在于通过组内样本的相对奖励来优化策略模型，而不是依赖传统的价值函数模型（如PPO中的批评家模型）。它通过采样一组输出，利用这些输出的奖励值来计算相对优势，从而简化了训练过程。

# 工作原理:

采样与奖励计算：对于每个输入问题，GRPO从当前策略中采样一组输出，并计算每个输出的奖励值。

·相对优势估计：通过将每个输出的奖励值与组内平均奖励值进行比较，计算出每个输出的相对优势。

·策略更新：根据相对优势，GRPO更新策略模型，优先选择相对优势更高的输出。同时，它通过KL散度约束来控制策略更新的幅度，确保策略分布的稳定性

在DeepSeek-R1中，GRPO被用于优化模型在数学推理、代码生成等复杂任务中的表现。通过生成多个候选解并比较其表现，GRPO能够显著提升模型的推理能力和性能

# GRPO 学习

<html><body><table><tr><td>特性</td><td>GRPO</td><td>PPO</td></tr><tr><td></td><td>价值网络无需价值网络，直接使用组内奖励计算相对优势</td><td>依赖价值网络估计优势函数，计算复杂度高</td></tr><tr><td>奖励计算</td><td>通过组内奖励归一化计算相对优势</td><td>使用广义优势估计（GAE），依赖未来奖励的折扣总和</td></tr><tr><td>策略更新</td><td>使用KL散度约束，直接在损失函数中加入KL散度项 使用裁剪概率比，限制策略更新幅度</td><td></td></tr><tr><td>计算效率</td><td>显著提高，降低内存占用</td><td>计算效率较低，内存占用大</td></tr><tr><td>稳定性</td><td>通过组内相对奖励减少策略更新方差</td><td>通过裁剪概率比保持训练稳定性</td></tr></table></body></html>

GRPO学习优势：

·计算效率高：避免了价值网络的训练和维护，显著降低了内存占用和计算开销。·稳定性强：通过组内相对奖励减少策略更新的方差，确保训练过程更加稳定。·适用性强：特别适合大规模语言模型的微调，尤其在推理任务中表现出色

# GRPO 如何引导形成推理能力

Thinking：GRPO可以帮助产生<reasoning>么？因为R1模型是推理模型，所以会有<reasoning></reasoning>这种，因为数据集GSM8K只有question,answer两列，不包含<reasoning>，那<reasoning>训练的数据是哪里来的？

GRPO是一种引导式奖励策略优化方法，它通过多个奖励函数引导模型生成特定格式的内容奖励函数设计：

reward_funcs $\mathbf { \sigma } = \mathbf { \sigma }$ ［xmlcount_reward_func, #检查XML标签的正确使用soft_format_reward_func, #检查是否包含<reasoning>和<answer>标签strict_format_reward_func， #严格检查格式int_reward_func, #检查答案是否为数字correctness_reward_func, #检查最终答案是否正确

# GRPO 如何引导形成推理能力

关于<reasoning>的训练，GSM8k确实只有问题和答案，但GRPO不需要显式的reasoning训练数据

它通过奖励函数来引导模型生成reasoning:

def soft_format_reward_func(completions, \*\*kwargs) $- >$ list[float]: """检查输出是否包含正确的格式""" pattern $\mathbf { \tau } = \mathbf { \tau }$ r"<reasoning>.\*?</reasoning>\s\*<answer>.\*?</answer>" responses $\mathbf { \tau } = \mathbf { \tau }$ [completion[O]["content"] for completion in completions] matches $\mathbf { \tau } = \mathbf { \tau }$ [re.match(pattern, r) for r in responses] return [0.5 if match else O.0 for match in matches]

# 训练过程中：

模型会生成多个候选输出（num_generations $= 6$ ）·每个输出都会被所有奖励函数评估高奖励的输出会被用来指导模型改进生成策略

模型逐渐学会：

1.使用正确的XML标签格式  
2.在<reasoning>中展示解题步骤  
3.在<answer>中给出正确答案

Llama模型本身已经具备推理能力，GRPO通过奖励机制引导模型，将内部的推理过程显式地写出来，使用特定的格式（XML标签）组织输出，确保最终答案的正确性

# 继DeepSeek后的Al趋势

# 1、小模型将成为主流 (大模型蒸馏)

通过蒸馏技术将大型模型的推理能力成功迁移到小型模型中显著提升了小型模型的性能。

DeepSeek-R1-Distill-Qwen-7B在AIME2024竞赛中击败了32B模型

2、使用合成数据进行训练成为主流蒸馏数据是公开的秘密，但很多表现不是蒸馏能解释的。比如v3的中文能力，很多用词和表达方式非常接地气，可能是用了数据合成方法做的预训练。

3、AI模型将自我迭代： 强化学习新范式让模型自己出题自己做，自己检查。

Step1，模型自己出题自己做，比如出了1亿道题目

Step2，模型自己检查对不对

Step3，筛选验证对的内容，将结果与思维链合成新的数据比如1亿道题目中，自己检查后发现有100万道能验证是对的。那么解出这100万道题的思维链就成了新的训练数据。不断迭代，探索出之前人类没有探索到的地方（类似AlphaGo-Zero)

# Thank You Using data to solve problems