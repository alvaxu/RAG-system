{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a4450-45f4-45f3-ad11-97c450123344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca63834f-b0d4-4909-a859-32d06f044aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('BAAI/bge-m3', cache_dir='D:\\\\AInewModels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4601cc-9c1e-4109-83fe-07ee86170fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install FlagEmbedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d389ea12-9783-4dd5-a844-0aefe83a9108",
   "metadata": {},
   "source": [
    "这段Python代码展示了如何使用**BGE-M3**模型（一种多功能嵌入模型）来计算文本之间的语义相似度。以下是逐步解析：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 模型初始化**\n",
    "```python\n",
    "model = BGEM3FlagModel('C:\\\\Users\\\\Administrator\\\\AInewModels\\\\BAAI\\\\bge-m3', use_fp16=True)\n",
    "```\n",
    "- **功能**：加载预训练的BGE-M3模型。\n",
    "  - **模型路径**：`C:\\Users\\Administrator\\AInewModels\\BAAI\\bge-m3`，指向本地下载的模型文件。\n",
    "  - **`use_fp16=True`**：启用半精度浮点数（FP16）加速计算，牺牲少量精度换取更快的推理速度。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 输入文本准备**\n",
    "```python\n",
    "sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model...\", \"BM25 is a bag-of-words...\"]\n",
    "```\n",
    "- **两组句子**：\n",
    "  - `sentences_1`：包含两个查询问题。\n",
    "  - `sentences_2`：包含对应的答案或定义。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 生成嵌入向量**\n",
    "```python\n",
    "embeddings_1 = model.encode(sentences_1, batch_size=12, max_length=1024)['dense_vecs']\n",
    "embeddings_2 = model.encode(sentences_2)['dense_vecs']\n",
    "```\n",
    "- **`model.encode()`**：将文本转换为稠密向量（dense embeddings）。\n",
    "  - **参数**：\n",
    "    - `batch_size=12`：批量处理12个句子，优化GPU利用率。\n",
    "    - `max_length=1024`：限制输入文本的最大token长度（默认支持8192，缩短可加速处理）。\n",
    "  - **输出**：`dense_vecs`是稠密向量，用于语义相似度计算。\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 计算相似度**\n",
    "```python\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "```\n",
    "- **操作**：矩阵乘法（`@`）计算两组嵌入的相似度矩阵。\n",
    "  - **结果**：`similarity[i][j]`表示`sentences_1[i]`与`sentences_2[j]`的相似度分数（值越高越相似）。\n",
    "  - **示例**：若`sentences_1[0]`（\"What is BGE M3?\"）与`sentences_2[0]`（\"BGE M3 is an embedding model...\"）相似度高，则对应分数接近1.0。\n",
    "\n",
    "---\n",
    "\n",
    "### **关键点总结**\n",
    "- **BGE-M3特性**：支持稠密检索、稀疏检索和多向量交互，适用于多语言和长文本场景。\n",
    "- **性能优化**：通过`use_fp16`和`max_length`平衡速度与精度。\n",
    "- **应用场景**：常用于检索增强生成（RAG）、语义搜索等任务。\n",
    "\n",
    "如需进一步了解BGE-M3的架构或训练细节，可参考[官方教程](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f384c8-ee27-4a93-aef3-4c422b2dc5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6259036  0.34749582]\n",
      " [0.34986773 0.6782464 ]]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('D:\\\\AInewModels\\\\BAAI\\\\bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
    "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n",
    "\n",
    "embeddings_1 = model.encode(sentences_1, \n",
    "                            batch_size=12, \n",
    "                            #max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            max_length=1024,\n",
    "                            )['dense_vecs']\n",
    "embeddings_2 = model.encode(sentences_2)['dense_vecs']\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd50199-6684-4a30-80b9-7d557f7f817d",
   "metadata": {},
   "source": [
    "- \"What is BGE M3?\" 与\"BGE M3 is an embedding model...\" 的相似度为0.6265（较高）\n",
    "- \"What is BGE M3?\"与\"- BM25 is a bag-of-words retrieval function...\" 的相似度为0.3477（较低）\n",
    "- \"Defination of BM25\" 与\"BGE M3 is an embedding model...\" 的相似度为0.3499（较低）\n",
    "- \"Defination of BM25\" 与\"BM25 is a bag-of-words retrieval function...\" 的相似度为0.678（较高）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f0b89-ffde-41ed-b17e-e5c5b7dc517e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
